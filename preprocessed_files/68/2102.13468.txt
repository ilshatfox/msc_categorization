pr eli m in ar y the interspeech 2021 computational paralinguistics challenge covid 19 cough covid 19 speech escalation primates bj rn w schuller 1 2 anton batliner 2 3 christian bergler 3 cecilia mascolo 4 jing han 4 iulia lefter 5 heysem kaya 6 shahin amiriparian 2 alice baird 2 lukas stappen 2 sandra ottl 2 maurice gerczuk 2 panagiotis tzirakis 1 chlo brown 4 jagmohan chauhan 4 andreas grammenos 4 apinan hasthanasombat 4 dimitris spathis 4 tong xia 4 pietro cicuta 4 leon j m rothkrantz 5 joeri zwerts 6 jelle treep 6 casper kaandorp 6 1 glam group on language audio music imperial college london uk 2 eihw chair of embedded intelligence for health care and wellbeing university of augsburg germany 3 pattern recognition lab fau erlangen nuremberg germany 4 university of cambridge uk 5 delft university of technology the netherlands 6 faculty of science utrecht university the netherlands schuller ieee org abstract the interspeech 2021 computational paralinguistics chal lenge addresses four different problems for the first time in a research competition under well defined conditions in the covid 19 cough and covid 19 speech sub challenges a binary classification on covid 19 infection has to be made based on coughing sounds and speech in the escalation sub challenge a three way assessment of the level of escalation in a dialogue is featured and in the primates sub challenge four species vs background need to be classified we describe the sub challenges baseline feature extraction and classifiers based on the usual compare and boaw features as well as deep unsupervised representation learning using the audeep toolkit and deep feature extraction from pre trained cnns using the deep spectrum toolkit in addition we add deep end to end sequential modelling and partially linguistic analysis index terms computational paralinguistics challenge covid 19 escalation primates 1 introduction in this interspeech 2021 computational paralin guistics challenge compare the thirteenth since 2009 1 we address four new problems within the field of computa tional paralinguistics 2 in a challenge setting in the covid 19 cough sub challenge ccs and covid 19 speech sub challenge css coughing sounds or speech are used to binary classify covid 19 or not infection in the present pandemic situation great potential lies in low cost anywhere and anytime accessible real time pre diagnosis of covid 19 infection to date the possibility has been shown 3 yet a controlled challenge test bed is lacking in the escala tion sub challenge ess participants are faced with three way classification of the level of escalation in human dialogues a range of applications exists including human to computer inter action computer mediated human to human conversation or public security finally in the primate sub challenge prs we classify four species of primates versus background noise real life applications include wild life monitoring in habitats e g to save species from extinction for all tasks a target class has to be predicted for each case contributors can employ their own features and machine learning algorithms standard feature sets and procedures are provided participants have to use the pre defined partitions for each sub challenge they may report results obtained from the train ing dev elopment set preferably with the supplied evaluation setups but have only five trials to upload their results on the test set per sub challenge whose labels are unknown to them each participation must be accompanied by a paper presenting the results which undergoes peer review and has to be accepted for the conference in order to participate the organ isers preserve the right to re evaluate the findings but will not participate in the challenge as evaluation measure we employ in all sub challenges unweighted average recall uar as used since the first challenge from 2009 1 especially because it is more adequate for unbalanced multi class classifications than weighted average recall i e accuracy 2 4 ethical approval for the studies has been obtained from the pertinent committees in section 2 we describe the challenge corpora section 3 details baseline experiments metrics and baseline results concluding remarks are given in section 4 2 the four sub challenges 2 1 the covid 19 cough sub challenge ccs and the covid 19 speech sub challenge css for the ccs and css we employ two subsets from the cam bridge covid 19 sound database 5 6 the database was collected via the covid 19 sounds app since its launch in april 2020 aiming at collecting data to inform the diagnosis of covid 19 based primarily on voice breathing and coughing participants were able to provide audio samples together with their covid 19 test results via multiple platforms a webpage an android app and an ios app the participants also provided basic demographic medical information and reported symp toms for the ccs and the css only cough sounds and voice recordings with covid 19 positive negative test results were included separately and only audio data and the corresponding covid 19 test labels are provided the quality of these data was manually checked as they were crowd sourced the orig inal audio data had varying sampling rates and formats all of them were resampled and converted to 16 khz and mono 16 bit and further normalised recording wise to eliminate varying loud ar x iv 2 10 2 13 46 8 v 1 ee ss a s 2 4 f eb 2 02 1 pr eli m in ar y ness for the ccs 929 recordings from 397 participants were provided in total 1 63 hrs in each cough recording the partici pant provided one to three forced coughs for the css we use 893 recordings from 366 participants in total 3 24 hrs in each speech recording the participant recorded speech content i hope my data can help to manage the virus pandemic in one language english italian or german etc one to three times for each recording a covid 19 test result was available which was self reported by the participant to create the two class clas sification task the original covid 19 test results were mapped onto either positive denoted as p or negative n 2 2 the escalation sub challenge ess for the ess the interspeech compare escalation corpus is provided consisting of the dataset of aggression in trains tr 7 and the stress at service desk dataset sd 8 both present unscripted interactions between actors where friction appears as they spontaneously react to each other based on short scenario descriptions while the datasets share the same proce dure for eliciting interactions the topics the number of partici pants in the scene and amount of overlapping speech as well as the recording quality differ the tr dataset consists of 21 scenarios of unwanted behaviours in trains and train stations e g harassment theft travelling without a ticket played by 13 subjects it was annotated based on aggression levels on a 5 point scale by 7 raters krippendorff s alpha 0 77 here the annotation based on audio footage is used the sd dataset con tains scenarios of problematic interactions situated at a service desk e g a slow and incompetent employee while the customer has an urgent request it contains 8 subjects and the recordings were annotated for stress levels on a 5 point scale by 4 raters krippendorff s alpha 0 74 based on audio visual footage all original labels were mapped onto a 3 point scale sd classes 1 and 2 and tr class 1 onto low sd class 3 and tr class 2 onto medium and the rest of the data onto high escalation the language spoken in the escalation corpus is dutch two scenar ios from sd where english was spoken were excluded manual transcriptions are provided the corpus has been re segmented based on linguistic information resulting in 410 and 501 test segments of an average length of 5 seconds the challenge task is to use the sd dataset for training and to recognise escalation levels in the tr dataset 2 3 the primates sub challenge prs for the prs the primate vocalisations corpus described in zwerts et al 9 is used the global biodiversity crisis calls for effective monitoring methods to measure manage and con serve wildlife using acoustic recordings is a non invasive and potentially cost effective way to identify and count species for environments like tropical forests where opportunities for vi sual monitoring are limited several studies have applied auto matic acoustic monitoring for a variety of taxa ranging from birds 10 to forest elephants 11 and sporadically also for primates 12 13 14 zwerts et al 9 recently collected acous tic data from a primate sanctuary in cameroon the recorded species were chimpanzees pan troglodytes mandrills man drillus sphinx red capped mangabeys cercocebus torquatus and a mixed group of guenons cercopithecus spp the sanc tuary houses primates under semi natural conditions making background noise relatively comparable to natural forests albeit less rich in biodiversity and also containing human related noise recordings were made between december 2019 and january 2020 with a timespan of 32 days using audiomoth v 1 1 0 table 1 databases number of instances per class in the train dev test splits test split distributions are blinded during the ongoing challenge and will be given in the final version train dev test ccs covid 19 cough c 19 c corpus no covid 19 215 183 blinded blinded covid 19 71 48 blinded blinded 286 231 208 725 css covid 19 speech c 19 s corpus no covid 19 243 153 blinded blinded covid 19 72 142 blinded blinded 315 295 283 893 ess escalation at service desks and in trains cest l 156 69 blinded blinded m 74 33 blinded blinded h 63 15 blinded blinded 293 117 501 911 prs primate vocalisations corpus pvc c 2 217 2 217 blinded blinded m 874 874 blinded blinded r 208 209 blinded blinded g 158 159 blinded blinded background 3 458 3 459 blinded blinded 6915 6918 6923 20756 recorders 15 mounted either on the fence or nearby the respec tive species enclosure with 48 khz sampling rate and 30 6 db gain yielding 358 gbs of acoustic data with a total duration of 1 112 hours 9 a semi automatic annotation process speeded up the manual annotation efforts with 1 initial annotation based on spectrogram analysis and listening 2 vocalisation detec tion based on energy variation in certain frequency sub bands 150 hz 2 khz and 3 final annotation based on spectrogram analysis and listening yielding over 10 k annotated vocalisa tions for the background class the recordings not annotated as vocalisation were sampled so as to exactly match the duration distribution of the annotated chunks of each species 9 3 experiments and results for all corpora the segmented audio was converted to single channel 16 khz 16 bits pcm format table 1 shows the number of cases for train dev and test for the databases partitions for ccs css and ess were gender balanced 3 1 approaches compare acoustic feature set the official baseline fea ture set is the same as has been used in the eight previous editions of the compare challenges starting from 2013 16 it contains 6 373 static features resulting from the computation of function als statistics over low level descriptor lld contours 17 16 a full description of the feature set can be found in 18 bag of audio words boaws these have been applied suc cessfully for e g acoustic event detection 19 and speech based emotion recognition 20 audio chunks are represented as histograms of acoustic llds after quantisation based on a codebook one codebook is learnt for the 65 llds from the compare feature set and another one for the 65 deltas of these llds in table 2 results are given for different code pr eli m in ar y table 2 results for the four sub challenges the official baselines for test are highlighted bold and greyscale there are no official baselines for dev c complexity parameter of the svm for all from 10 5 to 1 only best result n codebook size for bag of audio words boaw splitting the input into two codebooks compare llds compare lld deltas of the same given size with 50 assignments per frame densenet 121 pre trained cnn used for extraction of deep spectrum features x threshold power levels for s 2 sae under which was clipped dife linguistic feature extraction pipeline and svm end 2 you end to end learning with convolutional recurrent neural network hidden units nh uar unweighted average recall ccs covid 19 coughing css covid 19 speech ess escalation sub challenge prs primates sub challenge ci on test confidence intervals for test see explanation in text ccs css ess prs uar uar uar uar dev test ci on test dev test ci on test dev test ci on test dev test ci on test c opensmile compare functionals svm 61 4 65 5 56 1 74 3 66 1 67 2 57 9 72 1 66 0 77 8 70 2 71 1 70 5 58 6 53 5 63 3 55 2 58 3 82 4 82 2 80 5 83 9 78 8 79 6 n openxbow compare boaw svm 125 60 7 66 7 59 5 75 3 64 5 65 5 66 0 63 6 57 6 69 6 62 0 63 2 72 2 55 8 50 2 61 0 52 6 56 4 250 60 7 63 3 54 1 72 3 60 8 62 0 60 6 60 4 54 5 66 3 60 9 61 9 69 0 53 0 47 8 57 8 50 9 53 3 80 0 80 9 79 2 82 5 78 8 79 5 500 66 4 67 6 59 3 76 7 65 7 66 8 64 2 64 7 58 7 70 4 62 6 63 7 70 1 49 4 44 4 54 0 47 3 49 3 83 1 82 4 80 6 84 0 80 1 80 8 1000 66 2 69 1 60 6 77 5 69 3 70 2 62 6 68 7 62 9 74 2 66 0 67 0 69 7 56 8 52 0 61 8 55 7 56 9 83 3 83 9 82 2 85 5 81 4 81 9 2000 64 7 72 9 64 4 80 5 71 5 72 2 66 3 68 7 62 9 74 2 64 4 66 4 70 6 59 8 54 8 64 7 56 3 58 2 network deepspectrum svm densenet 121 63 3 64 1 55 7 72 8 65 9 67 1 56 0 60 4 55 9 64 9 57 8 58 7 64 2 56 4 51 5 61 3 53 6 55 2 81 3 78 8 76 9 80 6 76 1 76 8 x db audeep s 2 sae svm 30 60 7 55 2 47 6 61 9 51 9 53 5 65 8 59 9 53 6 65 4 58 2 59 3 39 1 35 3 30 0 40 4 34 8 37 3 70 6 69 7 67 7 71 8 69 1 69 5 45 64 1 60 5 51 8 69 5 61 0 62 0 66 3 55 2 49 1 61 0 54 1 55 2 41 3 43 1 37 8 48 6 38 5 42 0 80 3 82 3 80 6 83 8 80 5 81 3 60 67 6 67 6 60 3 75 4 64 9 65 8 59 4 53 3 47 4 59 4 52 2 53 5 42 0 44 3 39 2 49 6 41 7 44 1 81 6 84 1 82 5 85 6 82 4 83 2 75 64 0 64 6 56 1 72 6 61 0 62 3 58 4 52 2 45 9 57 7 52 0 52 9 49 0 52 2 47 2 56 9 50 1 52 0 80 7 83 0 81 5 88 0 81 1 82 0 fused 65 4 64 2 57 0 72 2 62 1 63 1 62 2 64 2 63 1 74 3 62 3 64 2 46 8 45 0 39 8 50 4 45 1 47 5 84 6 86 6 85 1 88 0 84 6 85 2 features dife transformer svm plain 51 2 36 8 32 2 41 7 38 8 41 2 plain blatt 50 3 45 2 39 4 50 8 44 0 45 3 sent 56 5 44 1 38 4 49 7 40 9 44 2 sent blatt 47 3 47 2 41 8 52 9 46 9 47 8 tuned blatt 43 5 44 9 40 0 50 3 43 7 45 3 nh rnn end 2 you cnn lstm rnn 64 61 8 64 7 56 2 73 5 70 5 68 7 63 1 74 3 64 1 54 0 48 8 59 5 72 70 70 8 68 8 72 9 fusion of best 73 9 66 0 82 6 71 1 65 4 76 3 59 7 55 0 64 4 87 5 86 0 88 9 book sizes codebook generation is done by random sampling from the llds deltas in the training data each lld delta is assigned to the 10 audio words from the codebooks with the lowest euclidean distance both boaw representations one from the llds and one from their deltas are concatenated finally a logarithmic term frequency weighting is applied to compress the numeric range of the histograms llds are ex tracted with the opensmile toolkit boaws are computed using openxbow 21 deep spectrum the feature extraction deep spectrum toolkit 1 is applied to obtain first deep representations from the in put audio data utilising pre trained convolutional neural networks cnns 22 deep spectrum features have been shown to be effective e g for speech processing 23 first audio signals are transformed into mel spectrogram plots using a hanning window of width 32 ms and an overlap of 16 ms from these 128 mel frequency bands are computed the spectrograms are then forwarded through densenet 121 24 a pre trained cnn and the activations of the avg pool layer of the network are extracted resulting in a 2 048 dimensional feature vector audeep another feature set is obtained through unsupervised representation learning with recurrent sequence to sequence au toencoders using audeep 2 25 26 these explicitly model the inherently sequential nature of audio with recurrent neural net works rnns within the encoder and decoder networks 25 26 here mel scale spectrograms are first extracted from the raw waveforms in a data set in order to eliminate some background noise power levels are clipped below four different given thresh olds in these spectrograms which results in four separate sets 1 https github com deepspectrum deepspectrum 2 https github com audeep audeep of spectrograms per data set subsequently a distinct recurrent sequence to sequence autoencoder is trained on each of these sets of spectrograms in an unsupervised way i e without any label information the learnt representations of a spectrogram are then extracted as feature vectors for the corresponding in stance finally these feature vectors are concatenated to obtain the final feature vector for the results shown in table 2 the autoencoders hyperparameters were not optimised dife escalation is marked by an increase in arousal coming from acoustic rather than linguistic features yet semantic con notations might additionally play a role 27 28 to this aim we developed a lightweight dutch linguistic feature extractor dife pipeline similar to 29 and last year s challenge 30 to utilise linguistic features for ess 3 transformer language em beddings recently showed tremendous success over a wide range of natural language processing tasks for the vectorisation dife either utilises a a standard pre trained dutch bert model plain b a fine tuned version on an external sentiment sent task 31 or c a fine tuned version on the escalation train and validation partitions tuned next a 768 dimensional con text embedding vector for each word of a segment of the last 4 layers is extracted and summed up over the last four layers 32 the sequence of encoded words is then either summed up again across the time dimension or fed into a feature compression block to obtain a single feature vector for the entire segment for compression the pipeline uses a bidirectional long short term memory lstm rnn with an attention module blatt fol lowed by two feedforward layers the output of this last layer is used as feature input for the svm evaluation end 2 you we utilise the multimodal profiling toolkit 3 https github com lstappen dife https github com deepspectrum deepspectrum https github com audeep audeep https github com lstappen dife pr eli m in ar y ne ga tiv e po sit ive predicted label ne ga tiv e po sit ive tr ue la be l 149 81 4 34 18 6 25 52 1 23 47 9 ccs uar 64 7 openxbow 0 0 0 2 0 4 0 6 0 8 1 0 ne ga tiv e po sit ive predicted label ne ga tiv e po sit ive tr ue la be l 121 79 1 32 20 9 90 63 4 52 36 6 css uar 57 9 opensmile 0 0 0 2 0 4 0 6 0 8 1 0 l m h predicted label l m h tr ue la be l 49 72 1 18 26 5 1 1 5 1 2 9 22 64 7 11 32 4 0 0 0 4 25 0 12 75 0 ess uar 70 6 openxbow 0 0 0 2 0 4 0 6 0 8 1 0 b c g m r predicted label b c g m r tr ue la be l 2957 85 5 281 8 1 50 1 4 140 4 0 31 0 9 303 13 7 1846 83 3 21 0 9 35 1 6 12 0 5 9 5 7 5 3 1 142 89 3 2 1 3 1 0 6 98 11 2 9 1 0 6 0 7 745 85 2 16 1 8 20 9 6 7 3 3 5 2 4 11 5 3 166 79 4 prs uar 84 6 audeep 0 0 0 2 0 4 0 6 0 8 1 0 figure 1 confusion matrices for ccs css ess and prs the individual approach hyperparameters performing on dev for the best test result without fusion were chosen given on top of each figure in the cells absolute number and percent of classified as of the class displayed in the respective row percentage also indicated by colour scale the darker the higher end 2 you 33 4 to perform end to end learning for our pur poses we utilise the emo 18 34 deep neural network that uses a convolutional network to extract features from the raw time representation and then a subsequent recurrent network with gated recurrent units grus which performs the final classifi cation for training the network we split the raw waveform into chunks of 100 ms each except for the prs sub challenge with chunks of 70 ms these are fed into a three layer convolutional network comprised of a series of convolution and pooling oper ations which try to find a robust representation of the original signal the extracted features are passed to a two layer gru to capture the temporal dynamics in the raw waveform 3 2 challenge baselines and interpretation for the sake of transparency and reproducibility of the baseline computation in line with previous years we use an open source implementation of svms with linear kernels the provided scripts employ the scikit learn toolkit with its class lin earsvc for the classification based on functionals boaw audeep dife and deep spectrum features all feature representations were scaled to zero mean and unit standard devi ation minmaxscaler of scikit learn using the parame ters from the respective training set when train and dev were fused for the final classifier the parameters were calculated on this fusion the complexity parameter c was always optimised during the development phase each sub challenge package includes scripts that allow participants to reproduce the baselines and perform the testing in a reproducible and automatic way including pre processing model training model evaluation on dev and scoring by the competition and further measures this year we provide the six approaches outlined above the same way as in the last three years we chose the highest results on test for defining the baselines irrespective of the corresponding results on dev in order to prevent participants from surpassing the official baseline by simply repeating or slightly modifying other constellations that can be found in table 2 a fusion of the best configurations each different approach with its best parameters with majority voting is given in the last row as can be seen in table 2 for ccs the baseline is fusion of best with uar 73 9 for ccs the baseline is based on compare with uar 72 1 for ess boaws define the baseline with uar 59 8 and for prs the baseline is fusion of best with uar 87 46 we provide two types of 95 confidence intervals see the column ci on test in table 2 first we did 1000 x bootstrap ping for test random selection with replacement and computed uars based on the same model that was trained with train and 4 https github com end 2 you end 2 you dev the ci for these uars is given before the slash then we did 100 x bootstrapping 5 for the corresponding combination of train and dev and employed the different models obtained from these combinations to get uars for test 6 and subsequently cis as displayed after the slash note that for this type of ci the test results are often above the ci sometimes within and in a few cases below obviously reducing the variability of the samples in the training phase with bootstrapping results on average in somehow lower performance figure 1 displays the confusion matrices for the four sub challenges for dev corresponding to the best result on test e g for css best test result without fusion is 72 9 uar for n 2000 displayed is the confusion matrix corresponding to the uar of 64 7 especially for ccs but for css as well positive is frequently confused with negative which may be tuned in a use case for ess confusion between the extreme classes l and h are almost non existent the high uar for prs is mirrored by the high values in the diagonal all five classes are predicted in a range of 10 absolute from 79 to 89 4 concluding remarks this year s challenge is new by four new tasks covid 19 cough and speech escalation and primates all of them highly relevant for applications besides the by now classic ap proaches compare and bag of audio words boaws we further featured sequence to sequence autoencoder based audio features by the audeep toolkit deep spectrum a dutch linguistic feature extractor dife as well as end 2 end deep sequence modelling for all computation steps scripts are pro vided that can but need not be used by the participants we expect participants to obtain better performance measures by employing novel combinations of procedures and features in cluding such tailored to the particular tasks 5 acknowledgements we acknowledge funding from the dfg s reinhart koselleck project no 442218748 audi 0 nomous the eu s hori zon 2020 grant no 115902 radar cns and the erc project no 833296 ear 5 for prs only 10 x was executed because of the large number of data points 6 this holds apart from end 2 you that would have required too time consuming computation cycles https github com end 2 you end 2 you pr eli m in ar y 6 references 1 b schuller a batliner s steidl and d seppi recognising realistic emotions and affect in speech state of the art and lessons learnt from the first challenge speech communication vol 53 pp 1062 1087 2011 2 b schuller and a batliner computational paralinguistics emotion affect and personality in speech and language processing chichester uk wiley 2014 3 m a ismail s deshmukh and r singh detection of covid 19 through the analysis of vocal fold oscillations arxiv preprint arxiv 2010 10707 2020 4 a rosenberg classifying skewed data importance weighting to optimize average recall in proc interspeech portland or 2012 pp 2242 2245 5 c brown j chauhan a grammenos j han a hasthanasombat d spathis t xia p cicuta and c mascolo exploring auto matic diagnosis of covid 19 from crowdsourced respiratory sound data in proc kdd san diego ca 2020 pp 3474 3484 6 j han c brown j chauhan a grammenos a hasthanasom bat d spathis t xia p cicuta and c mascolo exploring automatic covid 19 diagnosis via voice and symptoms from crowdsourced data in proc icassp toronto canada 2021 5 pages to appear 7 i lefter l j rothkrantz and g j burghouts a comparative study on automatic audio visual fusion for aggression detection us ing meta information pattern recognition letters vol 34 no 15 pp 1953 1963 2013 8 i lefter g j burghouts and l j rothkrantz an audio visual dataset of human human interactions in stressful situations journal on multimodal user interfaces vol 8 no 1 pp 29 41 2014 9 j a zwerts j treep c s kaandorp f meewis a c koot and h kaya introducing a central african primate vocalisa tion dataset for automated species classification arxiv preprint arxiv 2101 10390 2021 10 n priyadarshani s marsland and i castro automated birdsong recognition in complex acoustic environments a review journal of avian biology vol 49 no 5 pp jav 01 447 2018 11 p h wrege e d rowland s keen and y shiu acoustic mon itoring for conservation in tropical forests examples from forest elephants methods in ecology and evolution vol 8 no 10 pp 1292 1301 2017 12 s heinicke a k kalan o j wagner r mundry h luka shevich and h s k hl assessing the performance of a semi automated acoustic monitoring system for primates methods in ecology and evolution vol 6 no 7 pp 753 763 2015 13 p fedurek k zuberb hler and c d dahl sequential informa tion in a great ape utterance scientific reports vol 6 p 38226 2016 14 d j clink m c crofoot and a j marshall application of a semi automated vocal fingerprinting approach to monitor bornean gibbon females in an experimentally fragmented landscape in sabah malaysia bioacoustics vol 28 no 3 pp 193 209 2019 15 a p hill p prince j l snaddon c p doncaster and a rogers audiomoth a low cost acoustic device for monitoring biodiversity and the environment hardwarex vol 6 p e 00073 2019 16 b schuller s steidl a batliner a vinciarelli k scherer f ringeval m chetouani f weninger f eyben e marchi m mortillaro h salamin a polychroniou f valente and s kim the interspeech 2013 computational paralinguistics challenge social signals conflict emotion autism in proc interspeech lyon france 2013 pp 148 152 17 f eyben f weninger f gro and b schuller recent devel opments in opensmile the munich open source multimedia feature extractor in proc acm multimedia barcelona spain 2013 pp 835 838 18 f weninger f eyben b schuller m mortillaro and k r scherer on the acoustics of emotion in audio what speech music and sound have in common frontiers in emotion science vol 4 pp 1 12 2013 19 h lim m j kim and h kim robust sound event classi fication using lbp hog based bag of audio words feature representation in proc interspeech dresden germany 2015 pp 3325 3329 20 m schmitt f ringeval and b schuller at the border of acous tics and linguistics bag of audio words for the recognition of emotions in speech in proc interspeech san francisco ca 2016 pp 495 499 21 m schmitt and b w schuller openxbow introducing the passau open source crossmodal bag of words toolkit journal of machine learning research vol 18 pp 1 5 2017 22 s amiriparian m gerczuk s ottl n cummins m freitag s pugachevskiy and b schuller snore sound classification using image based deep spectrum features in proc interspeech 2017 stockholm sweden 2017 pp 3512 3516 23 s amiriparian m gerczuk s ottl n cummins s pugachevskiy and b schuller bag of deep features noise robust deep feature representations for audio analysis in proc ijcnn rio de janeiro brazil 2018 pp 2419 2425 24 g huang z liu l van der maaten and k q weinberger densely connected convolutional networks in proc cvpr 2017 pp 4700 4708 25 s amiriparian m freitag n cummins and b schuller se quence to sequence autoencoders for unsupervised represen tation learning from audio in proc dcase 2017 munich germany 2017 pp 17 21 26 m freitag s amiriparian s pugachevskiy n cummins and b schuller audeep unsupervised learning of representations from audio with deep recurrent neural networks journal of machine learning research vol 18 pp 1 5 2018 27 l stappen a baird e cambria and b w schuller senti ment analysis and topic recognition in video transcriptions ieee intelligent systems vol 36 no 2 2021 28 l stappen b schuller i lefter e cambria and i kompatsiaris summary of muse 2020 multimodal sentiment analysis emotion target engagement and trustworthiness detection in real life me dia in proceedings of the 28 th acm international conference on multimedia 2020 pp 4769 4770 29 l stappen g rizos m hasan t hain and b w schuller uncertainty aware machine support for paper reviewing on the interspeech 2019 submission corpus proc interspeech 2020 pp 1808 1812 2020 30 b w schuller a batliner c bergler e m messner a hamil ton s amiriparian a baird g rizos m schmitt l stap pen et al the interspeech 2020 computational paralinguis tics challenge elderly emotion breathing masks proc interspeech shanghai china isca 2020 31 w de vries a van cranenburgh a bisazza t caselli g van no ord and m nissim bertje a dutch bert model arxiv preprint arxiv 1912 09582 2019 32 j devlin m w chang k lee and k toutanova bert pre training of deep bidirectional transformers for language un derstanding in proceedings of the 2019 conference of the north american chapter of the association for computational linguistics human language technologies 2019 pp 4171 4186 33 p tzirakis s zafeiriou and b w schuller end 2 you the im perial toolkit for multimodal profiling by end to end learning arxiv preprint arxiv 1802 01115 2018 34 p tzirakis j zhang and b w schuller end to end speech emo tion recognition using deep neural networks in proc icassp 2018 pp 5089 5093 1 introduction 2 the four sub challenges 2 1 the covid 19 cough sub challenge ccs and the covid 19 speech sub challenge css 2 2 the escalation sub challenge ess 2 3 the primates sub challenge prs 3 experiments and results 3 1 approaches 3 2 challenge baselines and interpretation 4 concluding remarks 5 acknowledgements 6 references