the parametric cost function approximation a new approach for multistage stochastic programming warren b powell department of operations research and financial engineering saeed ghadimi university of waterloo january 4 2022 ar x iv 2 20 1 00 25 8 v 1 m at h o c 1 j an 2 02 2 abstract the most common approaches for solving multistage stochastic programming problems in the re search literature have been to either use value functions dynamic programming or scenario trees stochastic programming to approximate the impact of a decision now on the future by contrast common industry practice is to use a deterministic approximation of the future which is easier to understand and solve but which is criticized for ignoring uncertainty we show that a parameterized version of a deterministic optimization model can be an effective way of handling uncertainty with out the complexity of either stochastic programming or dynamic programming we present the idea of a parameterized deterministic optimization model and in particular a deterministic lookahead model as a powerful strategy for many complex stochastic decision problems this approach can handle complex high dimensional state variables and avoids the usual approximations associated with scenario trees or value function approximations instead it introduces the offline challenge of designing and tuning the parameterization we illustrate the idea by using a series of application settings and demonstrate its use in a nonstationary energy storage problem with rolling forecasts keywords stochastic optimization policy search stochastic programming approximate dy namic programming simulation optimization parametric cost function approximation 1 introduction there is an extensive history in the academic literature of solving sequential decision problems over time under uncertainty using the idea of approximating the future using sampled scenario trees dating to the original paper by dantzig 1955 since this time thousands of papers have been written using this approach for making decisions under uncertainty a parallel literature has evolved estimating the value of being in a downstream state using statistical approximations estimated using machine learning that have evolved under names such as approximate or adaptive dynamic programming neuro dynamic programming and more recently reinforcement learning although this is an umbrella for an entire family of methods in the arena of stochastic linear programs we can approximate the value of being in a state using bender s cuts a method known as stochastic dual dynamic programming other methods have evolved under the heading of robust optimization when used in the setting of sequential decision problems which optimizes over an uncertainty set all of these methods are computationally demanding which has sharply limited their use in practice at the same time when used in the context of complex operational problems our primary focus in this paper all of these methods are solving approximations of lookahead models which means that none of them are offering optimal or even asymptotically optimal policies an optimal solution of an approximate problem is not an optimal policy by contrast there has been a long history in industry of using deterministic optimization models to make decisions that are then implemented in a stochastic setting grid operators use deterministic forecasts of wind solar and loads to plan energy generation wallace fleten 2003 utilities use rolling estimates to plan the storage of natural gas lai et al 2008 airlines use deterministic esti mates of flight times to schedule aircraft and crews lan et al 2006 and retailers use deterministic estimates of demands and travel times to plan inventories harrison van mieghem 1999 there is an extensive literature using deterministic lookahead models for dynamic vehicle routing problems pillac et al 2013 all of us use the solution of deterministic shortest path problems produced by in vehicle navigation systems to plan our route through stochastic dynamic traffic networks these models have been widely criticized by the research community for not accounting for uncertainty but this criticism ignores the creative use of parametric modifications that help these models account for uncertainty while these parameterized policies have been dismissed as industrial heuristics in the stochastic optimization community we argue that they parallel parametric functions that have been studied in statistics for over 100 years and widely used in practice the field of parametric statistics requires that a knowledgeable human choose the structure of a parameterized model which might be linear or nonlinear in the parameters this includes neural networks and then uses data and algorithms to choose the parameters our use of parameterized deterministic models similarly requires a domain expert to design the parameterized optimization problem after which algorithms have to be used to find the best values of the parameters with parametric statistics we need a training dataset to estimate the parameters with a parameterized policy we need a model of the problem that is captured in a dataset although it is possible to do online training in the field in this paper we characterize these modified deterministic models as parametric cost function approximations cfas which puts them into the same category as other parameterized policies that have been used in computer science for solving simpler problems under the umbrella of policy search policy search is the same as stochastic search applied to the context of tuning parameterized policies for sequential decision problems the difference is that using a parameterized optimization model for a policy allows us to solve dramatically more complex problems than we can with sim ple analytical functions that are the focus of the policy search literature sutton barto 2018 deisenroth et al 2013 levine koltun 2013 instead of controlling a robot or computer game parametric cost function approximations can be used to dispatch thousands of trucks schedule an airline manage a network of hydroelectric reservoirs or plan energy generation for a power grid our paper takes these ideas into the arena of multistage stochastic programming problems which has been dominated in the research community by methods based on lookahead policies using sce nario trees or for special cases benders decomposition birge louveaux 2011 sen zhou 2014 bayraksan morton 2009 zhao et al 2013 our use of modified linear programs is new to the policy search literature where policies are typically parametric models such as linear models affine policies structured nonlinear models such as s s policies for inventories or neural networks such as han e 2016 these more classical policies are limited to scalar or low dimensional problems which could not be applied to the domain of high dimensional resource allocation problems there are two dimensions to our approach 1 the design of the parameterized lookahead model which serves as the policy for making decisions 2 the optimization of the parameters so that the policy performs as well as policy in expectation the process of designing the parameterization involves the same art as the design of any statistical model or parametric policy it requires exploiting the structure of the problem which would be associated with a particular problem domain the optimization of the parameters is a more classical algorithmic challenge which we formulate and solve as a stochastic optimization problem more precisely a stochastic search problem both dimensions are challenging but the end result is a model that is no harder to solve than a classical deterministic lookahead model by contrast policies based on stochastic programming produce stochastic lookahead models that are much harder to solve in the field than a parameterized deterministic model in addition we argue below that a parameterized deterministic model may easily produce higher quality solutions we feel that classical stochastic programs based on scenario trees suffer from several limitations they are much harder to solve than deterministic lookahead models two stage stochastic programs which are the most widely used approach in stochastic pro gramming require a number of modeling approximations including 2 the replacement of fully sequential multistage stochastic decision problems with two stage approximations the need to use a sample of outcomes and often a fairly small sample the inability to model interactions between decisions and exogenous information relevant in some applications there are many problems where the effect of uncertainty on the behavior of the policy is well known yet two stage stochastic programs using modest numbers of scenarios do not provide any mechanism to capture this intuition methods based on value function approximations which includes sddp pereira pinto 1991 birge louveaux 2011 kall wallace 2009 and approximate dynamic programming powell 2011 bertsekas 2017 represent an alternative approach for optimizing over a stochastic lookahead model there are many complex problems where adp is not a viable method for solving the full model but might be a useful approach for solving an approximate stochastic lookahead model we revisit this issue later this paper formalizes the idea that an effective way to solve certain classes of complex stochastic optimization problems is to shift the modeling of uncertainty from an approximate lookahead model to the stochastic base model typically implemented as a simulator but which might also be the real world tuning a model in a stochastic simulator makes it possible to handle arbitrarily complex dynamics avoiding the many approximations that are standard in stochastic programming this strategy also means that we transition from methods that are hard to solve in the field to methods that are relatively easy to solve in the field but which require serious research in the laboratory to design and tune model parameterizations the parametric cfa makes it possible to incorporate problem structure for handling uncertainty some examples include supply chains handle uncertainty by introducing buffer stocks hospitals can handle uncertainty in blood donations and the demand for blood by maintaining supplies of o minus blood which can be used by anyone people will use the estimated time for the shortest path when driving to a destination but will then leave early to accommodate the uncertainty in the travel times grid operators handle uncertainty in generator failures as well as uncertainty in energy from wind and solar by requiring reserve generator capacity central to our approach is the ability to manage uncertainty by recognizing effective strategies for responding to unexpected events we would argue that this structure is apparent in many settings especially in complex resource allocation problems we offer that our approach represents an interesting and very practical alternative to classical stochastic programming we also argue 3 that this approach is a perfectly valid form of stochastic optimization that may easily outperform methods based on solutions of approximate stochastic lookahead models our presentation is organized as follows section 2 provides a canonical model for sequential decision problems and describes two strategies for designing policies policy search which searches over classes of policies and policies within a class to find the ones that work best over time sec tion 3 provides more detail and policies based on lookahead models which make good decisions by creating the best approximation of the impact of a decision now on the future section 4 provides more detail basic cost function approximations fall within the policy search class but we will be considering hybrids that combine deterministic lookaheads from the lookahead class with param eterizations from the policy search class section 5 provides a series of examples of cost function approximations divided between parameterizations of the objective function and parameterizations of the constraints then section 6 illustrates the idea of a parameterized deterministic lookahead policy using the setting of a time dependent energy storage problem using rolling forecasts a com mon modeling device which has been largely overlooked in the research literature section 7 offers some closing remarks 2 canonical model and solution strategies we begin by writing a sequential decision problem as the sequence s 0 x 0 w 1 st xt wt 1 st where st is our state at time t which includes physical state variables rt this captures physical resources such as inventories or the location on a graph information it this could be prices speeds weather and beliefs bt which captures what we know about uncertain quantities and parameters let ct st xt be the cost we incur given what we know in st and our decision xt wt 1 is information that arrives after we make a decision which can depend on st and or xt we make decisions xt using a method we call a policy that we write as x st the state variable evolves according to a transition function st 1 s m st xt x st wt 1 where the transition can span updating inventories tracking the movement of a vehicle updating prices and weather or updating estimates or beliefs about uncertain quantities and parameters our goal is to find a policy that solves min e t t 0 ct st x t st s 0 1 where st 1 s m st x t st wt 1 and given an information model for s 0 w 1 wt we will sometimes refer to 1 as the base model which will typically be a simulator but is sometimes the real world 4 this canonical model is the foundation for a wide range of sequential decision problems powell 2019 b describes two fundamental strategies for designing policies policy search where we search over classes of functions for making decisions to find the function that works the best over time and policies based on lookahead approximations which combine the immediate cost of a decision plus an approximation of costs incurred as a result of the decision each of these two strategies can be applied in two different ways creating four classes of policies which encompass every possible method for making decisions powell 2019 b policy search strategies 1 policy function approximations pfas these are analytical functions that map states to decisions 2 cost function approximations cfas these are parameterized static or single period optimization problems that yield a decision policies based on lookahead approximations 3 policies based on value function approximations vfas a value function approximation estimates the future cost from the next state we land in after making decision xt now 4 policies based on approximations of direct lookahead models dlas here we form an approximate model over some horizon which we solve to make a decision now lookahead models can be divided into two broad classes deterministic lookahead models stochastic lookahead models we can also create hybrids as we will below when we create a parameterized deterministic lookahead policy that is a hybrid dla cfa policy function approximations have received considerable attention in computer science under the banner of policy search ng jordan 2000 peshkin et al 2000 hu et al 2007 mannor et al 2003 parameterized policies such as order up to inventory ordering policies have been studied since 1960 clark scarf 1960 along with a host of other specially structured policies in dynamic programming puterman 2014 powell 2022 chapter 14 policy search uses classical stochastic search methods applied to parameterized analytical functions for making decisions drawing on an extensive literature in derivative based and derivative free methods dating to 1951 we review this literature in section 3 3 policies based on value functions and value function approximations have been investigated ex tensively building on the foundation of bellman s equation puterman 2014 bertsekas 2017 or hamilton jacobi equations kirk 2012 stengel 1986 sontag 1998 sethi 2019 lewis et al 2012 value function approximations have been studied under names such as approximate dynamic pro gramming powell 2011 bertsekas 2017 with applications in trucking simao et al 2009 rail bouzaiene ayari et al 2014 and health bartroff lai 2010 reinforcement learning sutton barto 1998 2018 with applications in robotics si et al 2004 deisenroth et al 2013 and 5 games fu 2017 adaptive dynamic programming lewis vrabie 2009 wang et al 2009 neuro dynamic programming bertsekas tsitsiklis 1996 and heuristic dynamic programming si et al 2004 the stochastic programming community has developed the idea of approximating value functions using bender s cuts under the heading of stochastic dual dynamic programming sddp pereira pinto 1991 shapiro et al 2014 with applications in hydroelectric planning shapiro 2011 philpott et al 2000 direct lookahead policies represent an umbrella for a number of strategies that have been studied under names such as model predictive control camacho alba 2013 and stochastic programming birge louveaux 2011 kall wallace 2009 bayraksan morton 2009 zhao et al 2013 sen zhou 2014 this approach has been developed in many books and thousands of papers with applications that include unit commitment jin et al 2011 hydroelectric planning carpentier et al 2015 and transportation lium et al 2009 another form of direct lookahead policy has evolved more recently using robust optimization which replaces scenario trees with uncertainty sets ben tal et al 2009 wiesemann et al 2014 ben tal et al 2005 illustrates robust optimization as a lookahead policy for an inventory problem see bertsimas et al 2011 for a review of other applications of robust optimization cost function approximations however have been largely overlooked by the academic research community cfas are parameterized optimization models where the specific parameterization is designed as would be the case with any parametric model in machine learning to incorporate intuition into how uncertainty would affect the solution the use of parameterized optimization models has been widely used in industry but in an ad hoc manner there is one example of a cost function approximation which has received extensive attention from the research literature the problem of finding the best performer out of a discrete set of alternatives drugs products ads is known as the multiarmed bandit problem a widely studied class of policies are known as upper confidence bounding first introduced by lai robbins 1985 a simple version introduced by kaelbling 1993 is given by xucb st arg max x nx n x 2 where nx is the current estimate of the performance of discrete alternative x x 1 xm e g the expected sales from advertising product x and nx is the standard deviation of n x note that st bt n x n x there is by now an extensive literature proving various regret bounds on the performance of the policy xucb st see e g bubeck cesa bianchi 2012 this is particularly important for this paper since xucb st is first and foremost a class of parametric cost function approximation given that there is an imbedded arg maxx within the policy the regret bounds that have been derived for ucb policies of which 2 is just one example represents a rare set of provable bounds for the quality of a cfa policy pfas and vfas tend to be limited to problems that are relatively simple or which enjoy special structure that can be exploited to estimate the required function the policy or the value function stochastic lookaheads have attracted considerable attention in the research literature but relatively 6 little of this work has made its way into practice for this reason the most common approach used in practice for more complex decision problems is a deterministic approximation which may be either static or single period optimization models or deterministic lookaheads in this paper we want to shine a light on the power of using parameterized deterministic models particularly for the complex problems that often arise in real applications we are not going to argue that this is a panacea that can replace all other methods but we do feel that it is a powerful and overlooked strategy that belongs alongside widely studied but rarely used methods such as stochastic programming or approximate dynamic programming we next provide an overview of the policies based on policy search where we cover pfas and static single period cfas section 3 and policies based on lookahead approximations where we cover policies based on vfas and deterministic or stochastic dlas section 4 ultimately we are going to take the idea of policy search that originated with pfas and apply it to the idea of parameterized optimization models with special emphasis on parameterized deterministic lookaheads we are going to argue that this is often going to be a more effective strategy in practice for complex sequential decision problems than policies based on vfas or stochastic dlas 3 policies based on policy search we begin with the principle of creating parameterized policies broadly defined which we divide between policy function approximations and cost function approximations 3 1 policy function approximations pfas policy function approximations are analytical functions that map states to decisions the functions can be lookup tables parametric functions linear or nonlinear or nonparametric in particular locally parametric pfas using any of a wide range of approximation strategies have been widely studied in the computer science literature under the umbrella of policy search see e g sutton barto 2018 chapter 13 hadjiyiannis et al 2011 lillicrap et al 2015 levine abbeel 2014 although limited to relatively simple problems the fundamental idea of policy search is quite powerful and an idea that we are going to exploit an example of a pfa is a linear decision rule also known as an affine policy which can be written x st f f f f st 3 where f st is a feature drawn from the information in st and f is the coefficient for that feature we might wish to use a nonlinear model to choose the price xbidt to bid for a set of keywords to 7 maximize ad clicks such as x st e f f f f st 1 e f f f f st more generally we could represent a policy using a neural network in which case might have many thousands or millions of parameters we have to choose the type of function f f for a neural network f would specify the network structure number of layers and nodes per layer for a given function type f we have to choose f the choice of function f is the art of policy search just as it is with machine learning if this were a machine learning problem where we are given a training dataset xn yn nn 1 we would be solving min f f f n n 1 yn f xn 2 4 policy search for pfas would be written similarly min f f f e t t 0 ct st x t st s 0 5 where st 1 s m st xt x st wt 1 and where we are given a model for s 0 w 1 wt we note that machine learning 4 requires a training dataset while policy search for sequential decision problems in 5 requires a model of the decision problem costs constraints transition function and exogenous information model the comparison with machine learning hints at the limitation of pfas they only work for relatively simple decision problems for example the response y is typically scalar although it might be a low dimensional vector we could never use a pfa to say schedule machines dispatch a fleet of trucks find a shortest path or optimize flows in a supply chain 3 2 cost function approximations cfas the second class of policy is cost function approximations cfas which are parametrically modi fied optimization problems eventually we are going to include optimization problems that extend into the future but for now we limit ourselves to static or single period optimization models we emphasize that with the notable exception of upper confidence bounding policies equation 2 for multiarmed bandit problems cfas have received virtually no attention in the research literature we can create a cfa by modifying either the objective function or the constraints for this 8 reason we begin by defining c t st xt the modified objective function as determined by the policy where rep resents the tunable parameters x t the modified set of constraints that is the feasible region determined by policy with tunable parameters we might modify the objective function with a linear correction factor which we could write c t st xt c st xt f f f f st xt 6 as an illustration of how constraints can be modified assume that we start with linear constraints atxt bt 7 xt ut 8 xt 0 9 we might modify these using a t a x t b bt c 10 xt ut u 11 xt 0 12 where b bt is the element by element product of the vector bt with the similarly dimensioned vector of coefficients b plus a shift vector c we can enter schedule slack by parameterizing the matrix a t a we then reduce the upper bounds ut by a shift vector u and possibly raise the lower bounds by our constraints are now parameterized by the possibly high dimensional vector a b c u a parametric cfa policy can then be written xcfa st arg min xt x t c st xt 13 we provide examples of cfas in section 5 3 3 algorithms for policy search pfas and cfas both involve tuning a vector once we have the structure of the policy f f which is typically chosen by a knowledgeable human guided by intuition we tune using min e t t 0 ct st x t st s 0 14 9 the tools for optimizing the parameters using 14 fall under the broad umbrella of stochastic search which can be approached using both derivative based algorithms with a literature that dates to robbins monro 1951 and derivative free algorithms with a literature that dates to box wilson 1951 the derivative based stochastic optimization literature is extensive beginning with the body of research building off of robbins monro 1951 in the 1950 s and later see e g dvoretzky 1956 initially for unconstrained problems a separate literature evolved in the context of con strained stochastic gradient problems shor 1979 ermoliev 1983 in addition the simulation optimization community see fu 2015 has developed powerful tools for taking derivatives of sim ulations see glasserman 1991 ho 1992 kushner yin 2003 cao 2008 a nice tutorial is given in chau et al 2014 much of this literature focuses on derivatives of discrete event simula tions but there is an equally extensive literature on methods based on numerical derivatives such as spsa spall 2003 nesterov spokoiny 2017 ghadimi lan 2013 more recently is work on derivatives of parameterized policies for discrete dynamic programs from the reinforcement learning literature under the umbrella of the policy gradient theorem sutton et al 2000 sutton barto 2018 chapter 13 there is a parallel literature in derivative free algorithms for stochastic search which is equally extensive this literature spans active learning problems settles 2010 multiarmed bandit problems gittins et al 2011 and optimal learning powell ryzhov 2012 see powell 2022 chapter 7 for an overview of this rich field while both pfas and cfas require parameter tuning the characteristics of the tuning problems for pfas and cfas tend to be quite different it is well known that scaling is a major issue in stochastic search consider the linear decision rule in equation 3 the scaling of each coefficient f depends heavily on the characteristics of the feature f st by contrast the coefficients used in a parametric cfa tend to be scaled by the structure of the deterministic optimization model in section 6 we demonstrate a parametric cfa for a stochastic inventory control problem where the optimal coefficients are all equal to 1 0 if the forecasts are perfect given imperfect forecasts the optimal coefficients all appear to be in the interval 0 2 4 policies based on lookahead approximation the second strategy for creating policies is to construct policies based on approximations of the downstream impact of a decision xt made while in state st an optimal policy can be written x t st arg min xt xt ct st xt e min e t t t 1 ct st x t st st 1 st xt 15 equation 15 is called a lookahead policy not surprisingly this is computationally intractable for any realistic problem this includes all the problems that we are interested in just as we divided the policy search strategy into two classes pfas and cfas there are two 10 classes of policies that we can use to approximate equation 15 these are policies based on value function approximations vfa policies and policies based on approximations of the direct lookahead approximations dla policies we describe these in more detail in sections 4 1 and 4 2 below 4 1 policies based on value function approximations equation 15 is basically bellman s equation although it is more conventional to write x t st arg min xt xt ct st xt e vt 1 st 1 st xt 16 where vt 1 st 1 min e t t t 1 ct st x t st st 1 17 alternatively we can write the expression for value functions recursively using vt st min xt ct st xt e vt 1 st 1 st xt 18 equation 18 is the most common way of writing bellman s equation but it is mathematically equivalent to equation 15 assuming that we can compute vt st using either 17 or 18 which is never the case for the problems that we are interested in when xt is a vector it is customary to eliminate the expectation in 16 and 18 by using the post decision state variable sxt see powell 2011 and shapiro 2011 we then replace the post decision value function v xt s x t which we could never compute with an approximation v x t s x t giving us the policy xv fat st arg min xt ct st xt v x t s x t 19 where v xt s x t might be a linear model separable piecewise linear functions or benders cuts while this approach has attracted considerable attention in the literature see e g powell et al 2004 powell 2011 bertsekas 2011 sutton barto 2018 it is limited to a surprisingly narrow set of problems for example while sddp has been widely studied in the stochastic programming community applications are generally limited to fairly simple resource allocation problems such as hydroelectric planning problems shapiro 2011 philpott de matos 2012 which is known as a single layer resource allocation problem water is the only resource for example you could never use sddp for dynamic vehicle routing problems complex inventory planning problems or dynamic shortest path problems as an indication of how easy it is to break approximate dynamic programming adp or sddp is very effective for solving a blood management problem as long as the surgeries requiring blood 11 having to be completed at a particular point in time this might be some time during a week if we have elective surgeries that can be delayed creating effective value functions becomes dramatically more difficult even more complex problems include dynamic vehicle routing where you have to optimize the movement of vehicles and the timing of deliveries or scheduling machines to complete a series of tasks in short value function approximations are effective when there is structure such as linearity or convexity that can be exploited later we are going to illustrate a cfa in the solution of a time dependent inventory problem with rolling forecasts rolling forecasts create complex high dimensional state variables that are completely intractable using methods based on value function approximations 4 2 policies based on direct lookahead approximations the most commonly used approach used to solve complex time dependent problems is to solve an approximate lookahead model on a rolling basis here we first create an approximate sequential decision problem that we are going to use as an approximate model of the future we represent this approximate sequential decision problem as s tt x tt w tt s tt x tt w t t 1 where s tt x tt and w t t 1 are approximations of st xt and wt for a decision we are making at time t while there are a variety of strategies for approximating lookahead models the two that have received the most attention are deterministic lookaheads this is the approach most widely used in practice but it has a substantial academic following under the umbrella model predictive control using a deter ministic lookahead model reduces equation 15 to xdlat st arg min x tt c s tt x tt min x t t 1 x t t h t h t t 1 c s tt x tt arg min x tt x t t 1 x t t h t h t t c s tt x tt 20 equation 20 is so widely used it is known under a number of names including rolling hori zon procedure receding horizon procedure model predictive control or deterministic direct lookahead sethi sorger 1991 camacho alba 2013 powell 2022 chapter 19 stochastic programming first introduced by dantzig 1955 the most common approach is to replace the fully sequential decision problem in the future with a two stage approximation which means our sequence of decisions and information looks like x tt w t t 1 w t t 2 w t t h x t t 1 x t t 1 x t t h 12 this model assumes we make a single decision now x tt then observe a complete sample path of realizations w t t 1 w t t 2 w t t h and then make a complete set of decisions for each sample path x t t 1 x t t 1 x t t h this approach insures that the decision now xt x tt does not depend on what outcome happens in the future but future decisions x tt for t t are allowed to see the entire history of future information next we create a set of scenarios of w tt that we denote t our policy is then written x t st arg min x tt x tt t h t t 1 t t h c s tt x tt t t h c s tt x tt 21 the optimization problem in 21 is typically around t times bigger than the deterministic problem in equation 20 but at least it is solvable the deterministic lookahead which is also known as model predictive control in the optimal control literature is largely dismissed by the stochastic optimization community as little more than a deterministic approximation by contrast the policy based on the two stage stochastic program in 21 has appeared in thousands of academic publications often without recognizing that it is a suboptimal policy for the original optimization problem in equation 1 see powell 2019 a for a discussion of the limitation of scenario trees for the stochastic unit commitment problem 4 3 discussion policies based on solving lookahead models depend on the accuracy of the model to produce good de cisions the problem is that the solution of full multistage stochastic decision problems is inherently intractable forcing the use of very strong approximations such as two stage stochastic programs we propose to extend the idea of a parametric cost function approximation which we first intro duced for state or single period problems in section 3 2 to deterministic lookahead models then instead of depending on developing an accurate stochastic lookahead model we exploit structure in the problem to parameterize the deterministic lookahead model to produce behaviors that make the optimal solution more robust we then depend on the tuning using a realistic stochastic simulator equation 1 to produce the best values of the parameters some advantages of this approach include the tuning is done in a realistic simulator equation 1 that does not need to make simplifi cations such as an exogenous information process that is independent of decisions 13 the simulator can capture any level of detail in the dynamics of the system the parameterization of the policy can exploit structure and the modeler s intuition about how uncertainty is likely to affect the solution an assumption that is made in virtually all parametric models in machine learning the resulting policy will generally have the same computational demands as a classical unpa rameterized deterministic lookahead which would be much easier than solving any stochastic lookahead model this idea has been widely used in industry in an ad hoc manner specifically industrial ap plications will insert parameters without a recognizing that they are creating a class of policy that is a solution albeit a suboptimal one to the optimization problem 1 and b without recognizing that the parameters need to be tuned using the framework of 1 at the same time we have to consider we need to have the intuition into how uncertainty changes the solution we would get from a deterministic lookahead model despite over 60 years of research into stochastic search parameter tuning remains difficult but the difficult part is in the research lab where it belongs not in the field a major goal of this paper is bring to the attention of the research community in stochastic optimization that a parameterized deterministic lookahead is as valid an approach to the stochastic optimization in 1 as any policy based on a stochastic lookahead we believe that there are problems where the parameterized deterministic lookahead in addition to its computational advantages may outperform a two stage stochastic program in terms of its ability to solve 1 5 examples of cost function approximations recall that there are two ways to parameterize an optimization problem in the objective function as we did in equation 6 and in the constraints as we did in equations 10 12 in this section we are going to provide more concrete examples 5 1 cfas for dynamic assignment problems the truckload trucking industry matches drivers to loads over time let xtd 1 if we assign driver d to load at time t 0 otherwise ctd the contribution of assigning driver d dt to load lt at time t including the revenue generated by the load the cost of moving empty to the load as well as penalties for late pickup or delivery 14 we can perform a myopic assignment of drivers to loads by solving xassign st arg max xt d dt lt ctd xtd 22 a potential problem with a myopic policy is that there may be loads that are not assigned and are then held in the hope that a driver may be found to move the load at a later time however the load may be in a location where we do not traditionally have drivers we can create an artificial incentive let t the amount of time that load has been held at time t now consider the following modified optimization problem xassign st arg min xt d dt lt ctd t xtd 23 xassign st is now a parameterized cost function approximation with a modified objective function 5 2 a dynamic shortest path problem everyone is familiar with the process of navigation systems repeatedly solving shortest path problems to a destination as it receives updates to estimates of travel times around the network this is of course a fully sequential decision problem that can be modeled as a dynamic program we can model the problem as a sequential decision problem let rt be the location of the traveler at time t and let c tij be our estimate of the cost of traversing link i j given what we know at time t the estimates c t evolve over time according to c t 1 ij c t ij c t 1 ij 24 where c t 1 ij is the change in the estimate of c tij given new observations of traffic we assume that at each time t we are at an intersection where we have to make a decision given by xtij 1 if we traverse link i to j when we are at i at time t 0 otherwise we make this decision using a policy x st where the state st is given by st rt c t 15 models of shortest path problems typically overlook the need to include the vector of estimates c t in the state variable this is precisely why we cannot solve this problem at least not optimally using classical shortest path algorithms our challenge is to then find the best policy x st that solves min f e t t 0 i j c tijx st s 0 25 where c tij is the actual cost we experience traversing link i j at time t 1 a natural strategy is to fix the vector of estimates of link costs c t and solve a shortest path problem to the destination updating the shortest path as c t evolves to c t 1 and the traveler transitions to a new node this is a lookahead policy based on a lookahead model that uses fixed estimates c t rather than modeling their stochastic evolution the question is can we do better a limitation of the classical approach of solving sequences of deterministic lookaheads is that it fails to recognize that some links can have long tails which introduces the risk of arriving late an alternative is to replace c tij with the percentile of the distribution for each link let c t ij the percentile of the travel time for link i j given our estimate at time t this means we still have a deterministic shortest path problem but now we have to tune using 25 using our tools from stochastic search 6 an energy storage example with rolling forecasts one of the most overlooked modeling issues in operations research is the proper handling of rolling forecasts we use the setting of an energy storage system depicted in figure 1 which draws energy from a wind farm or the power grid to serve a time varying load with a finite capacity storage device and fixed transmission constraints to help smooth the variations our energy system has some important characteristics that make it unusually difficult as a stochastic optimization problem the energy demands follow a highly time dependent diurnal pattern see figure 2 a the energy from wind is highly stochastic we have rolling forecasts updated every hour but these rolling forecasts evolve considerably over time as indicated in figure 2 b our rolling wind forecast data was obtained courtesy of pjm interconnections there is unlimited power available from the grid but at highly stochastic prices we can buy from and sell to the grid 16 0 2000 4000 6000 0 50 100 150 battery storage demand load wind energy figure 13 5 alternate without internet images grid prices figure 1 energy storage system including a renewable source wind energy from the grid at real time prices battery storage and a load the battery has fixed capacity and the transmission lines are also capacitated which limits our ability to transmit and store power for this reason the ability to anticipate surges and dips in wind energy requires that we be able to plan into the future we use a martingale model of forecast evolution mmfe heath jackson 1994 graves et al 1986 where forecasts for energy from the wind farm and the demand evolve according to fet 1 t f e tt e t 1 t 26 fdt 1 t f d tt d t 1 t 27 where et 1 t n 0 2 e and d t 1 t n 0 2 d represents the exogenous change in the forecast of energy from the wind farm and the demand for time t 2 4 6 8 10 12 14 16 18 20 22 24 hour of day lo ad d em an d energy storage optimization forecasts evolve over time as new information arrives actual rolling forecasts updated each hour forecast made at midnight 2 4 6 8 10 12 14 16 18 20 22 24 hour of day e ne rg y ge ne ra tio n fro m w in d a b figure 2 a energy load by hour of day and b rolling forecast updated hourly 17 6 1 a model of the energy system our model consists of five elements state variables st decision variables xt exogenous information variables wt 1 the transition function st 1 s m st xt wt 1 and the objective function these are given below state variables the state of the system at time t is all the information we need to model our system from time t onward which means the information need to compute costs and constraints making a decision and compute the transition function for the energy problem this information is dt demand load for power during hour t et energy generated from renewables wind solar during hour t rt amount of energy stored in the battery at time t ut limit on how much generation can be transmitted at time t this is known in advance pt price to be paid for energy drawn from the grid at time t fdtt forecast of dt made at time t fett forecast of et made at time t these variables make up our state variable st rt dt et f d tt t t f e tt t t rolling forecasts are widely used in dynamic models but the recognition that the forecast itself belongs in the state variable has been recognized by only a small handful of authors including chen et al 1999 iida zipkin 2006 and lai et al 2008 decision variables these are the flows between each of the elements of our energy system xt planned generation of energy during hour t which consists of the following elements xedt flow of energy from wind to demand xebt flow of energy from wind to battery xgdt flow of energy from grid to demand xgbt flow of energy from grid to battery xbdt flow of energy from battery to demand we would normally write out the constraints that these flows have to satisfy these consist of the flow conservation constraints as well as upper bounds due to transmission constraints in addition to nonnegativity constraints on all the variables except xgbt since energy is allowed to flow both ways 18 between the grid and the battery for compactness we are going to represent the constraints using atxt rt xt ut xt 0 exogenous information for the variables with forecasts demand and wind energy the exoge nous information is the change in the forecast or the deviation between forecast and actual dt 1 change in the forecast of demand for 1 periods in the future that we first learn at time t 1 or the deviation between actual and forecast for 1 et 1 change in the forecast of wind energy for 1 periods in the future that we first learn at time t 1 or the deviation between actual and forecast for 1 we assume that prices evolve purely exogenously with deviations p t 1 change in grid prices between t and t 1 our exogenous information is then wt 1 d t 1 e t 1 1 p t 1 transition function the variables that evolve exogenously are fdt 1 t f d tt d t 1 t t 1 t t 2 dt 1 f d t 1 t d t 1 1 fet 1 t f e tt e t 1 t t 1 t t 2 et 1 f e t 1 t e t 1 1 pt 1 pt p t 1 the energy in storage evolves according to rt 1 rt x eb t x gb t x bd t these equations make up our transition function st 1 s m st xt wt 1 objective function our single period contribution function is c st xt pt xgbt x gd t 19 our objective function then would be max f f f f e t t 0 c st x st s 0 28 where st 1 s m st xt x st wt 1 and given a model of the uncertainty that enters our system through the initial state s 0 and the exogenous information sequence w 1 w 2 wt as in the past we can estimate this objective function by simulating our policy which we present next 6 2 designing a policy there is a very small literature that addresses inventory problems while explicitly recognizing rolling forecasts ft f d tt f e tt t t iida zipkin 2006 shows that an order up to policy parameterized by ft is optimal but does not attempt to compute the multidimensional function ft lai et al 2008 considers price forecasts in the context of natural gas storage formulating the dynamic program with ft in the state variable and showing that an order up to policy ft is optimal but also proposes an approximation based on supporting hyperplanes chen et al 1999 formulates an inventory problem with rolling forecasts and attempts to use approximate dynamic programming but is limited to about a half dozen dimensions none of these papers considers the much more difficult problem of bounds on order quantities and storage which makes the problem much harder and invalidates the optimality proofs of order up to policies that can be written as ft they have to be time dependent and state dependent t st where st includes all the elements of the state variable drawing on the framework of a parametric cost function approximation we are going to start with a classical deterministic lookahead model we begin by creating the decision variables for our lookahead model x tt x ed tt x eb tt x gd tt x gb tt x bd tt t 1 t t h which parallels the elements of xt for each time t in the future this is a time dependent problem with complex interactions between the uncertain supply of wind energy the price of energy from the grid and the time dependent nature of demands that have to be satisfied over a capacitated grid it seems natural to start by creating a policy based on a deterministic lookahead model given by xdla st arg max xt x tt t t 1 t h pt x gb t x gd t t h t t 1 p tt x gb tt x gd tt 29 20 subject to the following constraints first for time t we have xbdt x gb t x eb t rt 30 r t t 1 xgbt x eb t x bd t rt 31 xedt x bd t x gd t dt 32 xebt x ed t et 33 xgdt x eb t x ed t x bd t 0 34 then for t t 1 t h we have x bdtt x gb tt x eb tt r tt 35 r t t 1 x gbtt x eb tt x bd tt r tt 36 x edtt x bd tt x gd tt f d tt 37 x ebtt x ed tt f e tt 38 the weakness in this model is the forecasts of wind energy fett and demand f d tt one idea would be to discount these forecasts by multiplying each forecast with coefficients et t and d t t that depend on how far into the future we are trying to forecast using this approach we create a parameterized policy by replacing equations 37 and 38 with x edtt x bd tt x gd tt d t tf d tt 39 x ebtt x ed tt e t tf e tt 40 the problem of tuning the parameter vector is not easy but there are a number of strategies we can draw on we designed an algorithm ghadimi powell 2022 using a stochastic gradi ent algorithm based on spall s simultaneous perturbation stochastic approximation algorithm spall 2003 which is well suited to problems with multidimensional parameters we then compared the performance of the optimized parametric policy against a base policy with 1 the results are shown in figure 3 for optimized using a range of different starting points where the left most bar uses an initial starting point of 0 1 most of the results show improvements of 20 to 50 percent we claim that the roughly 30 percent improvement is quite significant given that it does not come at any additional computational cost in the field at the same time we observe that there is no alternative computational strategy that would be guaranteed to be better two stage stochastic programs do not even attempt to model the evolution of estimates of the forecasts approximate dynamic programming would not be able to capture the complex nonlinearities of forecasts in the state variable especially for the capacitated problem that we are solving 21 p ct i m pr ov em en t o ve r b en ch m ar k lo ok ah ea d starting point 1 0 1 0 5 1 5 1 0 2 0 50 40 30 20 10 0 10 figure 3 relative performance of optimized parameterized lookahead policy to the performance of a deterministic lookahead with 1 7 closing remarks the major goal of this paper is to make the argument that a parameterized deterministic optimization model is a perfectly valid basis for a policy for stochastic sequential decision problems the research community needs to accept that tuning a parameterized policy using a stochastic base model such as that given in equation 1 is a form of stochastic optimization even if the policy requires solving a deterministic optimization problem deterministic lookahead models are widely used in practice because they easily handle complexity and are relatively easy to solve the use of parameterized cost function approximations enjoys several significant strengths especially in the context of complex problems some examples are the parametric cost function approximations naturally handles the dynamics of a highly time dependent problem with complicating constraints as we encountered in the energy storage problem that emphasize the importance for planning into the future although a time dependent problem requires time dependent behavior the effect of incorporat ing a rolling forecast produces a stationary policy the function x st is not time dependent and the vector is not time dependent this property significantly simplifies the search process for rolling forecasts arise in many settings yet introduce complex stochastic interactions between forecasts and decisions in the future which impact the decisions made now capturing this in a stochastic lookahead policy is exceptionally difficult but is quite easy in a simulator parametric deterministic lookahead policies can capture complex state variables the rolling forecast is just one example much more easily than policies based on stochastic lookaheads similarly simulating complex state variables for the purpose of parameter tuning is also quite easy 22 the parametric deterministic lookahead policy is generally easy to compute in the field while this approach is both attractive since it is easy to implement and promising see figure 3 serious research issues remain designing the best parameterization is difficult but closely parallels the challenges of model design in machine learning for our energy storage problem we might say that multiplying coefficients times the forecasts is intuitively appealing but other parameterizations are possi ble such as ensuring that the energy in storage in future time periods stays above a minimum level as a reserve and below a maximum level so we can store unexpected surges in wind we suspect that most industrial applications at best use intuitive parameterizations without the benefits of experimental testing in a simulator stochastic search remains a challenge for example the objective function 28 is nonconvex in when we limit the search over simulating policies can also be quite noisy while it is natural to assume that we can tune the parameters using a simulator there will be many settings where a simulator is not available or would not be trusted an important research challenge is to perform online parameter tuning in the field so that the policy adapts to changing conditions we hope that the thoughts in this paper encourage the stochastic optimization community to include parameterized deterministic models as valid policies for stochastic optimization problems this initiative is likely to be warmly endorsed by industry that is already implementing parameterized deterministic models without the benefits of careful design of the parameterization and parameter tuning references bartroff j lai t l 2010 approximate dynamic programming and its applications to the design of phase i cancer trials statistical science 25 2 245 257 bayraksan g morton d p 2009 assessing solution quality in stochastic programs via sam pling tutorials in operations research 5 102 122 ben tal a el ghaoui l nemirovski a 2009 robust optimization 53 3 464 501 ben tal a golany b nemirovski a vial j p 2005 retailer supplier flexible commit ments contracts a robust optimization approach manufacturing service operations man agement 7 3 248 271 bertsekas d p 2011 dynamic programming and optimal control 3 rd edition vol ii belmont ma athena scientific bertsekas d p 2017 dynamic programming and optimal control approximate dynamic pro gramming 4 edn athena scientific belmont ma 23 bertsekas d p tsitsiklis j n 1996 neuro dynamic programming athena scientific bertsimas d brown d b caramanis c 2011 theory and applications of robust optimiza tion siam review 53 3 464 501 birge j r louveaux f 2011 introduction to stochastic programming springer science business media bouzaiene ayari b cheng c das s fiorillo r powell w b 2014 from single com modity to multiattribute models for locomotive optimization a comparison of optimal integer programming and approximate dynamic programming transportation science pp 1 24 box g e p wilson k b 1951 on the experimental attainment of optimum conditions journal of the royal statistical society series b 13 1 1 45 bubeck s cesa bianchi n 2012 regret analysis of stochastic and nonstochastic multi armed bandit problems foundations and trends in machine learning 5 1 1 122 camacho e f alba c b 2013 model predictive control springer science business media cao x r 2008 stochastic learning and optimization a sensitivity based approach ifac pro ceedings volumes 41 2 3480 3492 carpentier p l gendreau m bastin f 2015 managing hydroelectric reservoirs over an ex tended horizon using benders decomposition with a memory loss assumption ieee transactions on power systems 30 2 563 572 chau m fu m c qu h ryzhov i o 2014 simulation optimization a tutorial overview and recent developments in gradient based methods in proceedings of the 2014 winter simulation conference ieee press pp 21 35 chen v c p ruppert d shoemaker c a 1999 applying experimental design and regres sion splines to high dimensional continuous state stochastic dynamic programming operations research 47 1 38 53 clark a j scarf h 1960 optimal policies for a multi echelon inventory problem management science 6 4 363 505 dantzig g b 1955 linear programming with uncertainty management science 1 197 206 deisenroth m p neumann g peters j 2013 a survey on policy search for robotics foundations and trends in robotics 2 1 2 1 142 dvoretzky a 1956 on stochastic approximation in j neyman ed proceedings 3 rd berkeley symposium on mathematical statistics and probability university of california press pp 39 55 ermoliev y m 1983 stochastic quasigradient methods and their application to system optimiza tion stochastics 9 1 36 24 fu m c 2017 markov decision processes alphago and monte carlo tree search back to the future in tutorials in operations research pp 68 88 fu m c ed 2015 handbook of simulation optimization springer ghadimi s lan g 2013 stochastic first and zeroth order methods for nonconvex stochastic programming siam journal on optimization 23 4 2341 2368 ghadimi s powell w b 2022 stochastic search for a parametric cost function approxima tion energy storage with rolling forecasts technical report gittins j glazebrook k d weber r r 2011 multi armed bandit allocation indices john wiley and sons new york glasserman p 1991 gradient estimation via perturbation analysis vol 116 springer science business media graves s c meal h c dasu s qui y 1986 two stage production planning in a dynamic environment in a s s c s e eds multi stage production planning and inventory control lecture notes in economics and mathematical systems number 266 springer verlag berlin pp 9 43 hadjiyiannis m j goulart p j kuhn d 2011 an efficient method to estimate the subop timality of affine controllers ieee transactions on automatic control 56 12 2841 2853 han j e w 2016 deep learning approximation for stochastic control problems arxiv preprint arxive 1611 07422 harrison j m van mieghem j a 1999 multi resource investment strategies operational hedging under demand uncertainty european journal of operational research 113 1 17 29 heath d c jackson p l 1994 modeling the evolution of demand forecasts with applica tion to safety stock analysis in production distribution systems iie transactions institute of industrial engineers 26 3 17 30 ho y c 1992 discrete event dynamic systems analyzing complexity and performance in the modern world ieeepress new york hu j fu m c ramezani v r marcus s i 2007 an evolutionary random policy search algorithm for solving markov decision processes informs journal on computing 19 2 161 174 iida t zipkin p h 2006 approximate solutions of a dynamic forecast inventory model manufacturing service operations management 8 4 407 425 jin s ryan s m watson j p woodruff d l 2011 modeling and solving a large scale generation expansion planning problem under uncertainty energy systems 2 3 4 209 242 kaelbling l p 1993 learning in embedded systems mit press cambridge ma 25 kall p wallace s w 2009 stochastic programming vol 10 john wiley and sons hoboken nj kirk d e 2012 optimal control theory an introduction dover new york kushner h j yin g 2003 stochastic approximation and recursive algorithms and applications vol 35 springer science business media lai g margot f secomandi n 2008 an approximate dynamic programming approach to benchmark practice based heuristics for natural gas storage valuation working paper tepper school of business carnegie mellon university lai t l robbins h 1985 asymptotically efficient adaptive allocation rules advances in applied mathematics 6 1 4 22 lan s clarke j p barnhart c 2006 planning for robust airline operations optimizing aircraft routings and flight departure times to minimize passenger disruptions transportation science 40 1 15 28 levine s abbeel p 2014 learning neural network policies with guided policy search under unknown dynamics in advances in neural information processing systems pp 1071 1079 levine s koltun v 2013 guided policy search 30 th international conference on machine learning icml 2013 28 part 2 1038 1046 lewis f l vrabie d 2009 reinforcement learning and adaptive dynamic programming for feedback control ieee circuits and systems magazine 9 3 32 50 lewis f l vrabie d syrmos v l 2012 optimal control 3 rd edn john wiley and sons hoboken nj lillicrap t p hunt j j pritzel a heess n erez t tassa y silver d wierstra d 2015 continuous control with deep reinforcement learning arxiv preprint arxiv 1509 02971 lium a g crainic t g wallace s w 2009 a study of demand stochasticity in service network design transportation science 43 2 144 157 mannor s rubinstein r y gat y 2003 the cross entropy method for fast policy search in icml pp 512 519 nesterov y spokoiny v 2017 random gradient free minimization of convex functions foun dations of computational mathematics 17 2 527 566 ng a y jordan m 2000 pegasus a policy search method for large mdps and pomdps in pro ceedings of the sixteenth conference on uncertainty in artificial intelligence morgan kaufmann publishers inc pp 406 415 pereira m f pinto l m v g 1991 multi stage stochastic optimization applied to energy planning mathematical programming 52 359 375 26 http arxiv org abs 1509 02971 peshkin l kim k e meuleau n kaelbling l p 2000 learning to cooperate via policy search in proceedings of the sixteenth conference on uncertainty in artificial intelligence morgan kaufmann publishers inc pp 489 496 philpott a b de matos v l 2012 dynamic sampling algorithms for multi stage stochastic programs with risk aversion european journal of operational research 218 2 470 483 philpott a b craddock m waterer h 2000 hydro electric unit commitment subject to uncertain demand european journal of operational research 125 2 410 424 pillac v gendreau m gue ret c medaglia a l 2013 a review of dynamic vehicle routing problems european journal of operational research 225 1 1 11 powell w b 2011 approximate dynamic programming solving the curses of dimensionality 2 edn john wiley and sons powell w b 2019 a perspectives on stochastic unit commitment from scenario trees to parametric cost function approximations powell w b 2019 b a unified framework for stochastic optimization european journal of op erational research 275 3 795 821 powell w b 2022 reinforcement learning and stochastic optimization a unified framework for sequential decisions john wiley and sons new york powell w b ryzhov i o 2012 optimal learning john wiley and sons hoboken nj powell w ruszczyn ski a topaloglu h 2004 learning algorithms for separable ap proximations of discrete stochastic optimization problems mathematics of operations research 29 4 814 836 puterman m l 2014 markov decision processes discrete stochastic dynamic programming john wiley sons robbins h monro s 1951 a stochastic approximation method the annals of mathematical statistics 22 3 400 407 sen s zhou z 2014 multistage stochastic decomposition a bridge between stochastic pro gramming and approximate dynamic programming siam journal on optimization 24 1 127 153 sethi s sorger g 1991 a theory of rolling horizon decision making annals of operations research 29 1 387 415 sethi s p 2019 optimal control theory applications to management science and economics 3 edn springer verlag boston settles b 2010 active learning sciences new york 27 shapiro a 2011 analysis of stochastic dual dynamic programming method european journal of operational research 209 1 63 72 shapiro a dentcheva d ruszczyn ski a 2014 lectures on stochastic programming modeling and theory technology p 447 shor n k 1979 the methods of nondifferentiable op timization and their applications naukova dumka kiev si j barto a g powell w b wunsch d 2004 handbook of learning and approximate dynamic programming chichester u k simao h p day j george a p gifford t powell w b nienow j 2009 an approxi mate dynamic programming algorithm for large scale fleet management a case application transportation science 43 2 178 197 sontag e 1998 mathematical control theory 2 nd ed springer pp 1 544 spall j c 2003 introduction to stochastic search and optimization estimation simulation and control vol 65 john wiley sons stengel r f 1986 stochastic optimal control theory and application john wiley and sons hoboken nj sutton r s barto a g 1998 reinforcement learning an introduction vol 1 mit press cambridge sutton r s barto a g 2018 reinforcement learning an introduction 2 nd edn mit press cambridge ma sutton r s mcallester d singh s p mansour y 2000 policy gradient methods for reinforcement learning with function approximation advances in neural information processing systems 12 22 1057 1063 wallace s w fleten s e 2003 stochastic programming models in energy handbooks in operations research and management science 10 637 677 wang f y zhang h liu d 2009 adaptive dynamic programming an introduction ieee computational intelligence magazine may 39 47 wiesemann w kuhn d sim m 2014 distributionally robust convex optimization op erations research 62 6 1358 1376 zhao c wang j watson j p guan y 2013 multi stage robust unit commitment consider ing wind and demand response uncertainties ieee transactions on power systems 28 3 2708 2717 28 1 introduction 2 canonical model and solution strategies 3 policies based on policy search 3 1 policy function approximations pfas 3 2 cost function approximations cfas 3 3 algorithms for policy search 4 policies based on lookahead approximation 4 1 policies based on value function approximations 4 2 policies based on direct lookahead approximations 4 3 discussion 5 examples of cost function approximations 5 1 cfas for dynamic assignment problems 5 2 a dynamic shortest path problem 6 an energy storage example with rolling forecasts 6 1 a model of the energy system 6 2 designing a policy 7 closing remarks