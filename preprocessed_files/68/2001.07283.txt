in the field monitoring of functional calls is it feasible oscar cornejo daniela briola daniela micucci leonardo mariani department of informatics systems and communication university of milano bicocca milan italy email address oscar cornejo daniela briola daniela micucci leonardo mariani unimib it oscar cornejo daniela briola daniela micucci leonardo mariani preprint submitted to elsevier january 22 2020 ar x iv 2 00 1 07 28 3 v 1 cs s e 2 0 ja n 20 20 in the field monitoring of functional calls is it feasible oscar cornejo daniela briola daniela micucci leonardo mariani department of informatics systems and communication university of milano bicocca milan italy abstract collecting data about the sequences of function calls executed by an ap plication while running in the field can be useful to a number of applications including failure reproduction profiling and debugging unfortunately collect ing data from the field may introduce annoying slowdowns that negatively affect the quality of the user experience so far the impact of monitoring has been mainly studied in terms of the overhead that it may introduce in the monitored applications rather than con sidering if the introduced overhead can be really recognized by users in this paper we take a different perspective studying to what extent collecting data about sequences of function calls may impact the quality of the user experience producing recognizable effects interestingly we found that depending on the nature of the executed operation and its execution context users may tolerate a non trivial overhead this information can be potentially exploited to collect significant amount of data without annoying users keywords monitoring dynamic analysis user experience 1 introduction behavioral information collected from the field can complement and com plete the inherently partial knowledge about applications gained with in house testing and analysis activities for instance observing applications that run in the field can produce data otherwise difficult to obtain such as information about the behavior of the application when executed with actual production data and within various real execution environments indeed collecting field data is common practice in the area of software experimentation 1 2 3 where controlled experiments are performed to evaluate how a change may impact the user experience email address oscar cornejo daniela briola daniela micucci leonardo mariani unimib it oscar cornejo daniela briola daniela micucci leonardo mariani preprint submitted to elsevier january 22 2020 a diversity of data can be collected to study the behavior of software appli cations 4 5 6 7 8 9 10 11 12 13 in this paper we focus on sequences of function calls which is a specific but extremely common type of data recorded and used by analysis techniques studying the behavior of an application in terms of the function calls produced under different circumstances is in fact both common and useful for example sequences of function calls extracted from the field can be used to reproduce failures 12 detect malicious behav iors 14 debug applications 15 profile software 9 optimize applications 16 and mine models 17 18 19 collecting information from the field is challenging since it slows down the application and this may imply a negative effect on the quality of the user experience if the slowdowns are frequent the usability can be compromised up to the point users may stop using the application it is thus extremely important to understand how the slowdowns introduced into an application can affect users the impact of monitoring has been mainly studied in terms of its relative overhead that is by measuring how much the execution time of a given op eration is increased due to the presence of the monitor although this is an important information it does not reflect how and if this overhead can be per ceived by the users of the application for instance increasing by 20 the time that every menu item requires to open may introduce a small but annoying slowdown to operations that should be instantaneous from a user perspective on the contrary taking 20 more time on the execution of a query might be ac ceptable for users as long as the total time does not exceed their expectation it is thus important to investigate the relation between the overhead introduced by monitoring techniques and the user experience to understand how to seamlessly and feasibly collect data from the field in our initial study 20 we discovered that a non trivial overhead can be tolerated by users and that the overhead can be tolerated differently depending on the nature of the operation that is executed this paper extends this ini tial study considering a larger number of operations exposed to overhead new experiments to study how the availability of the computational resources may affect overhead a study based on human subjects and additional analyses of the empirical data the results show that function calls can be frequently col lected without impacting on the user experience regardless of the availability of the computational resources but specific operations may require ad hoc sup port to be monitored without affecting users these evidences can be exploited to design better monitor and analysis procedures running in the field this paper is organized as follows section 2 describes our experimental setup sections 3 and 4 report the results obtained when studying the impact of the overhead on the users with good and poor availability of computational resources respectively section 5 describes the results obtained with our study involving human subjects section 6 discusses threats to validity section 7 summarizes our findings section 8 discusses related work section 9 provides final remarks 3 2 experiment design this section describes the research questions that we have addressed and the design of the experiments to answer the research questions 2 1 research questions the general objective of our study is understanding how collecting field data can affect the user experience we investigated this question in a specific al though common scenario that is while recording the sequence of function calls executed by applications we thus organized our study around three main research questions that investigate the impact of monitoring in different conditions rq 1 how is the user experience affected by monitoring function calls this research question analyzes the relation between the overhead pro duced by the monitoring activity and its impact on the user experience rq 1 is further organized in three sub research questions rq 1 a what is the overhead introduced by monitoring function calls rq 1 a measures the overhead introduced in an application by the mon itoring activity rq 1 b what is the impact of monitoring function calls on the user experience rq 1 b studies if the overhead introduced with monitoring can be recognized by the user of the application rq 1 c what is the tolerance of the operations to the introduced overhead rq 1 c studies how different user operations tolerate overhead before producing slowdowns recognizable by users rq 1 d do failures change the overhead introduced by function calls monitoring rq 1 d studies if the overhead introduced by the monitor is different in the context of failures rq 2 what is the impact of monitoring function calls when the availability of computational resources is limited this research question investigates if and how much the overhead produced by collecting function calls changes with the availability of the computational resources the study focuses on the availability of the two most relevant resources cpu and memory as captured by the following two sub research questions rq 2 a what is the impact of cpu availability on the intrusive ness of monitoring rq 2 a studies how the overhead introduced by moni toring function calls is affected by different levels of cpu utilization rq 2 b what is the impact of memory availability on the intru siveness of monitoring rq 2 b studies how the overhead introduced by monitoring function calls is affected by different levels of memory utilization since we investigate rq 1 and rq 2 referring to the classification of the sys tem response time as proposed by seow 21 we consider the following research 4 question to investigate the alignment between the user behavior and the adopted classification rq 3 how do expert computer users react to the overhead produced by function calls monitoring compared to the results obtained with rq 1 and rq 2 this research question analyzes the alignment between the behavior of expert computer users recruited from our cs department and the results reported in rq 1 and rq 2 with a study involving human subjects 2 2 experiment design this section presents the design of the experiment that we performed to answer to our research questions to study the impact of monitoring we se lected four widely used interactive applications notepad 6 9 21 paint net 4 0 122 vlc media player 2 2 43 and adobe reader dc 20194 to collect sequences of function calls from these applications we instru mented the applications using a probe that we implemented with the intel pin binary instrumentation tool 5 pin supports the instrumentation of compiled binaries including shared li braries that are loaded at runtime and optimizes performance by automatically in lining routines that have no control flow changes 22 our probe is a cus tom plug in utility written in c that intercepts and logs every function call included nested calls the probe uses a buffer of 50 mb to store data in memory before saving to file we used this value based on the results we obtained in our preliminary experiment where 50 mb resulted to produce the best compromise between cpu and memory consumption 20 to run each application we have implemented a sikulix 6 test case that can be automatically executed to run multiple functionalities of the monitored applications the test cases simulate rich usage scenarios for instance adobe reader dc is executed by opening a pdf file moving inside the document up and down several times changing the view to full screen inserting comments in the text searching for a specific word in the document highlighting text and closing the document notepad is executed by writing a java program opening different files copying and pasting text in a document counting the occurrences of a given word marking the occurrences of a given word and closing all the opened tabs paint net is executed by loading an image resizing it drawing several shapes and shaded shapes rotating the image applying different filters to the image black and white sepia inverting the colors of the 1 https notepad plus plus org 2 http www getpaint net 3 http www videolan org 4 http get adobe com reader 5 https software intel com en us articles pin a dynamic binary instrumentation tool 6 http sikulix com 5 https notepad plus plus org http www getpaint net http www videolan org http get adobe com reader https software intel com en us articles pin a dynamic binary instrumentation tool http sikulix com image and closing it vlc media player is executed by opening several video files reproducing them pausing them adding and editing the subtitles of the video playing videos from a given time and creating and managing play lists of different videos in the experiments we first executed the test cases when the applications are not monitored and then with the monitor to answer our rqs we collected two main measures the system response time overhead and the estimated impact on the user experience we detail in the next section how we measured them to accurately report the impact of the monitoring activity we collected data about functions calls at the granularity of the individual operations performed in the tests an operation is a complete interaction between a user and an application it starts with a user input e g a user click and ends with the application that has processed the input and is ready to accept the next input for instance an operation may start with a click on a menu and end with the menu being displayed so if a test case executes operations o 1 on we collect functions calls and measure the overhead and its impact on the user experience for every operation oi since our study targets interactive applications we collect traces composed of user operations it is possible to precisely distinguish the portion of the trace that corresponds to each operation by exploiting the knowledge of the name of the functions that implement the operations this information is typically available if the organization that defines the monitoring strategy and the one that implements the application are the same otherwise traces can still be split based on interactions with the gui but it implies a more sophisticated analysis of the collected traces to respond to rq 1 we only executed the test cases and the monitored applications that is no processes were running in addition to the basic operat ing system processes to answer rq 2 we selectively saturated computational resources occupying 60 75 and 90 of both cpu and ram we performed linear sampling of the memory because we could not predict when there would be observable consequences we considered saturation up to 90 for both resources since higher values would not allow us to satisfy the minimum requirements of pin to saturate resources in a controlled way we used cpustress 1 0 0 17 and heavyload 3 48 to mitigate any effect due to non determinism we repeated each test 5 times and reported mean values the overall study implied collecting and processing more than 10 000 samples about operations and their duration all available at http github com ocornejo fieldmonitoringfeasibility 7 https blogs msdn microsoft com vijaysk 2012 10 26 tools to simulate cpu memory disk load 8 https www jam software com heavyload 6 http github com ocornejo fieldmonitoringfeasibility https blogs msdn microsoft com vijaysk 2012 10 26 tools to simulate cpu memory disk load https blogs msdn microsoft com vijaysk 2012 10 26 tools to simulate cpu memory disk load https www jam software com heavyload 2 3 measuring overhead and its estimated impact on the user experience measuring the overhead is straightforward that is we measure the differ ence in the duration of the same operations when executed with and without monitoring here it is important to discuss how we estimated the effect of the monitor on the user experience in principle assessing if a given overhead may or may not annoy users requires direct user involvement however user studies are expensive and can be hardly designed to cope with a volume of samples like the ones that we collected which would require involving users in the evaluation of the duration of thousands of operations to estimate the impact of the overhead on users we thus exploited the re sults already available from the human computer interaction domain and we strengthen the collected evidence with a human study focusing on a restricted number of cases in particular we used the well known and widely accepted classification proposed by seow 21 of the system response time srt i e the time taken by an application to respond to a user request that can be asso ciated with each operation based on its nature in this classification operations are organized according to four categories which have been derived from direct user engagement instantaneous these are the most simple operations that can be performed on an application such as entering inputs or navigating throughout menus users expect to receive a response by 100 200 ms at most immediate these are operations that are expected to generate acknowledg ments or very simple outputs users expect to receive a response by 0 5 1 s at most continuous these are operations that are requested to produce results within a short time frame to not interrupt the dialog with the user they are ex pected to produce a response in 2 5 s at most depending on the complexity of the operation that is executed we assume simple continuous operation to produce a response by 2 3 5 s and more complex continuous operations to produce a response by 3 5 5 s captive these are operations requiring some relevant processing for which users will wait for results but will also give up if a response is not produced within a certain time these operations are expected to produce a response by 7 10 s in our study we estimate the impact of the overhead on users by measuring the number of operations that change their category due to overhead intu itively an operation that takes the time that is usually taken by an operation of higher category to complete its execution is an operation that does not satisfy the expectation of the user we refer to these operations as the slow operations measuring the number of operations that become slow due to overhead provides an estimate of how often users are likely to be annoyed while using a monitored application we attribute categories to the operations performed by the tests 7 based on their execution time when no overhead is introduced in the system and considering the lower limit of the execution time of each category for instance operations that take at most 100 ms are classified as instantaneous while operations that take more than 100 ms but less than 0 5 s are classified as immediate if the execution time of an operation executed while the application is monitored exceeds the lower limit of its category the operation is considered to be slow this strategy allows us to use the srt classification as a continuous scale using the lowest limit for both the categorization of the operations and the identification of the slow operations we thus obtained a conservative measure of the slow operations that is the real number of slow operations reported by users are likely to be lower than the ones reported with this metric we use the overhead and the number of slow operations as the main variables to answer our research questions 3 rq 1 how is the user experience affected by monitoring function calls this section reports the results obtained for each sub research question rq 1 a rq 1 d and finally discusses the overall results obtained for rq 1 since the monitored applications are desktop applications we executed all the experiments in a machine running windows 7 pro with a 3 47 ghz intel xeon x 5690 processor and 4 gb of ram 3 1 rq 1 a what is the overhead introduced by monitoring function calls figure 1 shows the overhead that we observed for operations in each category and for each subject application note that not all types of operations occur in every application for instance captive operations are present in paint net only the overhead profile per category is quite consistent in the case of instan taneous operations the overhead is always close to 0 this is probably due to the nature of instantaneous operations that perform simple operations that imply the execution of a limited amount of logic and thus produce a limited number of function calls a similar result can be observed for immediate operations where the overhead is small for adobe reader dc and notepad paint net represents an exception because its overhead is higher the overhead profile is again quite consistent across operations in the continuous simple and complex categories with the overhead ranging between 0 and 200 although there are similarities for operations in the same category even if present in different applications we can also observe that there are exceptions in fact there are several outliers represented in the boxplot with some of them showing very different overhead values compared to the rest of the samples for example we had two continuous simple operations in notepad selecting the java highlighting and dismissing a save operation with a high overhead 8 figure 1 overhead per category and application the two outliers compared to the other operations which experienced 100 overhead at most figure 2 shows the percentage of operations in each category affected by overhead levels within specific ranges collecting function calls produces an overhead in the interval 0 10 in the majority of the cases 65 of the executed operations in 8 of the cases operations are exposed to an overhead between 10 and 30 in 12 of the cases monitoring produced an overhead in the interval 30 80 and for less than 15 of the operations the overhead is higher figure 2 percentage of operations undergoing a specific overhead interval 9 we can conclude that the observed behavior within operations of a same category is not significantly different although specific operations may violate this pattern figure 1 moreover collecting function calls exposes operations to an overhead that is lower than 10 in the large majority of cases and is seldom higher than 80 figure 2 estimating if and how much this overhead can be intrusive with respect to user activity is studied with the next research question adobe reader dc operation category total instantaneous immediate continuous simple continuous complex captive captive slow operations instantaneous 55 50 5 0 0 0 0 9 immediate 15 0 15 0 0 0 0 0 continuous simple 90 0 0 69 16 5 0 23 continuous complex 15 0 0 0 13 1 1 13 captive 0 0 0 0 0 0 0 0 notepad operation category total instantaneous immediate continuous simple continuous complex captive captive slow operations instantaneous 45 40 0 5 0 0 0 11 immediate 20 0 19 1 0 0 0 5 continuous simple 70 0 0 48 6 11 5 31 continuous complex 5 0 0 0 5 0 0 0 captive 0 0 0 0 0 0 0 0 paint net operation category total instantaneous immediate continuous simple continuous complex captive captive slow operations instantaneous 35 35 0 0 0 0 0 0 immediate 25 0 0 24 0 1 0 100 continuous simple 55 0 0 45 7 3 0 18 continuous complex 40 0 0 0 29 11 0 28 captive 60 0 0 0 0 57 3 5 vlc media player operation category total instantaneous immediate continuous simple continuous complex captive captive slow operations instantaneous 30 30 0 0 0 0 0 0 immediate 0 0 0 0 0 0 0 0 cont simple 125 0 0 99 26 0 0 21 cont complex 30 0 0 0 24 6 0 20 captive 0 0 0 0 0 0 0 0 table 1 slow operations per application 3 2 rq 1 b what is the impact of monitoring function calls on the user expe rience table 1 reports the analytical results obtained for the operations recorded as slow in the four subject applications for each application the table shows the number of operations in each category that have been executed in the experi ment and how the operation has been classified once affected by the overhead caused by function calls monitoring the overhead is not recognizable by users if the category does not change with monitoring overhead a perfect result im plies having all 0 s outside the values in the diagonal highlighted with a grey 10 figure 3 percentage of slow operations with respect to the srt categories background when an operation changes its category the table shows what the new category of the operation is the column captive shows the number of operations whose duration is longer than the maximum allowed for a captive operation the last column slow operations specifies the percentage of slow operations across all the executions figure 3 visually illustrates how slow operations distribute across operations categories the last column in each category shows the percentage of slow operations for that category across all subject applications the empirical data suggests that instantaneous operations seldom present a slowdown that affects the user experience in fact only 6 of the cases produced a recognizable slowdown we obtained a similar result for immediate operations with the exception of paint net where the slowdown has been significant for every immediate operation that has been executed this result is coherent with the exceptional overhead reported for immediate operations in paint net for rq 1 a this is likely caused by the nature of the immediate operations in paint net which execute non trivial logic e g the operation that closes an image and are more expensive to monitor when the portion of logic of the application that is executed increases the percentage of operations that become slow also increases as observed for contin uous operations that in some cases become even slower than captive operations see table 1 for instance the execution time of five continuous simple op erations in notepad exceeded the time expected for a captive operation the higher cost of monitoring continuous operations is visible also in figure 3 where more than 20 of the continuous operations both simple and complex have been significantly slowed down in average compared to instantaneous and immediate operations where about 5 of the operations have been slowed down if we do not consider those from paint net which is a special case extremely long tasks such as captive operations seem to tolerate well the overhead caused by function calls monitoring however since they are present in one application only it is hard to distill a more general lesson learnt 11 figure 4 percentage of slow operations for different overhead intervals we can conclude that the operations that are likely to be perceived as slowed down are quite limited in number 20 overall and mostly concentrated in the continuous operations moreover applications that implement small pieces of logic that must be executed quickly as paint net does might be particularly hard to monitor in fact its immediate operations have been all significantly slowed down when collecting function calls 3 3 rq 1 c what is the tolerance of the operations to the introduced overhead since we exposed operations in different categories to various overhead lev els this research question studies how often a certain overhead is the cause of operations resulting in a too slow response time figure 4 shows the percentage of operations for all the categories reported to be slow for overhead within a given range and for operations in all categories in our previous study 20 we identified 30 80 and 180 as interesting overhead values that may produce different reactions by users so we used these ranges in this study to analyze the collected data we obtained a similar result with this experiment an overhead level be tween 30 and 80 is hard to tolerate for operations in any category with the exception of instantaneous operations while overhead values higher than 80 can be prohibitive we can conclude that overhead levels up to 30 are not harmful but higher overhead levels must be introduced wisely with the exception of instantaneous operations that seem to tolerate overhead slightly better than operations in the other categories 12 figure 5 percentage of operations undergoing a specific overhead interval 3 4 rq 1 d do failures change the overhead introduced by function calls mon itoring this research question investigates if monitoring functional calls may affect failures differently than regular executions to compare the impact of monitor ing when exactly the same operations terminate correctly or terminate with a failure we inject faults into our subject applications to this end we config ure pin to modify the first instruction of a function if it is a mov instruction with the ax register as a destination the change consists of multiplying the destination address by a constant value with this process we achieved two applications failing abruptly vlc media player and paint net and two applications presenting various misbehaviours adobe reader dc and notepad in the former case the execution simply stopped prematurely without producing any noticeable difference in terms of overhead in the latter case we obtained misbehaviors such as adobe reader dc failing to open files and notepad failing to load graphical elements we collected and analyzed the overhead values for adobe reader dc and notepad figure 5 shows the percentage of operations in each category affected by overhead levels within specific ranges the result is very similar to the one presented in figure 2 when the execution terminates correctly in particular collecting function calls during a failure produced an overhead in the interval 0 10 in the majority of the cases for operation in any category 65 of the operations that have been executed we also observe that 2 91 of operations produced an overhead in the range 10 30 9 27 of the operations produced an overhead in the interval 30 80 and for less than 20 of operations the overhead was higher in summary failures do not change the cost of function calls monitoring according to our observations 13 3 5 discussion collecting function calls exposed the operations performed in the subject applications to various overhead levels often below 10 65 73 of the cases and sometime to higher levels 8 48 12 02 13 77 of the cases in the ranges 10 30 30 80 80 respectively we investigated if these overhead levels can be recognized by the users and we found that an overhead up to 30 is well tolerated while higher overhead levels can be tolerated for operations that usually execute fast we do not consider the cost of aggregating and elaborating data on the client side since techniques such as obfuscation 23 24 25 and distributive monitor ing 26 27 usually consider this step as an offline process to be executed after the actual events have been collected without impacting on the user experience these results suggest that monitoring activity in particular collecting data about sequences of function calls can be safely executed in many cases but it must be controlled for those operations that execute an excessive amount of logic compared to their expected execution time monitoring techniques should be aware of these differences between operations and optimize their intrusiveness accordingly predicting and identifying operations expensive to monitor is an interesting challenge which might be addressed with both static analysis and profiling techniques elaborating solutions in these directions is part of future work in this area 4 rq 2 what is the impact of monitoring function calls when the availability of computational resources is limited in this section we study the impact of the monitoring activity when the computational resources cannot be completely allocated to the monitored ap plications but they are also allocated to other tasks we first discuss the impact of cpu availability and then we discuss the impact of memory availability similarly to rq 1 we study the impact of collecting function calls by ana lyzing the overhead and studying the number of operations changing category when cpu and ram are under stress 4 1 rq 2 a what is the impact of cpu availability on the intrusiveness of monitoring figure 6 shows the system response time presented in log scale of the executed operations per operation category we report timing information con sidering four cpu load levels 0 60 75 and 90 the figure includes two types of boxplots the orange boxplot corresponds to the execution time observed when monitoring is in place while the brown boxplot corresponds to the execution time when no monitoring is in place the trend is quite similar for all classes of operations with the exception of immediate operations which show decreasing values of the overhead for higher cpu load values we conducted a kruskal wallis test to check if the overhead 14 figure 6 execution time for various cpu load levels per operation category treatment chi square p value df instantaneous 2 2107 0 5298 3 immediate 1 1327 0 7692 3 continuous simple 3 3914 0 3351 3 continuous complex 4 4726 0 2147 3 captive 1 54 0 6731 3 table 2 kruskal wallis test results per operation category introduced for a given cpu load and a given class of operations differs from the overhead for the same class of operations exposed to a different cpu load significance expected for p value 0 05 the test revealed no significant differences see table 2 suggesting that the impact of monitoring is not affected by a significant degree by the cpu load level that is an application is slowed down similarly by function calls monitoring regardless of the cpu availability figure 7 percentage of slow operations for various cpu load levels per application we also considered how monitoring affects the number of slow operations per application shown in figure 7 and the number of slow operations per operation 15 figure 8 percentage of slow operations for various cpu load levels per operation category category shown in figure 8 the usage of a loaded cpu already generates a number of slow operations for each application adding function calls moni toring further increases the number of operations that have been slowed down we can however notice that the only addition of monitoring makes the user experience worse by a similar degree across cpu load levels confirming that the cpu load level is not a significant factor when considering the impact of monitoring to confirm this intuition we computed the linear regression of the number of slow operations for the instrumented and non instrumented version of each application and considered the difference between the angular coefficients of the computed lines we further considered the percentage of operations with a different classification when the cpu saturates to 100 highest saturation possible based on the computed trends table 3 reports the results for each application we indicate the difference between the angular coefficients on the left and the percentage of operations with a different categorization on the right we can notice that the difference in the increase of the number of slow op erations is between 2 66 and 14 33 of the operations indicating a similar trend i e slope for the two cases with and without monitoring the small positive values of the difference between the coefficients indicates that when a difference is observed e g 14 33 of the operations in paint net the satu ration of the cpu increases the number of slow operations by a lower degree when monitoring is active the plot of the data per operation category figure 8 reveals that instanta neous operations behave better than the other operations in terms of their ability to tolerate monitoring in fact the number of slow operations does not change adobe reader dc notepad paint net vlc media player cpu 0 047 2 66 0 123 8 78 0 308 14 33 0 256 13 85 table 3 trend analysis for cpu 16 significantly when monitoring is introduced in the system even if captive op erations behave similarly to instantaneous operations it is hard to generalize the result since they are present in one application only on the other hand instantaneous operations are more sensitive to the load of the cpu in fact more than 80 of the instantaneous operations are slow when the cpu load reaches 90 we can conclude that the cpu load level does not significantly affect the intrusiveness of function calls monitoring in fact the impact of the addition of monitoring tends to be the same regardless of cpu availability and when a difference is observed monitoring results to be slightly less intrusive with a higher saturation of the cpu 4 2 rq 2 b what is the impact of memory availability on the intrusiveness of monitoring to better discuss the results for rq 2 b we report the memory usage of each application as summarized in table 4 the maximum memory consumption observed during the execution of our tests for adobe reader dc is 353 mb of ram for notepad is 278 mb of ram for paint net is 520 mb of ram and for vlc media player is 303 mb of ram figure 9 shows the overhead introduced in the system response time pre sented in log scale per operation category when varying the amount of occupied memory up to 90 the orange boxplot corresponds to the execution time observed when function calls are collected while the brown boxplot corresponds to the execution time when no monitoring is in place figure 9 execution time for different ram availability per operation category adobe reader dc notepad paint net vlc media player max ram 356 mb 278 mb 520 mb 303 mb table 4 maximum memory used during experimentation 17 treatment chi square p value df instantaneous 3 5831 0 3101 3 immediate 3 5298 0 3169 3 continuous simple 2 1604 0 5398 3 continuous complex 1 1438 0 7665 3 captive 0 1913 0 979 3 table 5 kruskal wallis test results per operation category similar to section 4 1 we check for statistical differences between groups using a kruskal wallis test see table 5 obtaining no significant difference between different levels of ram load particularly the results show a clearly negligible effect of the memory on the overhead indeed the overhead is similar for different values of memory occupation we also investigated how memory occupation impacts on the operations that become slow figure 10 shows the number of slow operations per application while figure 11 shows the number of slow operations per operation category the behavior of the applications does not reveal any trend to confirm this intuition we computed the linear regression of the number of slow operations for the instrumented and non instrumented version of each app and considered the difference between the angular coefficients of the computed lines we fur ther considered the percentage of operations with a different classification when the memory saturates to 100 highest saturation possible based on the com puted trends table 6 reports the results for each application we indicate the difference between the angular coefficients on the left and the percentage of operations with a different categorization on the right we can notice negli gible difference in the coefficients and the number of slow operations suggesting similar trend for the two cases the results per operation category confirm the same behavior we observed for cpu utilization instantaneous operations better tolerate low availability of the computational resources compared to operations in the other categories anyway memory occupation does not produce relevant effects when analyzing the results per operation category either adobe reader dc notepad paint net vlc media player ram 0 084 4 77 0 059 4 25 0 023 1 08 0 054 2 91 table 6 memory trend analysis we can conclude that the memory load level does not affect the intrusiveness of function calls monitoring by a significant degree in fact the monitoring overhead tends to be the same regardless of memory availability 4 3 discussion the analysis of the impact of function calls monitoring on the user expe rience when the availability of the computational resources is limited revealed 18 figure 10 percentage of slow operations for various ram load levels per application figure 11 percentage of slow operations for various ram load levels per operation category little influence of the computational resources as a consequence the logic of the monitoring can be activated and deactivated with limited attention to computational resources only in the case of cpu saturation higher than 90 monitoring should be avoided since this could turn the application unresponsive finally results revealed that instantaneous operations are less sensitive to memory availability compared to other kinds of operations 5 rq 3 how do expert computer users react to the overhead pro duced by function calls monitoring compared to the results ob tained with rq 1 and rq 2 this research question investigates the coherence between the classification of the operations as resulting by the application of the criteria proposed by seow and the feedback provided by actual users from our cs department on the applications and operations considered in our study to this end we asked 19 overhead range operation category instantaneous immediate continuous simple continuous complex captive 0 30 4 2 10 5 3 30 80 4 2 2 80 180 1 1 3 180 1 1 1 table 7 number of operations for the different combinations of overhead ranges and cate gories a number of users to assess operations of different categories while exposed to a range of overhead values and we compared the results to the ones obtained with the classification criteria by seow in the following we present the design of the empirical study the results and their critical discussion 5 1 design we study how actual users perceive the system response time by considering operations exposed to overhead values in the ranges 0 30 30 80 80 180 180 and operations belonging to the five operation categories used in our study instantaneous immediate continuous simple continuous complex and captive to expose every participant to the same interactions and to exactly the same overhead we recorded videos showing the execution of the same operations executed for rq 1 and rq 2 while monitoring function calls each participant classifies each operation as either running slow or running in the expected amount of time the study involves 22 subjects who are members of our cs department and include students researchers and professors they all regularly use interactive applications all the participants experienced all the operation categories and all the overhead values for the evaluation we selected eight meaningful tasks that cover the five possible operation categories and the four possible overhead ranges selecting fewer tasks would not allow us to cover enough cases while using more cases would be too demanding for the participants the distribution of the operations included in the tasks are shown in table 7 the laboratory session started with the participants receiving an instruction sheet with general information about the structure of the experiment and a de scription of the tasks to be assessed including text and screen shots to avoid misunderstandings subjects were not told about the specific aim of the ex periment nor about the presence of function calls monitoring after the video corresponding to a task has been reproduced and before moving to the next task the participant evaluated the response time of each operation according to two possible levels running slow or running as expected 5 2 results to evaluate the consistency between the assessment based on the classifica tion by seow and the responses provided by the human subjects we reclassified 20 figure 12 consistency in the evaluation of srt between our approach and user perception each operation assessed by users according to the classification by seow and measure their coherence figure 12 shows the results each bar is an operation category the label at the top of the bar shows its classification as running slow or running as expected according to our definition of slow operation and the percentage shows the number of participants who responded coherently with the label at the top based on these results we conclude that our classification strategy and the participants agree on the operations that should not be considered slow in fact all the operations labeled as expected are classified in the same way by a percentage of the participants ranging from 75 to 100 our classification and the participants tend to agree on the continuous operations that should be considered slow in fact there is agreement in considering continuous simple operations exposed to more than 80 overhead and the continuous complex operations exposed to more than 30 overhead as slow operations the participants tolerate overhead better than revealed by our classification for quick operations in fact there is disagreement on the instantaneous and immediate operations exposed to overhead higher than 80 5 3 discussion we can conclude that identifying slow operations based on the seow clas sification is conservative the operations that we identify running as expected are fine also for the actual users on the other hand there might be operations that we consider too slow but are instead running as expected for the users in practice developers following the recommendations resulting from our work encounter into a negligible risk of introducing noticeable overheads in their ap plications 21 6 threats to validity the main threat to internal validity of our empirical investigation is the usage of the system response time categories defined by seow 21 to identify the operations that have been slowed down up to a level that can be recognized by the users involving a significant number of human subjects in the evaluation and asking them to evaluate every individual operation that is executed in different applications and contexts is however nearly infeasible this is why we decided to rely on a well known categorization of user operations that let us work with a significant number of samples to mitigate this issue we investigated the coherence between our evaluation and the assessment performed by actual users for a subset of the operations and discovered that our findings provide a slightly conservative picture of how users perceive overhead another potential threat is the representativeness of the participants we used in the empirical experiment to respond to rq 3 however the use of popular applications also known to non computer experts mitigates the potential bias introduced by the selected subjects another potential threat is the choice of the individual operations that have been used in the study although we can potentially design the test cases in many different ways we mitigated the issue of choosing the operations by focus ing on the most relevant functionalities of each application possibly including a large number of operations from most of the categories the main threat to external validity of our empirical evaluation is the gen eralizability of the results the study focuses on function calls monitoring for regular desktop applications and although some results might have a broader applicability they should be interpreted mainly in that context considering other contexts require the replication of this study another potential threat is related to the sample size of actual subjects we used in the empirical experiment to respond to rq 3 to further validate the results achieved it is necessary to extend the experiment in such a way as to include many more subjects and potentially with different backgrounds 7 findings in this section we summarize the main findings that result from the empirical experience reported in this paper overhead up to 30 is likely to be well tolerated by users our results show that an overhead up to 30 is seldom the cause of operations recognized as slow this suggests that enriching applications running in the field with processes that collect data and analyze executions is feasible collecting sequences of function calls from the field is feasible in most of the cases our results show that the actual overhead produced by function calls monitored is below 30 in the vast majority of the cases furthermore less than the 20 of the executed operations are likely to 22 be perceived as slowed down although the cases where the impact of monitoring is heavier must be carefully handled results suggest that ex tensively collecting data about sequences of function calls from the field is possible specific operations require special handling of monitoring fea tures in our experiment we reported operations that presented an excep tionally high overhead this happened both across categories and specif ically in one application that is the immediate operations present in paint net this result suggests that applications must be carefully ana lyzed before being instrumented so that the overhead introduced by the probes can be properly controlled detecting these special cases computational resources have little influence on the impact of monitoring results show that cpu and ram availability have not a significant impact on the relative cost of collecting function calls it is however true that the cumulative effect of cpu load and monitoring may introduce a prohibitive overhead reaching a peak of more than 50 of the operations perceived as slowed down in contrast with the normal impact of monitoring that affected less than 20 of the operations in the worst case instantaneous operations are more resilient to overhead instan taneous operations demonstrated to tolerate well overhead also when the cpu is extremely busy this is probably due to the intrinsic nature of these operations that can be executed fast almost without interruption even if the cpu is busy moreover human subjects demonstrated to toler ate particularly well the overhead introduced in instantaneous operations these findings can be exploited by organizations that use experimentation techniques to improve their products and processes indeed they can refine data collection strategies to be less intrusive while collecting significant amount of data about the behavior of the software in particular our findings may impact requirements engineers who may exploit field monitoring solutions to profile users and evolve requirements following the usage scenarios discov ered in the field however in order to be used without impacting on the user experience the overhead introduced by profiling techniques should not exceed 30 software developers who may exploit highly optimized monitoring pro cedures to collect software behavioral data discovering how software is actually used in the field and improve their products accordingly since computational resources have little influence on the impact of monitor ing environmental conditions for monitoring should not be a problem for developers 23 testers who may exploit fine grained monitoring to collect accurate in formation about the behavior of a deployed product with a specific focus on failures to reveal and fix faults earlier we demonstrated that func tion calls monitoring is feasible in most of the cases if applied carefully it could be of great impact to the software testing community consider ing that function calls monitoring has been widely used for several tasks including debugging and fault failures reproduction 15 12 devops architects who engineer continuous monitoring solutions that can cost effectively support continuous deployment and the development pro cess more in general as long as they monitor between the boundaries we identified in this study such as keeping overhead under 30 and priori tizing instantaneous operations because of their resilience to monitoring overhead 8 related work in this section we relate our work to software experimentation to studies about the impact of srt delays on the quality of the user experience and to studies on field monitoring and analysis regarding software experimentation there are several approaches and stud ies that exploited software systems to collect actual evidence especially using field data a systematic way to collect field data is to perform randomised controlled experiments e g a b tests for instance to study how a new fea ture or a change may impact the user experience continuous deployment is a well known practice that benefits directly from controlled experiments 28 for instance companies such as microsoft reported to run more than 200 ex periments concurrently every day within their products 1 relevantly kevic et al 2 presented an empirical characterisation of an experimentation process when applied to the bing web search engine and fagerholm et al 3 provided a model that enables continuous customer experiments aimed to software quality improvement our work relates to these studies because it provides evidence that can help engineers designing better data collection solutions that do not affect the user experience regarding the perception of the srt there are several studies in the context of the research in human computer interaction hci and controlled experi ments the importance of controlled experiments has been extensively discussed and demonstrated for instance fabijan et al 28 reported the benefits of online controlled experiments for software development processes studying how using customer and product data could support decisions throughout the product lifecyle killeen et al 29 demonstrated that users are unlikely to recognize time variations inferior to 20 of the original value our results are aligned with this study since delays up to 30 do not generate slowdowns recognizable by future users 24 ceaparu et al 30 studied how interactions with a personal computer may cause frustrations in their experiment users were asked to describe in a written form sources of frustration in human computer interactions the participants declared that applications not responding in an appropriate amount of time and web pages taking long time to process are the main sources of frustration in human computer interactions thus confirming the relevance of our investigation other studies stressed user tolerance in specific settings for instance nah et al 31 analyzed how long users are willing to wait for a web page to be downloaded the results of the experiment showed that users start noticing the slowdowns after two seconds delays and that do not tolerate slowdowns longer that 15 s a threshold of 15 s has been reported as the maximum that can be tolerated before perceiving an interruption in a conversation with an application also in other studies 32 33 an experiment conducted by hoxmeier and di cesare 34 studied how fixed slowdowns 3 6 9 and 12 s to interactions may affect the user appreciation and perception results show that a limit for the user tolerance is 12 s and that a linear relationship exists between srt and user satisfaction kohavi et al 35 show that slowdowns in web applications may affect the user experience causing loss of money for companies for example amazon reported a loss of 1 in sales because of a 100 ms slowdown and microsoft similarly reported a loss of 1 in user queries when adding a slowdown of one second to their search site differently from these studies our investigation does not aim to identify the maximum overhead that can be tolerated nor the cost to companies but rather to identify delays and overhead levels that cannot be even recognized by users in the scope of monitoring techniques there are techniques that implemented mechanisms to limit the overhead introduced in the monitored system 36 for instance distributive monitoring can be used to divide the monitoring workload between several instances of a same application in order to lower the overhead introduced by monitoring activities 27 26 briola et al 37 38 and ancona et al 39 exploited a similar intuition to cost efficiently monitor multi agent systems alternatively information can be collected at run time only with a given probability or according to a strategy 40 this strategy has been exploited in the context of debugging 7 8 program verification 41 and profiling 42 finally monitoring can be optimized carefully balancing in memory and sav ing operations 43 the results reported in this paper can be exploited by these techniques and by practitioners 44 to further optimize their monitoring strategy collecting more data without affecting users depending on the kind of collected data monitoring solutions may introduce overhead levels up to 10000 12 as it is in the case of function calls monitoring also slow software is one of the main reasons why users stop using applications as reported in 30 34 33 delaying too much some functionalities may cause loss of users and consequently the failure of the project the results obtained with our study may help practitioners to design context aware techniques that 25 achieve a better compromise between collecting data and impacting on the user experience such as proposed in 41 our findings provide initial insights in this direction 9 conclusions collecting data from the field is extremely useful to discover how applica tions are actually used and support software engineering tasks for instance several monitoring techniques collect sequences of function calls to reproduce failures 12 detect malicious behaviors 14 debug applications 15 profile software 9 optimize applications 16 and mine models 17 18 if retrieving this data is indeed useful knowing the impact of the monitoring activity on the user experience is also extremely important in fact monitoring techniques can be feasibly applied only if they work seamlessly this paper presented a study about the impact of function calls monitor ing considering both the monitoring overhead and the operations that may be perceived as slowed down by the users results show that an overhead up to 30 can be likely introduced in the operations without annoying users and that function calls monitoring often produce an overhead below this limit we found however operations that are slowed significantly and that require special care when monitored these findings suggest that monitoring capabilities cannot be introduced blindly but they must be customized to the characteristics of the monitored program results also suggest that computational resources ram and cpu have little influence on the impact of monitoring future work consists of exploiting the results obtained with this study to design monitoring techniques that can collect function calls from the field with out being recognized by users we will further consider the categorization of the monitoring metrics we introduced in this work into useful usability groups that can be reused at large scale experiments as recommended in the study by rodden et al 45 also we will consider applications with longer response time such as scientific experiments and job processing applications studying the feasibility of monitoring in these fields acknowledgment this work has been partially supported by the h 2020 learn project which has been funded under the erc consolidator grant 2014 program erc grant agreement n 646867 and the gauss national research project which has been funded by the miur under the prin 2015 program contract no 2015 kwremx references 1 r kohavi a deng b frasca t walker y xu n pohlmann on line controlled experiments at large scale in proceedings of the cm sigkdd international conference on knowledge discovery and data min ing kdd 2013 26 2 k kevic b murphy l williams j beckmann characterizing experi mentation in continuous deployment a case study on bing in proceed ings of the international conference on software engineering software engineering in practice track icse sep 2017 3 f fagerholm a s guinea h ma enpa a j mu nch the right model for continuous experimentation journal of systems and software 123 2017 292 305 4 eclipse community eclipse http www eclipse org visited in 2019 5 microsoft windows 10 http www microsoft com visited in 2019 6 n delgado a q gates s roach a taxonomy and catalog of runtime software fault monitoring tools ieee transactions on software engineer ing tse 30 12 2004 859 872 7 g jin a thakur b liblit s lu instrumentation and sampling strategies for cooperative concurrency bug isolation acm sigplan notices 45 10 2010 241 255 8 b liblit a aiken a x zheng m i jordan bug isolation via remote program sampling acm sigplan notices 38 5 2003 141 154 9 s elbaum m diep profiling deployed software assessing strategies and testing opportunities ieee transactions on software engineering tse 31 4 2005 312 327 10 p ohmann d b brown n neelakandan j linderoth b liblit opti mizing customized program coverage in proceedings of the international conference on automated software engineering ase 2016 11 c pavlopoulou m young residual test coverage monitoring in pro ceedings of the international conference on software engineering icse 1999 12 w jin a orso bugredux reproducing field failures for in house debug ging in proceedings of the international conference on software engi neering icse 2012 13 j clause a orso a technique for enabling and supporting debugging of field failures in proceedings of the international conference on software engineering icse 2007 14 a gorji m abadi detecting obfuscated javascript malware using se quences of internal function calls in proceedings of the acm southeast regional conference se 2014 27 http www eclipse org http www microsoft com 15 s murtaza a hamou lhadj n h madhavji m gittens towards an emerging theory for the diagnosis of faulty functions in function call traces in proceedings of the fourth semat workshop on general theory of software engineering gtse 2015 16 z zhao b wu m zhou y ding j sun x shen y wu call sequence prediction through probabilistic calling automata in proceedings of the acm international conference on object oriented programming systems languages applications oopsla 2014 17 l mariani a marchetto c d nguyen p tonella a i baars rev olution automatic evolution of mined specifications in proceedings of the international symposium on software reliability engineering issre 2012 18 l mariani f pastore m pezze dynamic analysis for diagnosing inte gration faults ieee transactions on software engineering tse 37 4 2011 486 508 19 f pastore d micucci l mariani timed k tail automatic inference of timed automata in proceedings of the ieee international conference on software testing verification and validation icst 2017 20 o cornejo d briola d micucci l mariani in the field monitoring of interactive applications in proceedings of the international conference on software engineering new ideas and emerging results track icse nier 2017 21 s c seow designing and engineering time the psychology of time per ception in software addison wesley professional 2008 22 s wallace k hazelwood superpin parallelizing dynamic instrumenta tion for real time performance in proceedings of the international sym posium on code generation and optimization cgo 2007 23 l motiwalla x b li developing privacy solutions for sharing and analyz ing healthcare data international journal of business information systems 13 2 24 c c aggarwal s y philip privacy preserving data mining models and algorithms springer science business media 2008 25 v ciriani s d c di vimercati s foresti p samarati microdata pro tection in secure data management in decentralized systems springer 2007 pp 291 321 26 j bowring a orso m j harrold monitoring deployed software using software tomography in proceedings of the workshop on program anal ysis for software tools and engineering paste 2002 28 27 a orso d liang m j harrold r lipton gamma system continuous evolution of software after deployment in proceedings of the international symposium on software testing and analysis issta 2002 28 a fabijan p dmitriev h h olsson j bosch the benefits of controlled experimentation at scale in proceedings of the euromicro conference on software engineering and advanced applications seaa 2017 29 p r killeen n a weiss optimal timing and the weber function psy chological review 94 4 1987 455 468 30 i ceaparu j lazar k bessiere j robinson b shneiderman deter mining causes and severity of end user frustration international journal of human computer interaction 17 3 2004 333 356 31 f f nah a study on tolerable waiting time how long are web users willing to wait behavior and information technology 23 3 2004 153 163 32 l nielsen user interface directions for the web communications of the acm 42 1 1999 65 72 33 r b miller response time in man computer conversational transactions in proceedings of all joint computer conference part i 1968 34 j a hoxmeier c d cesare system response time and user satisfaction an experimental study of browser based applications in proceedings of the association of information systems americas conference amcis 2000 35 r kohavi r longbotham d sommerfield r m henne controlled ex periments on the web survey and practical guide data mining and knowl edge discovery 18 1 2009 140 181 36 o cornejo flexible in the field monitoring in proceedings of the inter national conference on software engineering companion icse c 2017 37 d briola v mascardi d ancona distributed runtime verification of jade and jason multiagent systems with prolog in proceedings of the italian conference on computational logic cilc 2014 38 v mascardi d briola d ancona on the expressiveness of attribute global types the formalization of a real multiagent system protocol lec ture notes in computer science including subseries lecture notes in ar tificial intelligence and lecture notes in bioinformatics 8249 lnai 2013 300 311 39 d ancona d briola a ferrando v mascardi mas drive a practical approach to decentralized runtime verification of agent interaction proto cols vol 1664 2016 pp 35 43 29 40 o cornejo d briola d micucci l mariani fragmented monitoring in proceedings second international workshop on pre and post deployment verification techniques ifm 2017 41 e bartocci r grosu a karmarkar s a smolka s d stoller j n seyster adaptive runtime verification in proceedings of the international conference on runtime verification rv 2012 42 m hirzel t chilimbi bursty tracing a framework for low overhead temporal profiling in proceedings of the acm workshop on feedback directed and dynamic optimization fddo 4 2001 43 o cornejo d ginelli d briola d micucci l mariani field monitoring with delayed saving ieee access 7 2019 85913 85924 44 t barik r deline s drucker d fisher the bones of the system a case study of logging and telemetry at microsoft in proceedings of the international conference on software engineering companion icse c 2016 45 k rodden h hutchinson x fu measuring the user experience on a large scale user centered metrics for web applications in proceedings of the sigchi conference on human factors in computing systems chi acm 2010 pp 2395 2398 30 1 introduction 2 experiment design 2 1 research questions 2 2 experiment design 2 3 measuring overhead and its estimated impact on the user experience 3 rq 1 how is the user experience affected by monitoring function calls 3 1 rq 1 a what is the overhead introduced by monitoring function calls 3 2 rq 1 b what is the impact of monitoring function calls on the user experience 3 3 rq 1 c what is the tolerance of the operations to the introduced overhead 3 4 rq 1 d do failures change the overhead introduced by function calls monitoring 3 5 discussion 4 rq 2 what is the impact of monitoring function calls when the availability of computational resources is limited 4 1 rq 2 a what is the impact of cpu availability on the intrusiveness of monitoring 4 2 rq 2 b what is the impact of memory availability on the intrusiveness of monitoring 4 3 discussion 5 rq 3 how do expert computer users react to the overhead produced by function calls monitoring compared to the results obtained with rq 1 and rq 2 5 1 design 5 2 results 5 3 discussion 6 threats to validity 7 findings 8 related work 9 conclusions