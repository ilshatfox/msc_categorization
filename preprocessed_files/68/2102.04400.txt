1 rapid classification of glaucomatous fundus images hardit singh 1 simarjeet saini 2 and vasudevan lakshminarayanan 2 3 1 cameron heights collegiate institute 301 charles st e kitchener on n 2 g 2 p 8 canada 2 department of electrical and computer engineering university of waterloo 200 university ave w waterloo on n 2 l 3 g 1 canada 3 school of optometry and vision science university of waterloo 200 university ave w waterloo on n 2 l 3 g 1 canada hsharditsingh gmail com abstract we propose a new method for training convolutional neural networks which integrates reinforcement learning along with supervised learning and use it for transfer learning for classification of glaucoma from colored fundus images the training method uses hill climbing techniques via two different climber types viz random movement and random detection integrated with supervised learning model through stochastic gradient descent with momentum sgdm model the model was trained and tested using the drishti gs and rim one r 2 datasets having glaucomatous and normal fundus images the performance for prediction was tested by transfer learning on five cnn architectures namely googlenet densenet 201 nasnet vgg 19 and inception resnet v 2 a 5 fold classification was used for evaluating the performance and high sensitivities while maintaining high accuracies were achieved of the models tested densenet 201 architecture performed the best in terms of sensitivity and area under the curve auc this method of training allows transfer learning on small datasets and can be applied for tele ophthalmology applications including training with local datasets 2020 optical society of america under the terms of the osa open access publishing agreement 1 introduction glaucoma is the silent killer of eyesight and is the second leading cause for blindness 1 an estimated 64 million people suffer from glaucoma worldwide if left undiagnosed it causes irreversible damage to the optic nerve which eventually leads to blindness presently there is no cure and treatment only slows the progression of the disease hence it is imperative that glaucoma is diagnosed at an early stage unfortunately eye care is not integrated with the universal health care system in most countries and it is estimated that over 70 of cases are left undiagnosed in early stages 2 the prevalence of glaucoma increases with age and the population of the elderly is expected to double in the next decade putting a strain on an already stretched health care systems 3 thus a paradigm shift in glaucoma detection is required where patients can be screened rapidly and accurately all at a low cost detection of glaucoma typically relies on examination of structural damage to the optic nerve combined with measurements of visual function such as visual fields and intraocular pressure a review of glaucoma diagnostic methods is given in the paper by sharma et al 2008 4 currently there are three common tests for diagnosis of glaucoma these include the contact or non contact tonometry to measure the intraocular pressure iop in the eye evaluation of the visual field and evaluation of the optic nerve head onh using optical means such as ophthalmoscopy and or fundus imaging 5 iop measurements are not enough on their own as many patients do not show signs of elevated iop further in many cases by the time there https doi org 10 1364 oa license v 1 2 is an appreciable iop increase neural damage has already occurred the visual field only gets affected at advanced stages of the disease and thus again misses the early stages of the disease imaging technologies for evaluating the onh are only available in tertiary health centers and patients are only referred to them once the vision has been affected complete diagnosis requires further tests including gonioscopy a measurement of the angle of theanterior chamber of the eye and pachymetry which measures the thickness of the cornea 5 the complete set of tests are time consuming and expensive and requires a glaucoma specialist thus ophthalmologists are looking for accurate screening methods evaluation of the onh is considered to have the most promise for effective screening 6 figure 1 shows the onh region cropped from a fundus image some of the features which are used to screen for glaucoma are also indicated in the figure the onh consists of the optic disc and the optic cup the optic disc is the point for the exit of the ganglion cell axons which make up the optic nerve the optic cup area is the brightest part of the optic disc evaluation of the cup to disc ratio is used to detect glaucoma as the reduction of the optic nerve fibers creates optic disc cupping and thus increasing the ratio of the cup diameter to the disc diameter typically cup to disc ratio of a healthy eye is smaller than 0 5 if the ratio is greater than 0 7 it is considered to be an indicator of severe glaucoma 7 however the cup to disc ratio is not always correct as this cupping can also be hereditary and would remain stable with time also these measurements are made more difficult because the disc can be tilted thus diagnosis is made by evaluating the progression of the cup to disc ratio cdr over several months or even years there is also the so called isnt rule the onh region is divided into different regions called the inferior i at the south pole of the onh superior s at the north pole of the onh nasal n on the inside of the eye facing the nose and temporal t on the outside of the eye the thickness of the neuroretinal rim which is the space between the optic disc edge and optic cup edge is measured and for a normal eye follows the pattern called the isnt rule the rule states that in normal eyes the thickness of the neuroretinal rim for the i s n t on its own the isnt rule is also not accurate enough in a study conducted by harizman et al 2006 on 109 subjects the isnt rule was intact in 79 of normal eyes and 28 of glaucoma eyes 8 thus 28 of people who had glaucoma would be incorrectly diagnosed as normal creating false negatives a recent study showed that it was more accurate to measure the neuroretinal rim to optic disk ratio rdr as compared to cdr or isnt rule 9 classification accuracy of 84 4 75 9 and 92 2 was achieved using cdr isnt and rdr respectively currently in practice ophthalmologists manually segment different parts of the image and use their experience to diagnose looking at different features the quality of the diagnosis is only as good as the judgment of the doctor in a study done by almazroa et al 2017 10 it was found that the accuracy for segmenting the optic disc and optic cup varied between 75 to 90 for 6 different ophthalmologists the ophthalmologists only agreed with each other in approximately 75 of the cases ophthalmologists also look at other features in the retinal images e g how the blood vessels kink in the onh region these features are missed when classification is done based only on image segmentation the problem of classifying images is where computer vision excels and thus there has been tremendous effort around the world to use computer vision to classify normal and glaucomatous images over the last few years 11 12 feature based classification can be broadly split up into two different categories based on the classification methods used features of the image like the pixel intensity histograms 13 wavelets 14 fourier transforms and spline coefficients higher order spectra analysis and texture based features 15 are extracted from preprocessed images the identified features are used to train a classifier and make a decision based on the prevalence of specific features classification is done either through simple classifiers like support vector 3 machines or random forest classification or through deep learning methods involving convolutional neural networks 16 fig 1 examples of features used to detect glaucoma in a colored fundus image a describes the markings for the cup and the disc which can be used to calculate the cup to disc ratio b marks the inferior i superior s nasal n and temporal t thicknesses of the neuroretinal rim which can be used for classifying healthy eyes of the various classification methods cnn has shown the most promise in cnn feature extraction feature learning and classification all happen simultaneously a network or model is created by many layers called neurons which transform an input image to outputs glaucoma present absent there are three major strategies that use cnns training and building a network from scratch using pre trained cnn features and conducting unsupervised cnn pre training with supervised fine tuning table 1 summarizes the work done using cnns chen et al 2015 17 used a 6 layer cnn to classify optic disc and optic cup and achieved 83 1 accuracy using a private dataset al bander et al 2017 18 used a 23 layer cnn architecture to extract the features but did the classification using an svm algorithm and achieved an accuracy of 88 2 fu et al 2018 19 created an ensemble of 4 cnn s to create a new architecture called d net to achieve an accuracy of 83 2 and 66 6 on two different data sets respectively table 1 cnn models for classification of glaucoma a comparison author year method number of images success rate acc al bander et al 2017 18 cnn 23 layers and svm 255 normal 200 glaucoma accuracy 88 2 sensitivity 84 78 chai et al 2018 39 mb nn 2 554 accuracy 91 51 sensitivity 92 33 specificity 90 90 chen et al 2015 17 cnn 6 layers 482 normal 168 glaucoma auc 83 1 christopher et al 2018 9 transfer learning with resnet vgg 16 and inception v 3 9189 normal 5633 glaucoma sensitivity 88 specificity 95 4 fu et al 2018 19 ensemble of 4 cnns 482 normal 168 glaucoma sensitivity 84 78 specificity 83 80 li et al 2018 20 transfer learning with inception network 48116 sensitivity 95 6 specificity 92 0 shibata et al 2018 41 transfer learning with resnet 1768 normal 1364 glaucoma auc 96 5 singh et al 2019 21 transfer learning with inception v 3 191 images accuracy 92 5 sensitivity 92 3 specificity 93 6 impressive results have been achieved by li et al 2018 20 using an inception v 3 cnn architecture 21 on a private database with 48 000 images to detect glaucomatous images the images of the dataset were labeled by 21 ophthalmologists a local space average color substitution was implemented to account for varying levels of illumination the large dataset with a deep model led to high accuracies approaching 94 singh et al 2019 22 showed that transfer based learning can be used to train inception v 3 for small datasets and achieved an accuracy of 92 5 for deep learning methods to be used effectively by ophthalmologists it is also important to win their confidence one of the problems is simply this deep learning methods act as black boxes with limited visibility into decision making and thus has restricted clinical use recent explainability studies aim to show the features that influence the decision of a model the most the reader is referred to singh et al 2021 23 for a review of the new field of explainable ai another major problem is that fundus cameras and lighting conditions vary and to win that confidence it may be necessary to train the deep learning networks with local datasets that may not have a large number of images deep learning algorithms are prone to overfitting the data in this paper we propose and implement a new training algorithm combining supervised learning with reinforcement based learning for transfer training of cnn architectures the aim of this research was to develop a new transfer learning training method that allows for rapid training on cnn architectures the performance of 5 different architectures viz densenet 201 24 vgg 19 25 nasnet 26 inception resnet v 2 27 googlenet 28 was evaluated using cross validation for classification of glaucomatous fundus images the trained models were optimized for sensitivity while maintaining high accuracy 2 methodology 2 1 dataset description two publically available datasets namely rim one r 2 and drishti gs were chosen for this study the rim one r 2 dataset is a sub dataset of the open retinal image database for optic nerve evaluation rim one 29 the dataset consists of 455 colored fundus images out of which 255 are normal and 200 have glaucoma or suspected to have glaucoma the dataset has a region of interest the onh region already extracted with different image sizes drishti gs 30 is a public dataset that consists of 101 uncropped fundus images the images have been annotated by 5 clinicians into normal and glaucoma classes an image is considered to be glaucomatous or not based on the majority decision seventy of the images 5 in the dataset are in class glaucoma while 31 images are normal the images of drishti gs have a field of view of 30 with an image size of 2896 1944 pixels the patients age varied from 40 to 80 years with roughly equal numbers of males and females a critical overview of fundus image databases for deep learning ophthalmic diagnosis is given by sengupta et al 2020 31 2 2 statistical analysis a 5 fold cross validation was used to train the models and test the performance the normal and glaucomatous images were first randomly rank ordered and then a venetian blind was used to create the five folds each fold consisted of 204 normal images from the rim one r 2 dataset 25 normal images from drishti gs one fold had 24 normal images from drishti gs for a total of 229 normal images 160 glaucomatous images from the rim one r 2 and 56 images from drishtigs for a total of 216 glaucomatous images for every fold the performance was tested on 51 independent normal images and 40 independent glaucomatous images from rim one r 2 dataset 6 normal images one fold had 7 normal images and 14 glaucomatous images from drishti gs within an epoch 80 of the images used in the training data set were used for training while 20 were used for validation of the models the split was done randomly and the images were shuffled at the beginning of each epoch 2 3 pre processing the only pre processing used was to format the image to the correct input size for which the cnn architectures were originally trained on for googlenet vgg 19 and densenet 201 the input size of the image was adjusted to 224 224 pixels while for inception resnet v 2 the image size was adjusted to 299 299 pixels and for nasnet the image size was adjusted to 331 331 pixels no algorithms were used to correct the lighting conditions or to increase the contrast rim one r 2 images already have the onh region cropped and the images were used as is for the drishti gs images an algorithm was developed for autocropping the retinal images to extract the onh region autocropping was based on the idea that the onh region is the largest brightest region in the fundus image and the various steps are shown in figure 2 to successfully target the region the images were preprocessed to enhance the contrast first the red channel of the rgb image was extracted as it showed a clear onh region compared to other color channels the extracted red channel image was then contrast enhanced by scaling the brightest pixel intensity to the maximum value of 255 and scaling the other pixels according to their intensity values the image was then segmented into 50 superpixels using the simple linear iterative clustering slic algorithm 32 the algorithm connects pixels with similar intensities together reducing the complexity of segmentation after superpixelation was done the intensity values of each superpixel were averaged to create a coarse image that highlights the od further the average intensity superpixel image was then thresholded to create a binary image superpixels above the threshold were converted to an intensity of 255 and superpixels below the threshold are converted to 0 a threshold of 254 gave the highest accuracy in extracting the region of interest once the image was thresholded the largest region in the image was isolated and the centroid and area of the region were extracted using morphological operations 33 the centroid was considered to be the center of the onh region and a square with the length of pixels desired was used to crop the original image the method took 7 seconds to crop the image on a lenovo thinkpad t 480 equipped with intel i core 5 8 th generation processor and 8 gb of memory the accuracies of the roi extraction are summarized in table 2 the accuracy of the implemented roi extraction method was calculated on the drishti gs database and magrabia and observing wh almazroa et a technique to images and ac our method nearly half w drishti gs d implemented fig 2 show 2 4 data aug since cnns a to prevent th randomly on t 0 2 to 0 2 zoo each epoch a training set t patches of the 2 5 transfer normally in t network mod weights of th functions e g d bin rushed hether the on al 2017 10 find the brigh chieved succes of using supe while requiring database the for these imag ws the methods fo table datase drishti magrabia bin rushe gmentation are deep learni his data augme the training set om up to 10 a combination o create partia e required imag r learning tra the training ph del is given tr he deep layers adam optim images of the nh region wa had previousl htest regions f ss rates of 93 6 erpixels and m g much lower roi was no ges or extracting the op e 2 comparison o et o i a ed ing architectur entation was im t within the spe horizontal fli n of these ope ally overlappin ge size were ran aining method hase for transf raining images s are changed mizer 35 h riga dataset as completely ly used morph for onh extra 6 and 94 8 morphological computing res ot correctly c ptic nerve head im of the results for r our work ah 98 0 97 9 97 4 res there is a m mplemented t ecified ranges p width and h erations was ra ng views of th ndomly genera d fer learning su s and accurac the weights however this c 34 the accu within the s hological opera action on mag respectively operations dec sources only centered a m mage from a full siz roi extraction hmed et al 2017 n a 93 6 94 8 major concern the following o rotation 10 height shift up andomly appli he data during ated during eac upervised learn cy of classific are changed can lead to ov uracy was mea square cropped ations and type grabia and bin y for the two d creased the fa for two imag manual extract zed color fundus im 10 n of overfitting operations wer to 10 degree to 10 at th ied to an imag g the training ch iteration ning is used w cation measure based on opt verfitting in sm 6 asured by d region e i fuzzy n rushed databases ailures by ges in the tion was mage the data re applied es shear he start of ge in the further where the ed as the imization mall size 7 datasets a new way of training was developed which integrated reinforcement learning along with supervised learning the training dataset was grouped into mini batches of size 64 and in an epoch the complete dataset was covered multiple hill climbers were created and randomly assigned a mode one of the climbers was assigned to optimize using stochastic gradient descent with momentum sgdm while other climbers were randomly assigned one of the two modes viz random movement and random detection when the random detection mode was assigned climbers try to repeat the same movement as their last step if a climber did not make progress that is larger than model variation strength it would randomly activate one of its detectors to find a better path once one of the detectors found slopes where climbers can improve in a value that is larger than model variation strength the climbers would stop activating the remaining detectors and start to move in that direction as for the mode random movement climbers just move completely randomly in trying to optimize the weights of the network at the end of the epoch the climber which achieves the highest validation accuracy survives and new climbers are created and randomly assigned to one of the modes the process continues until the validation accuracy starts to decrease at which time the training stops or maximum number of epochs are achieved table 3 summarizes the hyper parameters including the maximum number of epochs initial learning rate for sgdm mode and the number of iterations in an epoch for transfer learning on the different cnn architectures table 3 training settings used for the transfer learning process starting network learning rate no of epochs no of iterations per epoch n googlenet 1 10 3 60 6 inception resnet v 2 7 10 4 100 6 vgg 19 1 10 4 30 6 densenet 201 1 10 3 30 6 nasnet 1 10 3 20 22 2 6 performance metrics to evaluate the performance of the trained models the receiver operating characteristics roc were created and the area under the curve auc was utilized the auc does not depend upon the classification threshold and thus can be used to compare different models also three other criteria were used for evaluating the performance namely accuracy sensitivity and specificity and we also looked at the four outcomes for an image that is classified viz true positive tp false negative fn true negative tn and false negative fn since our goal was to create rapid screening algorithms it is important that no glaucomatous image is missed as the cost to the patient is very high thus a higher emphasis was put on the sensitivity obviously specificity should also be considered as a large number of false positives is not optimal 2 7 freezing initial layers it has been previously suggested that retraining all the layers is more effective than freezing the initial layers 22 though no comparisons were made this is because the fundus images are quite different as compared to the images used in the imagenet dataset that the cnn 8 architectures have been previously trained on we compared the results by freezing the first 10 layers with retraining all the layers for one fold of one of nasnet and found that the results were better by freezing the first 10 layers as such for the extensive studies done the first 10 layers were frozen for all networks 3 results and discussion 3 1 freezing layers the cnn networks being used for training were originally built to classify objects quite different from color fundus images the low level and mid level features of the images could be very different an initial experiment was run to see whether to freezing the first 10 layers while training or allowing training to change the weights in all layers the experiment was run by training nasnet network and testing on one fold to see which method achieved the highest accuracy nasnet was chosen for the network as it is the deepest network of those studied table 4 summarizes the tp tn fn and fp for the two experiments training done with freezing first 10 layers achieved higher accuracy than having to optimize the weights in all the layers the first 10 layers only look into very low level features in the image and while the images are quite different for glaucoma the low level features are probably not that different these networks had originally been trained with over 1 million images and thus the robustness of the weights for the first 10 layers is very strong by choosing to train them over only 450 images we may have not reached optimal values table 4 comparison of the results when first 10 layers are frozen versus optimizing the weights of all the layers the experiment was done on a single fold by transfer learning on nasnet architecture freezing weights of first 10 layers changing weights of all layers true positive 35 34 false negative 5 6 true negative 50 47 false positive 1 4 3 2 comparison of cnn architectures for rim one r 2 database figure 3 shows the confusion matrices for the 5 networks studied the nasnet achieved the highest accuracy and specificity and did equally well in diagnosing glaucoma and normal cases densenet 201 achieved the highest sensitivity only classifying 7 glaucomatous images incorrectly the sensitivity and specificity for densenet 201 was 96 5 and 91 0 respectively and thus and only has a 3 5 chance of missing a case the vgg 19 architecture had consistent results in both sensitivity and specificity but performed worse than nasnet and densenet 201 in overall accuracy making it an unviable architecture in previous work vgg 19 had shown the best performance among cnn architectures but nasnet and densenet 201 were not studied 36 googlenet and inception resnet v 2 performed similarly both achieving a sensitivity of 89 5 and specificities of around 91 although the googlenet architecture achieved lower accuracies than the previously mentioned networks it should be noted that all the images were classified in rapid times 0 5 seconds and thus it may be suitable for mobile platforms an important parameter to judge the network s ability to accurately classify images is to investigate the results achieved over different folds a stable and accurate network should have consistent performance in different folds figure 4 plots the accuracy sensitivity and 9 specificity for the different networks over the 5 independent folds and the values along the standard deviations are summarized in table 5 if we look at the overall accuracy over the different folds densenet 201 nasnet and vgg 19 had consistent performance over the folds with standard deviations sd of 1 43 2 20 and 1 74 respectively the sd for googlenet and inception resnet v 2 was 4 10 and 3 84 however when we also consider the sensitivity and specificity densenet 201 has the most consistent performance and achieved values greater than 90 sensitivity for all folds in some folds nasnet achieved 100 sensitivity but has lower specificity for those same folds interestingly different networks performed relatively differently in different folds demonstrating that these networks were focusing on different features of the images googlenet had the highest variation showing that the training was not stable over the network fig 3 confusion matrices for the test data set for different trained cnn architectures results from the test samples from five folds were combined 10 figure 4 comparison of a accuracy b sensitivity and c specificity for 5 different folds of each network densenet 201 and vgg 19 had the most consistent results over the five folds 11 table 5 comparison of the accuracy sensitivity and specificity for the cnn architectures sd values are the standard deviations over the 5 different folds auc area under the curve network accuracy sensitivity specificity auc overall sd overall sd overall sd tested on rim one r 2 dataset googlenet 91 4 4 07 89 5 10 81 92 9 5 11 0 970 inception resnet v 2 91 0 3 84 89 5 6 94 92 2 3 10 0 973 vgg 19 92 3 1 74 92 0 4 47 92 5 2 56 0 983 densenet 201 93 4 1 43 96 5 3 79 91 0 3 28 0 990 nasnet 94 5 2 20 95 0 5 86 94 1 6 04 0 978 tested on drishti gs dataset densenet 201 90 1 3 54 98 6 3 19 71 0 7 22 0 91 vgg 19 86 1 2 18 97 1 3 91 61 3 7 60 0 87 nasnet 89 1 2 30 100 0 64 5 8 25 0 90 roc curves are plotted in figure 5 a for the 5 different architectures studied and the auc values are also summarized in table 5 all the networks had an auc over 97 with densenet 201 performing the best with auc of 0 990 nasnet which achieved the highest accuracy among the networks had a lower auc of 0 977 the roc curves show that all the networks classified the images robustly considering all the performance metrics densenet 201 has the best performance among the networks studied for the imagenet dataset densenet 201 performs worse than vgg 19 and nearly the same as inception resnet v 2 the results here show that the accuracy on the imagenet dataset may not correspond to the same accuracy when the networks are trained for other applications and hence one needs to consider all networks for transfer training for the densenet 201 network if the threshold for classification is reduced to 0 4 instead of 0 5 then 98 of glaucoma images are classified correctly while the accuracy only drops to 93 2 from 93 5 there is only one extra false positive to understand the robustness and stability of the densenet 201 network roc curves were analyzed for each of the five folds a stable network should have consistent results over all the folds figure 5 b plots the roc curves for each fold along with the roc curve for all the images the auc varied from 0 980 to 0 998 with a standard deviation of 0 007 figure 5 a roc curved for the 5 different cnn architectures studied b roc curved for 5 different folds for densenet 201 network 12 3 3 comparison of cnn architectures for drishti gs database for drishti gs the top performing models viz densenet 201 vgg 19 and nasnet were tested and the confusion matrices are shown in figure 6 the results are also summarized in table 5 densenet 201 achieved the highest overall accuracy of 89 1 but misclassified one glaucomatous image as normal nasnet classified all glaucomatous images correctly but misclassified two normal images more than densenet 201 while the sensitivity was high for the three networks the specificity was much lower the images have large light intensity variations and reflection artifacts in them further the 5 ophthalmologists all agree only on 52 images roughly half of the dataset these fifty two images were correctly classified by the densenet 201 and nasnet and the corresponding number for vgg 19 was 51 fig 6 confusion matrices for drishti gs for densenet 201 vgg 19 and nasnet 13 the class decision on the dataset was made as a majority decision by 5 ophthalmologists with varying experience from 3 20 years we recall richard feynman s 1988 37 caveat about taking averages with people of varying expertise in his analysis of the space shuttle challenger crash investigation when results from people with different experiences are averaged the average brings down the accuracy not increases it if the criterion is changed such that the image is glaucomatous if only two experts classify it as glaucoma then the classification results improve and are summarized in table 6 densenet 201 outperformed the other networks and achieved an accuracy of 93 1 sensitivity of 97 3 and specificity of 80 8 table 6 comparison of the accuracy sensitivity and specificity for drishti gs if an image is classified as glaucoma provided two of the 5 ophthalmologists classify it as glaucoma nasnet vgg 19 densenet 201 true positive 72 71 73 false negative 3 4 2 true negative 17 17 21 false positive 9 9 5 accuracy 88 1 87 1 93 1 sensitivity 96 0 84 7 97 3 specificity 65 4 65 4 80 8 3 5 comparison with other works table 7 summarizes some of the pertinent results of our work and compares it with other cnn classifications for glaucoma as can be seen from the table the results we have achieved are better than that reported so far both in terms of accuracy and sensitivity the difference is a lot more significant when you compare methods using the same database for example results that were achieved with vgg 19 are much better than those achieved by gomez valverde et al 2019 36 and diaz pinto et al 2019 38 table 7 comparison of our work with previous studies using cnn architectures author name method database no of images tested accuracy sensitivity specificity our work densenet rim one r 2 455 93 4 93 2 96 5 98 0 91 0 90 6 our work densenet drishti 101 90 1 98 6 71 0 our work vgg 19 rim one r 2 455 92 3 92 0 93 5 our work googlenet rim one r 2 455 91 4 89 5 92 9 singh et al 22 inception v 3 rim one 187 92 5 92 3 93 5 gomez valverde et vgg 19 rim one r 2 191 88 5 88 6 88 4 14 al 36 gomez valverde et al 36 vgg 19 drishti 17 88 2 91 7 80 0 li et al 21 inception v 3 private database 48116 95 6 92 0 chai et al 39 mb neural network private database 2554 91 5 92 3 90 9 cerentini et al 41 googlenet drishti 101 86 2 diaz pinto et al 37 vgg 19 acrima drishti rimone shchoi 86 hrf hrf 90 7 92 4 88 5 4 conclusion and discussion a new method was developed for transfer learning by combining supervised learning with reinforcement learning different cnn networks such as googlenet vgg 19 densenet 201 nasnet and inception resnet v 2 have been used to classify glaucomatous images these networks were used in this paper googlenet was designed for smartphone applications and is a very quick network to train vgg 19 has previously shown the highest accuracy for the rim one r 2 dataset 30 nasnet is really deep and has been shown to be accurate for the imagenet dataset densenet 201 has similar accuracies to vgg 19 though at a slower classification time however this network has never been tried before inception resnet v 2 is resnet with added inception layers and gives higher accuracies than vgg 19 for the imagenet dataset the proposed algorithm is tested on publically available dataset and the results show that the model can be used to effectively train and deploy cnn architectures for glaucoma diagnosis the method was tested for using 5 fold classification for glaucomatous images and achieved very high sensitivities while maintaining high accuracies transfer learning on densenet 201 performed the best achieving an auc of 0 99 and sensitivity of 96 5 for the rimone r 2 dataset if the threshold is changed to 0 4 then the sensitivity increases to 98 for drishti gs dataset densenet 201 achieved an accuracy of 90 1 auc of 0 91 and sensitivity of 98 6 the results achieved in this work are better than what has been reported so far for the same datasets before demonstrating that the new training method improves the accuracy of prediction in the future the authors plan to deploy the densenet 201 model into the medical field for real world testing this will allow to see what effects the model takes when tested blindly on new images the authors also plan to train the model on images with different stages of glaucoma as this can indicate the correct treatment a glaucomatous patient needs 15 acknowledgments vl is also with the departments of physics and systems design engineering at uw and acknowledges support by a discovery grant from nserc disclosures the authors declare no conflicts of interest references 1 lakshminarayanan v 2012 the global problem of blindness and visual dysfunction in photonic innovations and solutions for complex environments and systems pisces vol 8482 p 84820 a international society for optics and photonics doi 10 1117 12 928050 2 susanna r de moraes c g cioffi g a ritch r 2015 why do people still go blind from glaucoma translational vision science technology 4 2 https doi org 10 1167 tvst 4 2 1 3 vajaranant t s wu s torres m varma r 2012 the changing face of primary open angle glaucoma in the united states demographic and geographic changes from 2011 to 2050 american journal of ophthalmology 154 2 303 314 4 sharma p sample p a zangwill l m schuman j s 2008 diagnostic tools for glaucoma detection and management survey of ophthalmology 53 6 s 17 s 32 5 mills r p budenz d l lee p p noecker r j walt j g siegartel l r evans s j doyle j j 2006 categorizing the stage of glaucoma from pre diagnosis to end stage disease american journal of ophthalmology 141 1 24 30 6 greenfield d s weinreb r n 2008 role of optic nerve imaging in glaucoma clinical practice and clinical trials american journal of ophthalmology 145 4 598 603 7 iester m mikelberg f s swindale n v drance s m 1997 roc analysis of heidelberg retina tomograph optic disc shape measures in glaucoma canadian journal of ophthalmology 32 6 382 388 8 harizman n oliveira c chiang a tello c marmor m ritch r liebmann j m 2006 the isnt rule and differentiation of normal from glaucomatous eyes archives of ophthalmology 124 11 1579 1583 9 christopher m belghith a bowd c proudfoot j a goldbaum m h weinreb r n et al 2018 performance of deep learning architectures and transfer learning for detecting glaucomatous optic neuropathy in fundus photographs scientific reports 8 1 1 13 doi 10 1038 s 41598 018 35044 9 10 almazroa a sun w alodhayb s raahemifar k lakshminarayanan v 2017 optic disc segmentation for glaucoma screening system using fundus images clinical ophthalmology auckland nz 11 2017 2029 11 leopold h a zelek j lakshminarayanan v deep learning methods applied to retinal image analysis in biomedical signal processing in big data e sejdic and t falk eds crc press boc raton fl pages 329 367 2018 12 balasubramanian k ananthamoorthy n p 2020 state of the art techniques in optic cup and disc localization for glaucoma diagnosis research results and issues critical reviews in biomedical engineering 48 1 63 83 13 sakthivel k narayanan r 2015 an automated detection of glaucoma using histogram features international journal of ophthalmology 8 1 194 200 14 dua s acharya u r chowriappa p sree s v 2011 wavelet based energy features for glaucomatous image classification ieee transactions on information technology in biomedicine 16 1 80 87 15 acharya u r bhat s koh j e bhandary s v adeli h 2017 a novel algorithm to detect glaucoma risk using texton and local configuration pattern features extracted from fundus images computers in biology and medicine 88 72 83 16 raghavendra u fujita h bhandary s v gudigar a tan j h acharya u r 2018 deep convolution neural network for accurate diagnosis of glaucoma using digital fundus images information sciences 441 41 49 17 chen x xu y yan s wong d w k wong t y liu j 2015 automatic feature learning for glaucoma detection based on deep learning in international conference on medical image computing and computer assisted intervention pp 669 677 springer cham 18 al bander b al nuaimy w al taee m a zheng y 2017 automated glaucoma diagnosis using deep learning approach in 2017 14 th international multi conference on systems signals devices ssd pp 207 210 ieee doi 10 1109 ssd 2017 8166974 19 fu h cheng j xu y zhang c wong d w k liu j cao x 2018 discaware ensemble network for glaucoma screening from fundus image ieee transactions on medical imaging 37 11 2493 2501 16 20 li z he y keel s meng w chang r t he m 2018 efficacy of a deep learning system for detecting glaucomatous optic neuropathy based on color fundus photographs ophthalmology 125 8 1199 1206 21 szegedy c vanhoucke v ioffe s shlens j wojna z 2016 rethinking the inception architecture for computer vision in proceedings of the ieee conference on computer vision and pattern recognition pp 2818 2826 doi 10 1109 cvpr 2016 308 22 singh a sengupta s lakshminarayanan v 2019 glaucoma diagnosis using transfer learning methods in applications of machine learning vol 11139 p 111390 u international society for optics and photonics https doi org 10 1117 12 2529429 23 singh a sengupta s lakshminarayanan v 2020 explainable deep learning models in medical image analysis j imaging 6 52 doi 10 3390 jimaging 6060052 2020 24 huang g liu z van der maaten l weinberger k q 2017 densely connected convolutional networks in proceedings of the ieee conference on computer vision and pattern recognition pp 4700 4708 arxiv preprint arxiv 1608 06993 25 simonyan k zisserman a 2014 very deep convolutional networks for large scale image recognition arxiv preprint arxiv 1409 1556 26 zoph b vasudevan v shlens j le q v 2018 learning transferable architectures for scalable image recognition in proceedings of the ieee conference on computer vision and pattern recognition pp 8697 8710 doi 10 1109 cvpr 2018 00907 27 szegedy c ioffe s vanhoucke v alemi a a 2017 inception v 4 inceptionresnet and the impact of residual connections on learning in thirty first aaai conference on artificial intelligence https dl acm org doi 10 5555 3298023 3298188 28 szegedy c liu w jia y sermanet p reed s anguelov d erhan d vanhoucke v rabinovich a 2015 going deeper with convolutions in proceedings of the ieee conference on computer vision and pattern recognition pp 1 9 doi 10 1109 cvpr 2015 7298594 29 fumero f alay n s sanchez j l sigut j gonzalez hernandez m 2011 rim one an open retinal image database for optic nerve evaluation int sym on cbms 1 6 10 1109 cbms 2011 5999143 doi 10 1109 cbms 2011 5999143 30 sivaswamy j krishnadas s r joshi g d jain m tabish a u s 2014 april drishti gs retinal image dataset for optic nerve head onh segmentation in 2014 ieee 11 th international symposium on biomedical imaging isbi pp 53 56 doi 10 1109 isbi 2014 6867807 31 sengupta s singh a leopold h a gulati t lakshminarayanan v 2020 ophthalmic diagnosis using deep learning with fundus images a critical review artificial intelligence in medicine 102 101758 https doi org 10 1016 j artmed 2019 101758 32 ren c y reid i 2011 gslic a real time implementation of slic superpixel segmentation university of oxford department of engineering technical report 1 6 33 comer m l delp e j 1999 morphological operations for color image processing j electronic imaging 8 3 279 289 34 almazroa a alodhayb s osman e ramadan e hummadi m dlaim m alkatee m raahemifar k lakshminarayanan v 2018 retinal fundus images for glaucoma analysis the riga dataset in medical imaging 2018 imaging informatics for healthcare research and applications vol 10579 p 105790 b international society for optics and photonics https doi org 10 1117 12 2293584 35 zhang z 2018 p improved adam optimizer for deep neural networks in 2018 ieee acm 26 th international symposium on quality of service iwqos pp 1 2 doi 10 1109 iwqos 2018 8624183 36 g mez valverde j j ant n a fatti g liefers b herranz a santos a s nchez c i ledesma carbayo m j 2019 automatic glaucoma classification using color fundus images based on convolutional neural networks and transfer learning biomedical optics express 10 2 892 913 37 feynman r p 1988 an outsider s inside view of the challenger inquiry physics today 41 2 26 37 38 diaz pinto a morales s naranjo v k hler t mossi j m navea a 2019 cnns for automatic glaucoma assessment using fundus images an extensive validation biomedical engineering online 18 1 29 https doi org 10 1186 s 12938 019 0649 y 39 chai y liu h xu j 2018 glaucoma diagnosis based on both hidden features and domain knowledge through deep learning models knowledge based systems 161 147 156 40 shibata n tanito m mitsuhashi k fujino y matsuura m murata h asaoka r 2018 development of a deep residual learning algorithm to screen for glaucoma from fundus photography scientific reports 8 1 1 9 https doi org 10 1038 s 41598 018 33013 w 41 cerentinia a welfera d d ornellasa m c haygertb c j p dottob g n 2018 automatic identification of glaucoma using deep learning methods stud health technol inform 2017 245 318 321