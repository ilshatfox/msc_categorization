pay attention to relations multi embeddings for attributed multiplex networks joshua melton michael ridenhour and siddharth krishnan dept of computer science university of north carolina at charlotte charlotte usa jmelto 30 uncc edu mridenh 7 uncc edu skrishnan uncc edu abstract graph convolutional neural networks gcns have become effective machine learning algorithms for many down stream network mining tasks such as node classification link prediction and community detection however most gcn meth ods have been developed for homogenous networks and are limited to a single embedding for each node complex systems often represented by heterogeneous multiplex networks present a more difficult challenge for gcn models and require that such techniques capture the diverse contexts and assorted interactions that occur between nodes in this work we propose rahmen a novel unified relation aware embedding framework for attributed heterogeneous multiplex networks our model incorporates node attributes motif based features relation based gcn approaches and relational self attention to learn embeddings of nodes with respect to the various relations in a heterogeneous multiplex net work in contrast to prior work rahmen is a more expressive embedding framework that embraces the multi faceted nature of nodes in such networks producing a set of multi embeddings that capture the varied and diverse contexts of nodes we evaluate our model on four real world datasets from amazon twitter youtube and tissue ppis in both transduc tive and inductive settings our results show that rahmen consistently outperforms comparable state of the art network embedding models and an analysis of rahmen s relational self attention demonstrates that our model discovers interpretable connections between relations present in heterogeneous multiplex networks index terms network embedding multiplex networks se mantic attention graph convolutional network i introduction deep graph neural network gnn embeddings have be come increasingly successful in graph based learning tasks like node classification link prediction and network recon struction 1 6 while graph convolutional networks gcns have been instrumental in the development of the embedding techniques most proposed works in the gcn family consider only homogeneous networks which consist of a single node and edge type however real world networks are often com plex and their properties are characterized via multiple at tributed edge and node types examples include gene protein interaction networks and knowledge graphs nodes in such networks participate in a variety of different contexts and relate to one another in different ways this multi faceted nature of nodes in such networks belies the limited expressiveness of gcn models designed for homogeneous networks therefore to accomplish downstream learning tasks on such attributed heterogeneous multiplex networks we require network em bedding techniques with a greater expressive capacity such frameworks must capture the varied contextual features along with differing local structural topology of nodes in these networks in order to best perform predictive and classification tasks in downstream graph learning objectives fig 1 example attributed heterogeneous multiplex network with multiple types of nodes and edges each node type consists of their own set of attributes blue nodes with 3 attributes and yellow nodes with 4 attributes towards that end in this work we propose a unified network embedding framework for real world complex structures networks that are attributed multiplex and heterogeneous recent research studies focus on developing network em bedding models for multiplex networks single node type and multiple edge types 7 9 heterogeneous networks multiple node and edge types 10 11 and heterogeneous multiplex networks multi level network with multiple node and edge types 12 14 our present work introduces a more expressive embedding framework by asserting the multi faceted nature of nodes throughout the embedding process and eschewing the reliance on a single embedding as insufficient for capturing a node s proximity to others in such complex networks we therefore design a novel graph convolutional approach for network embedding with relational self attention that produces a set of information rich multi embeddings of nodes present in an attributed heterogeneous multiplex net work ahmen example shown in figure 1 an instance of ar x iv 2 20 3 01 90 3 v 1 cs l g 3 m ar 2 02 2 an ahmen appears for example in protein protein interaction networks 15 where a set of proteins interact in a number of different tissues by incorporating the interactions and protein features across multiple tissue layers in the network our embedding framework can better capture the many sided functional roles of a protein and can better predict unseen protein protein interactions in various tissue layers present work in this work we present rahmen relation aware embeddings for attributed heterogeneous multiplex networks an expressive and interpretable frame work to learning spatial embeddings of nodes in ahmens we make the following contributions and observations to the cur rent line of research on network embedding with rahmen 1 we employ a set relational graph convolutional operators that incorporate a relation specific view of a node s self information with the node s local relation specific neighborhood providing an enhanced context of the node in its neighborhood with respect to each relation in the network 2 to the sequence of relation specific node representa tions we apply semantic self attention to share informa tion across each relation type in the network informing the node s context across all relation types present in the network this produces a set of robust multi embeddings for the node characterizing its various contexts in the network 3 the latent representations learned with our framework give state of the art performance on both transductive and inductive link prediction tasks with four real world datasets 4 we propose to incorporate higher order subgraph fea tures as node attributes we observe that such structural features can enhance the node representations learned by the framework and allow for inductive learning on networks without node attributes 5 our self attention mechanism uncovers interpretable connections between relations in the network for exam ple we show that our model confirms domain knowledge by establishing connections between proteins found in the brain central nervous system and nervous system ii related work network embeddings traditional network embedding mod els 3 4 use random walks to preserve the local and global neighborhoods of a node advances in deep graph neural net works have introduced the message passing paradigm 16 and convolutional graph embedding models like graph convolu tional networks gcn 2 and graphsage 1 utilize node attributes and neighborhood aggregation to capture a node s local context graph attention networks gat 6 further improved such graph convolutional approaches by introducing node level attention mechanisms during neighbor aggregation these frameworks wed spatial and spectral interpretations of networks by incorporating node attributes and the contextual information of a node s local receptive field defined by the node s neighborhood in the graph 2 5 with learnable filter functions providing efficient implementations and generating node embeddings with provable representation power 1 2 17 parameter sharing allows such models to scale to large datasets and to be applied in inductive contexts 1 18 heterogeneous and multiplex graph embeddings such convolutional approaches were originally limited to homoge neous networks but in recent years a number of heteroge neous multiplex network embedding frameworks have been proposed to extend graph neural network embedding models to such networks frameworks like r gcn 12 and han 14 extend the idea of gcn 2 and gat 6 respectively for heterogeneous networks by applying relation or metapath specific convolutional operations frameworks like mne 8 and gatne 13 employ a base node embedding which is augmented by edge type specific messages generated by graph convolutional operations han and gatne have both employed semantic attention 19 as a means to implicitly learn aggregation weights for edge type or metapath specific embeddings respectively to date most frameworks consider only a single embedding for each node 8 12 14 and do not sufficiently capture the variety of contexts and interactions between nodes in hetero geneous multiplex networks in rahmen we emphasize that nodes in such real world networks are related to one another in different ways and thus cannot be accurately modeled by a single embedding we therefore propose a more expressive multi embedding framework for nodes in heterogeneous mul tiplex networks called rahmen that incorporates a set of relation specific graph convolutional operators which learn the individual semantic contexts for each node and a relational self attention mechanism which shares information across a node s various relational contexts to produce a set of robust multi embeddings for each node in a heterogeneous multiplex network iii problem definition the complete set of notations used in this paper is given in table i we define a homogeneous network as g v e where v denotes the set of all nodes in the graph and e denotes the set of all edges in the graph based on this fundamental representation of a homogeneous network we define the following definition 1 attributed network we define an attributed homogeneous network as g v e a where each node v v is associated with a set of node features a xi vi v where xi is the feature vector associated with node vi definition 2 heterogeneous multiplex network a het erogeneous network is defined as g v e tv te where v and e are the universal sets of nodes and edges each node v v is associated with mapping function v v tv and each edge e e is associated with mapping function e e te where tv and te denote the sets of node types and edge types respectively if tv te 2 the network is termed heterogeneous the network is termed multiplex if multiple types of edges may exist between a pair of nodes fig 2 two four node connected motifs considered for structural feature representations table i important notations and their definitions notation description g the input network v e the node and edge sets of g tv te the node type and edge type sets of g a the attribute set of g r the set of canonical relations of g v e a node and edge in the graph x the set of attributes of a node n the neighborhood of a given node k neighborhood levels or hops z the latent representation of a node d the dimension of the final overall embeddings da the dimension of the attention vector ws wn the self and neighbor transformation matrices definition 3 motif given a graph g v e a motif is defined as a subgraph m vm em consisting of m nodes vm v and with all edges em e connecting vm nodes in g 20 for this work we consider two four node connected motifs shown in figure 2 definition 4 canonical relation given a heterogeneous graph g with tv node types and te edge types we de fine the canonical relation r as a 3 tuple representing the source node type edge type and destination node type of the relation the set r consists of all canonical relations r tiv t i j e t j v tiv tjv tv t i j e te and uniquely defines the set of relations present in the network problem definition ahmen embedding given an ah men g v e a r where v is a set of vertices e is a set of edges a is a set of node attributes and r is a set of all canonical relations the goal is to learn a transformation function that gives a set of low dimensional representations for each node v v with respect to canonical relations r r such that nodes which are similar are closer to one another in the embedding space that is learn a function f v v rd r r where d v iv methodology in this section we describe the proposed rahmen frame work we explain the model architecture its ability to be applied in an inductive context and describe an efficient semi supervised method for model optimization the model pipeline is illustrated in figure 3 a rahmen framework in the rahmen framework we learn an inductive transfor mation function for each node v with respect to all relations in the network defined by the set of canonical relations r this transformation function produces a set of low dimensional spatial representations of a node by jointly incorporating relation specific views of the self node s attributes with its multiple network contexts defined by canonical relations r at each k level corresponding to the k hop neighborhood of node v we apply relational graph convolutional operations over the views of the network gr vr er a r r we then apply a relational semantic attention mechanism to differentiate and optimally combine the semantic specific latent representations of node v the initial representation h 0 v for each node v is given by the node features vector xv following the message passing paradigm described in 16 our neighbor message function is defined as kr n v r u n v r 1 n v r wkn r h k 1 u r 1 where n v r is node v s local neighborhood on relation r and hk 1 u r is each node u s latent representation from the previous layer the transformed and aggregated neighborhood message is then combined with a specific view of node v s self representation to generate a relation specific latent representa tion of node v h k v r wks r h k 1 v r k r n v r b k r 2 where is a non linear activation function such as elu 21 the self and neighbor transformation functions ws and wn may be any differentiable function such as a linear transformation or an mlp every node in an ahmen participates in multiple semantic relationships defined across the various relations in the net work the latent representations of node v along each relation r r reflect only one aspect of the node s semantic context and do not take into account interactions between the various relations in the network to learn a more comprehensive set of embeddings for a node we apply semantic attention 19 to this sequence of latent node representations an emphasis of rahmen is the irreducibility of a node s complex position in an ahmen to a single embedding as such we employ our relational semantic self attention to generate a set of multi embeddings for a node which blend the various relational contexts of a node while preserving a focus on a particular relation within each individual embedding first we stack each of the relation specific representations of node v from equation 2 as the sequence below with shape r d h k v concat h k v r r r 3 fig 3 overview of steps proposed in rahmen in step 1 k level neighborhood samples are taken for the target node for each k level neighborhood sample in step 2 relation specific graph convolutions incorporate a relational view of the node s self attributes with its local relation specific neighborhood in step 3 self attention is applied to the sequence of latent representations to share information across relations in the network steps 2 and 3 are repeated for each neighborhood level up to k to produce a set of embeddings z for the target node to learn the optimal set of attention weights for each relation with respect to every other relation in the network we transform the embeddings using a nonlinear transformation and compute the similarity with a relation level attention matrix the attention weights for each relation are obtained by normalizing the above results using the softmax function akv softmax w k rel tanh w k attn h k v 4 where wkrel is a trainable relation attention matrix of shape r da r and w kattn is a trainable transformation matrix with size r d da the final latent representation for node v at level k is therefore hkv a k v h k v r r r 5 the overall set of multi embeddings of node v at k with shape r d is zv h k v 6 b model optimization in this section we describe the semi supervised training process for the proposed rahmen framework following 4 13 we use random walks to generate sequences of node se quences and optimize our model to learn node representations that maximize the similarity of co occuring nodes we conduct random walks along each relation view gr vr er a given schema m v 1 v 2 vm vl where l is the length of the metapath schema the transition probability at step t is p u v m 1 n v r vt 1 v u r 0 v u r 7 where n v r denotes neighborhood of node v along relation r the random walker conducts a set of walks for each node along each relation r in which it participates capturing the various semantic contexts for each relation in the network a random walk with length l along relation r defines a path p vp 1 vp 2 vpl given a context sliding window size c we define the context of v as c vpj vpi p i j c j 6 i therefore given a node v with context c along a path the objective is to minimize the negative log likelihood log p u u c v u s log p u v 8 where represents the model parameters we utilize the softmax function as the probability of node u given v p u v exp ctr zv r u vc exp c t u zv r 9 where u vm cu is the context embedding of node u and zv r is the embedding for node v on relation r to approximate the objective function we use negative sampling for each node context triple v r u as e log ctu zv r l l 1 evi pr u log c t u zv r 10 where is the sigmoid function l corresponds to the number of negative samples draw for each positive sample and vi is a node drawn randomly from noise distribution pr v defined on node u s corresponding node set and pr v may be either a uniform distribution or a log uniform distribution ordered by node degree the time complexity of the random walk based training algorithm is o v r d l where v is the number of nodes r is the number of relations d is the embedding dimension and l is the number of negative samples per training sample the memory complexity of our algorithm is o v r d da v experiments we evaluate our model in both transductive and inductive experiments against state of the art network embedding meth ods the code for our analysis is available here 1 sections v a and v b outline the datasets and baseline models used for 1 https anonymous 4 open science r rahmen anon b 817 readme md table ii properties of heterogeneous multiplex network datasets used for evaluation of rahmen framework dataset nodes edges relations amazon 10 099 135 761 2 twitter 28 473 91 726 3 youtube 2 000 1 310 544 5 tissue ppi 4 360 527 850 10 our analysis our experiments seek to answer the following questions 1 can we utilize the rahmen framework for link pre diction on four real world datasets in a transductive setting section v c 2 can we apply our model for link prediction in an inductive setting section v d 3 does rahmen s self attention discover explainable connections between relations in networks section v e 4 can we improve the performance of gnns by using network motifs to enhance node features section v f 5 how sensitive is our model to various hyperparameters including embedding dimension neighborhood sample size number of k levels and attention dimension sec tion v g a datasets we utilize four publicly available datasets for our experi ments table ii provides the network properties of our datasets descriptions of each dataset are as follows amazon 2 we utilize the dataset 3 as provided by 13 which consists of only the product metadata of the electronics cate gory from amazon com each product has a set of attributes including price sales rank brand and category twitter 4 the twitter dataset contains tweets and user inter actions related to the discovery of the higgs boson in 2012 for this work we extract the multiplex network consisting of the reply retweet and mention networks between all users that had at least one reply link in the original data youtube 5 we utilize the youtube dataset provided by 13 which consists of a multiplex network describing the co occurence of friends subscriptions favorited videos sub scribers and a layer representing contacts between users tissue ppi 6 the tissue specific protein interaction network consists of protein interactions for 107 tissue types network layers in the human body we extract multiplex network consisting of the ten largest network layers present in the dataset b baseline comparisons we evaluate rahmen against two categories of base lines homogeneous network embedding models and multiplex 2 http jmcauley ucsd edu data amazon 3 https github com thudm gatne 4 https snap stanford edu data higgs twitter html 5 http socialcomputing asu edu datasets youtube 6 http snap stanford edu ohmnet heterogeneous network embedding models the embedding dimension for all models is set to 200 the complete hyperpa rameter settings for each model are listed in the appendix we utilize two well established graph embedding methods originally designed for homogeneous networks node 2 vec 4 and deepwalk 3 to adapt these methods to ahmens we learn node embeddings for each separate relation layer in the multiplex networks we compare rahmen against five state of the art ahmen embedding frameworks mne 8 r gcn 12 r graphsage gatne 13 and han 14 we implement an r graphsage framework following the r gcn design principles and use graphsage in place of gcn r gcn r graphsage and han aggregate messages from relation specific graph convolutional operations gcn 2 graphsage 1 and gat 6 respectively by contrast mne and gatne both learn a common base node embedding which is augmented with an edge specific embedding that captures the information contained in various edge types mne applies a fixed weight to each individual edge type message while gatne implicitly learns aggregation weights when combining the individual edge type messages for the inductive experiment we compare rahmen against r graphsage gatne and han since these models support both inductive learning and attributed networks in contrast to the heterogeneous embedding frameworks listed above rahmen learns an inductive transformation function that leverages both node attributes and the multi relational structure of the network at each k level in the model we learn relation specific convolutional operations that transform and combine a node s self attributes and its local context in the network we then employ self attention to share information across relation types in the network learning a set of latent representations of a node that capture the multi faceted semantic contexts of a node in ahmens specific implementation details may be found in the appendix we evaluate the performance of rahmen by comparing against the aforementioned baseline models on link prediction exper iments in both transductive and inductive settings c transductive experiment results for our experiments in a transductive setting we use a classic link prediction task which is common in both academic and industrial contexts in our experimental setup we mask a set of edges and non edges from the original network and train the models on the remaining network we utilize the train val test splits provided by 13 for the amazon and youtube datasets for the twitter dataset we follow the same procedure and create validation and test sets that consist of 5 and 10 randomly selected positive edges respectively with an equivalent number of randomly selected negative edges for each relation type for the tissue ppi dataset we use 5 fold cross validation with 20 of the edges in the network held out and split into validation and test sets we report the area under the roc curve roc auc and the f 1 score of all models on the link prediction task to avoid the thresholding effect we assume the number of hidden edges in the test set table iii performance roc auc and f 1 of embedding frameworks on the link prediction task in a transductive context rahmen outperforms all baseline models on the amazon youtube and tissue ppi datasets unpaired t test p 0 05 and equals the performance of han on the twitter dataset unpaired t test p 0 096 oot out of time 24 hrs amazon twitter youtube tissue ppi roc auc f 1 roc auc f 1 roc auc f 1 roc auc f 1 node 2 vec 94 47 87 88 72 58 71 94 71 21 65 36 51 30 64 04 deepwalk 94 20 87 38 76 88 72 42 71 11 65 52 58 48 67 16 mne 90 28 83 25 oot oot 82 30 75 03 oot oot r graphsage 94 88 89 39 74 31 70 77 87 02 79 93 66 61 61 59 r gcn 94 96 90 08 92 75 85 85 80 21 73 36 84 19 75 98 gatne 96 25 91 36 92 94 86 20 84 47 76 83 79 83 71 78 han 95 28 90 43 94 81 88 44 80 43 73 43 93 05 85 98 rahmen 96 78 92 39 94 58 88 31 88 64 80 58 94 88 87 99 is given 13 22 we report the mean performance over five trials for each dataset both metrics are uniformly averaged over all relation types in the datasets the experimental results are presented in table iii the results clearly illustrate the strong performance of rahmen across all four of the ahmen datasets rahmen outper forms all baseline models on the amazon youtube and tissue ppi datasets unpaired t test p 0 05 and achieves comparable results to han on the twitter dataset unpaired t test p 0 096 mne did not complete training on the twitter and tissue ppi datasets due to the out of time issue 24 hrs we note that the baseline models excluding rah men demonstrate variable performance across the experi mental datasets each of the baseline frameworks aside from gatne generate a single embedding to represent a node in an ahmen and gatne relies largely on its base self node embedding as the edge type information is highly compressed into a low dimension space this fact limits the expressiveness of other heterogeneous embedding frameworks and requires that these models compromise on the extent to which they emphasize the self node or emphasize the multi relational local context of the node as such depending on the par ticular characteristics of the network rahmen by contrast demonstrates consistently strong performance across all of the experimental datasets in the rahmen framework we learn a transformation function that computes multi embeddings for a target node which integrates a multi faceted view of the self node s attributes and the multi relational structure of the network this provides rahmen with a greater expressive capacity than other heterogeneous graph embedding models producing latent representations of nodes that better capture the diverse interactions between entities in ahmens overall our experiments demonstrate that rahmen is able to integrate multi relational local graph structures in the network and to share information across layers producing information rich embeddings that capture the complex structure of attributed heterogeneous multiplex networks d inductive experiment results in addition to our experiments in a transductive setting we conduct an evaluation of rahmen in an inductive context on the tissue ppi dataset for this experiment we mask 15 of nodes in the graph to consider as test nodes unseen during model training from the reduced graph we mask an additional 20 randomly sampled positive edges with an equivalent number of randomly sampled negative edges as a validation set for hyperparameter tuning and early stopping for evaluation we create a test graph from the training graph by adding 50 of the edges incident on the hidden nodes which provide the local structure of the hidden nodes during neighbor sampling our link prediction task is to predict the remaining 50 of edges from the unseen nodes with an equiv alent number of randomly sampled negative edges incident on the hidden nodes table iv illustrates our inductive ex periment results which demonstrate that rahmen maintains its performance advantage over all other heterogeneous graph embedding frameworks in an inductive context unpaired t test p 0 05 e self attention explainability the relational self attention is critical for the enhanced ex pressiveness of rahmen as it facilitates information sharing across the varied contexts present in ahmens in addition to improving the performance of rahmen embeddings on downstream learning tasks we hypothesize that our relational self attention discovers interpretable connections in the net work which provides a level of explainability to rahmen that is not present in other heterogeneous graph embedding frameworks to further examine our attention mechanism we analyze the learned attention weights for each relation in the tissue ppi dataset where each relation layer in this dataset corresponds to a particular human tissue type for our experi ments we extract the graph consisting of the ten largest tissue layers in the network the relational layers are blood brain central nervous system cns fetus heart kidney leukocyte lung nervous system ns and testis figure 4 illustrates the distribution of attention values for every protein node in the network with respect to each tissue type in our dataset and as figure 4 demonstrates we can see that rahmen s self attention mechanism identifies the importance of biologically related tissues in predicting protein interactions figure 4 a shows the attention distributions for the brain layer where we note that both the cns and nervous system layers are attended to by the model when learning latent representations fig 4 distributions of tissue self attention values for each protein for the a brain b cns c leukocyte and d ns layers in the tissue ppi dataset shows the significance of self attention mechanism of the rahmen framework in identifying biologically related tissues table iv performance roc auc and f 1 of embedding frameworks on the link prediction task in an inductive context rahmen outperforms all baseline models unpaired t test p 0 05 tissue ppi roc auc f 1 r graphsage 71 16 65 46 r gcn 78 08 70 98 gatne 61 36 57 64 han 87 53 57 66 rahmen 87 93 79 94 of proteins in the brain similarly figure 4 b shows that attention distributions for the cns layer where both the brain and ns layers are important by contrast figure 4 d shows that the more general nervous system layer does not attend as highly over the brain and cns layers compared to the other tissue layers lastly figure 4 c shows the attention values for the leukocyte layer where we see that white blood cell proteins attend to other proteins found in the blood in all our analysis of rahmen s learned attention weights in the tissue ppi model illustrates that our relational self attention produces interpretable and biologically consistent importance weights across the tissue types present in the protein protein interaction network f graph motif features meaningful initial feature representations are essential for generating high quality embeddings with inductive graph em bedding models such models learn an inductive transfor mation function that embeds a node in the latent feature space based on the characteristics of the self node and feature distributions of a node s local neighborhood in many cases initial node features must be generated through transductive embedding frameworks or other models an attractive al ternative is to generate initial feature representations from graph structures both as initial node representations or as a method of feature augmentation in this work we generate structural feature representations by counting the participation of each node in various network motifs motifs are small graph substructures that appear with regular frequency in a graph 23 in our experiment we consider connected motifs containing two three or four nodes we limit the counted motifs to four nodes because the number of motif types grows exponentially as you increase the number of participating nodes diagrams of the three and four node motifs are shown in figure 2 to obtain motif counts we use a modified version of the graphlet counting algorithm presented by 24 our modifications allow us to count motif participation with respect to each individual node in the graph as opposed to producing the total count for each motif type in the graph we account for network heterogeneity by counting motif participation on each relation layer independently our results fig 5 performance of rahmen on the amazon dataset with structural motif features given node features and a combination of given and structural features with the motif based node features are shown in figure 5 and indicate that structural representations can augment node features or serve as a viable alternative to learning initial node embeddings using deepwalk 3 or other transductive embedding frameworks as was the case for the youtube and twitter datasets 13 g ablation study parameter sensitivity to demonstrate the necessity of rahmen s attention mech anism we conduct an ablation test where rahmen s rela tional self attention is removed from the model in this variant the relation specific graph convolution operations transform and aggregate information about a node s self attributes and its local graph neighborhood this information is then propagated through the model along each relation axis without any sharing of contextual information from the other relations present in the network figure 6 illustrates the performance of rahmen on the youtube and tissue ppi with no self attention as can be seen removal of rahmen s relational self attention reduces the overall performance of the model on both datasets though the extent of the performance decrease varies between the two datasets in the tissue ppi dataset where network relations correspond to biological tissues removing the infor mation sharing from the self attention leads to a significant drop in performance this aligns with biological knowledge that many proteins are present in multiple tissues and partic ipate in functionally similar roles across different tissues a fact reinforced by our analysis of rahmen s attention presented in section v e in the youtube dataset the drop in performance is more muted suggesting that cross relational information sharing is less significant for users in the youtube social network we also investigate the sensitivity of rahmen to various hyperparameters including embedding dimension d neigh borhood sample size number of k levels and self attention dimension figure 7 a illustrates the performance of rahmen when changing the size of the embedding dimension from which we can conclude that the performance of rahmen is fig 6 ablation study and dimension analysis of rahmen s relational self attention for the tissue ppi dataset information sharing across relations is highly important for model performance stable within a large range of embedding sizes though per formance drops when the embedding dimension is too small figure 7 b illustrates rahmen s performance when modifying the size of the neighborhood sampled per node at each level in the model the performance of rahmen when altering the number of k level convolutional layers is depicted in figure 7 c and we can see that a single level of neighborhood aggregation results in lower performance compared to two levels of aggregation three levels of aggregation does not result in a significant increase in performance and does not offset the resulting increase in computational costs vi conclusion in this work we introduce rahmen a framework for relation aware embeddings for attributed heterogeneous multi plex networks our framework applies the graph convolutional approaches to learn a node s spatial embedding with respect to each canonical relation in the network rahmen allows embeddings to be generated for unseen nodes by training a set of relation specific graph convolutional operators that learn a set of relation specific characterizations for each node by in corporating relational semantic self attention which uncovers the importance of each relation to one another and facilitates sharing information across all relations in the network rah men produces a set of information rich multi embeddings for each node in the network that capture the diverse nature of nodes in ahmens rahmen outperforms state of the art benchmarks on the link prediction in both transductive and inductive contexts in datasets in both social and biological settings we also demonstrate the utility and interpretability of rahmen s self attention over the relations present in a network and we show the potential of incorporating low level subgraph features as node attributes a number of potential future directions are possible for rahmen including incorporating node level at tention in neighborhood aggregation motif based attention for local structure aware neighborhood sampling and extensions to higher order structures such as hypergraphs fig 7 the sensitivity of rahmen when varying the a size of the embedding dimension b modifying the size of the neighborhood sample at each k level and c altering the number of k level neighbor aggregations we observe that rahmen is stable within a large range of embedding sizes and insensitive to neighborhood sample size though performance drops when the embedding dimension is too small a single level of neighborhood aggregation results in lower performance compared to two levels of aggregation three levels of aggregation does not result in a significant increase in performance and does not offset the resulting increase in computational costs references 1 w hamilton z ying and j leskovec inductive representation learning on large graphs in neurips 2017 pp 1024 1034 2 t n kipf and m welling semi supervised classification with graph convolutional networks in iclr 2017 3 b perozzi r al rfou and s skiena deepwalk online learning of social representations in sigkdd 2014 p 701 710 4 a grover and j leskovec node 2 vec scalable feature learning for networks in sigkdd 2016 pp 855 864 5 g cui j zhou c yang and z liu adaptive graph encoder for attributed graph embedding in sigkdd 2020 pp 976 985 6 p velic kovic g cucurull a casanova a romero p lio and y bengio graph attention networks in iclr 2018 7 m zitnik and j leskovec predicting multicellular function through multi layer tissue networks bioinformatics vol 33 no 14 pp 190 198 2017 8 h zhang l qiu l yi and y song scalable multiplex network embedding in ijcai vol 18 2018 pp 3082 3088 9 a bagavathi and s krishnan multi net a scalable multiplex network embedding framework in international conference on complex net works and their applications springer 2018 pp 119 131 10 y dong n v chawla and a swami metapath 2 vec scalable representation learning for heterogeneous networks in sigkdd 2017 pp 135 144 11 c shi b hu w x zhao and s y philip heterogeneous information network embedding for recommendation tkde vol 31 no 2 pp 357 370 2018 12 m schlichtkrull t n kipf p bloem r van den berg i titov and m welling modeling relational data with graph convolutional networks in the semantic web springer international publishing 2018 pp 593 607 13 y cen x zou j zhang h yang j zhou and j tang representation learning for attributed multiplex heterogeneous network in sigkdd 2019 pp 1358 1368 14 m schlichtkrull t n kipf p bloem r van den berg i titov and m welling modeling relational data with graph convolutional net works in the semantic web cham springer international publishing 2018 pp 593 607 15 a halu m de domenico a arenas and a sharma the multiplex network of human diseases npj systems biology and applications vol 5 no 1 pp 1 12 2019 16 j gilmer s s schoenholz p f riley o vinyals and g e dahl neural message passing for quantum chemistry in proceedings of the 34 th international conference on machine learning ser proceedings of machine learning research d precup and y w teh eds vol 70 pmlr 06 11 aug 2017 pp 1263 1272 17 c yang a pal a zhai n pancha j han c rosenberg and j leskovec multisage empowering gcn with contextualized multi embeddings on web scale multipartite networks in sigkdd 2020 p 2434 2443 18 r ying r he k chen p eksombatchai w l hamilton and j leskovec graph convolutional neural networks for web scale rec ommender systems in sigkdd 2018 pp 974 983 19 z lin m feng c d santos m yu b xiang b zhou and y bengio a structured self attentive sentence embedding arxiv vol abs 1703 03130 2017 20 r a rossi n k ahmed e koh s kim a rao and y abbasi yadkori a structural graph representation learning framework in wsdm houston tx usa 2020 pp 483 491 21 d clevert t unterthiner and s hochreiter fast and accurate deep network learning by exponential linear units elus in 4 th international conference on learning representations iclr 2016 san juan puerto rico may 2 4 2016 conference track proceedings y bengio and y lecun eds 2016 22 l tang s rajan and v k narayanan large scale multi label classification via metalabeler in proceedings of the 18 th international conference on world wide web ser www 09 new york ny usa association for computing machinery 2009 p 211 220 online available https doi org 10 1145 1526709 1526738 23 r milo s shen orr s itzkovitz n kashtan d chklovskii and u alon network motifs simple building blocks of complex networks science vol 298 no 5594 pp 824 827 2002 24 n k ahmed j neville r a rossi and n duffield efficient graphlet counting for large networks in 2015 ieee international conference on data mining ieee 2015 pp 1 10 25 d kingma and j ba adam a method for stochastic optimization iclr 12 2014 https doi org 10 1145 1526709 1526738 appendix in the appendix we describe the implementation details of our proposed model along with details of the experiments conducted we provide detailed descriptions of datasets and the parameter configurations for all methods utilized herein a implementation details the experiments were conducted using a windows pc with an intel r core tm i 7 7600 cpu 2 80 ghz 32 gb ram and an nvidia 2060 super 8 gb our models were imple mented using pytorch 1 8 17 and dgl 0 6 18 in python 3 8 our experimental code may be separated into two components random walk generation and model training evaluation the random walk component of our model uses dgl s hetero geneous network random walk functionality with reference to gatne s pytorch implementation 9 and a reference im plementation of gatne t using dgl 10 these references along with references to graphsage 11 and a reference im plementation of graphsage in dgl 12 were utilized to de velop our model implementation and to develop the training procedure we utilize the relgraphconv implementation of r gcn in dgl 13 and the dgl implementation of han along with the author s original repository 14 our evaluation of model performance uses functions from scikit learn 15 in cluding roc auc score f 1 score precision recall curve and auc model parameters are optimized using stochastic gradient descent and updated using the adam optimizer 25 for the amazon and youtube results we report the results as presented by 13 and utilize the same model parameters for our experiments on the twitter and tissue ppi datasets a parameter configuration we set the hidden and em bedding dimension d for all experiments to 200 the number of random walks for each node is 20 and the length of each walk is 10 the sliding window size for node contexts is set to 5 we use 5 negative samples for each training sample the number of training epochs is capped at 50 and the model will stop early if the validation roc auc does not improve for 3 consecutive epochs the dimension of the attention used in the model is set to 20 we utilize the default adam optimizer with learning rate set to 0 001 for the twitter dataset the number of k levels was set to 1 for the remaining experiments k was set to 2 levels of aggregation b baseline comparisons for all baseline models the embedding size is set to 200 for random walk methods we use 20 random walks of length 10 with sliding window of size 5 the number of skip gram iterations is set to 100 we use the author s code repository for 7 https pytorch org 8 https www dgl ai 9 https github com thudm gatne 10 github com dmlc dgl tree master examples pytorch gatne t 11 https github com williamleif graphsage 12 https github com dmlc dgl tree master examples pytorch graphsage 13 https docs dgl ai api python nn pytorch html relgraphconv 14 https github com jhy 1993 han 15 https scikit learn org stable table v original datasets dataset nodes edges relations amazon 312 320 7 500 100 4 twitter 456 626 15 367 315 4 youtube 15 088 13 628 895 5 tissue ppi 4 510 3 666 563 107 node 2 vec deepwalk mne and gatne and the dgl im plementations of r gcn r graphsage and han training and evaluation for all graph neural network models follows the same random walk based procedure described in section iv b node 2 vec 4 we utilize the code from the author s github repository 16 parameter p is set to 2 and parameter q is set to 0 5 deepwalk 3 we utilize the code from the author s github repository 17 r graphsage 1 we implement a graphsage model in dgl using the dgl sageconv layer r gcn 12 we implement an r gcn model in dgl using the dgl relgraphconv layer mne 8 we utilize the code from the author s github repository 18 gatne 13 we utilize the code from the author s github repository 19 c datasets we utilize four publicly available datasets for our experi ments table v describes the statistics of the original datasets we utilize the processed datasets for amazon and youtube from 13 and utilize their train val test splits for the amazon and youtube experiments results for node 2 vec deepwalk mne and gatne were reproduced from 13 experiments with r gcn r graphsage han and rahmen were re peated for five trials for the twitter dataset similarly five trials were conducted using our train val test split for each model variant for the tissue ppi dataset the transductive experiments were conducted with 5 fold cross validation because of the necessity of training a separate model for each layer of the network experiments were conducted using a single cross validation split for deepwalk and node 2 vec mne did not finish training within 24 hours for the twitter and tissue ppi datasets for the inductive experiment five trials were repeated for r gcn r graphsage gatne han and rahmen 16 https github com aditya grover node 2 vec 17 https github com phanein deepwalk 18 https github com hkust knowcomp mne 19 https github com thudm gatne i introduction ii related work iii problem definition iv methodology iv a rahmen framework iv b model optimization v experiments v a datasets v b baseline comparisons v c transductive experiment results v d inductive experiment results v e self attention explainability v f graph motif features v g ablation study parameter sensitivity vi conclusion references appendix a implementation details b baseline comparisons c datasets