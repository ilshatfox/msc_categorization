optimised allgatherv reduce scatter and allreduce communication in message passing systems andreas jocksch 1 noe ohana 2 emmanuel lanti 2 vasileios karakasis 1 and laurent villard 2 1 cscs swiss national supercomputing centre via trevano 131 ch 6900 lugano switzerland 2 ecole polytechnique fe de rale de lausanne epfl swiss plasma center spc ch 1015 lausanne switzerland abstract collective communications namely the patterns allgatherv reduce scatter and allreduce in message passing systems are opti mised based on measurements at the installation time of the library the algorithms used are set up in an initialisation phase of the com munication similar to the method used in so called persistent collec tive communication introduced in the literature for allgatherv and reduce scatter the existing algorithms recursive multiply divide and cyclic shift bruck s algorithm are applied with a flexible number of communication ports per node the algorithms for equal message sizes are used with non equal message sizes together with a heuristic for rank reordering the two communication patterns are applied in a plasma physics application that uses a specialised matrix vector multiplication for the allreduce pattern the cyclic shift algorithm is applied with a prefix operation the data is gathered and scattered by the cores within the node and the communication algorithms are applied across the nodes in general our routines outperform the non persistent counterparts in es tablished mpi libraries by up to one order of magnitude or show equal performance with a few exceptions of number of nodes and message sizes keywords mpi collective communication allgatherv reduce scatter allre duce 1 introduction collective communication is a component of the message passing interface mpi library 14 while point to point communication provides basic functionality collective communication can accommodate more complex algorithms inside the ar x iv 2 00 6 13 11 2 v 1 cs d c 2 3 ju n 20 20 library these algorithms can be very efficient with respect to the execution time 30 persistent collective communication which corresponds to communication patterns repeatedly called plays an increasingly important role it allows for even more sophisticated optimisations to be provided within the library which are currently with a setup of the algorithm in every communication call ineffi cient due to the expensive initialisation phase the implementation of persistent communication as non blocking communication has been discussed in the liter ature 15 24 16 in this contribution we introduce optimised collective commu nication for the patterns allgatherv reduce scatter and allreduce with the additional initialisation phase without restriction of the generality of our approach our collectives are blocking overall we obtain speedups of up to one order of magnitude with respect to cray mpi and a factor of five for mvapich more specifically we generalise the recursive multiplication division and cyclic shift algorithms for allreduce and recursive scatter to allow for different factors for different steps as done in 10 for allreduce only using recursive exchange in this way the underlying algorithms are adjusted for the particular network based on measurements this procedure provides the majority of the performance improvement observed for the different cases additionally for allgatherv and reduce scatter with non equal message sizes we apply a heuristic for rank reordering in order to use the recursive mul tiplication division and cyclic shift algorithms in an efficient way at this point we achieve speedups of 20 the rank order is determined in the initialisation phase of the communication furthermore we generalise for allreduce the prime factor decomposition for recursive multiplying 10 to a factorisation with multiple consecutive calls of bruck s algorithm the prime factors are combined using a greedy approach for the case of the allreduce operation bruck s algorithm is further optimised by storing the results of the prefix operation instead of the actual data this allows shorter messages to be communicated today supercomputers are typically composed of many connected shared memory nodes which provide fast communication between processor cores on the same node and slow communication between cores on different nodes this property has been considered for optimising communication algorithms by sev eral authors 31 33 22 34 23 18 3 good speedups compared to standard imple mentations mpich mvapich openmpi have been shown however only part of this work has been included in publicly available implementations we exploit the shared memory on the node by gathering and scattering messages between cores on the node before and after sending them over the network re spectively as done in the literature in order to accommodate all optimisations efficiently we generate a bytecode in the initialisation phase which is interpreted in the execution phase as demonstrated in 18 subsequently we present in sec 2 the basic algorithms of our contribution and show in sec 3 1 and 3 2 optimised routines for allgather and reduce scatter block respectively the heuristic for non equal message sizes is introduced in sec 3 3 furthermore in sec 3 4 an optimised routine for allreduce is introduced how the parameters of the algorithms are determined based on measurements at the library s installation time is discussed in sec 4 the implementation details are discussed in sec 5 benchmarks made on a drag onfly and on an infiniband network are presented in sec 6 in sec 7 the routines allgatherv and reduce scatter are applied to matrix vector multiplications of a discrete fourier transform dft fourier filter of a plasma physics ap plication namely the orb 5 global electromagnetic particle in cell gyrokinetic turbulence code 19 related work is reviewed in sec 8 and finally we draw our conclusions in sec 9 2 background several network topologies have emerged during the years fat tree hypercube torus and dragonfly are some examples beside their topology networks are characterised by other properties like bandwidth and latency which determine their performance simplified models like the logp model 10 are applied the network properties lead to various different algorithms for collective communica tion e g recursive multiplying bruck s algorithm 5 and the ring algorithm for allgather operations but also store and forward algorithms for personalised communication with small message sizes the networks we are optimising for are the dragonfly network of a cray xc 40 knl and an infiniband network al though our algorithms including the implementation are also efficient on other network architectures for the optimisation of collective communication we consider mainly the literature about a fully connected network with a simple bandwidth latency model for the communication cost the network is assumed to have multiple ports for the communication the ports are the connections of a node to the other nodes all algorithms discussed are assumed to operate between nodes and only optionally between cores of the same node data exchange or data rearrangement within the node is assumed to have zero cost for our simple model the basic algorithms in this contribution are recursive multiplication di vision and cyclic shift fig 1 figure 2 shows the data arrangement for the fig 1 recursive multiplying dividing doubling halfing left versus cyclic shift also radix 2 right recursive multiplication and cyclic shift algorithms the top blocks show the buffers filled with the initial data in the two execution steps for radix 2 from 0 1 2 3 0 1 2 3 0 1 2 3 0 0 1 1 2 2 3 3 0 0 1 1 2 2 3 3 0 0 0 1 1 1 2 2 2 3 3 3 1 1 2 2 3 3 0 0 2 3 0 1 3 0 1 2 n 0 n 0 n 1 n 1 n 2 n 2 n 3 n 3 fig 2 scheme of recursive multiplying left and cyclic shift right radix 2 initial data top after step 1 middle after step 2 bottom nodes n 0 n 3 top to bottom the buffers are filled further with the data communicated from the other nodes the recursive multiplying algorithm has the advantage that with the last step the data is at the target and no local rearrangement on the node is necessary as for the cyclic shift algorithm it is also an option to communicate the source data top to the destination bottom directly using 1 step with 3 so called substeps for recursive multiplication division s logr p steps are re quired for p nodes and a radix r rs p within a step r 1 messages need to be sent to different nodes which can be done in our nomenclature with r 1 ports 3 adaptations of the algorithms we take the shared memory of the nodes into account and execute our algorithms according to the following steps i rearrangement of the data of all tasks on the node locally in a shared memory segment ii communication of the single node data to all nodes with our allgatherv reduce scatter or allreduce algorithm and iii distribution of the data to all tasks on the node locally 3 1 allgather the allgather operation transmits from every participating rank a piece of in formation to every rank thus at the end of the operation every rank contains the same information which is the collected data of all ranks here we discuss equal message sizes for non equal ones see sec 3 3 there are several options conceivable to perform the operation in the literature the most commonly used ones which reduce the number of communication steps are based on recursive multiplying typically doubling or cyclic shift bruck s algorithm 5 see fig 1 in difference to the naive algorithm which sends all information directly these algorithms do not send the information directly from the source to the destination rank but apply forwarding thus the amount of data sent through the network remains unchanged but the number of communication steps is reduced in fig 1 the algorithms are based on radix 2 at every step the information on every task is doubled see fig 2 the data of both communication partners from before the step is on both partners after the step we would like to emphasise that recursive multiplying and cyclic shift can be performed with radixes larger than two 5 28 or different factors for different steps figure 3 shows a communication done in two steps with factors 5 and 3 we speak about fig 3 recursive multiplying dividing with factors 5 and 3 factors f 1 f 2 fs p since the formula with the radix rs p is not valid in this case the naive algorithm is equivalent to recursive multiplying or cyclic shift with the radix of number of nodes any combination of these algorithms seems to be possible but we will not discuss the combination of cyclic shift and recursive multiplying further within the steps the exchange of messages can be done in parallel with a number of ports equal to the factor minus one since the size of the messages grows from step to step we consider this flexibility as essential and use a different number of ports for the different steps of these two algorithms see sec 4 the algorithmic complexity for equal factors fi r is tcomm logr p p 1 r 1 p n 1 as shown in 5 where tcomm is the time spent in communication n p the number of bytes sent per node assuming equal message sizes p the number of nodes participating r the radix of the algorithm the time required for a single step and the time required for a single byte sent per node 3 2 reduce scatter block the optimisation of reduce scatter operations has been the key aspect of sev eral studies 32 4 for reduction operations one distinguishes between algorithms for commutative operations and non commutative operations see 30 and ref erences therein in this contribution we consider commutative reduction oper ations only the cyclic shift algorithm known from allgather has also been applied in a similar way to reduce scatter block 4 however we believe that it should be formalised as the allgather algorithm the reduce scatter opera tion can be considered as the reversed allgatherv operation in the same way as reduce is the reversed operation of broadcast therefore the same algorithms are applied in reversed order recursive division and cyclic shift figure 4 shows an example of the reversed bruck algorithm there is one major difference n 0 n 1 n 2 n 3 n 4 00 10 20 30 40 11 21 31 41 01 22 32 42 02 12 33 43 03 13 23 44 04 14 24 34 n 0 n 1 n 2 n 3 n 4 10 20 30 21 31 41 32 42 02 43 03 13 04 14 24 00 01 11 12 22 23 33 34 44 40 n 0 n 1 n 2 n 3 n 4 00 01 03 11 12 14 22 23 20 33 34 31 44 40 42 10 13 21 24 32 30 43 41 04 02 n 0 n 1 n 2 n 3 n 4 00 01 03 04 02 11 12 14 10 13 22 23 20 21 24 33 34 31 32 30 44 40 42 43 41 fig 4 cyclic shift for reduce scatter radix r 2 nodes n 0 n 4 numbers 0 4 are messages to be reduced on the corresponding destination node subscripts 0 4 indicate the source node is the reduction operation however while in the allgatherv case buffers might be used for sending with multiple ports at the same time this is only possible with an intra node reduc tion for the reduce scatter case thus the memory requirement is higher for reduce scatter since we assume that the receiving rank first gets the data in an empty buffer and second performs the arithmetic operation thus the memory requirements are increasing with an increasing number of ports as the algo rithms for allgatherv and reduce scatter differ in the direction of execution only the algorithmic complexity is the same for both cases except that the cost of reduction needs to be added for reduce scatter it is tcomm logr p p 1 r 1 p n p 1 r 1 p n 2 where is the computational cost per byte for the reduction 3 3 non equal message sizes for the collective communications allgather and reduce scatter block with non equal message sizes allgatherv reduce scatter the principle that every rank performs the same number of operations with the same message sizes which is due to symmetry does not apply any more this gives room for optimisations however in our approach we will leave the basic algorithms unmodified we exploit the option of rank reordering for the algorithm not for the network our heuristic for non equal message sizes is to pair small messages with large messages in the different communication steps fig 5 the different ranks are 1 n 0 3 n 1 6 n 2 9 n 3 3 n 16 n 2 1 n 09 n 3 3 n 16 n 21 n 09 n 3 h h h h hhj s sw s sw step 1 step 2 fig 5 pairing of messages nodes numbers 1 3 6 and 9 are sizes of the messages superscripts n 0 n 3 are the nodes grouped in a tree like order for every communication step for an odd number of messages the largest message is taken out and remains for the rest of the messages as for an even number of messages the smallest one will be paired with the largest one the second smallest one with the second largest one and so on the two messages within one pair are sorted the sums of the message sizes of the pairs become the message sizes of the next step for example in fig 5 the nodes will be ordered n 1 n 2 n 0 n 3 while for equal message size recursive multiplying and cyclic shifting requires the same execution time for non equal message sizes it does not the example in fig 6 shows both algorithms applied to reordered messages of size 1 1 0 2 at the start of the communication with an communication time tcomm 4 for recursive multiplying left and tcomm 5 the cyclic shift right assuming zero latency and a bandwidth of one in this case but not in general all arrangements other than the one presented for the recursive multiplying algorithm give tcomm 5 and for the cyclic shift all arrangements give tcomm 5 if zero message sizes occur the algorithms can be interpreted in an alternative way an example is the ordered messages of size 0 1 0 1 0 1 0 1 the algorithm with radix 2 corresponds to four scatter operations between pairs of two nodes followed by two parallel allgather operations with four members each in the current implementation the rank reordering procedure is executed redundantly on all nodes for each pairing step using the quicksort algorithm thus the algorithmic complexity of the initialisation phase is treorder p log 2 p 2 3 where is the time required for a basic operation compare and swap if necessary of the sorting algorithm and p the number of nodes other solutions for the sorting as a distributed sorting can be used 17 which become relevant for many mpi tasks 2 in order to reduce the algorithmic complexity of the initialisation n 0 n 1 n 2 n 3 tstep 1 tstep 2 tstep 1 tstep 2 fig 6 modelled execution for non equal message sizes for recursive doubling left and cyclic shift right with radix 2 nodes n 0 n 3 with initial message sizes 1 1 0 2 execution times of the two substeps tstep 1 and tstep 2 proportional to the message size different hashes indicate different message tags the longest message horizontal extent determines the communication time otherwise the initialisation would dominate from a very large number of tasks and above see eqns 1 2 and 3 3 4 allreduce we follow the literature and base our allreduce collective operation for small messages on the same algorithm as allgatherv a naive implementation would use the allgather algorithm without any modification as illustrated in figure 7 left for cyclic shift we assume that the reduction operation is a sum here a a a a a a a a a a a 9 9 9 9 9 9 9 9 9 9 0 0 0 0 0 0 0 0 0 0 8 8 8 8 8 8 8 8 8 1 1 1 1 1 1 1 1 1 7 7 7 7 7 7 7 7 2 2 2 2 2 2 2 2 6 6 6 6 6 6 6 3 3 3 3 3 3 3 5 5 5 5 5 5 4 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 3 3 3 3 6 6 6 6 2 2 2 7 7 7 1 1 8 8 0 9 n 0 n 1 n 2 n 3 n 4 n 5 n 6 n 7 n 8 n 9 na 0 i 0 i 1 i 0 i 2 i 0 i 3 i 0 i 7 i 0 i a i 0 i 1 i 1 i 2 i 1 i 3 i 1 i 4 i 1 i 8 i 1 i a i 1 i 0 i 0 i a i a i a i a i 0 i 0 i a i a i 1 i 0 i a i a i 2 i 0 i a i a i 6 i 0 i a i a i 9 i 0 i n 0 n 1 na fig 7 cyclic shift algorithm adapted from allgather nodes n 0 na with messages 0 a hexadecimal notation radix 2 horizontal lines indicate the end of every step x are the lines required for allreduce left for sum reduction inclusive scans from top to bottom which are actually stored and communicated right we modify the scheme and do not store the values at the lines but column wise the partial sum inclusive scan from the top to the bottom fig 7 right while the l th line shifted by k columns to the left is the l k th line in the original scheme fig 7 left in our modified version for a block of lines from 1 st to n th shifted by k columns to the left and k lines to the bottom the prefix sum for the shifted lines is computed by adding the prefix sum of line k 1 fig 7 right this idea allows for less lines to be communicated since for computing the final result on the bottom line for r 2 only the lines which are marked with an x on the left are required the rest of the lines are not needed and are displayed for the illustration of the algorithm only in case of complete steps e g for a radix of r 2 and 2 n nodes the algorithmic complexity becomes equivalent to the one of the binary exchange algorithm in the more general case if always f 1 1 f 2 1 fi 1 lines according to the non optimised algorithm are communicated including the last step only one line per substep needs to be communicated this case corresponds to the approach of 29 where the node count is decomposed in a product of prime numbers the message exchange is done for each single factor separately with a reduction followed in 29 this special case was applied to the recursive exchange algorithm for non 2 n nodes but a radix r 2 more lines need to be communicated in every step than the last line this is due to the incomplete last step of the cyclic shift the additional lines double at most the data volume the generalisation to non equal arbitrary factors is straightforward it might be efficient to set fi 2 see sec 4 as discussed already the allreduce communication can be executed inde pendently for every factor of the node count therefore the node count is de composed in prime factors if the prime factors are smaller than a target factor fi e g fi 13 they are combined according to a greedy approach for prime factors much larger than the target factor fi e g fi 167 we apply cyclic shift operations with multiple steps for every prime factor i e two factors 13 for 167 for long messages we use rabenseifner s algorithm 30 and perform a reduce scatter followed by an allgatherv these two routines were discussed in the previous sections with the cyclic shift algorithm for these routines we are not bound to any particular node count such as the 2 n used in the liter ature in this case the algorithmic complexity is the sum of the complexity of reduce scatter and allgatherv alternative algorithms have been described in the literature e g the binary blocks algorithm 30 this algorithm considers the node count as sum of 2 ni parts which are executed independently and are reduced with each other at the end for example it requires 4 steps for 7 nodes for very short messages our algorithm corresponds to allgather and it outperforms the binary block algo rithm since it requires less steps 3 steps for 7 nodes comparisons of all message sizes and possible combinations of the algorithms are still under investigation 4 parametrisation the simple bandwidth latency network model does not give any indication which factors fi or radix r to choose for bandwidth dominated communication fur thermore it is not always clear how many ports per node are available in order to choose the optimal parameters we apply a tuning approach at the instal lation phase of the library measurements of communication times are done for different message sizes based on that the factors fi are chosen for all possible combinations of factors the communication time is estimated from interpolations of the measurements performed during installation the algorithmic complexity of this try all method is nop f log 2 p i max p 2 n n n 4 the total number of messages between nodes is in any case smaller with our shared memory approach than for a naive implementation since messages are merged this is advantageous with respect to network congestion the option of splitting the messages between nodes and using multiple senders and receivers for their transmission is not exploited here however the load on the network might still affect the communication that is why the measurement runs are done with different loads on the network in the background using the gpcnet benchmark 9 the parameters of the algorithms can be adapted to the network load our experiments show that it is efficient to apply high and low radixes for short messages and long messages respectively this is supported by the findings in 26 where a saturation effect for long messages is described the backload of the network boosts this effect the r 1 ports can be physical ports in the sense of multiple cores performing communication or logical ports if one core performs multiple non blocking point to point communications in this contribution we restrict ourselves to physical ports see also 18 if the factors fi allow the recursive multiply divide is applied otherwise the cyclic shift since the former seems to be advantageous for non equal message sizes sec 3 3 we apply one exception to the tuning approach the target factor fi is fixed to the number of cores per node plus one for allreduce with small message sizes 5 implementation details the separation of the initialisation phase of the algorithms from the actual com munication is beneficial since a significant amount of computation has to be done in order to determine the parameters algorithms single step message sizes and communicating ranks the execution time of the initialisation is approxi mately independent from the message size and thus not negligible especially for short messages see sec 6 the cost of initialisation is amortised by repeated calls of the execution routines which are highly optimised therefore we have chosen to encode the whole algorithm in a special bytecode in the initialisation phase without any ifs jumps 18 in the execution phase this bytecode is inter preted we have many algorithmic choices in the code generation phase without disadvantage in the execution phase our collective communication routines are based on the mpi point to point communication routines mpi irecv mpi isend mpi waitall and mpi sendrecv the latter one is used for all reduce scatter implementations only since the algorithms are purely deterministic numerical results of the reduc tions are bit reproducible current limitations of the implementation are the following the same num ber of cores must be involved in the collective communication on all nodes it is not allowed to let the last node partially idle if a certain overall number of cores is required which is a realistic use pattern in the automatic determination of the algorithmic parameters no tradeoff between performance and memory us age is implemented inter node communicators 20 are not implemented yet non contiguous data types are not supported it is not necessary to rewrite our routines for equal message sizes allgather and reduce scatter block since they would not perform better than the ver sions for non equal message sizes but wrappers might be convenient for this feature furthermore the operations bcast and reduce are covered by setting up allgatherv and reduce scatter respectively with all message sizes equal to zero except of one then our algorithms simplify to a tree algorithm with out explicit implementation in difference to the interface planned in mpi 4 0 which foresees non blocking persistent collective communication only our col lective communication is blocking for the current blocking implementation we see various applications already besides the one of plasma physics shown the source code of our collective communication implementation will be made publicly available on github if the contribution will be accepted 6 benchmarks benchmarks are made on an empty cray xc 40 knl cluster comprising 64 core intel r xeon phi tm cpu 7230 processors running at 1 30 ghz the proces sors are configured in flat memory mode with quadrant clustering using mc dram the network topology is dragonfly with aries routing furthermore we utilise an empty infiniband cluster using 17 nodes with two 12 core haswell tm e 5 2650 v 3 2 66 ghz cpus we follow the osu microbenchmarks 6 which were adapted for our com munication routines figure 8 left shows the communication time in relation to the message size for our persistent allgatherv routine and for the non persistent mpi allgatherv routine for 9600 cores on 160 nodes for our routine the cray nodes were used in two different ways either groups of 12 cores each forming 5 virtual nodes per knl or one group of 60 cores per knl the 12 cores per virtual node were chosen in order to mimic systems with 12 cores per node the message sizes refer to the message of the send buffer before communica tion our routine is faster than the one of cray mpi cray mpich 7 7 10 on the cray network especially for small message sizes for very large message sizes not shown the performance becomes equal between the two implementations for the infiniband inf network our routine mostly outperforms mvapich 2 2 for both one group of 24 cores per node and two groups of 12 cores per node figure 8 right shows the same properties for reduce scatter while from here on we restrict ourselves to virtual nodes per knl dual socket haswell with 100 1000 10000 100000 1 x 10 6 1 10 100 1000 ti m e s message size bytes allgatherv 60 cores allgatherv 12 cores cray mpi allgatherv 100 1000 10000 100000 1 x 10 6 1 x 10 7 10 100 1000 ti m e s message size bytes reduce scatter cray mpi reduce scatter 10 100 1000 10000 1 10 100 1000 ti m e s message size bytes allgatherv 24 cores allgatherv 12 cores mvapich allgatherv 100 1000 10000 100000 10 100 1000 ti m e s message size bytes reduce scatter mvapich reduce scatter fig 8 allgatherv left and reduce scatter right on 160 nodes with 9600 tasks cray top and on 17 nodes with 408 tasks infiniband bottom 12 cores each for all subsequent benchmarks contrary to the definition of the osu microbenchmarks the message size refers to the message in the receive buffer after communication the speedup of our routine compared to cray mpi cray network and to mvapich inf network is significant we believe that besides our algorithmic improvements implemented for the communication also the local reduction on the node is done more efficiently in our routines figure 9 shows the results for allreduce for small and large message sizes our routines outperform the cray mpi but for medium message sizes about 10 kb 100 kb the routines show approximately equal performance our routine is mostly faster than mvapich for allgatherv on the cray network for the smallest message size the initialisation is 5700 times more expensive than the single execution of the algorithm for the longest message it is a factor of 56 figures 10 11 and 12 show the performance of the different routines for a varying number of nodes on the cray network with a fixed message size our results are comparable with the speedups of end et al 11 who have implemented a k port allreduce and have made a comparison with openmpi 1 6 5 our routines outperform cray mpi in all cases except reduce scatter at small message sizes for a few number of nodes the peaks in all graphs especially in the ones for small messages sizes are intermittent slowdowns of the system and not caused by the algorithms 10 100 1000 10000 100000 1 x 10 6 10 100 1000 10000 100000 1 x 10 6 1 x 10 7 ti m e s message size bytes allreduce cray mpi allreduce 10 100 1000 10000 100000 1 x 10 6 10 100 1000 10000 100000 1 x 10 6 1 x 10 7 ti m e s message size bytes allreduce mvapich allreduce fig 9 allreduce on 160 nodes with 9600 tasks cray left and on 17 nodes with 408 tasks infiniband right 0 100 200 300 400 500 600 700 0 200 400 600 800 1000 1200 1400 1600 1800 2000 ti m e s number of tasks allgatherv cray mpi allgatherv 0 2000 4000 6000 8000 10000 12000 14000 16000 0 200 400 600 800 1000 1200 1400 1600 1800 2000 ti m e s number of tasks allgatherv cray mpi allgatherv fig 10 allgatherv with 8 bytes left and 4096 bytes right 0 50 100 150 200 250 300 350 400 450 500 0 200 400 600 800 1000 1200 1400 1600 1800 2000 ti m e s number of tasks reduce scatter cray mpi reduce scatter 0 10000 20000 30000 40000 50000 60000 70000 80000 90000 100000 0 200 400 600 800 1000 1200 1400 1600 1800 2000 ti m e s number of tasks reduce scatter cray mpi reduce scatter fig 11 reduce scatter with 8 bytes left and 4096 bytes right 0 20 40 60 80 100 120 140 160 180 0 200 400 600 800 1000 1200 1400 1600 1800 2000 ti m e s number of tasks allreduce cray mpi allreduce 20000 40000 60000 80000 100000 120000 140000 160000 180000 200000 220000 0 200 400 600 800 1000 1200 1400 1600 1800 2000 ti m e s number of tasks allreduce cray mpi allreduce fig 12 allreduce with 8 bytes left and 33554432 bytes right 7 fourier filter the optimised allgather and reduce scatter routines are applied to a fourier filter which is part of the plasma physics application orb 5 19 its task is to transform data on a regular 3 d mesh which is periodic in two directions from real space to spectral space in these two periodic directions and to select a fraction of modes to be processed further the reverse spectral space to real space transformation is also part of the procedure the data arrangement of the code is the following the application uses a toroidal computational domain for parallelisation a 1 d domain decomposition in toroidal direction and an additional domain cloning technique the filter reduces the number of fourier modes to a band in poloidal toroidal mode numbers for general configurations the number of fourier modes processed further in the field solver is not a multiple of the number of nodes allocated the messages have non equal size it might even happen that part of the nodes are idling during the field solve procedure and will either receive or send messages only figure 13 illustrates the fourier modes retained or set to zero for a certain radial coordinate the filter varies in radial direction two a 11 a 12 0 0 a 21 a 22 a 23 0 a 32 a 33 a 34 a 76 a 77 a 78 0 a 87 a 88 a 89 0 0 a 98 a 99 fig 13 fourier modes retained in the r const plane options are implemented in the code a solution with fast fourier transforms ffts and one with a dft matrix other ones are possible in the dft matrix approach the real space vector r is transformed to the spectral space vector s by multiplying with the matrix f s fr 5 this operation with n 2 complexity is efficient in our case since the transfor mation matrix f is very sparse here the start is a fft in poloidal direction followed by the matrix vector multiplication thus the matrix f 0 l 0 n l 1 n l n 1 n m 0 n m 1 n m n 1 n 0 n e i 2 n 6 transforms a single line in toroidal direction only the values necessary to be com puted are communicated the computation and communication from r which is distributed over the nodes to s is done such that s is distributed as equal as possible over the nodes for the backward transformation the reverse operations apply benchmarks are performed with a simplified version of the plasma physics application 25 parameters are n 512 n 1024 nr 512 in toroidal poloidal and radial direction respectively with 12 clones and 107 markers the number of timesteps chosen is 10 with the size t 1 apart from the small number of markers this is a typical production run but we note here that the fourier filter operations apply to grid data only and are therefore independent of the number of markers only two fourier modes are kept in toroidal direction thus two messages of 90464 bytes are gathered and distributed to all nodes participating and the reverse is done for the reduction figure 14 left shows the performance of the allgatherv and the reduce scatter routines in comparison to the cray mpi reference implementation in order to quantify the effect of rank reordering we included graphs fig 14 right for a worst case ordering messages sorted according to size for the cray mpi reference implementation the rank orders are chosen randomly for up to 128 12 cores all benchmarks were done on physical nodes with 12 cores per node higher core counts were realised on 128 nodes with multiple groups of 12 cores on each node 8 related work many efforts were made in order to optimise the collective communication for message passing especially by exploiting the shared memory on the nodes alma si et al 1 optimised the collectives operations for the bluegene l chakraborty et al 7 developed mpi collectives using shared memory on the 0 01 0 1 1 1000 to ta l e x e c u ti o n t im e s number of cores allgatherv reduce scatter cray mpi allgatherv cray mpi reduce scatter 0 01 0 1 1 1000 to ta l e x e c u ti o n t im e s number of cores allgatherv reduce scatter fig 14 execution times of collective operations of plasma physics application rank reordering and reference left and without rank reordering right nodes with kernel assistance chan et al 8 reimplemented all mpi collective communication routines faraj and yuan 12 implemented mpi collective com munication with an autotuning approach graham and shipman 13 optimised shared memory collective communication karwande et al 21 developed a mpi library which selects parameters of the algorithms during compile time of the code patarasuk and yuan 27 proposed a bandwidth optimal allreduce algo rithm for smp clusters 9 conclusions in this paper we optimised the collective communication operations allgatherv reduce scatter and allreduce where we made extensive use of an initialisa tion phase the initialisation phase allowed for several optimisations namely an extensive choice of algorithms such as recursive multiplication division cyclic shifting bruck s algorithm and their derivatives with different factors number of ports substeps for different steps the proper algorithms and their parame ters are chosen according to network performance measurements at the instal lation time of the library for allgatherv and reduce scatter we considered explicitly the occurrence of non equal message sizes in our algorithms with a rank reordering heuristic our allreduce for small messages is based on a prime fac tor decomposition of the number of nodes and allgather for long messages our optimised allgatherv and reduce scatter are consecutively called the existing implementation of cray mpi is outperformed significantly for small and medium message sizes for allgatherv for very large messages the performance is equal between the two options although our reduce scatter is slower than the reference for small messages and a small number of nodes it outperforms the reference clearly for all other cases our allreduce is faster than the existing implementation for short messages and long messages for medium message sizes it shows performance equal to the reference implementation we mostly outperform mvapich with our three routines for non equal message sizes our routines show additional speedups if the ranks are reordered this has been shown on the example of a plasma physics application the implementation of our algorithms as non blocking versions is future work references 1 alma si g heidelberger p archer c j martorell x erway c c moreira j e steinmacher burow b zheng y optimization of mpi collective commu nication on bluegene l systems in proceedings of the 19 th annual international conference on supercomputing pp 253 262 acm 2005 2 balaji p buntinas d goodell d gropp w kumar s lusk e thakur r tra ff j l mpi on a million processors in european parallel virtual ma chine message passing interface users group meeting pp 20 30 springer 2009 3 bayatpour m hashmi j chakraborty s subramoni h kousha p panda d salar scalable and adaptive designs for large message reduction collectives september 2018 4 bernaschi m iannello g lauria m efficient implementation of reduce scatter in mpi journal of systems architecture 49 3 89 108 2003 5 bruck j ho c t kipnis s upfal e weathersby d efficient algorithms for all to all communications in multiport message passing systems ieee transac tions on parallel and distributed systems 8 11 1143 1156 1997 6 bureddy d wang h venkatesh a potluri s panda d omb gpu a micro benchmark suite for evaluating mpi libraries on gpu clusters september 2012 7 chakraborty s subramoni h panda d k contention aware kernel assisted mpi collectives for multi many core systems in 2017 ieee international con ference on cluster computing cluster pp 13 24 ieee 2017 8 chan e heimlich m purkayastha a van de geijn r collective communica tion theory practice and experience concurr comp pract e 19 13 1749 1783 2007 9 chunduri s groves t mendygral p austin b balma j kandalla k kumaran k lockwood g parker s warren s wichmann n wright n gpcnet designing a benchmark suite for inducing and measuring contention in hpc networks in proc int conf high performance computing networking storage and analysis sc 19 argone national laboratory november 2019 10 culler d karp r patterson d sahay a schauser k e santos e subramo nian r von eicken t logp towards a realistic model of parallel computation in acm sigplan notices vol 28 pp 1 12 acm 1993 11 end v simmendinger c yahyapour r alrutz t butterfly like algorithms for gaspi split phase allreduce international journal on advances in systems and measurements 9 2016 12 faraj a yuan x automatic generation and tuning of mpi collective commu nication routines in proceedings of the 19 th annual international conference on supercomputing pp 393 402 acm 2005 13 graham r l shipman g mpi support for multi core architectures optimized shared memory collectives in european parallel virtual machine message pass ing interface users group meeting pp 130 140 springer 2008 14 gropp w d gropp w lusk e skjellum a using mpi portable parallel programming with the message passing interface vol 1 mit press 1999 15 hoefler t schneider t optimization principles for collective neighborhood com munications in sc 12 proceedings of the international conference on high per formance computing networking storage and analysis pp 1 10 ieee 2012 16 holmes d j morgan b skjellum a bangalore p v sridharan s planning for performance enhancing achievable performance for mpi through persistent collective operations parallel computing 81 32 57 2019 17 jeon m kim d parallel merge sort with load balancing international journal of parallel programming 31 1 21 33 2003 18 jocksch a kraushaar m daverio d optimized all to all communication on multicore architectures applied to ffts with pencil decomposition concurr comp pract e p e 4964 2018 19 jolliet s bottino a angelino p hatzky r tran t m mcmillan b sauter o appert k idomura y villard l a global collisionless pic code in magnetic coordinates comput phys commun 177 5 409 425 2007 20 kang q agrawal a choudhary a liao w k optimal algorithms for half duplex inter group all to all broadcast on fully connected and ring topologies in sc 18 proc int conf high performance computing networking storage and analysis ieee 2018 21 karwande a yuan x lowenthal d k cc mpi a compiled communication capable mpi prototype for ethernet switched clusters in acm sigplan notices vol 38 pp 95 106 acm 2003 22 li s hoefler t hu c snir m improved mpi collectives for mpi processes in shared address spaces cluster computing 17 4 1139 1155 2014 23 li s zhang y hoefler t cache oblivious mpi all to all communications based on morton order ieee trans parall and distr syst 29 3 542 555 2018 24 message passing interface forum mpi a message passing interface standard version 4 0 2019 www mpi forum org mpi 40 25 ohana n jocksch a lanti e tran t brunner s gheller c hariri f villard l towards the optimization of a gyrokinetic particle in cell pic code on large scale hybrid architectures in journal of physics conference series vol 775 p 012010 iop publishing 2016 26 parsons b s accelerating mpi collective communications through hierarchical algorithms with flexible inter node communication and imbalance awareness ph d thesis perdue university 2015 27 patarasuk p yuan x bandwidth optimal all reduce algorithms for clusters of workstations journal of parallel and distributed computing 69 2 117 124 2009 28 qian y afsahi a high performance rdma based multi port all gather on multi rail qsnet ii in 21 st international symposium on high performance com puting systems and applications hpcs 07 pp 3 3 ieee 2007 29 ruefenacht m bull m booth s generalisation of recursive doubling for allre duce now with simulation parallel comput 69 24 44 2017 30 thakur r rabenseifner r gropp w optimization of collective communica tion operations in mpich the international journal of high performance com puting applications 19 1 49 66 2005 31 tipparaju v nieplocha j panda d fast collective operations using shared and remote memory access protocols on clusters in proceedings international parallel and distributed processing symposium pp 10 pp ieee 2003 32 tra ff j l an improved algorithm for non commutative reduce scatter with an application in european parallel virtual machine message passing interface users group meeting pp 129 137 springer 2005 www mpi forum org mpi 40 33 tu b zou m zhan j zhao x fan j multi core aware optimization for mpi collectives in 2008 ieee international conference on cluster computing pp 322 325 ieee 2008 34 venkatesh a potluri s rajachandrasekar r luo m hamidouche k panda d k high performance alltoall and allgather designs for infiniband mic clusters in 2014 ieee 28 th international parallel and distributed processing symposium pp 637 646 ieee 2014 optimised allgatherv reduce scatter and allreduce communication in message passing systems