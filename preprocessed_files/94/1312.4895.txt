ar x iv 1 31 2 48 95 v 1 st at m l 1 7 d ec 2 01 3 technical report 1 recursive compressed sensing nikolaos m freris member ieee orhan o c al student member ieee and martin vetterli fellow ieee abstract we introduce a recursive algorithm for performing compressed sensing on streaming data the approach consists of a recursive encoding where we sample the input stream via overlapping windowing and make use of the previous measurement in obtaining the next one and b recursive decoding where the signal estimate from the previous window is utilized in order to achieve faster convergence in an iterative optimization scheme applied to decode the new one to remove estimation bias a two step estimation procedure is proposed comprising support set detection and signal amplitude estimation estimation accuracy is enhanced by a non linear voting method and averaging estimates over multiple windows we analyze the computational complexity and estimation error and show that the normalized error variance asymptotically goes to zero for sublinear sparsity our simulation results show speed up of an order of magnitude over traditional cs while obtaining significantly lower reconstruction error under mild conditions on the signal magnitudes and the noise level index terms compressed sensing recursive algorithms streaming data lasso machine learning optimization mse i introduction in signal processing it is often the case that signals of interest can be represented sparsely by using few coefficients in an appropriately selected orthonormal basis or frame for example the fourier basis is used for bandlimited signals while wavelet bases are used for piecewise continuous signals with applications in communications for the former and image compression for the latter while a small number of coefficients the authors are with the school of computer and communication sciences e cole polytechnique fe de rale de lausanne epfl ch 1015 lausanne switzerland nikolaos freris orhan ocal martin vetterli epfl ch this work was submitted to ieee transactions on information theory dec 2013 a preliminary version of this work was presented at the 51 st allerton conference 2013 1 march 15 2018 draft http arxiv org abs 1312 4895 v 1 2 technical report in the respective basis may be enough for high accuracy representation the celebrated nyquist shannon sampling theorem suggests a sampling rate that is at least twice the signal bandwidth which in many cases is much higher than the sufficient number of coefficients 2 3 the compressed sensing cs also referred to as compressive sampling framework was introduced for sampling signals not according to bandwidth but rather to their information content i e the num ber of degrees of freedom this sampling paradigm suggests a lower sampling rate compared to the classical sampling theory for signals that have sparse representation in some fixed basis 2 or even non bandlimited signals 3 to which traditional sampling does not even apply the foundations of cs have been developed in 4 5 although the field has been extensively studied for nearly a decade performing cs on streaming data still remains fairly open and an efficient recursive algorithm is to the best of our knowledge not available this is the topic of the current paper where we study the architecture for compressively sampling an input data stream and analyze the computational complexity and stability of signal estimation from noisy samples the main contributions are 1 we process the data stream by successively performing cs in sliding overlapping windows sampling overhead is minimized by recursive computations using a cyclic rotation of the same sampling matrix a similar approach is applicable when the data are sparsely representable in the fourier domain 2 we perform recursive decoding of the obtained samples by using the estimate from a previously decoded window to obtain a warm start in decoding the next window via an iterative optimization method 3 in our approach a given entry of the data stream is sampled over multiple windows in order to enhance estimation accuracy we propose a three step procedure to combine estimates corresponding to a given sample obtained from different windows support detection amounts to estimating whether or not a given entry is non zero this is accomplished by a voting strategy over multiple overlapping windows containing the entry ordinary least squares is performed in each window on the determined support averaging of estimates across multiple windows yields the final estimate for each entry of the data stream 4 extensive experiments showcase the merits of our approach in terms of substantial decrease in both run time and estimation error similar in spirit to our approach are the works of garrigues and el ghaoui 6 boufounos and asif 7 and asif and romberg 8 in 6 a recursive algorithm was proposed for solving lasso based on warm draft march 15 2018 freris et al recursive compressed sensing 3 start in 7 the data stream is assumed sparse in the frequency domain and streaming greedy pursuit is proposed for progressively filtering measurements in order to reconstruct the data stream in 8 the authors analyze the use of warm start for speeding up the decoding step our work is different in that a it both minimizes sampling overhead by recursively encoding the data stream as well as b produces high accuracy estimates by combining information over multiple windows at the decoding step the organization of the paper is as follows section ii describes the notation definitions and the related literature on cs section iii introduces the problem formulation and describes the key components of recursive cs rcs recursive sampling and recursive estimation we analyze the proposed method and discuss extensions in section iv experimental results on resilience to noise and execution time of rcs are reported in section vi ii background this section introduces the notation and definitions used in the paper and summarizes the necessary background on cs a notation throughout the paper we use capital boldface letters to denote matrices e g a and boldface lowercase letters to denote vectors e g x we use xi to denote the i th entry of vector x ai to denote the ith column of matrix a and aij to denote its i j entry the i th sampling instance e g ith window of the input stream ith sampling matrix ith sample is denoted by superscript e g x i a i y i the cardinality of a set s is denoted by s and we use xi as shorthand notation for the infinite sequence xi i 0 1 last we use ex to denote the conditional expectation ex e x b definitions and properties in the following we summarize the key definitions related to compressed sensing definition 1 sparsity for a vector x rn we define the support supp x i xi 6 0 the 0 pseudonorm is x 0 supp x we say that a vector x is sparse if and only if x 0 definition 2 mutual coherence for a matrix a rm n the mutual coherence is defined as the march 15 2018 draft 4 technical report largest normalized inner product between any two different columns of a 9 a max 0 i j n 1 i 6 j a i aj ai 2 aj 2 1 2 definition 3 restricted isometry property let a rm n for given 0 n the matrix a is said to satisfy the restricted isometry property rip if there exists 0 1 such that 1 x 22 ax 22 1 x 22 3 holds for all x rn sparse vectors for a constant 0 sufficiently small 2 the value is called the restricted isometry constant of a for sparse vectors evidently an equivalent description of rip is that every subset of columns of a approximately behaves like an orthonormal system 10 hence ax is approximately an isometry for sparse vectors unfortunately rip is np hard even to verify for a given matrix as it requires n eigendecompositions the success story lies in that properly constructed random matrices satisfy rip with overwhelming probability 2 for example 1 sampling n random vectors uniformly at random from the m dimensional unit sphere 2 2 random partial fourier matrices obtained by selecting m rows from the n dimensional fourier matrix f uniformly at random where f 1 n 1 1 1 2 n 1 n 2 n n 1 2 for ei 2 n 3 random gaussian matrices with entries drawn i i d from n 0 1 m 4 random bernoulli matrices with ai j 1 m 1 m with equal probability or 11 aij 1 with probability 1 6 0 with probability 2 3 1 with probability 1 6 4 draft march 15 2018 freris et al recursive compressed sensing 5 having the added benefit of providing a sparse sampling matrix for the last two cases a satisfies a prescribed for any c 1 m log n with probability p 1 2 e c 2 m where constants c 1 and c 2 depend only on 12 the important result here is that such matrix constructions are universal in the sense that they satisfy rip which is a property that does not depend on the underlying application as well as efficient as they only require random number generation it is typically the case that x is not itself sparse but is sparsely representable in a given orthonormal basis in such case we write x where now is sparse compressed sensing then amounts to designing a sensing matrix rm n such that a is a cs matrix luckily random matrix constructions can still serve this purpose definition 4 coherence let be two orthonormal bases in rn the coherence between these two bases is defined as 2 m n max 1 k j n k j 5 it follows from elementary linear algebra that 1 m n for any choice of and the basis rn n is called the sensing basis while rn n is the representation basis compressed sensing results apply for low coherence pairs 5 a typical example of such pairs is the fourier and canonical basis for which the coherence is 1 maximal incoherence most notably a random basis generated by any of the previously described distributions for m n when orthonormalized is incoherent with any given basis with high probability m 2 log n 2 the sensing matrix can be selected as a row subset of therefore designing a sensing matrix is not different than for the case where x is itself sparse i e in n the identity matrix for ease of presentation we assume in the sequel that in n unless otherwise specified but the results also hold for the general case c setting given linear measurements of vector x rn y ax 6 y rm is the vector of obtained samples and a rm n is the sampling sensing matrix our goal is to recover x when m n this is an underdetermined linear system so for a general vector x it is essentially ill posed the main result in cs is that if x is sparse and cm log n k this is march 15 2018 draft 6 technical report possible equivalently random linear measurements can be used for compressively encoding a sparse signal so that it is then possible to reconstruct it in a subsequent decoding step the key concept here is that encoding is universal while it is straightforward to compress if one knows the positions of the non zero entries the cs approach works for all sparse vectors without requiring prior knowledge of the non zero positions to this end searching for the sparsest vector x that leads to the measurement y one needs to solve min x 0 s t ax y p 0 unfortunately this problem is in general np hard requiring search over all subsets of columns of a 12 e g checking n linear systems for a solution in the worst case d algorithms for sparse recovery since p 0 may be computationally intractable for large instances one can seek to approximate it by other tractable methods in this section we summarize several algorithms used for recovering sparse vectors from linear measurements at the decoding phase with provable performance guarantees 1 basis pursuit cande s and tao 10 have shown that solving p 0 is equivalent to solving the 1 minimization problem min x 1 s t ax y bp for all sparse vectors x if a satisfies rip with 2 2 1 the optimization problem bp is called basis pursuit since the problem can be recast as a linear program solving bp is computationally efficient e g via interior point methods 13 even for large problem instances as opposed to solving p 0 whose computational complexity may be prohibitive 2 orthogonal matching pursuit orthogonal matching pursuit omp is a greedy algorithm that seeks to recover sparse vectors x from noiseless measurement y ax the algorithm outputs a subset of columns of a via iteratively selecting the column minimizing the residual error of approximating y by projecting to the linear span of previously selected columns assuming x is sparse the resulting measurement y can be represented as the sum of at most columns of a weighted by the corresponding nonzero entries of x let the columns of a be normalized to have unit 2 norm for iteration index t let rt denote the residual vector let ct rt be the solution to the least squares problem at iteration t st set of indices and ast the submatrix obtained by extracting draft march 15 2018 freris et al recursive compressed sensing 7 columns of a indexed by st the omp algorithm operates as follows initially set t 1 r 0 y and s 0 at each iteration the index of the column of a having highest inner product with the residual vector i e st argmaxi rt 1 ai is added to the index set yielding st st 1 st since one index is added to st at each iteration the cardinality is st t then the least squares problem ct arg min c rt y t j 1 cjasj 2 is solved in each iteration a closed form solution is ct a stast 1 a sty and the residual vector is updated by rt y t j 1 ctjasj with rt obtained as such the residual vector at the end of iteration t is made orthogonal to all the vectors in the set ai i st the algorithm stops when a desired stopping criterion is met such as y astct 2 for some threshold 0 despite its simplicity there are guarantees for exact recovery omp recovers any sparse signal exactly if the mutual coherence of the measurement matrix a satisfies a 1 2 1 14 both bp and omp handle the case of noiseless measurements however in most practical scenaria noisy measurements are inevitable and we address this next 3 least absolute selection and shrinkage operator lasso given measurements of vector x rn corrupted by additive noise y ax w 7 one can solve a relaxed version of bp where the equality constraint is replaced by inequality to account for measurement noise min x 1 s t ax y 2 8 this is best known as least absolute selection and shrinkage operator lasso in the statistics literature 15 the value of is selected to satisfy w 2 by duality the problem can be posed equivalently 13 as an unconstrained 1 regularized least squares problem min ax y 22 x 1 9 march 15 2018 draft 8 technical report where is the regularization parameter that controls the trade off between sparsity and reconstruction error still by duality an equivalent version is given by the following constrained optimization problem minimize ax y 2 subject to x 1 10 remark 1 equivalent forms of lasso all these problems can be made equivalent in the sense of having the same solution set for particular selection of parameters this can be seen by casting between the optimality conditions for each problem unfortunately the relations obtained depend on the optimal solution itself so there is no analytic formula for selecting a parameter from the tuple given another one in the sequel we refer to both 8 9 as lasso the distinction will be made clear from the context the following theorem characterizes recovery accuracy in the noisy case through lasso theorem ii 1 error of lasso 2 if a satisfies rip with 2 2 1 the solution x to 8 obeys x x 2 c 0 x x 1 c 1 11 for constants c 0 and c 1 where x is the vector x with all but the largest components set to 0 theorem ii 1 states that the reconstruction error is upper bounded by the sum of two terms the first is the error due to model mismatch and the second is proportional to the measurement noise variance in particular if x is sparse and 2 2 1 then x x 2 c 1 additionally for noiseless measurements w 0 0 we retrieve the success of bp as a special case note that the requirement on the restricted isometry constant is identical this assumption is satisfied with high probability by matrices obtained from random vectors sampled from the unit sphere random gaussian matrices and random bernoulli matrices if m c log n where c is a constant depending on each instance 2 typical values for the constants c 0 and c 1 can be found in 2 and 5 where it is proven that c 0 5 5 and c 1 6 for 2 k 1 4 a different approach was taken in 16 where the replica method was used for analyzing the mean squared error of lasso the difficulty of solving p 0 lies in estimating the support of vector x i e the positions of the non zero entries one may assume that solving lasso may give some information on support and this is indeed the case 17 to state the result on support detection we define the generic sparse model definition 5 generic sparse model let x rn denote a sparse signal and i x supp x be its support set supp x i xi 6 0 signal x is said to be generated by generic sparse model if draft march 15 2018 freris et al recursive compressed sensing 9 1 support i x 1 2 n of x is selected uniformly at random and ix 2 conditioned on i x the signs of the non zero elements are independent and equally likely to be 1 and 1 theorem ii 2 support detection 17 assume a c 1 log n for some constant c 1 0 x is generated from generic sparse model c 2 n a 22 log n for some constant c 2 0 and w n 0 2 i if min i ix xi 8 2 log n the lasso estimate obtained by choosing 4 2 log n satisfies supp x supp x sgn x i sgn xi i ix with probability at least 1 2 n 1 2 logn ix n o 1 n 2 log 2 and ax ax 22 c 3 log n 2 with probability at least 1 6 n 2 log 2 n 1 2 log n 1 2 for some positive constant c 3 another result on near support detection or as alternatively called ideal model selection for lasso is given in 18 based on the so called irrepresentable condition of the sampling matrix introduced therein remark 2 algorithms for lasso there is a wealth of numerical methods for lasso stemming from convex optimization lasso is a convex program in all equivalent forms and the unconstrained problem 9 can be easily recast as a quadratic program which can be handled by interior point methods 19 this is the case when using a generalized convex solver such as cvx 20 additionally iterative algorithms have been developed specifically for lasso all these are inspired by proximal methods 21 for non smooth convex optimization fista 22 and sparsa 23 are accelerated proximal gradient methods 21 salsa 24 is an application of the alternative direction method of multipliers these methods are first order methods 19 in essence generalizations of the gradient method for error defined as g x t g x where g x is the objective function of lasso in 9 x t is the estimate at iteration number t and x argminxg x is the optimal soultion the error decays as 1 t 2 for fista sparsa and salsa recently a proximal newton type method was devised for lasso 25 with substantial speedup the convergence rate is globally no worse than 1 t 2 but is locally quadratic i e goes to zero roughly like e ct 2 remark 3 computational complexity in iterative schemes computational complexity is considered at a per iteration basis a interior point methods require solving a dense linear system hence a cost of march 15 2018 draft 10 technical report o n 3 per iteration b first order proximal methods only perform matrix vector multiplications at a cost of o n 2 while the second order method proposed in 25 requires solving a sparse linear system at a resulting cost of o 3 the total complexity depends also on the number of iterations until convergence we analyze this in sec v b note that the cost of decoding dominates that of encoding which requires a single matrix vector multiplication i e o mn operations our approach is generic in that it does not rely on a particular selection of numerical solver it uses warm start for accelerated convergence so using an algorithm like 25 may yield improvements over the popular fista that we currently use in experiments we conclude this section by providing optimality conditions for lasso which can serve in determining termination criteria for iterative optimization algorithms we show the case of unconstrained lasso but similar conditions hold for the constrained versions 8 10 remark 4 optimality conditions for lasso for unconstrained lasso cf 9 define x to be an optimal solution and g a y ax the necessary and sufficient kkt conditions for optimality 19 are gi 2 sgn x i for i x i 6 0 gj 2 for j x j 0 12 as termination criterion we use optimality for some 0 suffieciently small gi 2 sgn x i for i x i 6 0 gj 2 for j x j 0 13 iii recursive compressed sensing we consider the case that the signal of interest is an infinite sequence xi i 0 1 and process the input stream via successive windowing we define x i xi xi 1 xi n 1 14 to be the ith window taken from the streaming signal if x i is known to be sparse one can apply the tools surveyed in section ii to recover the signal portion in each window hence the data stream however the involved operations are costly and confine an efficient online implementation in this section we present our approach to compressively sampling streaming data based on recursive encoding decoding the proposed method has low complexity in both the sampling and estimation parts which makes the algorithm suitable for an online implementation draft march 15 2018 freris et al recursive compressed sensing 11 fig 1 illustration of the overlapping window processing for the data stream x i i 0 1 a problem formulation from the definition of x i rn we have x i 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 x i 1 0 0 0 1 xi n 1 15 which is in the form of a n dynamical system with scalar input the sliding window approach is illustrated in fig 1 our goal is to design a robust low complexity sliding window algorithm which provides estimates x i using successive measurements y i of the form y i a i x i w i 16 where a i is a sequence of measurement matrices this is possible if xi is sufficiently sparse in each window namely if x i 0 for each i where n or if this holds with sufficiently high probability and a i are cs matrices i e satisfy the rip as explained in the prequel note that running such an algorithm online is costly and therefore it is integral to design an alternative to an ad hoc method we propose an approach that leverages the signal overlap between successive windows consisting of recursive sampling and recursive estimation recursive sampling to avoid a full matrix vector multiplication for each y i we design a i so that we can reuse y i in computing y i 1 with low computation overhead or y i 1 f y i xi n xi march 15 2018 draft 12 technical report recursive estimation in order to speed up the convergence of an iterative optimization scheme we make use of the estimate corresponding to the previous window x i 1 to derive a starting point x i 0 for estimating x i or x i 0 g x i 1 y i b recursive sampling of sparse signals we propose the following recursive sampling scheme with low computational overhead it reduces the complexity to o m vs o mn as required by the standard data encoding we derive our scheme for the most general case of noisy measurements with ideal measurements following as special case at the first iteration there is no prior estimate so we necessarily have to compute y 0 a 0 x 0 w 0 we choose a sequence of sensing matrices a i recursively as a i 1 a i 1 a i 2 a i n 1 a i 0 a i p 17 where a i j is the j th column of a i where we have used the convention j 0 1 n 1 for notational convenience and p is a permutation matrix p 0 0 1 1 0 0 0 1 0 18 the success of this data encoding scheme is ensured by noting that if a 0 satisfies rip for given with constant then a i satisfies rip for the same due to the fact that rip is insensitive to permutations of the columns of a 0 given the particular recursive selection of a i we can compute y i 1 recursively as y i 1 a i 1 x i 1 w i 1 a i px i 1 w i 1 a i x i 1 0 n 1 xi n xi w i 1 y i xi n xi a i 1 w i 1 w i 19 draft march 15 2018 freris et al recursive compressed sensing 13 fig 2 illustration of v i for the first four windows the sampling of x i by the matrix a i is equivalent to sampling v i by a 0 i e a i x i a 0 v i where 0 n 1 denotes the all 0 vector of length n 1 this takes the form of a noisy rank 1 update y i 1 y i xi n xi a i 1 rank 1 update z i 1 20 where the innovation is the scalar difference between the new sampled portion of the stream namely xi n and the entry xi that belongs in the previous window but not in the current one above we also defined z i w i w i 1 to be the noise increment note that the noise sequence z i has independent entries if w i is an independent increment process our approach naturally extends to sliding the window by 1 n units in which case we have a rank update cf sec iv remark 5 the particular selection of the sampling matrices a i i 0 1 given in 17 satisfies a i x i a 0 p ix i defining v i p ix i 21 recursive sampling can be viewed as encoding v i by using the same measurement matrix a 0 with the particular structure of x i given in 14 all of the entries of v i and v i 1 are equal except v i i 1 thus the resulting problem can be viewed as signal estimation with partial information 1 recursive sampling in orthonormal basis so far we have addressed the case that for a given n z a given window x i of length n obtained from the sequence xi is spase x i 0 i march 15 2018 draft 14 technical report in general it might rarely be the case that x i is sparse itself however it may be sparse when represented in a properly selected basis for instance the fourier basis for time series or a wavelet basis for images we show the generalization below let x i rn be sparsely representable in a given orthonormal basis i e x i i where i is sparse assuming a common basis for the entire sequence xi over windows of size n we have a i x i a i i for the cs encoding decoding procedure to carry over we need that a i satisfy rip the key result here is that rip is satisfied with high probability for the product of a random matrix a i and any fixed matrix 12 in this case the lasso problem is expressed as minimize a i i y i 22 i 1 where the input signal is expressed as x i i and measurements are still given by y i a i x i w i lemma iii 1 recursive sampling in orthonormal basis let x i i where is an orthonormal matrix with inverse 1 then i 1 i n 1 n 1 i 1 0 i where p and 0 and n 1 denote the first and last row of respectively proof by the definition of x i 1 we have x i 1 x i 0 n 1 1 xi n xi since x i i it holds xi x i 0 1 0 n 1 i 1 xi n x i 1 n 1 0 n 1 1 i 1 using these equations along with i x i yields i 1 x i 1 x i 0 n 1 1 xi n xi x i xi n xi n 1 i n 1 n 1 i 1 0 i draft march 15 2018 freris et al recursive compressed sensing 15 2 recursive sampling in fourier basis the fourier basis is of particular interest in many practical applications e g time series analysis for such a basis an efficient update rule can be derived as is shown in the next corollary corollary iii 2 let be n n inverse discrete fourier transform idft matrix with entries i j ij n where i j 0 n 1 and ej 2 n in such case i 1 n i fn 1 n 1 i 1 0 i 22 where n is the n n diagonal matrix with n i i i and f 1 is the orthonormal fourier basis proof circular shift in the time domain corresponds to multiplication by complex exponentials in the fourier domain i e f nf and the result follows from f i remark 6 complexity of recursive sampling in an orthonormal basis in general the number of computations for calculating i 1 from i is o n 2 for the particular case of using fourier basis the complexity is reduced to only o n i e we have zero overhead for sampling directly on the fourier domain c recursive estimation in the absence of noise estimation is trivial in that it amounts to successfully decoding the first window i e g by bp then all subsequent stream entries can be plainly retrieved by solving a redundant consistent set of linear equations y i 1 y i xi n xi a i 1 where the only unknown is xi n for noisy measurements however this approach is not a valid option due to error propagation it is no longer true that x i x i so computing xi n via 19 leads to accumulated errors and poor performance for recursive estimation we seek to find an estimate x i 1 x i 1 0 x i 1 n 1 leveraging the estimate x i x i 0 x i n 1 and using lasso x i 1 arg min x a i 1 x y i 1 22 x 1 in iterative schemes for convex optimization convergence speed depends on the distance of the starting point to the optimal solution 26 in order to accelerate convergence we leverage the overlap between windows and set the starting point as x i 0 x i 1 1 x i 1 2 x i 1 n 1 march 15 2018 draft 16 technical report where x i 1 j for j 1 n 1 is the portion of the optimal solution based on the previous window we set x i 1 j j 0 1 n 1 to be the estimate of the j 1 th entry of the previous window i e of xi 1 j the last entry x i n 1 denoted by above can be selected using prior information on the data source for example for randomly generated sequence the maximum likelihood estimate e x i 1 xi n 1 may be a reasonable option or we can simply set x i n 1 0 given that the sequence is assumed sparse by choosing the starting point as such the expected number of iterations for convergence is reduced cf section v for a quantitative analysis in the general case where the signal is sparsely representable in an orthonormal basis one can leverage the recursive update for i 1 based on i so as to acquire an initial estimate for warm start in recursive estimation e g e i 1 i d averaging lasso estimates one way to enhance estimation accuracy i e to reduce estimation error variance is to average the estimates obtained from successive windows in particular for the ith entry of the streaming signal xi we may obtain an estimate by averaging 1 the values corresponding to xi obtained from all windows that contain the value i e x j j i n 1 i x i 1 n i j i n 1 x j i j 23 by jensen s inequality we get 1 n i j i n 1 x j i j xi 2 1 n i j i n 1 x j i j xi 2 x i xi 2 which implies that averaging may only decrease the reconstruction error defined in the 2 sense in the following we analyze the expected 2 norm of the reconstruction error x i xi 2 we first present an important lemma establishing independence of estimates corresponding to different windows lemma iii 3 independence of estimates let y i a i x i w i i 0 1 and w i be independent zero mean random vectors the estimates x i obtained by lasso x i argmin x ax y i x 1 1 for notational simplicity we consider the case i n 1 whence each entry i is included in exactly n overlapping windows the case i n 1 can be handled analogously by considering i 1 estimates instead draft march 15 2018 freris et al recursive compressed sensing 17 are independent conditioned on the input stream x xi 2 proof the objective function of lasso x w 7 f x w a i x a i x i w 22 x 1 is jointly continuous in x w and the mapping obtained by minimizing over x w 7 min x f x w g w is continuous hence borel measurable thus the definition of independence and the fact that w i w j are independent for i 6 j concludes the proof the expected 2 norm of the reconstruction error satisfies ex x i xi 2 ex 1 n i j i n 1 x j i j xi 2 ex x i 0 xi 2 1 n ex x i 0 ex x i 0 2 where we have used cov x j i j x k i k 0 for j 6 k j k i n 1 i which follows from independence the resulting equality is the so called bias variance decomposition of the estimator note that as the window length is increased the second term goes to zero and the reconstruction error asymptotically converges to the square of the lasso bias 3 we have seen that averaging helps improve estimation accuracy however averaging alone is not enough for good performance cf sec vi since the error variance is affected by the lasso bias even for large values of window size n in the sequel we propose a non linear scheme for combining estimates from multiple windows which can overcome this limitation e the proposed algorithm in the previous section we pointed out that leveraging the overlaps between windows through averaging lasso estimates cannot yield an unbiased estimator and the error variance does not go to 0 for large values of window size n the limitation is the indeterminacy in the support of the signal if the signal support is known then applying least squares estimation lse to an overdetermined linear systems yields an unbiased estimator in consequence it is vital to address support detection 2 this accounts for the general case of a random input source x where noise w i is independent of x 3 lasso estimator is biased as a mapping from rm rn with m n march 15 2018 draft 18 technical report recursive sampling recursive estimation support detection lse on support set delay averaging delay fig 3 architecture of rcs we propose a two step estimation procedure for recovering the data stream at first we obtain the lasso estimates x i which are fed into a de biasing algorithm for de biasing we estimate the signal support and then perform lse on the support set in order to obtain estimates x i the estimates obtained over successive windows are subsequently averaged the block diagram of the method and the pseudocode for the algorithm can be seen in figure 3 and algorithm 1 respectively in step 8 we show a recursive estimation of averages applicable to an online implementation in the next section we present an efficient method for support detection with provable performance guarantees algorithm 1 recursive compressed sensing input a 0 rm n xi i 0 1 0 output estimate x i i 0 1 1 initialize signal estimate x 0 2 for i 0 1 2 do 3 x i xi xi 1 xi n 1 4 y i a i x i w i encoding 5 x i argmin x rn a i x y i 22 x 1 lasso 6 i supp x i support estimation 7 x i argmin x rn xic 0 a i x y i 22 lse 8 x i j ki j 1 x i j x i j ki j for j 0 n 1 where ki j min i 1 n j update average estimates 9 a i a i 1 p for recursive sampling 10 end for draft march 15 2018 freris et al recursive compressed sensing 19 f voting strategy for support detection recall the application of lasso to signal support estimation covered in section ii in this section we introduce a method utilizing supports estimated over successive windows for robust support detection even in high measurement noise at first step lasso is used for obtaining estimate x i which is then used as input to a voting algorithm for estimating the non zero positions then ordinary least squares are applied to the overdetermined system obtained by extracting the columns of the sampling matrix corresponding to the support the benefit is that since lse is an unbiased estimator averaging estimates obtained over successive windows may eliminate the bias and so it is possible to converge to true values as the window length increases in detail the two step algorithm with voting entails solving lasso x i argmin x rn a i x y i 22 x 1 then identifying the indices having magnitude larger than some predetermined constant 1 0 in order to estimate the support of window x i by i i j x i j 1 24 the entries of this set are given a vote the total number of votes determines whether a given entry is zero or not formally we define the sequence containing the cumulative votes as vi and the number of times an index i is used in lse as li at the beginning of the algorithm vi and li are all set to zero for each window we add votes on the positions that are in the set i i as v i i i v i i i 1 where the subscript i i i is used to translate the indices within the window to global indices on the streaming data by applying threshold 2 z on the number of votes vi we get indices that have been voted sufficiently many times to be accepted as non zeros and store them in ri j vj i 2 j 0 n 1 25 note that the threshold 2 1 n is equal to the delay in obtaining estimates this can be chosen such that ri m hence yielding an overdetermined system for the lse subsequently we solve the overdetermined least squares problem based on these indices in ri x i argmin x rn xrc i 0 a i x y i 22 26 this problem can be solved in closed form x i ri a i ri a i ri 1 a i ri y i where x i ri is the vector obtained by extracting elements indexed by ri and a i ri is the matrix obtained by extracting columns march 15 2018 draft 20 technical report of a i indexed by ri subsequently we increment the number of recoveries for the entries used in lse procedure as lri i lri i 1 and the average estimates are updated based on the recursive formula x i j li j 1 li j x i j 1 li j x j for j ri iv extensions in this section we present various extensions to the algorithm a sliding window with step size consider a generalization in which sensing is performed via recurring windowing with a step size 0 n i e x i xi xi 1 xi n 1 we let i denote the sampling efficiency that is the ratio of the total number of samples taken until time n i to the number of retrieved entries n i for one window sampling efficiency is m n by the end of ith window we have recovered n i 1 elements while having sensed im many samples the asymptotic sampling efficiency is lim i i lim i im n i 1 m the alternative is to encode using a rank update i e by recursively sampling using the matrix obtained by circularly shifting the sensing matrix times a i 1 a i p in this scheme for each window we need to store scalar parameters for instance this can be accomplished by a least squares fit of the difference y i 1 y i in the linear span of the first columns of a i cf 20 the asymptotic sampling efficiency becomes 4 lim i m i 1 n i 1 1 in the latter case the recursive sampling approach is asymptotically equivalent to taking one sample for each time instance note however that the benefit of such an approach lies in noise suppression by taking overlapping windows each element is sensed at minimum n many times hence collaborative decoding using multiple estimates can be used to increase estimation accuracy 4 note that when m recording samples y i directly as opposed to storing parameters yields better efficiency m 1 draft march 15 2018 freris et al recursive compressed sensing 21 b alternative support detection the algorithm explained in sec iii f selects indices to be voted by thresholding the lasso estimate as in 24 an alternative approach is by leveraging the estimates obtained so far since we have prior knowledge about the signal at ith window x i 0 from i 1 th window x i 1 we can annihilate the sampled signal as y i y i a i x i 0 if the recovery of the previous window was perfect y i would be equal to a i n xi n 1 w i and thus xi n 1 can be estimated by lse as xi n 1 a i n y i however since the previous window will have estimation errors this does not hold in such case we can again use lasso to find the estimator for the error between the true signal and estimate as x i arg min x a i x y i 22 x 1 and place votes on the 3 z indices of highest magnitudes i e s i t j x i j z 3 27 instead of 24 the rest of the estimation method remains the same since the noise is i i d the expected number of votes a non support position collects is less than 3 thus the threshold 2 in 25 needs to satisfy 3 2 n in order to eliminate false positives last in the spirit of recursive least squares rls 27 we consider joint identification over multiple windows with exponential forgetting let t be the horizon i e the number of past windows considered in the estimation of the current one also let 0 1 for the i th window 5 we solve x argmin x rn t i j i t j i a j x j y j 22 x j 1 28 where the decision vector x corresponds to xi t xi n 1 and we set x i x t x n t it is interesting to point out that this optimization problem can be put into standard lasso form by weighting the entries of the decision vector at a pre processing step so standard numerical schemes can be applied note that the computational complexity is increasing with t and unlike traditional rls t has to be finite 5 we consider the case i t and 1 for notational simplicity march 15 2018 draft 22 technical report c expected signal sparsity we have considered so far the case that each window x i is sparse however the most general case is when the data stream is sparse on average in the sense that lim sup n 1 n n i 0 1 xi 6 0 in such case one can simply design rcs based on some value and leverage theorem ii 1 to incorporate the error due to model mismatch in the analysis cf theorem v 1 for both analysis and experiments we adopt a random model in which each entry of the data stream is generated i i d according to fxi x 1 p x 1 2 p if x a a 0 o w 29 where p 0 1 this is the density function of a random variable that is 0 with probability 1 p and sampled uniformly over the interval a a otherwise 6 the average sparsity of the stream is p we can calculate the mean error due to model mismatch by e x x 1 a n k 1 n k pk 1 p n k k i 1 1 i k 1 where x denotes an n dimensional random vector with entries generated i i d from 29 and xk is obtained from x by setting all but its largest entries equal to zero the result is a function of the window length n the sparsity used in designing sensing matrices e g we can take mean sparsity pn and the probability of an element being nonzero p in place of the rather lengthy yet elementary algebraic calculations we illustrate error due to model mismatch in fig 4 we point out that we can analytically establish boundedness for all values of p n so our analysis in sec v carries over unaltered the analysis of other distributions on the magnitudes of non zero entries can be carried out in a similar way v analysis in this section we analyze the estimation error variance and computational complexity of the proposed method 6 note that the case p 0 is trivially excluded since then the data stream is an all zero sequence draft march 15 2018 freris et al recursive compressed sensing 23 1000 1500 2000 2500 3000 3500 4000 4500 5000 0 2 0 205 0 21 0 215 0 22 0 225 0 23 0 235 expected deviation from signal sparsity window length e x p e c te d n o r m p 0 02 p 0 08 p 0 16 a e x x 1 vs window length n np 0 0 02 0 04 0 06 0 08 0 1 0 12 0 14 0 16 0 18 0 2 0 13 0 14 0 15 0 16 0 17 0 18 0 19 0 2 0 21 0 22 0 23 expected norm vs p p e x p e c te d n o r m n 1000 b e x x 1 vs probability of non zero p np n 1000 fig 4 error due to model mismatch expected deviation from signal sparsity e x x 1 a estimation error variance given xi we give a bound on the normalized error variance of each window defined as ne i e x i x i 2 x i 2 note that for an ergodic data source this index its inverse expressed in log scale corresponds to average signal to residual ratio srr theorem v 1 normalized error of rcs under the assumptions of theorem ii 2 and given a 0 satisfying rip with for xi i 0 1 satisfying x i 0 ne i satisfies ne i pn c 1 1 n log n 1 pn c 2 c 3 m log n where c 1 c 2 and c 3 are constants and pn 1 2 n 2 log n 2 n 2 o 1 n 2 log 2 2 n 1 proof defining the event s 2 n 1 support is detected correctly on 2 n 1 consecutive windows 7 we have the following equality for ne given xi i 0 1 ne i p s 2 n 1 ex s 2 n 1 x i x i 2 x i 2 1 p s 2 n 1 ex sc 2 n 1 x i x i 2 x i 2 where dropping the subscript 2 n 1 and using s as a shorthand notation for s 2 n 1 we have p s 1 2 n 2 log n 2 n 2 o 1 n 2 log 2 2 n 1 7 note that the definition of the success set is very conservative march 15 2018 draft 24 technical report by theorem ii 2 in s by lse we get ex s x i x i 2 x i 2 1 x i 2 ex s x i x i 22 a x i 2 n 1 where a follows from ex s x i x i 22 ex s j i x i j xi j 2 ex s j i 1 n n 1 t 0 x i j t j t xi j 2 j i n 1 t r 0 ex s x i j t j t xi j x i j r j r xi j n 2 1 n 2 j i n 1 t 0 ex s x i j t j t xi j 2 b 1 n 2 j i n 1 t 0 2 1 2 n 1 where i supp x i is also equal to supp x i given s and b follows since the covariance matrix of lse is 2 ati ai 1 and by rip we have all of the eigenvalues of a i ai greater than 1 since 1 x 22 ax 22 for all x sparse to bound the estimation error in sc note that independent of the selected support by triangle inequality we have a i x i y i 2 a y i 2 a i x i 2 w i 2 1 x i 2 w i 2 and y i 2 a i x i y i 2 a i x i 2 y i 2 1 x i 2 y i 2 where a follows since x i argmin x rn xic 0 a i x i y i 2 draft march 15 2018 freris et al recursive compressed sensing 25 from these two inequalities we have x i 2 2 1 y i 2 2 1 1 x i 2 w i 2 by applying triangle inequality once more we get x i x i 2 x i 2 x i 2 x i 2 1 2 1 1 2 w i 2 1 thus in sc we have ex sc x i x i 2 x i 2 1 x i 2 ex sc x i x i 2 b 1 x i 2 ex sc x i x i 2 1 2 1 1 2 1 e w i 2 x i 2 where b follows from jensen s inequality we get the result by taking the expectation over xi i 0 1 and noting by the assumptions of theorem ii 2 we have e w i 2 m xi j 8 2 log n where j supp x i and x i 0 corollary v 2 for sublinear sparsity o n 1 non zero data entries with magnitude logn and obtained samples m o log n where n is the window length the normalized error goes to 0 as n proof for o n 1 we have pn 1 o 1 n logn 2 n 1 and from the assumptions we have c 3 m logn constant we get the result by noting pn goes to 1 as the window length n goes to infinity remark 7 error of voting note that the exact same analysis applies directly to voting by invoking stochastic dominance for any positive threshold 1 8 log n correct detection occurs in a superset of s 2 n 1 defined by requiring perfect detection in all windows i e 1 0 2 n remark 8 dynamic range 8 note that in a real scenario it may be implausible to increase the window length arbitrarily because the dynamic range condition min i ix xi 8 2 log n may be violated this observation may serve to provide a means for selecting n the good news being that the lower bound 8 the authors would like to thank pr yoram bresler for a fruitful comment on dynamic range march 15 2018 draft 26 technical report increases very slowly in window length only as log n in multiple simulations we have observed that this limitation is actually negligible n can be selected way beyond this barrier without any compromise in increasing estimation accuracy last we note that it is possible to carry out the exact same analysis for general step size as expected the upper bound on normalized error variance is increasing in but we skip the details for length considerations b computational complexity analysis in this section we analyze the computational complexity of rcs let i be the window index a i r m n be the sampling matrix and recall the extension on the number of shifts between successive windows by the end of ith window we have recovered n i 1 many entries as discussed in section iii the first window is sampled by a 0 x 0 this requires o mn basic operations additions and multiplications after the initial window sampling of the window x i xi xi 1 xi n 1 is achieved by recursive sampling having rank update with complexity o m thus by the end of ith window total complexity of sampling is o mn o m i the encoding complexity is defined as the normalized complexity due to sampling over the number of retrieved entries ce lim i ce i n i 1 30 where ce i denotes the total complexity of encoding all stream entries 0 1 i for recursive sampling ce o m while for non recursive we have ce o mn note that by recursively sampling the input stream the complexity is reduced by n the other contribution to computational complexity is due to the iterative solver where the expected complexity can be calculated as the number of operations of a single iteration multiplied by the expected number of iterations for convergence the latter is a function of the distance of the starting point to the optimal solution 26 which we bound in the case of using recursive estimation as follows lemma v 3 using x i 0 x i 1 x i 1 n 1 0 as the starting point we have x i 0 x i 2 c 0 x i 1 x i 1 1 c 0 x i x i 1 c 1 x i n x i n 1 2 where c 0 and c 1 are constants draft march 15 2018 freris et al recursive compressed sensing 27 proof defining e i x i 1 x i 1 n 1 0 x i 0 x i n 1 e i x i x i we have e i x i 1 x i 1 n 1 0 x i x i x i taking the norm and using triangle inequality yields e i 2 e i 1 2 e i 2 x i n x i n 1 2 using theorem ii 1 we get e i 2 c 0 x i 1 x i 1 1 c 0 x i x i 1 c 1 x i n x i n 1 2 31 exact computational complexity of each iteration depends on the algorithm minimally iterative solver for lasso requires multiplication of sampling matrix and the estimate at each iteration which requires o mn operations in an algorithm where cost function decays sublinearly e g 1 t 2 as in fista the number of iterations t required for obtaining x t such that g x t g x where x is the optimal solution is proportional to x 0 x 2 e g x 0 x 2 where x 0 is the starting point of the algorithm 22 from this bound it is seen that average number of iterations is proportional to the euclidean distance of the starting point of the algorithm from the optimal point lemma v 4 expected number of iterations 9 for the sequence xi i 0 1 where x i 0 with the positions of non zeros chosen uniformly at random and max j 0 n 1 x i j o log n for all i the expected number of iterations for convergence of algorithms where cost function decays as 1 t 2 is o log n n for noiseless measurements and o m for i i d measurement noise 9 we note in passing that this bound on the expected number of iterations is actually conservative and can be improved based on a homotopy analysis of warm start 6 8 this is beyond the scope of the current paper march 15 2018 draft 28 technical report proof since x i is sparse the terms x i 1 x i 1 1 and x i x i 1 vanish in 31 by xi o log n and uniform distribution of non zero elements we have e x i n x i n 1 2 log n n with noisy measurements the term c 1 is related to the noise level since noise has distribution w n 0 2 i the squared norm of the noise w i 22 has chi squared distribution with mean 2 m and standard deviation 2 2 m probability of the squared norm exceeding its mean plus 2 standard deviations is small hence we can pick 2 2 m 2 2 m 5 to satisfy the conditions of theorem ii 1 using this result in 31 we get o log n n o m where the second term dominates since n not to leave out any element of the signal and m o log n hence it is found that the expected number of iterations is o m in the noisy case the other source of complexity is the lse in each iteration which requires solving a linear system that needs o 3 operations finally averaging can be performed using o n operations for each given entry we define the decoding complexity as the normalized complexity due to estimation over the number of retrieved entries cd lim i cd i n i 1 32 where cd i denotes the total complexity of decoding all stream entries 0 1 i it follows that decoding complexity is equal to cd o m 3 2 n 3 using recursive estimation to conclude the asymptotic total complexity per retrieved stream entry c ce cd is dominated by lasso and lse based on the facts that m 1 n 1 therefore c o m 3 2 n 3 33 in table i we demonstrate the total complexity for various sparsity classes based on the fundamental relation m o log n 12 note that the computational complexity is decreasing in while error variance is increasing in this trade off can be used for selecting window length n and step size based on desired estimation accuracy and real time considerations draft march 15 2018 freris et al recursive compressed sensing 29 table i computational complexity per entry as function of window length n and step size for different sparsity classes computational complexity o 1 o n log n 3 2 o log n o n log n log n log n 3 2 o n o n 3 2 o n o n 3 vi simulation results the data used in the simulations are generated from the random model 29 10 with p 0 05 the measurement model is y i a i x i w i with w i n 0 2 i where r and the sampling matrix is a 0 rm n where m 6 pn and n is equal to the window length in the sequel we test rcs as described in sections iii e iii f we have also experimented extensively on the extensions presented in sec iv b but do not present the results here because a the exponential forgetting approach alone does not improve estimation accuracy while it incurs computation overhead and b the performance and run time of generalized voting is no different than that of standard voting a runtime we experimentally test the speed gain achieved by rcs by comparing the average time required to estimate a given window while using fista for solving lasso rcs is compared against so called na ve approach where the sampling is done by matrix multiplication in each window and fista is started from all zero vector the average time required to recover one window in each case is shown in figure 5 b support estimation we present the results of experiments on the support estimation using lasso in the measurements x r 6000 x 0 60 a rm 6000 is generated by i i d gaussian distribution with ai j n 0 1 m and w has 0 1 as suggested in theorem ii 2 for these parameters lasso is solved with 10 we also tested the case where the values of non zero entries are generated i i d from a gaussian distribution even though this model may violate the dynamic range assumption cf rem 8 the results are very similar march 15 2018 draft 30 technical report 0 500 1000 1500 2000 2500 3000 0 0 5 1 1 5 2 2 5 runtime plot window size t im e s naive approach rcs fig 5 average processing time of rcs vs traditional non recursive cs over a single time window 4 2 log n and the nonzero entries of x are chosen so that min i 1 2 n xi 3 34 by sampling from u 4 34 3 34 3 34 4 34 in simulations we vary the number of samples taken from the signal m and study the accuracy of support estimation by using true positive rate detected support true support true support false positive rate detected support true support n true support where denotes the cardinality of a set and is the set difference operator the support is detected by taking the positions where the magnitude of the lasso estimate is greater than threshold 1 for values 0 01 0 1 1 figure 6 shows the resulting curves obtained by randomly generating the input signal 20 times for each m and averaging the results it can be seen that the false positive rate can be reduced significantly by properly adjusting the threshold on the resulting lasso estimates c reconstruction error as was discussed in section iii f lasso can be used together with a voting strategy and least squares estimation to reduce error variance figure 7 shows the comparison of performance of a averaged lasso estimates b debiasing and averaging with voting strategy and c debiasing and averaging without voting the figure is obtained by using fixed x i e a single window and taking multiple measurements each draft march 15 2018 freris et al recursive compressed sensing 31 300 400 500 600 700 800 0 0 2 0 4 0 6 0 8 1 number of samples m true positive rate and false positive rate threshold 0 01 threshold 0 10 threshold 1 00 fig 6 support set estimation using lasso for n 6000 0 1 min xi 3 34 threshold 1 0 01 0 10 and 1 00 circles depict true positive rate and squares depict false positive rate 0 200 400 600 800 1000 10 2 10 1 10 0 10 1 voting measurements e s ti m a ti o n e r r o r averaged estimate voting debiasing fig 7 error plots for a averaged estimates b voting strategy and c debiasing without voting being an m dimensional vector corrupted by i i d gaussian noise it can be seen that the error does not decrease to zero for averaged estimate which is due to lasso being a biased estimator cf section iii whereas for the proposed schemes it does march 15 2018 draft 32 technical report 100 200 300 400 500 600 700 800 900 1000 0 04 0 06 0 08 0 1 0 12 0 14 0 16 0 18 0 2 rcs normalized error n o r m a li z e d e r r o r window length fig 8 normalized error variance vs window length for rcs on streaming data figure 8 shows the behavior of normalized error variance lim t t i 1 x i xi 2 t i 1 xi 2 as the window length n increases the signals are generated to be 5 sparse m is chosen to be 5 times the expected window sparsity and the measurement noise is w i n 0 2 i where 0 1 the non zero amplitudes of the signal are drawn from uniform distribution u 2 1 1 2 the figure shows that the normalized error variance decreases as the window length increases which is in full agreement with our theoretical analysis vii conclusions and future work we have proposed an efficient online method for compressively sampling data streams the method uses a sliding window for data processing and entails recursive sampling and iterative recovery by exploiting redundancy we achieve higher estimation accuracy as well as reduced run time which makes the algorithm suitable for an online implementation extensive experiments showcase the merits of our approach compared to traditional cs a at least 10 x speed up in run time and b 2 3 orders of magnitude lower reconstruction error in ongoing work we study accelerating the decoding procedure by deriving a fast lasso solver directly applicable to rcs we also seek to apply the derived scheme in practical applications such as burst detection in networks and channel estimation in wireless communications draft march 15 2018 freris et al recursive compressed sensing 33 acknowledgement this work was supported in part by qualcomm san diego and erc advanced investigators grant sparsam no 247006 references 1 n freris o o c al and m vetterli compressed sensing of streaming data in 51 st allerton conference on communi cation control and computing 2013 2 e cande s and m wakin an introduction to compressive sampling ieee signal processing magazine vol 25 no 2 pp 21 30 march 2008 3 m vetterli p marziliano and t blu sampling signals with finite rate of innovation ieee transactions on signal processing vol 50 no 6 pp 1417 1428 2002 4 d l donoho compressed sensing ieee transactions on information theory vol 52 pp 1289 1306 2006 5 e candes and t tao near optimal signal recovery from random projections universal encoding strategies ieee transactions on information theory vol 52 no 12 pp 5406 5425 2006 6 p garrigues and l el ghaoui an homotopy algorithm for the lasso with online observations in proc nips 2008 7 p boufounos and m asif compressive sampling for streaming signals with sparse frequency content in information sciences and systems ciss 2010 44 th annual conference on 2010 pp 1 6 8 m s asif and j romberg sparse recovery of streaming signals using l 1 homotopy submitted to ieee transactions on signal processing june 2013 9 a m bruckstein d l donoho and m elad from sparse solutions of systems of equations to sparse modeling of signals and images siam review vol 51 no 1 pp 34 81 mar 2009 10 e cande s and t tao decoding by linear programming ieee transactions on information theory vol 51 no 12 pp 4203 4215 dec 2005 11 d achlioptas database friendly random projections in proceedings of the twentieth acm sigmod sigact sigart symposium on principles of database systems acm 2001 pp 274 281 12 r baraniuk m davenport r devore and m wakin a simple proof of the restricted isometry property for random matrices constructive approximation vol 28 no 3 pp 253 263 2008 13 d p bertsekas convex optimization theory athena scientific 2009 14 j a tropp greed is good algorithmic results for sparse approximation ieee transactions on information theory vol 50 pp 2231 2242 2004 15 r tibshirani regression shrinkage and selection via the lasso journal of the royal statistical society series b methodological vol 58 pp 267 288 1996 16 s rangan a fletcher and v goyal asymptotic analysis of map estimation via the replica method and applications to compressed sensing ieee transactions on information theory vol 58 no 3 pp 1902 1923 mar 2012 17 e cande s and y plan near ideal model selection by 1 minimization the annals of statistics vol 37 pp 2145 2177 2009 18 c h zhang and j huang the sparsity and bias of the lasso selection in high dimensional linear regression annals of statistics vol 36 no 4 pp 1567 1594 2008 19 s boyd and l vandenberghe convex optimization new york ny usa cambridge university press 2004 march 15 2018 draft 34 technical report 20 m grant and s boyd cvx matlab software for disciplined convex programming version 2 0 beta http cvxr com cvx sep 2012 21 n parikh and s boyd proximal algorithms foundations and trends in optimization 2013 22 a beck and m teboulle a fast iterative shrinkage thresholding algorithm for linear inverse problems siam journal on imaging sciences vol 2 no 1 pp 183 202 2009 23 s j wright r d nowak and m a t figueiredo sparse reconstruction by separable approximation signal processing ieee transactions on vol 57 no 7 pp 2479 2493 2009 24 m afonso j bioucas dias and m a t figueiredo fast image recovery using variable splitting and constrained optimization ieee transactions on image processing vol 19 no 9 pp 2345 2356 2010 25 n freris and p patrinos pn lasso a proximal newton algorithm for compressed sensing in preparation 26 d p bertsekas nonlinear programming athena scientific 1995 27 p r kumar and p varaiya stochastic systems estimation identification and adaptive control upper saddle river nj usa prentice hall inc 1986 draft march 15 2018 http cvxr com cvx i introduction ii background ii a notation ii b definitions and properties ii c setting ii d algorithms for sparse recovery ii d 1 basis pursuit ii d 2 orthogonal matching pursuit ii d 3 least absolute selection and shrinkage operator lasso iii recursive compressed sensing iii a problem formulation iii b recursive sampling of sparse signals iii b 1 recursive sampling in orthonormal basis iii b 2 recursive sampling in fourier basis iii c recursive estimation iii d averaging lasso estimates iii e the proposed algorithm iii f voting strategy for support detection iv extensions iv a sliding window with step size iv b alternative support detection iv c expected signal sparsity v analysis v a estimation error variance v b computational complexity analysis vi simulation results vi a runtime vi b support estimation vi c reconstruction error vii conclusions and future work references