ar x iv 1 10 3 59 85 v 1 cs i t 3 0 m ar 2 01 1 1 on empirical entropy paul m b vita nyi abstract we propose a compression based version of the empirical entropy of a finite string over a finite alphabet whereas previously one considers the naked entropy of possibly higher order markov processes we consider the sum of the description of the random variable involved plus the entropy it induces we assume only that the distribution involved is computable to test the new notion we compare the normalized information distance the similarity metric with a related measure based on mutual information in shannon s framework this way the similarities and differences of the last two concepts are exposed index terms empirical entropy kolmogorov complexity normalized information distance simi larity metric mutual information distance i introduction in the basic set up of shannon 20 a message is a finite string over a finite alphabet one is interested in the expected number of bits to transmit a message from a sender to a receiver when both the sender and the receiver consider the same ensemble of messages the set of possible messages provided with a probability for each message the expected number of bits is known as the entropy of the ensemble of messages this ensemble is also known as the source the empirical entropy of a single message is taken to be the entropy of a source that produced it as a typical element the notion of typicality is defined differently by different authors and we take here the intuitive meaning traditionally this source is a possibly higher order markov process this leads to the definition in example 2 4 here we want to liberate the notion so that it encompasses all computable random variables with finitely many outcomes consisting of finite strings over a finite alphabet moreover paul vita nyi is a cwi fellow with the national research center for mathematics and computer science in the netherlands cwi and emeritus professor of computer science at the university of amsterdam he was supported in part by the the esf qit programmme the eu noe pascal ii address cwi science park 123 1098 xg amsterdam the netherlands email paul vitanyi cwi nl may 29 2018 draft http arxiv org abs 1103 5985 v 1 since we are given only a single message but not the ensemble from which it is an element the new empirical entropy should provide both this ensemble and the entropy it induces if we are given just the entropy but not the ensemble involved then a receiver cannot in general reconstruct the message moreover we are given a single message which has a particular length say n therefore given the family of random variables we draw upon we can select one of them and compute the probability of every message of length n for fixed n this results in a bernoulli variable that has n outcomes we are thus led to a notion of empirical entropy that consists of a description of the bernoulli variable involved plus the related entropy of the message induced since we assume the original probability mass function to be computable the bernoulli variable is computable and its effective description length can be expressed by its kolmogorov complexity normalized information distance explained below between two finite objects is often confused with a similar distance between two random variables the last distance is expressed in terms of probabilistic mutual information we use our new notion to explain the differences between the former distance between two individual objects and the latter distance between two random variables this difference parallels that between the kolmogorov complexity of a single finite object and the entropy of a random variable the former quantifies the information in a finite object while the latter gives us the expected number of bits to communicate any outcome of a random variable known to both the sender and the receiver computability notions are reviewed in appendix a and kolmogorov complexity in appendix b a preliminaries we write string to mean a finite string over a finite alphabet other finite objects can be encoded into strings in natural ways the set of strings is denoted by we usually take 0 1 the length of a string x is the number of letters in in it denoted as x the empty string has length 0 identify the natural numbers n including 0 and 0 1 according to the correspondence 0 1 0 2 1 3 00 4 01 i 1 then 010 3 the emphasis here is on binary sequences only for convenience observations in every finite alphabet can be so encoded in a way that is theory neutral for example if a finite alphabet has cardinality 2 k then every element i can be encoded by i which is a block of bits of length k with this encoding every x satisfies that the kolmogorov complexity k x k x see appendix b for basic definitions and results on kolmogorov complexity up to an additive constant that 2 is independent of x ii the new empirical entropy let x be a random variable with outcomes in a finite alphabet x shannon s entropy 20 is h x x x p x x log 1 p x x there are three items involved in the new empirical entropy of data x a class of random variables like the set of bernoulli processes or the set of higher order markov processes from each element of this class we construct a bernoulli variable x with n outcomes of length n a selection of a random variable from this bernoulli class such that x is a typical outcome and a description of this random variable plus its entropy this is reminiscent of universal coding essentially due to kolmogorov 11 and of two part mdl due to rissanen 19 in its simplest form the former assuming a bernoulli process codes a string x of length n over a finite alphabet as follows a string containing a description of n and n ni 1 i and the index of x in the set constrained by these items the coding should be such that the individual substrings can be parsed except the description of the index which we put last this takes additive terms that are logarithmic in the length of the items except the last one the universal code takes o log n n n n 1 n n bits the two part mdl complexity of a string 19 is the minimum of the self information of that string with respect to a source and the number of bits needed to represent that source the source is not required to be markovian and the two part mdl takes into account its complexity however the methods of encoding are arbitrary an n length outcome x x 1 x 2 xn over is the outcome of a stochastic process x 1 x 2 xn characterized by a joint probability mass function pr x 1 x 2 xn x 1 x 2 xn for technical reasons we replace the list x 1 x 2 xn by a single bernoulli random variable x with outcomes in x n here the random variables xi may be independent copies of a single random variable as is the case wen the source stochastic process is a bernoulli variable but the source stochastic process may be a higher order markov chain making some or all xis dependent this depends on whether the order of the markov chain is greater then n for certain stochastic processes all xis are dependent for every n the stochastic process assigns a probability to every outcome in 3 definition 2 1 let n be an integer a finite alphabet x n be a string x a family of computable processes each process x producing possibly by repetition a sequence of possibly dependent random variables x x 1 x 2 xn with pr x x is computable and h x the empirical entropy of x with respect to x is given by h x x min x k x h x h x log 1 pr x x is minimal this means that the expected binary length of encoding an outcome of x is as close as possible to log 1 pr x x in the two part description the complexity part describes x and the entropy part is the ignorance about the data x in the set n given x remark 2 2 by assumption n is fixed by theorem 3 in 20 i e the asymptotic equidistribution property for ergodic markov sources the following is the case let h be the per symbol entropy of the source for example if the source is bernoulli with pr si p si si for 1 i then h i 1 p s 1 log 1 p si let x be the induced bernoulli variable with n outcomes consisting of sequences of length n over then for every 0 there is an n 0 such that the sequences of length n n 0 are divided into two classes one set with total probability less than and one set such that for every y in this set holds h 1 n log 1 pr x y note that h x nh thus for large enough n we are almost certain to have h x log 1 pr x x o n set for convenience we call the set of y s such that h x log 1 pr x y n with 0 and some n 0 depending on and n n 0 the typical outcomes of x the cardinality of the set s n of such y s satisfies 1 h x n s h x n see 7 theorem 3 1 2 lemma 2 3 assume definition 2 1 then k x k x x o 1 proof the family x consists of computable random variables that is in essence of computable probability mass functions the family of all lower semicomputable semiprobability mass functions can be effectively enumerated possibly with repetitions theorem 4 3 1 in 17 the latter family contains all computable probability mass functions hence it contains x thus if we know x x we can compute the x x of definition 2 1 by going through this list example 2 4 assume definition 2 1 let ni be the number of occurrences of the ith character of in x if w is a string then xw is the string obtained by concatenating the characters immediately following 4 occurrences of w in x the cardinality xw is the number of occurrences of w in x unless w occurs as a suffix of x in which case it is 1 less in 12 18 8 the kth order empirical entropy of x is defined by hk x 1 n i 1 ni log n ni for k 0 1 n w k xw h 0 xw for k 0 ii 1 the kth order empirical entropy of x can be reconstructed from x once we know k the kth order empirical entropy of x results from the probability induced by a kth order markov source x a bernoulli process is a 0 th order markov source let x to be the family of kth order markov sources a specific k 0 provided the transition probabilities are computable such a family is subsumed under definition 2 1 let x be a string over which is typically produced by such a markov source of order k the empirical entropy h x x of x is k x nhk x here x is the random variable associated with the kth order empirical entropy computed from x note that the empirical entropy hk x stops being a reasonable complexity metric for almost all strings roughly when k surpasses n 8 example 2 5 let x 10 n 2 for even n that is n 2 copies of the pattern 10 let x 1 be the family of binary bernoulli processes the empirical entropy h x x 1 is reached for i i d sequence x x 1 x 2 xn x 1 each xi being a copy of the same random variable y with outcomes in 0 1 with p y 1 1 2 then h x x 1 k x nh y then x can be computed from the information concerning n in o log n bits the particular x used in o 1 bits and a program of o 1 bits to compute x from this information in this way k x o log n moreover h y 1 so that h x x 1 n o log n let x 2 be the family of first order markov processes with 2 transitions each and with output alphabet 0 1 for each state the empirical entropy h x x 2 is reached for the n bit output of a deterministic parity markov process that is x x 1 x 2 xn and every xi gives the output at time i of the markov process with 2 states s 0 and s 1 defined as follows the transit probabilities are p s 0 s 1 1 and p s 1 s 0 1 while the output in state s 0 is 0 and in state s 1 is 1 the start state is s 0 in this way p x 10 n 2 1 while h x 0 then h x x 2 k x h x here k x o log n since we require a description of n the 2 state merkov process involved and a program to compute x from this information since the outcome is deterministic h x 0 so that h x x 2 o log n example 2 6 consider the first n bits of 3 1415 let x 1 be the family of bernoulli processes empirically it has been established that the frequency of 1 s in the binary expansion of is n 2 o n 5 that is the binary expaqnsion of is a typical pseudorandom sequence hence h x x 1 k x nh x where x x 1 x 2 xn x 1 and the xi s are n i i d distributed copies of y here y is a bernoulli process with p y 1 1 2 then k x o log n and h y 1 so that h x x 1 n o log n let x 2 be the family of computable random variables with as outcomes binary strings of length n we know that there is a small program say of about 10 000 bits incorporating an approximation algorithm that generates the successive bits of forever telling it to stop after n bits we can generate the computable bernoulli variable x x 2 assigning probability 1 to x and probability 0 to any other binary string of length n assume n 1 000 000 000 then we have k x log 1 000 000 000 c 30 c where the c additive term is the number of bits of the program to compute and a program required to turn the logarithmic description of 1 000 000 000 and the program to compute into the random variable x finally h x 0 therefore h x x 2 10 030 c example 2 7 consider printed english say just lower case and space signs ignoring the other signs the entropy of representative examples of printed english has been estimated experimentally by shannon 21 based on human subjects guesses of successive characters in a text his estimate is between 0 6 and 1 3 bits per character bpc and 22 obtained an estimate of 1 46 bpc for ppm based models which we will use in this example ppm prediction by partial matching is an adaptive statistical data compression technique it is based on context modeling and prediction and uses a set of previous symbols in the uncompressed symbol stream to predict the next symbol in the stream rather like a mechanical version of shannon s method consider a text of n characters over the alphabet used by 22 and let x be the class of ppm based models with n output characters over the used alphabet since the ppm machine can be described in o 1 bits its program is finite and the length n in o log n bits we have k x o log n hence h x x k x 1 46 n 1 46 n o log n in these examples we see that the empirical entropy is higher when the family of random variables considered is simpler for simple random variables the knowledge in the kolmogorov complexity part is neglible the empirical entropy with respect to a complex family of random variables can be lower than that with respect to a family of simple random variables by transforming the ignorance in the entropy part into knowledge in the kolmogorov complexity part we use this observation to consider the widest family of computable probability mass functions lemma 2 8 let x be the family of computable random variables x with h x and x with then h x x k x o 1 6 proof first let px be a shortest prefix program which computes x hence px k x by adding o 1 bits to it we have a program pp which computes a probability mass function p with p x 1 and p y 0 for y 6 x x y hence pp k x o 1 second let qp be a shortest prefix program which computes a probability mass function p with p x 1 and p y 0 for y 6 x x y thus qp pp adding o 1 bits to qp we have a program qx which computes x then k x qp o 1 altogether qp k x o 1 for the sequel of this paper we need to extend the notion of empirical entropy to joint probability mass functions definition 2 9 let n be an integer a finite alphabet x y n be strings z be the family of computable joint probability mass functions z z and x y an outcome of z let the probability mass function p x y p z x y have a finite joint entropy h z the empirical entropy of x y with respect to z is h x y z min z z k z h z h z log 1 p x y is minimal lemma 2 10 let z be the family of computable joint probability mass functions z with h z and x y with then h x y z k x y o 1 proof similar to that of lemma 2 8 iii normalized information distance the classical notion of kolmogorov complexity 11 is an objective measure for the information in a single object and information distance measures the information between a pair of objects 2 this last notion has spawned research in the theoretical direction see the many google scholar citations to the above reference research in the practical direction has focused on the normalized information distance nid also called the similarity metric which arises by normalizing the information distance in a proper manner the nid is defined by iii 2 below if we approximate the kolmogorov complexity through real world compressors 16 6 4 then we obtain the normalized compression distance ncd from the nid this is a parameter free feature free and alignment free similarity measure that has had great impact in applications only the compressor used can be viewed as a parameter or feature the ncd was preceded by a related nonoptimal distance 15 in 10 another variant of the ncd has been tested on all major time sequence databases used in 7 all major data mining conferences against all other major methods used the compression method turned out to be competitive in general and superior in heterogeneous data clustering and anomaly detection there have been many applications in pattern recognition phylogeny clustering and classification ranging from hurricane forecasting and music to to genomics and analysis of network traffic see the many papers referencing 16 6 4 in google scholar in 16 it is shown that the nid and in 4 that the ncd subject to mild conditions on the used compressor are metrics up to negligible discrepancies in the metric in equalities and that they are always between 0 and 1 the computability status of the nid has been resolved in 23 the ncd is computable by definition the information distance d x y between strings x and y is defined as d x y min p p u p x y u p y x where u is the reference universal turing machine above like the kolmogorov complexity k the distance function d is upper semicomputable define e x y max k x y k y x in 2 it is shown that the function e is upper semicomputable d x y e x y o loge x y the function e is a metric more precisely that it satisfies the metric in equalities up to a constant and that e is minimal up to a constant among all upper semicomputable distance functions d satisfying the mild normalization conditions y y 6 x 2 d x y 1 and x x 6 y 2 d x y 1 here and elsewhere in this paper log denotes the binary logarithm the normalized information distance nid e is defined by e x y e x y max k x k y iii 1 it is straightforward that 0 e x y 1 up to some minor discrepancies for all x y 0 1 rewriting e using a 1 yields e x y k x y min k x k y max k x k y iii 2 up to some lower order terms that we ignore lemma 3 1 let x be a string x z be the families of random variables with computable probability mass functions and computable joint probability mass functions respectively moreover for x x and z z we have h x h z then we can substitute the kolmogorov complexities in iii 2 by the corresponding empirical entropies as in iii 3 8 proof by lemma s 2 8 and 2 10 we know the following for x is the family of computable probability mass functions h x x k x h y x k y for z is the family of computable joint probability mass functions h x y z k x y hence e x y h x y z min h x x h y x max h x x h y x iii 3 ignoring lower order terms remark 3 2 in lemma 3 1 we can replace the computable random variables by the restriction to computable random variables that have a singleton support that is probability mass functions p with p x 1 for some x and p y 0 for all y 6 x alternatively we can replace it by the family of computable markov processes to see this for every x of length n there is a computable markov process m of order n 1 that outputs x deterministically and k x k m o 1 clearly if we replace the family of computable probability mass functions in the empirical entropies in lemma 3 1 by weaker subfamilies like the families based on computable bernoulli functions computable gaussians or computable first order markov processes then lemma 3 1 will not hold in general remark 3 3 the ncd is defined by ncdz x y z xy min z x z y max z x z y iii 4 where z x is the compressed version of x when it is compressed by a lossless compressor z we have substituted xy for the pair x y both for convenience and with ignorable consequences consider a simple compressor that uses only bernoulli variables for example a huffman code compressor the compressed version of a string is preceded by a header containing information identifying the compressor and the charcteristics used the relative frequencies in this case to compress the source string in general this is the case with every compressor in 3 the ncd based on compressors computing the static huffman code of a bernoulli variable is shown to be the total kullback leibler divergence to the mean we refrain from explaining these terms since are extraneous to our treatment thus z x is comprised of the header generated by z for x this header makes it possible to use the uncompress feature denoted here by z 1 so that z 1 z x x the header describes a random variable based on the compressor z the family of random variables induced by the compressor z can be denoted by xz in this way we can define the bernoulli variable x used to compress x the empirical entropy h x xz k x h x here k x is uncomputable we approximate it by the length of the header 9 say x the bernoulli variable x has entropy h x and z x x h x similarly for y and x y therefore ncdz x y xy h x y min x h x y h y max x h x y h y iii 5 ignoring lower order terms where x k x y k y and xy k xy iv mutual information in 25 1 13 9 26 14 the entropy and joint entropy of a pair of sequences is determined and this is directly equated with the kolmogorov complexity of those sequences the shannon type probabilistic version of iii 2 is eh x y h x y min h x h y max h x h y 1 max h x h y h x y min h x h y max h x h y 1 i x y max h x h y since the mutual information i x y between random variables x and y is i x y h x h y h x y and max h x h y min h x h y h x h y in this way eh x y is 1 minus the mutual information between random variables x and y per bit of the maximal entropy how do the cited references connect this distance between two random variables to iii 2 the distance between two individual outcomes x and y ostensibly one has to replace the entropy of random variables x and y by the empirical entropy according to definition 2 1 deduced from strings x and y to obtain the required result iii 2 one has to use families x y z of computable random variables such that k x h x x k y h y y and k x y h x y z in our framework this is possible only if x y are appropriate families of computable random variables and z is an appropriate family of computable joint random variables outside our framework the widest notion of empirical entropy is ii 1 and there it is not possible at all 10 to obtain computable approximations using a real world compressor z for x and y as in iii 4 we can take the empirical entropy based on compressor z as in iii 4 and iii 5 appendix a computability in 1936 a m turing 24 defined the hypothetical turing machine whose computations are intended to give an operational and formal definition of the intuitive notion of computability in the discrete domain these turing machines compute integer functions the computable functions by using pairs of integers for the arguments and values we can extend computable functions to functions with rational arguments and or values the notion of computability can be further extended see for example 17 a function f with rational arguments and real values is upper semicomputable if there is a computable function x k with x an rational number and k a nonnegative integer such that x k 1 x k for every k and limk x k f x this means that f can be computably approximated from above a function f is a function f is lower semicomputable if f is upper semicomputable a function is called semicomputable if it is either upper semicomputable or lower semicomputable or both if a function f is both upper semicomputable and lower semicomputable then f is computable a countable set s is computably or recursively enumerable if there is a turing machine t that outputs all and only the elements of s in some order and does not halt a countable set s is decidable or recursive if there is a turing machine t that decides for every candidate a whether a s and halts example a 1 an example of a computable function is f n defined as the nth prime number an example of a function that is upper semicomputable but not computable is the kolmogorov complexity function k in appendix b an example of a recursive set is the set of prime numbers an example of a recursively enumerable set that is not recursive is x n k x x b kolmogorov complexity informally the kolmogorov complexity of a string is the length of the shortest string from which the original string can be losslessly reconstructed by an effective general purpose computer such as a particular universal turing machine u 11 or the text 17 hence it constitutes a lower bound on how far a lossless compression program can compress in this paper we require that the set of programs of u is prefix free no program is a proper prefix of another program that is we deal with the prefix kolmogorov complexity but for the results in this paper it does not matter whether we use the plain 11 kolmogorov complexity or the prefix kolmogorov complexity we call u the reference universal turing machine formally the conditional prefix kolmogorov complexity k x y is the length of the shortest input z such that the reference universal turing machine u on input z with auxiliary information y outputs x the unconditional prefix kolmogorov complexity k x is defined by k x the functions k and k though defined in terms of a particular machine model are machine independent up to an additive constant and acquire an asymptotically universal and absolute character through church s thesis see for example 17 and from the ability of universal machines to simulate one another and execute any effective process the kolmogorov complexity of an individual finite object was introduced by kolmogorov 11 as an absolute and objective quantification of the amount of information in it the information theory of shannon 20 on the other hand deals with average information to communicate objects produced by a random source since the former theory is much more precise it is surprising that analogs of theorems in information theory hold for kolmogorov complexity be it in somewhat weaker form for example let x and y be random variables with a joint distribution then h x y h x h y where h x is the entropy of the marginal distribution of x similarly let k x y denote k x y where is a standard pairing function and x y are binary strings an example is x y defined by y x y 1 x y 2 where x and y are viewed as natural numbers as in i 1 then we have k x y k x k y o 1 indeed there is a turing machine ti that provided with p q as an input computes u p u q where u is the reference turing machine by construction of ti we have ki x y k x k y hence k x y k x k y o 1 another interesting similarity is the following i x y h y h y x is the probabilistic information in random variable x about random variable y here h y x is the conditional entropy of y given x since i x y i y x we call this symmetric quantity the mutual probabilistic information definition a 2 the algorithmic information in x about y is i x y k y k y x where x y are finite objects like finite strings or finite sets of finite strings it is remarkable that also the algorithmic information in one finite object about another one is symmetric i x y i y x up to an additive term logarithmic in k x k y this follows immediately from the symmetry of information property due to a n kolmogorov and l a levin they proved it for plain 12 kolmogorov complexity but in this form it holds equally for prefix kolmogorov complexity k x y k x k y x o log k x k y a 1 k y k x y o log k x k y references 1 d benedetto e caglioti and v loreto language trees and zipping i physical rev letters i vol 88 2002 p 048702 2 c h bennett p ga cs m li p m b vita nyi and w zurek information distance ieee transactions on information theory 44 4 1998 1407 1423 3 r l cilibrasi statistical inference through data compression ph d thesis university of amsterdam amsterdam the netherlands 2007 4 r l cilibrasi p m b vita nyi clustering by compression ieee trans information theory 51 4 2005 1523 1545 5 r l cilibrasi p m b vita nyi the google similarity distance ieee trans knowledge and data engineering 19 3 2007 370 383 6 r cilibrasi p m b vita nyi r de wolf algorithmic clustering of music based on string compression computer music j 28 4 2004 49 67 7 t m cover and j a thomas elements of information theory wiley new york 1991 8 t gagie large alphabets and incompressibility information processing letters 99 2006 246251 9 z dawy j hagenauer p hanus j c mueller mutual information based distance measures for classification and content recognition with applications to genetics proc ieee int conf communications vol 2 2005 820 824 10 e keogh s lonardi c a ratanamahatana l wei s h lee j h compression based data mining of sequential data data mining and knowledge discovery 14 1 2007 99 129 11 a n kolmogorov three approaches to the quantitative definition of information problems inform transmission 1 1 1965 1 7 12 s r kosaraju and g manzini compression of low entropy strings with lempel ziv algorithms siam j comput 29 1999 893911 13 a kraskov h stogbauer r g andrzejak p grassberger hierarchical clustering using mutual information europhysics letters 70 2 2005 278 284 14 a kraskov p grassberger pp 101 124 in mic mutual information based hierarchical clustering information theory and statistical learning f emmert streib m dehmer eds springer new york 2009 15 m li j h badger x chen s kwong p kearney and h zhang an information based sequence distance and its application to whole mitochondrial genome phylogeny bioinformatics 17 2 2001 149 154 16 m li x chen x li b ma p m b vita nyi the similarity metric ieee trans inform th 50 12 2004 3250 3264 17 m li and p m b vita nyi an introduction to kolmogorov complexity and its applications springer verlag new york 3 rd edition 2008 18 g manzini an analysis of the burrowswheeler transform j acm 48 2001 407430 19 j j rissanen stochastical complexity and statistical inquiry world scientific 1989 13 20 c e shannon a mathematical theory of communication bell syst tech j 27 1948 379 423 623 656 21 c e shannon prediction and entropy of printed english bell syst tech j 30 1951 50 64 22 w j teahan j g cleary the entropy of english using ppm based models in proc data compression conf 1996 53 62 23 s a terwijn l torenvliet p m b vita nyi nonapproximability of the normalized information distance j comput system sciences to appear 24 a m turing on computable numbers with an application to the entscheidungsproblem proc london mathematical society 42 2 1936 230 265 correction 43 i 1937 544 546 25 z g yu p jiang distance correlation and mutual information among portraits of organisms based on complete genomes phys lett a 286 2001 34 46 26 z g yu z mao l q zhou vo v anh a mutual information based sequence distance for vertebrate phylogeny using complete mitochondrial genomes proc ieee 3 rd int conf natural computation 2007 253 257 14 i introduction i a preliminaries ii the new empirical entropy iii normalized information distance iv mutual information appendix a computability b kolmogorov complexity references