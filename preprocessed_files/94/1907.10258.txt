1 adann adaptive neural network based equalizer via online semi supervised learning qingyi zhou fan zhang senior member ieee and chuanchuan yang senior member ieee abstract the demand for high speed data transmission has increased rapidly leading to advanced optical communication techniques in the past few years multiple equalizers based on neural network nn have been proposed to recover signal from nonlinear distortions however previous experiments mainly focused on achieving low bit error rate ber on certain dataset with an offline trained nn neglecting the generalization ability of nn based equalizer when the properties of optical link change the development of efficient online training scheme is urgently needed in this paper we ve proposed an adaptive online training scheme which can fine tune parameters of nn based equalizer without the help of an online training sequence by introducing data augmentation and virtual adversarial training the conver gence speed has been accelerated by 4 5 times compared with decision directed self training the proposed adaptive nn based equalizer is called adann its ber has been evaluated under two scenarios a 56 gb s pam 4 modulated vcsel mmf optical link 100 m and a 32 gbaud 16 qam modulated nyquist wdm system 960 km ssmf in our experiments with the help of adann ber values can be quickly stabilized below 1 e 3 after trained with 105 unlabeled symbols adann shows great performance improvement compared with non adaptive nn and conventional mlse index terms optical fiber communication adaptive nonlin ear equalizer neural network semi supervised learning i introduction w ith the continuous development of the internet higherbandwidth data transmission is required advanced modulation techniques together with novel algorithms have emerged to fulfill the requirements digital signal processing dsp is quite essential for improving the bit error rate ber performance and raising the optical links transmission rate in order to achieve large transmission capacity in short range optical interconnects researchers have tried out a va riety of conventional dsp techniques with feed forward equalization ffe the data rate of non return to zero nrz has reached 71 gb s 1 other conventional equalization techniques such as decision feedback equalizer dfe and maximum likelihood sequence estimator mlse have also been utilized 2 5 by utilizing pre emphasis 94 gb s and 107 gb s pam 4 transmission have been demonstrated by k szczerba et al 6 and j lavrencik et al 7 respectively manuscript received xx xx xxxx revised xx xx xxxx corresponding author chuanchuan yang the authors are with the state key laboratory of advanced op tical communication systems and networks department of electronics peking university beijing 100871 china e mail zhouqingyi pku edu cn yangchuanchuan pku edu cn fzhang pku edu cn this work is funded by national key r d program of china under grant 2018 yfb 1801702 and joint fund of the ministry of education under grant 6141 a 02033347 researchers have also been trying to exploit the potential of dsp algorithms for long reach optical communication sys tems volterra nonlinear equalizer vnle has been utilized to mitigate nonlinear distortions 8 a few works have been done to lower the complexity of vnle 9 10 other nonlinearity compensation techniques have also been investigated such as digital back propagation dbp 11 perturbation based compensation 12 and nonlinear kalman filter 13 all the above mentioned dsp algorithms are designed on rich expert knowledge and some can be proved optimal for tractable mathematical models however many nonlinearities modulation nonlinearity together with square law detection that exist in practical systems can only be approximately captured and are difficult to compensate with conventional dsp techniques 14 in order to solve this problem many dsp algorithms based on neural network have been proposed including artificial neural network ann based equalizer 15 16 convolutional neural network cnn based equalizer 17 and recurrent neural network rnn based equalizer 18 19 implemented in different optical communication systems these nn based equalizers have not only reached lower ber but also shown excellent capability of mitigating nonlinearity although researchers report to have achieved lower ber using nn there s one problem it s difficult for nn to general ize over varied channel condition in an actual communication system the external environment and channel parameters may change causing the distribution of received data to drift away for example in data centers fibers are in motion due to rack vibration which causes the channel properties to vary over time an nn based equalizer that performs well on training set test set may suffer from severe performance degradation 20 on the other hand it s too costly to train different nns for different communication systems due to the lack of the ability to adjust parameters adaptively existing nn based equalizers cannot adapt to channel variations and thus are not practical enough developing adaptive nn based equalizer is therefore impor tant a new training scheme is expected which does not rely on massive amount of collected labeled data some previous works on adaptive equalizers based on machine learning also require training sequence 21 22 unfortunately similar parameter adjustment method cannot be used directly for nn based equalizers we ve found that when the short training sequence is provided to an nn based equalizer the equalizer still suffers from degraded ber performance researchers in the field of wireless communication are also exploring possible applications of deep learning techniques 23 27 most of these relevant works still rely on pilots when model ar x iv 1 90 7 10 25 8 v 5 ee ss s p 1 0 a pr 2 02 0 2 parameters are changed adaptively 23 s schibisch et al have used error correcting codes ecc to construct labeled dataset for online training but this causes overhead and relies on special protocol 28 in 29 the authors claimed that channel estimation based on semi supervised learning is still an open subject in this paper we propose an adaptive online training scheme which can be used to fine tune nn based equalizer without the help of training sequence the proposed adaptive nn based equalizer is called adann the deployment of adann include both offline training stage and online training stage although labeled training set is still required at offline stage at online stage no labeled data needs to be provided we collect recently received data using a sliding window then fine tune parameters with the help of unlabeled data inspired by virtual adversarial training vat which is a semi supervised learning method we propose a loss function named aug vat which outperforms naive decision directed self training and leads to a 4 5 times speedup adann is evalu ated under two scenarios a 56 gb s pam 4 modulated short distance 100 m vcsel mmf optical interconnect system and a 32 gbaud 16 qam modulated nyquist wdm system 960 km ssmf experimental results indicate that the ber performance of adann is much better compared with non adaptive nn and mlse conclusions can be reached that without training sequence it s possible to construct adaptive nn based equalizer with acceptable computational cost justi fying the significance of our work the rest of this paper can be organized as follows section ii provides a detailed introduction of our proposed online training scheme in section iii the computational complexity of proposed adann is analyzed in section iv the ber performance of adann non adaptive nn and mlse are compared section v concludes the paper ii adann online training based on semi supervised learning a nonlinear equalizer based on nn the nn we use contains an input layer an output layer and several hidden layers each hidden layer contains r neurons as shown in fig 1 a b the total number of layers contained in this nn is denoted as lnn for the i th symbol the relationship between adjacent fully connected layers denoted as layer k and k 1 where k 1 lnn 1 follows a i k w kh i k 1 bl 1 h i k a i k 2 where w k is r r weight matrix bk is bias vector for layer k function stands for activation function with softmax chosen for the output layer and relu for hidden layers different activation functions are displayed in fig 1 c d 1 offline training stage at the offline training stage the loss function has the form of cross entropy which is widely used when dealing with multi class classification 30 denote x 1 xi d softmax function a b fully connected layer with relu activation c relu y max x 0 x hk i relu wkhk 1 i bk hk 1 i hk i ak i relulinear input fully connected relu fully connected relu fully connected relu fully connected softmax output fig 1 a the structure of nn based equalizer evaluated in this paper including input layer hidden layers and output layer b fully connected layer with relu activation note that only a few neurons are explicitly drawn c the relu activation function d the softmax function the total number of symbols contained in the sequence as nseq the training process can be formulated as min w k bk lloss min w k bk 1 nseq nseq i 1 m j 1 y i j ln o i j 3 where m means a symbol only belongs to one of m classes the loss function lloss measures the difference between predicted probability o i and ground truth y i the whole training dataset is divided into batches each containing a small portion of all nseq training samples the network parameters are updated iteratively using stochastic gradient descent sgd optimizer with momentum which is much faster compared with vanilla sgd 30 2 equalizing process during equalization we denote the received signal sequence after interpolation and zero mean normalization as r r 1 r 2 r nseq where vectors r 1 r nseq correspond to nseq received symbols following chronological order the feature vector v i for the i th symbol is constructed as v i r i l r i r i l 4 we denote the interpolation multiple as thus the dimension of input feature vector v i is 2 l 1 the input output relationship is given in eq 1 2 b proposed adann online training scheme suppose that we aim to classify symbols into m classes correctly for such a multi class classification problem data can be either labeled or unlabeled the term labeled means that for an input vector x i the true label y i which is a one hot vector is provided unlabeled on the other hand means that the exact classification result is not known during online training stage it is impossible to gather large amount of labeled data it is possible that the transmitter pro vide short training sequences for channel estimation parameter fine tuning unfortunately short training sequences are not 3 2 l 1 2 l 1 i i th window i 1 th window i nb 1 th window concatenate 2 l 1 batch size nb transmitted signal stream slide feature vector for i nb 1 th symbol feature vector for i th symbol fig 2 a schematic illustration of the sliding window which is used to collect batches parameters can be updated only if a batch size nb has been collected enough for training nn a possible solution is that although the exact labels are not known we can make use of the distribution of received signals to monitor the drift away process and use such information to fine tune our equalizer here the concept of semi supervised learning arises semi supervised learning is a class of machine learning tasks that make use of unlabeled data for training typically a small amount of labeled data with a large amount of unlabeled data unlabeled data helps us by providing information about the probability density distribution of input vectors 31 32 based on the idea of semi supervised learning we now explain the process of adann we focus on the online training stage in this part first during online stage a sliding window is utilized to collect data which is illustrated in fig 2 the window containing 2 l 1 symbols and denoted as bold line slides on the received signal sequence at each step t nb input feature vectors v 1 v nb are collected and serve as a batch gradient gt is calculated based on the loss function and parameters are updated when no labels are provided nn can work adaptively under decision directed mode however if conventional cross entropy loss function is used the following problems occur 1 when using vanilla cross entropy loss function the convergence speed is very slow 2 in communication systems signals are inevitably dis torted by different levels of noise without data augmentation the nn is not robust against noise we ve verified experimentally that both vat and data aug mentation can accelerate the training process greatly there fore we propose a loss function named augmented virtual adversarial training or aug vat for short aug vat combines model 33 and vat 34 considering that the loss function should be consistent with the communication scenario fig 3 shows the general structure of adann with aug vat the detailed algorithm will be given as follows model encourages consistent nn output between two x i stochastic data augmentation nn based equalizer decision y i compute aug vat loss functionupdate parameters o i add virtual adversarial perturbation r i vadv o i adv y i pseudo label clean data fig 3 the general structure of adann with aug vat serves as loss function adann s parameters are updated with the help of pseudo label y i realizations of one input vector under two different data aug mentation conditions denote g v as the input augmentation function the augmentation is done by generating a random vector and add it on v g v v n 0 2 i 2 l 1 5 when using aug vat as loss function in adann every single input feature vector v should first be replaced using g v then serve as the input feature vector in vat vat is closely related to adversarial training 35 the adversarial perturbation for the i th input vector can be defined as r i adv arg max r r 6 m j 1 y i j ln nn g v i r j 6 this equation implies that by adding a small perturbation r i adv satisfying r i adv 6 on v i the loss function calculated using the perturbed input tend to increase adversarial train ing means that during training the loss function is always calculated based on the perturbed input vectors rather than the clean ones so that nn s robustness can be improved when full label information y i is not available radv can only be approximated by computing rvadv which is derived efficiently using one time power iteration method see algorithm 1 the complete form of aug vat loss function for a single batch can be formulated as lloss 1 nb nb i 1 m j 1 y i j ln nn g v i j 1 nb nb i 1 y i ladv ln nn g v i ladv 7 where index ladv means that after adding adversarial perturba tion and noise the i th symbol is classified into class ladv ladv arg max k nn g v i r i vadv k 8 the final pseudocode of our proposed adann online train ing scheme with aug vat as loss function is given in algorithm 2 the flow chart of adann is displayed in fig 4 at step t the gradient gt is accumulated before all the data in batch bt have been utilized after that parameters t should be updated using gradient based optimizer here all gradient based optimization algorithms can be written in the following general form 36 t 1 t t g 1 gt g 1 gt 9 4 start i 1 lloss 0 t 1 collect batch bt with sliding window update parameters using batch bt t t 1 last batch end yes no compute gradient with i th sample v i i i 1 i nb yes get gradient no two predictions original adversarial compute radvpredict classify predict classify loss update gradient update fig 4 the complete flow chart of the adann process the box with thinner dashed lines represents the processing of a single batch the box with thicker dashed lines on the other hand represents the processing of the i th sample in the batch algorithm 1 virtual adversarial perturbation require v i i th input feature vector i z require parameters of the offline trained network require step size of gradient estimation default 0 1 require length of adversarial perturbation 1 generate random vector d i with gaussian distribution 2 r d i 3 o i nn g v i 4 o i nn g v i r 5 compute lloss dkl o i o i m j 1 o i j ln o i j o i j difference between two output vectors can be quantified using kullback leibler kl divergence 6 g i rlloss 7 r i vadv g i g i 8 r i vadv r i vadv where gt represents the gradient obtained at the t th time step t g 1 gt denotes the adaptive learning rate and g 1 gt is the gradient estimation several influential optimizers include sgd 37 momentum sgd 38 nes terov momentum 39 adagrad 40 rmsprop 30 and adam 41 choosing the right optimizer has great impact on adann s performance experimental results show that adam performs the best for our task c other choices for loss function when labels are not provided y i j in eq 3 should be replaced with pseudo label y i j the loss function still has the cross entropy form lloss 1 nb nb i 1 m j 1 y i j ln o i j 10 pseudo label y i j can be obtained in different ways corre sponding to different loss functions in the last subsection algorithm 2 adann require v i i th input feature vector i z require 1 parameters of the offline trained network require g add gaussian noise with deviation 1 for t 1 2 3 do 2 for i in 1 nb do 3 o i nn t g v i 4 l arg maxk o i k predicted label 5 compute r i vadv using algorithm 1 6 o i adv nn t g v i r i vadv 7 ladv arg maxk o i adv k perturbed label 8 y i ladv 1 one hot perturbed label vector 9 lloss lloss 1 nb m j 1 y i j ln o i j 10 end for 11 gt lloss t compute gradient on batch 12 update t using gradient based optimizer e g adam 13 end for we ve proposed aug vat as loss function which combines model with vat besides vanilla self training model and vat can also be used alone as loss function 1 self training for the i th input feature vector v i the output probability vector o i nn v i the pseudo label y i can be derived by y i j 1 if j arg maxk o i k 0 otherwise 11 self training is similar to decision directed mode of conven tional adaptive equalizers and thus serves as a baseline 2 model only the main difference between model and self training lies in data augmentation the output prob ability vector o i nn g v i where g follows eq 5 the derivation of y i is the same as eq 11 3 virtual adversarial training only when using vanilla vat the output probability vector o i adv nn v i r i vadv where r i vadv is the virtual adversarial perturbation vector cal 5 850 nm pd new focus 1484 a 50 data out dsp equalization rx digital analog converter dac 850 nm vcsel new focus 1784 data in tx om 4 mmf 100 m voa times interpolation online data compute gradients nn model equalization real time digital signal oscilloscope dso normalization update fig 5 experimental configuration of the 56 gb s pam 4 signal transmission system utilizing 850 nm vcsel and om 4 mmf culated from algorithm 1 the derivation of y i is the same as eq 11 all these loss functions are compared in section iv there are several other loss functions we haven t covered temporal ensemble 33 requires re evaluation of all training samples each time the nn parameters are updated which is too costly mean teacher 42 constructs an ensemble using current model and several past models during training our experi ments show that mean teacher has no difference compared with self training we also know that many semi supervised learning algorithms are based on low dimension manifold assumption which assumes that data lie on a manifold of much lower dimension compared with input space relevant algorithms include low dimension manifold model ldmm and curvature regularization cure 43 44 however the estimation of local dimension curvature requires access to all data points in a small area which cannot be guaranteed iii computational complexity in this section we focus on analyzing the computational cost of adann fully connected nns mainly involve two types of computations multiplications and activation func tions note that here rather than tanh relu activation function is used therefore when analyzing complexity acti vation doesn t need to be considered for a non adaptive deep neural network the calculation of output probability vector o is called forward propagation which follows eq 1 2 when equalizing a single symbol each layer can be viewed as a vector the number of neurons contained in all lnn layers are 2 l 1 r r r and m the number of floating point multiplications knn can be calculated as knn 2 l 1 r lnn 3 r 2 r m 12 as for adann all the parameters need to be adjusted on line according to appendix a for a single back propagation the number of required floating point multiplications kback can be calculated as kback 2 knn lnn 2 r m 2 knn 13 the computational cost of back propagation is slightly larger than two times the cost of forward propagation when model serves as loss function two forward prop agations and one back propagation are needed in a single iteration thus the computational cost of adann model as loss function should be approximately 4 times the cost of non adaptive nn when aug vat serves as loss function two forward propagations one back propagation and computing rvadv mainly includes one forward propagation and one back propagation are needed in a single iteration in total the computational cost of adann aug vat as loss function should be approximately 7 times the cost of non adaptive nn as a contrast the computational cost of adann self training as loss function is 3 times the cost of non adaptive nn iv experimental results in order to justify adann s wide applicability we ve con ducted experiments under two scenarios a 56 gb s pam 4 modulated vcsel mmf optical link 100 m and a 32 gbaud 16 qam modulated nyquist wdm system 960 km ssmf the results are analyzed in this section a different loss functions we first present the adaptive training results for all the 4 different loss functions in this subsection experiments are con ducted with a 56 gb s pam 4 modulated vcsel optical link which is depicted in fig 5 the system mainly consists of a directly modulated 850 nm vcsel 100 m om 4 mmf and a photodiode pd the received signal is sampled using a high speed real time digital signal oscilloscope dso the 850 nm vcsel is new focusr 1784 while pd is new focusr 1484 a 50 the om 4 mmf is chosen as yofcr maxbandr om 4 multimode fiber the dso is agilent dsax 96204 q with sampling rate of 160 gsa s we first 4 x resample the received signal as stated in 45 4 for all experiments the signal is then normalized and input feature vectors are constructed we ve generated two sets of pam 4 symbols with bit pattern generator bpg of shf 12104 a 56 gb s following 46 we did not use prbs pattern instead a binary sequence is first generated by applying sign function to an gaussian noise sequence generated in matlab then converted into two pam 4 sequences each of the two datasets denoted as 6 set 1 and set 2 contains 220 pam 4 symbols set 2 was collected 56 hours after we collected set 1 for both set 1 and set 2 the receive optical power rop is 2 7 dbm between these two experiments we rebuilt the experimental system and adjusted the position of optical fiber in order to simulate a realistic scenario where fiber properties change slightly our nn based equalizer with 4 hidden layers lnn 6 r 10 is first trained offline using 25 data in set 1 the tap number of nn is first optimized by testing l 1 3 5 7 9 11 then tap number is fixed as l 5 since it achieves satisfying ber performance the batch size is fixed as nb 16384 for offline training momentum sgd is used with initial learning rate 0 004 and moving average decay 0 9 the model is trained for 200 epochs an epoch represents a single pass through the entire training set meaning that all feature vectors in the training set have been used for exactly one time this ensures good convergence and a ber lower than 10 3 during online stage a sliding window is utilized to collect data as fig 2 shows set 1 and set 2 are concatenated and then processed sequentially the batch size for online training is nb 8192 adam optimizer is used with initial learning rate 0 01 1 0 9 and 2 0 999 when processing set 1 and set 2 sequentially the ber for set 1 will remain relatively low while for set 2 the ber will increase abruptly by utilizing an online training scheme hopefully the ber will then decrease to a low level ber curves obtained are smoothed by averaging the ber values of neighboring 8 batches we mainly focus on two quantities 1 convergence time defined as the number of batches it takes before adann satisfies two conditions reaching a ber lower than 10 3 on recent 8 batches and reaching an overall ber lower than 10 3 on set 2 2 final ber defined as adann s ber on set 2 at the end of online training stage the convergence time as well as final ber are summa rized in table i the numbers in bold represents the best performance among one class of training method only hyper parameters are changed from table i we can tell that while self training suffers from slow convergence adann can be 4 5 times faster 72 16 4 5 which indicates adann s effective usage of unlabeled data the final ber values on set 2 are also displayed in order to show adann s good generalization ability in the following experiments adann s loss function is fixed as aug vat 0 15 0 3 we ve also calculated the change of weight matrices before and after the online training stage the results are given in appendix b b 100 m vcsel mmf link in this subsection adann is evaluated in the 100 m vcsel mmf optical link described above during online stage set 1 and set 2 each containing 128 batches batch size nb 8192 are concatenated and processed sequentially the ber curve of adann is compared with multiple equalizers including non adaptive nn nn with training sequence and mlse table i convergence time and final ber performance set 2 of adann with different loss functions including self training model vat and aug vat the 95 confidence interval of ber estimations are also provided loss converge final ber 95 confidence function time batch 10 4 interval 10 4 self training 72 8 39 6 32 10 84 model 0 1 24 4 27 2 84 6 08 model 0 2 16 2 75 1 63 4 25 model 0 3 16 2 44 1 67 3 45 model 0 4 32 3 43 2 16 5 08 vat 0 1 40 5 34 3 72 7 34 vat 0 2 24 2 98 1 80 4 53 vat 0 3 16 2 44 1 67 3 45 vat 0 4 16 2 52 1 45 3 96 vat 0 5 24 2 82 1 69 4 34 aug vat 0 10 0 3 16 2 59 1 51 4 06 aug vat 0 15 0 3 16 2 29 1 28 3 68 aug vat 0 20 0 3 16 2 75 1 63 4 25 aug vat 0 30 0 3 16 3 36 2 10 4 99 1 compare non adaptive nn the ber performance of adann as well as a non adaptive nn is displayed in fig 6 the network structure of nn is the same as that of adann before online equalization both adann and the nn are trained offline using 25 data in set 1 received optical power 2 7 dbm when processing set 2 the ber of nn rises to about 1 6 10 2 abruptly and remains unchanged since it s non adaptive while adann s ber also rises when first encountering set 2 the ber soon drops below 10 3 note that it only takes about 40 batches before the ber stabilizes again we ve also tested another adann model which is initially trained offline using a different dataset the rop of the new dataset is 4 7 dbm surprisingly compared with adann trained 2 7 dbm adann trained 4 7 dbm can achieve very similar ber this indicates that the adaptive training process of adann is robust even when a different offline trained model is used 2 compare nn with training sequence we ve already demonstrated that adann can adjust its parameters without the help of labels it s still necessary to investigate the ber performance of normal nn when short training sequence can ber 1 e 3 adann non adaptive set 1 set 2 fig 6 the ber performance of a normal nn based equalizer compared with adann note that performance of adann trained with a different dataset received optical power is 4 7 dbm is also plotted in order to show adann s robustness 7 ber 1 e 3 set 1 set 2 fig 7 the ber performance of normal nn based equalizer trained in a supervised manner denotes the proportion of training sequence be provided in this part training sequences are provided at the beginning of set 1 and set 2 respectively concretely nn s ber on a single batch is measured immediately after nn has been trained on this batch by minimizing the cross entropy loss function the nn is trained using the training sequences for 100 iterations ensuring convergence denote the ratio of training sequence length to set 1 length or set 2 length as fig 7 shows the ber of nn based equalizer fine tuned with provided training sequence if 1 32 including more than 32768 symbols the performance is similar to adann for smaller the performance degradation becomes unacceptable our results show that adann can achieve better at least similar ber performance compared with nn based equalizer even when a portion of labels are provided to that nn there are cases when sending extra training sequence cannot be supported adann provides an effective alternative for these occasions 3 compare conventional mlse we have also compared adann with conventional mlse the memory length of mlse takes its value from lch 1 3 5 7 the channel response coefficients are estimated using least mean square lms algorithm note that true labels are provided when adann set 2 set 1 mlse lch 1 mlse lch 3 mlse lch 5 mlse lch 7 fig 8 the ber performance of adann and conventional mlse ber 1 e 3 set 1 set 2 fig 9 the ber performance when choosing different nb following chronological order different curves corresponds to different nb updating these coefficients indicating that mlse works adap tively in a supervised manner the update frequency of chan nel response coefficients is exactly the same as the update frequency of adann parameters the ber performance of both adann and mlse are displayed in fig 8 obviously once adann converges it has much lower ber 3 0 10 4 compared with mlse 2 4 10 3 the results show that adann s generalization ability is stronger than adaptive con ventional algorithms 4 discuss influence of batch size in previous paragraphs the batch size nb 8192 choosing different nb will influence the online training process concretely nb describes how much data needs to be collected before adann updates its parameters smaller nb seems beneficial since model parameters are updated more frequently unfortunately a very small nb causes new problem since it leads to very small batch containing few data which does not reflect the overall probability distribution fig 9 provides the ber performance when nb takes different values ranging from 128 to 32768 as can be seen from the case nb 128 when trained on very small batches adann may fail to converge when building an actual system nb should be chosen carefully depending on how frequently the link properties change c long distance optical transmission in fact adann can be used in many communication sys tems in this subsection in order to show its wide applicability adann is evaluated with a 32 gbaud 16 qam modulated nyquist wdm system which is depicted in fig 10 an arbitrary waveform generator operating at 64 gsa s generates 32 gbaud 16 qam baseband signals a root raised cosine rrc filter with a roll off factor of 0 1 is chosen for nyquist pulse shaping at the transmitter we use external cavity laser ecl with narrow linewidth of 25 khz the transmission link consists of 12 spans of 80 km ssmf with erbium doped fiber amplifier edfa only amplification at the receiver an optical band pass filter obpf with 45 ghz bandwidth is used as the receiving filter the coherent receiver consists of an optical local oscillator lo with 25 khz linewidth optical 8 data out dsp equalization rx arbitrary waveform generator awg ecls tx compute gradients nn model equalization real time digital signal oscilloscope dso update online data iq modulation edfa 80 km ssmf 12 obpf c d c o m p e n sa ti o n p h a se r e c o v e r y in te r p o la ti o n h y b r id fig 10 experimental configuration of the 32 gbaud 16 qam wdm system utilizing ecls and 960 km ssmf hd fec ber 3 8 e 3 set 1 set 2 set 1 set 2 fig 11 the ber performance of a normal nn based equalizer compared with adann after adann stabilizes its ber on set 2 can be lower than 3 8 10 3 which is the 7 hd fec limit hybrid and balanced detectors bd a real time oscilloscope operating at 80 gsa s stores the received signal the offline dsp has several stages first a fir filter roughly compensates for accumulated dispersion then carrier frequency recovery is conducted after synchronization carrier phase recovery is conducted and finally adann is used to mitigate nonlinear distortions several different sets of 16 qam symbols are collected each containing 50400 symbols two sets are chosen denoted as set 1 and set 2 and concatenated for set 1 the rop is 0 dbm while for set 2 the rop is 1 dbm the polarization of set 2 is different from set 1 the received signals are 4 x resampled 4 adann with 4 hidden layers lnn 6 r 10 is first trained offline using 50 data in set 1 during online stage the batch size is nb 1260 when processing set 1 and set 2 sequentially the ber performance on different batches are displayed in fig 11 again adann shows adaptivity and performs better than non adaptive nn v conclusion in this paper we propose an adaptive online training scheme which can be used to fine tune nn based equalizer without the help of training sequence the proposed adap tive nn based equalizer is called adann at the online stage recently received data are collected using a sliding window with the help of unlabeled data all the parame ters in our nn are fine tuned in an unsupervised manner which is similar to decision directed adaptive equalization the performance of adann is evaluated under two scenarios a 56 gb s pam 4 modulated vcsel optical link and a 32 gbaud 16 qam modulated optical transmission system 960 km ssmf heterogeneous datasets are concatenated to test adann s adaptivity our experimental results indicate that by introducing adann the ber performance can be improved compared with both non adaptive nn based equalizers and conventional mlse compared with self training which serves as a baseline adann s convergence speed can be 4 5 times faster the online training process has been proved robust when different offline trained models are used which shows adann s wide applicability the computational complexity of adann training scheme is also analyzed theoretically we conclude that it is feasible to construct adaptive nn based equalizer with acceptable computational cost when training sequences aren t provided the generalization ability of all nn based equalizers can be greatly improved using our pro posed method appendix a the complexity of back propagation for adann all the parameters including both weights and biases need to be adjusted online based on gradi ents w klloss and bklloss according to 30 all these gradients are calculated by implementing back propagation algorithm which is given in algorithm 3 consider the back propagation from layer k to layer k 1 the number of neurons contained in each layer can be denoted as rk and rk 1 it s straightforward to see that during the back propagation from layer k to layer k 1 g g ak requires rk multiplications w klloss gh k 1 requires rk 1 rk multiplications and g w k g requires rk 1 rk multiplications by summing over all layers we can conclude that for a single back propagation the number of required 9 algorithm 3 backward propagation require lnn the number of layers in the network require w k k 1 lnn 1 weight matrices require bk k 1 lnn 1 bias vectors require v the input feature vector 1 g olloss last layer 2 for k lnn 1 1 do 3 g aklloss g ak 4 bklloss g 5 w klloss gh k 1 6 g hk 1 lloss w k g continue to layer k 1 7 end for floating point multiplications kback can be calculated as kback 2 2 l 1 r r lnn 3 2 r 2 r 2 r m m 2 knn lnn 2 r m 14 appendix b weight changes it would be useful to know what changes does online training stage have on nn parameters we ve compared the weight matrices in two models the initial model trained offline and the final model which has been online trained with set 1 and set 2 our nn model has lnn 6 layers 1 input layer 4 hidden layers and 1 output layer there are 5 weight matrices between these 6 layers denoted as w 1 w 5 define function s w as summing up the absolute value of all elements in matrix w s w i j wij 15 we now calculate the following ratio for all layers k 1 2 5 rk s w k s w initk s w finalk w init k s w initk 16 the ratio rk reflect the change of weight matrix w init k the results are given in table ii table ii ratio rk for weight matrices between different layers k s w initk s w k rk 1 71 122 5 788 0 081 2 30 641 0 929 0 030 3 31 368 0 250 0 008 4 34 080 0 189 0 006 5 21 338 0 172 0 008 it can be concluded that only minor changes have occurred during online training stage on the other hand it can be observed that rk becomes larger for weight matrices near the input layer references 1 d kuchta a rylyakov f doany c schow j proesel c baks and a larsson a 71 gb s nrz modulated 850 nm vcsel based optical link ieee photon technol lett vol 27 no 6 pp 577 580 jan 2015 2 d kuchta higher speed vcsel links using equalization presented at the european conf on optical communication du sseldorf germany 2016 paper tu 1 a 2 3 z tan c yang y zhu z xu k zou f zhang and z wang high speed band limited 850 nm vcsel link based on time domain interference elimination ieee photon technol lett vol 29 no 9 pp 751 754 may 2017 4 f karinou et al experimental performance evaluation of equalization techniques for 56 gb s pam 4 vcsel based optical interconnects presented at the european conf on optical communication valencia spain 2015 paper p 4 10 5 t lengyel k szczerba p westbergh m karlsson a larsson and p andrekson sensitivity improvements in an 850 nm vcsel based link using a two tap pre emphasis electronic filter j lightw technol vol 35 no 9 pp 1633 1639 dec 2016 6 k szczerba t lengyel m karlsson p andrekson and a larsson 94 gb s 4 pam using an 850 nm vcsel pre emphasis and receiver equalization ieee photon technol lett vol 28 no 22 pp 2519 2521 nov 2016 7 j lavrencik v thomas s varughese and s ralph dsp enabled 100 gb s pam 4 vcsel mmf links j lightw technol vol 35 no 15 pp 31893196 aug 2017 8 y gao f zhang l dou z chen and a xu intra channel nonlin earities mitigation in pseudo linear coherent qpsk transmission systems via nonlinear electrical equalizer opt commun vol 282 no 12 pp 2421 2425 jun 2009 9 l ge w zhang c liang and z he threshold based pruned retraining volterra equalization for 100 gbps lane and 100 m optical interconnects based on vcsel and mmf j lightw technol vol 37 no 13 pp 3222 3228 apr 2019 10 s lu et al 81 7 complexity reduction of volterra nonlinear equalizer by adopting l 1 regularization penalty in an ofdm long reach pon presented at the european conf on optical communication gothen burg sweden 2017 paper p 1 sc 3 38 11 e ip and j kahn compensation of dispersion and nonlinear impair ments using digital backpropagation j lightw technol vol 26 no 20 pp 3416 3425 oct 2008 12 r dar m feder a mecozzi and m shtaif inter channel nonlinear interference noise in wdm systems modeling and mitigation j lightw technol vol 33 no 5 pp 1044 1053 dec 2014 13 c li f zhang y zhu m jiang z chen and c yang fiber nonlinearity mitigation in single carrier 400 g and 800 g nyquist wdm systems j lightw technol vol 36 no 17 pp 3707 3715 sep 2018 14 t o shea j hoydis an introduction to deep learning for the physical layer ieee trans cogn commun netw vol 3 no 4 pp 563 575 dec 2017 15 j estaran et al artificial neural networks for linear and non linear impairment mitigation in high baudrate im dd systems presented at the european conf on optical communication du sseldorf germany 2016 paper m 2 b 2 16 c chuang et al employing deep neural network for high speed 4 pam optical interconnect presented at the european conf on optical communication gothenburg sweden 2017 paper w 2 d 2 17 c chuang et al convolutional neural network based nonlinear classi fier for 112 gbps high speed optical link presented at the optical fiber communication conf san diego ca usa 2018 paper w 2 a 43 18 c ye et al recurrent neural network rnn based end to end non linear management for symmetrical 50 gbps nrz pon with 29 db loss budget presented at the european conf on optical communication rome italy 2018 paper mo 4 b 3 19 q zhou c yang a liang x zheng and z chen low computa tionally complex recurrent neural network for high speed optical fiber transmission opt commun vol 441 pp 121 126 jun 2019 20 b karanov m chagnon f thouin t eriksson h bu low d lavery p bayvel and l schmalen end to end deep learning of optical fiber communications j lightw technol vol 36 no 20 pp 4843 4855 oct 2018 21 g chen et al machine learning adaptive receiver for pam 4 modu lated optical interconnection based on silicon microring modulator j lightw technol vol 36 no 18 pp 4106 4113 sep 2018 10 22 m peng c nikias and j proakis adaptive equalization for pam and qam signals with neural networks in proc of the 25 th ieee asilomar conf on signals systems computers acssc vol 1 pp 496 500 1991 23 h ye g li and b juang power of deep learning for channel estimation and signal detection in ofdm systems ieee wireless commun lett vol 7 no 1 pp 114 117 sep 2017 24 h ye l liang g li and b juang deep learning based end to end wireless communication systems with conditional gan as unknown channel arxiv preprint arxiv 1903 02551 mar 2019 25 j zhang et al deep learning based on orthogonal approximate message passing for cp free ofdm in ieee int conf on acoustics speech and signal processing icassp pp 8414 8418 2019 26 v raj s kalyani backpropagating through the air deep learning at physical layer without channel models ieee commun lett vol 22 no 11 pp 2278 2281 nov 2018 27 t o shea t roy n west and b hilburn physical layer com munications system design over the air using adversarial networks in european signal processing conf eusipco pp 529 532 2018 28 s schibisch et al online label recovery for deep learning based communication through error correcting codes in int symposium on wireless communication systems iswcs pp 1 5 2018 29 e balevi and j andrews deep learning based channel estimation for high dimensional signals arxiv preprint arxiv 1904 09346 apr 2019 30 i goodfellow y bengio and a courville deep learning the mit press 2016 31 x zhu semi supervised learning literature survey university of wisconsin madison department of computer sciences 2005 online available at https minds wisconsin edu handle 1793 60444 32 d lee pseudo label the simple and efficient semi supervised learning method for deep neural networks in workshop on challenges in representation learning icml 2013 33 s laine t aila temporal ensembling for semi supervised learning in proc int conf on learning representations iclr 2017 34 t miyato s maeda s ishii and m koyama virtual adversarial training a regularization method for supervised and semi supervised learning ieee trans pattern anal jul 2018 35 i goodfellow j shlens and c szegedy explaining and harnessing adversarial examples in proc int conf on learning representations iclr 2015 36 h huang c wang and b dong nostalgic adam weighting more of the past gradients when designing the adaptive learning rate in proc int joint conf on artificial intelligence ijcai pp 2556 2562 2019 37 h robbins and s monro a stochastic approximation method ann math stat pp 400 407 sep 1951 38 d rumelhart g hinton and r williams learning representations by back propagating errors nature vol 323 pp 533 536 oct 1986 39 i sutskever j martens g dahl g hinton on the importance of initialization and momentum in deep learning in proc int conf on machine learning icml pp 1139 1147 2013 40 j duchi e hazan and y singer adaptive subgradient methods for online learning and stochastic optimization j mach learn res vol 12 pp 2121 2159 jul 2011 41 d kingma and j ba adam a method for stochastic optimization in proc int conf on learning representations iclr 2015 42 a tarvainen and h valpola mean teachers are better role mod els weight averaged consistency targets improve semi supervised deep learning results in advances in neural information processing systems nips pp 1195 1204 2017 43 s osher z shi and w zhu low dimensional manifold model for image processing siam j imaging sci vol 10 no 4 pp 1669 1690 oct 2017 44 b dong h ju y lu and z shi 2019 cure curvature regu larization for missing data recovery arxiv preprint arxiv 1901 09548 may 2019 45 a liang c yang c zhang y liu f zhang z zhang and h li experimental study of support vector machine based nonlinear equalizer for vcsel based optical interconnect opt commun vol 427 pp 641647 nov 2018 46 t eriksson h bu low and a leven applying neural networks in optical communication systems possible pitfalls ieee photon technol lett vol 29 no 23 pp 2091 2094 dec 2017 http arxiv org abs 1903 02551 http arxiv org abs 1904 09346 https minds wisconsin edu handle 1793 60444 http arxiv org abs 1901 09548 i introduction ii adann online training based on semi supervised learning ii a nonlinear equalizer based on nn ii a 1 offline training stage ii a 2 equalizing process ii b proposed adann online training scheme ii c other choices for loss function iii computational complexity iv experimental results iv a different loss functions iv b 100 m vcsel mmf link iv b 1 compare non adaptive nn iv b 2 compare nn with training sequence iv b 3 compare conventional mlse iv b 4 discuss influence of batch size iv c long distance optical transmission v conclusion appendix a the complexity of back propagation appendix b weight changes references