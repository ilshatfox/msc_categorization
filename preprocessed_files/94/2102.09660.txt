generative speech coding with predictive variance regularization w bastiaan kleijn 1 2 andrew storus 1 michael chinen 1 tom denton 1 felicia s c lim 1 alejandro luebs 1 jan skoglund 1 hengchin yeh 1 1 google llc san francisco usa 2 school of engineering and computer science victoria university of wellington new zealand abstract the recent emergence of machine learning based generative mod els for speech suggests a significant reduction in bit rate for speech codecs is possible however the performance of generative models deteriorates significantly with the distortions present in real world input signals we argue that this deterioration is due to the sensi tivity of the maximum likelihood criterion to outliers and the inef fectiveness of modeling a sum of independent signals with a single autoregressive model we introduce predictive variance regulariza tion to reduce the sensitivity to outliers resulting in a significant increase in performance we show that noise reduction to remove unwanted signals can significantly increase performance we pro vide extensive subjective performance evaluations that show that our system based on generative modeling provides state of the art cod ing performance at 3 kb s for real world speech signals at reasonable computational complexity index terms speech coding wavenet regularization 1 introduction in recent years it has become possible to generate high quality speech from a conditioning sequence with a very low information rate this suggests that generative synthesis forms a natural basis for the coding and enhancement of speech however it has been found that generative synthesis is sensitive to the quality of the data used for training and the conditioning sequences used for training and inference this can result in poor synthesized speech quality in this paper we discuss methods that significantly reduce the impact of distortions in the input signal on signal synthesis for speech coding wavenet 1 first showed that the generation of high quality speech from only a low rate conditioning sequence such as writ ten text is possible wavenet is based on an autoregressive struc ture that specifies a predictive distribution for each subsequent sig nal sample while wavenet uses a dilated convolution to determine the predictive distribution other recurrent neural networks structures such as wavernn 2 and the wavegru structure that we use in this paper have also been used successfully for this purpose al though autoregressive structures for synthesis are common feed forward structures are used by for example parallel wavenet 3 waveglow 4 wavegan 5 and gansynth 6 it is fair to state that while the more recent methods may have computational advan tages they do not surpass the basic synthesis quality of the original wavenet approach the high quality of generative speech synthesis has led to a sig nificant effort towards its usage for coding in contrast to synthesis from text synthesis for coding must be able to generate an unlim ited range of voices and its conditioning is variable as it is computed from input signals that may suffer from a range of distortions it was found that the synthesis of a wide range of voices with a single gen erative model is not a significant problem generative synthesis of a wide range of unknown voices with a single model results in only a minor reduction of speaker identifiability 7 however variability in the conditioning leads to a reduced output speech quality that can not be improved significantly with straightforward measures such as training with noisy conditioning and undistorted target signals hence despite extensive research e g 8 12 generative synthesis based speech coding has not yet seen major practical applications the contribution of this paper consists of the identification of causes of the sensitivity to distortion the development of methods to reduce this sensitivity and subjective testing of the new methods confirming the improvements we show that a major cause of the sensitivity is associated with an attribute of the log likelihood ll objective function the ll objective function incurs a high penalty if the model assigns a low probability to observed data hence in the context of autoregressive structures it encourages an overly broad predictive distribution when at least some training data are difficult to predict accurately from the past signal and conditioning which is true for real world training data we mitigate this effect by including predictive variance regularization in the objective function we also show with experiments that input noise suppression can improve performance significantly it is well known that a sum of low order linear autoregressive processes is in general not a low order autoregressive process this suggests that linear autoregres sive models are poor for sums of independent signals and our re sults indicate this also holds for nonlinear autoregressive models whereas traditional analysis by synthesis coding methods can com pensate for model inadequacies this is not true for generative synthe sis based coding explaining the effectiveness of noise suppression 2 problem formulation in this section we first describe how an autoregressive model is used to model a process the method is as proposed in 1 we then dis cuss a common problem that occurs when training such sequences consider a random process xi consisting of real valued ran dom samplesxi with a time index i z the joint distribution of a finite sequence p xi xi n can be expressed as a product of conditional distributions p xi xi n n j 0 p xi j xi j 1 xi n 1 where is conditioning information it follows from 1 that we can create an approximate realiza tion of a random process by recursively sampling from a model of the predictive distribution p xi xi 1 xi n for sufficiently large n it is convenient to use a standard form distribution q xi ar x iv 2 10 2 09 66 0 v 1 ee ss a s 1 8 f eb 2 02 1 with parameters as a model predictive distribution the standard form distribution can be a gaussian or a logistic mixture for exam ple this formulation allows us to predict the model parameters with a deterministic neural network xi 1 xi n w 7 where w is a vector of network parameters thus the predictive distribution for sample xi is now q xi xi 1 xi n w to find the parameters w a reasonable objective is to min imize the kullback leibler divergence between the ground truth joint distribution p xi xi n and the model distribution q xi xi n or equivalently the cross entropy between these distributions the latter measure is tractable even though p is only available as an empirical distribution it follows from 1 and our formulation of q xi that cross entropy based estimation of the parameters of can be implemented using maximum likelihood based teacher forcing for a database of m signal samples the maximum likelihood estimate of w can be written as w argmax w m i 1 log q xi xi 1 xi n w 2 note that 2 leads to rapid training as it facilitates parallel imple mentation for sufficiently large n and m the ll objective pro vides an upper bound on the differential entropy rate as h xi xi j xi n 1 m m i 1 log q xi xi 1 xi n w 3 where for notational convenience we considered the unconditioned case conversely 3 can be interpreted as a lower bound on a mea sure of uncertainty associated with the model predictive distribution this lower bound is associated with the process itself and not with the model although the differential entropy rate is subadditive for summed signals predictive models tend not to work well for summed signals in general a model of summed signals is essentially multiplicative in the required model configurations it is well known that the sum of finite order linear autoregressive models is in general not a finite order autoregressive model 13 it is relatively straightforward to reduce this problem with noise suppression a more difficult problem relates to well known drawbacks of the kullback leibler divergence and hence the ll objective of 2 when the model distribution q vanishes in the support region of the groundtruth p the kullback leibler divergence diverges in 2 this manifests itself as a severe penalty for training data xi that have a low model probability q xi xi 1 xi n w hence a few nonrepresentative outliers in the training data may lead the training procedure to equip the predictive model distribution with heavy tails such tails lead to signal synthesis with a relatively high entropy rate during inference in audio synthesis this corresponds to a noisy synthesized signal hence it is desirable to counter the severity of the penalty for low probability training data we can identify a second relevant drawback to the ml objec tive when the ml objective function is used the model distribu tion should converge to the groundtruth distribution with increasing database size however in practice the stochastic nature of the train ing data and the training method results in inaccuracies and this in turn means the method attempts to minimize the impact of such er rors for example the implicit description of pitch by the predictive distribution may be inaccurate a predictive model distribution with heavy tails for voiced speech then increases the likelihood of training data as it reduces the impact of the model pitch deviating from the groundtruth pitch from this reasoning we conclude that it is desir able to account for the audibility perception of distortions leading to empirically motivated refinements of the objective function the problems associated with the ll objective have been con sidered earlier in different contexts the vanishing support problem described above was addressed in the context of generative adver sarial networks gans 14 where the implicit jensen shannon objective function of the original method and the more general f divergence based method 15 suffer at least in principle from sim ilar problems the support problem in gans can be removed by using the 1 wasserstein distance 16 or with maximum mean dis crepancy mmd 17 18 however as these measures require as input two empirical distributions these methods are natural for static distributions and not for dynamic predictive distributions the meth ods also do not facilitate adjustment to account for perception an existing approach that attempts to compensate for overly broad pre dictive distributions is to lower the temperature during inference e g 19 the predictive distribution is typically raised to a power then renormalized this approach does not account for the implicit cost penalty in the basic training objective 3 objective functions for predictive distribution models in this section we discuss two related approaches that modify the maximum likelihood criterion to obtain improved performance both approaches aim to reduce the impact of data points in the train ing set that are difficult to predict the methods remove the need for heuristic modifications during inference while the principles of our methods are general we apply them to the mixture of logistics distribution that we use in our coding scheme cf section 4 3 1 encouragement of low variance predictive distributions we now discuss how to add a term to the objective function that encourages low variance predictive distributions in this approach we define the overall objective function for the weights w given a database x as j x w jll x w jvar x w 4 where the log likelihood over the database jll x w edata log q xi xi 1 xi n w is combined with a vari ance regulatization term jvar x w that is defined below and where is a constant that must be tuned 3 1 1 predictive variance computation the variance of the predictive distribution is an instantaneous pa rameter that varies over a database and jvar x w must be an average over the predictive distributions the predictive distribution of each sample has a distinct variance and the averaging method can be selected to have properties that are advantageous for the specific application as noted in section 2 the predictive distribution is a standard form distribution q x the predictive distribution q x is commonly a mixture dis tribution hence we must find an expression for the variance of a mixture distribution we first note that the mean of a mixture distri bution is simply eq x k k 1 k eqk x k k 1 k k 5 where eq is expectation over q and qk q k sk with q a mix ture component the variance of the mixture distribution is eq x eq x 2 k k 1 k 2 k 2 k 2 6 we now consider the specific case of a mixture of logistics in more detail the logistic distribution for component k is q x k sk e x k sk 1 e x k sk 2 7 where sk is the scale and k is an offset it is easily seen that the logistic distribution is symmetric around and that hence is the distribution mean the variance of the logistic distribution is ex q x ex q x 2 s 2 2 3 8 we can now write down the variance of the mixture of logistics model by combining 6 and 8 2 q k k 1 k s 2 k 2 3 2 k k k 1 k k 2 9 3 1 2 regularization terms the most obvious approach for reducing the prediction variance is to use the prediction variance 9 directly as variance regularization in the objective function 4 jvar x w edata 2 q 10 where edata indicates averaging over the database that is we en courage selection of weights w of the network that minimize 2 q straightforward optimization of 10 over a database may re sult in the prediction variance being reduced mainly for signal re gions where the conditional differential entropy 3 is large the conditional differential entropy can be decomposed into the sum of a scale independent term and a logarithmic scale signal variance dependency for speech the scale independent term is large for un voiced segments while the scale dependent term is large for voiced speech as it is relatively loud for signals that have uniform overall signal variance it may be desirable to encourage low predictive variance only for regions that have relatively low conditional differential entropy for speech that would correspond to encouraging low variance for voiced speech only this can be accomplished by a monotonically increasing con cave function of the predictive variance the logarithm is particu larly attractive for this purpose as it is invariant with scale the effect of a small variance getting smaller equals that of a large variance getting smaller by the same proportion we then have jvar x w edata log q a 11 with a providing a floor 3 2 baseline distribution approach for completeness we describe an alternative method for preventing the vanishing support problem of the kullback leibler divergence by using a baseline distribution to this purpose consider a mix ture distribution of the form qtrain xi 0 q xi 0 k k 1 k q xi xi 1 xi n wk 12 where the parameters 0 and 0 are set by the designer and where the first term is omitted during inference the other terms must be renormalized by a factor 1 1 0 by selecting 0 to provide an overly broad distribution the distribution used for inference will be of low variance 4 system architecture in this section we describe the architecture of our coding scheme the parameter settings of the scheme are provided in section 5 1 let us consider an input signal with a sampling rate s hz to avoid the need for modeling summed independent signals the input is pre processed with a real time tasnet 20 21 the encoder first converts the signal into a sequence of log mel spectra e g 22 a set of subsequent log mel spectra are stacked into a supervector that is subjected to a karhunen loe ve transform klt that is optimized off line the transformed stacked log mel spectra are encoded using split vector quantization with a small num ber of coefficients per split no other information is encoded the decoder first decodes the bit stream into a sequence of quan tized log mel spectra these spectra form the input to the condition ing stack which consists of a set of 1 d convolutional layers all ex cept the first with dilation the output is a vector sequence with a sampling rate equal to that of the mel spectra of the encoder and a dimensionality equal to the state of the gru unit discussed below the autoregressive network consists of a multi band wavegru which is based on gated recurring units gru 23 for our n band wavegru n samples are generated simultaneously at an up date rate of s n hz one sample for each frequency band for each update the state of the gru network is projected onto ann k 3 dimensional space that defines n parameter sets each set corre sponding to a mixture of logistics for a band the value of a next signal sample for each band is then drawn by first selecting the mix ture component a logistics distribution according to its probability and then drawing the sample from this logistic distribution by trans forming a sample from a uniform distribution for each set of n samples a synthesis filter bank produces n subsequent time domain samples which results in an output with sampling rate s hz the input to the wavegru consists of the addition of an autore gressive and conditioning components the autoregressive compo nent is a projection of the last n frequency band samples projected onto a vector of the dimensionality of the wavegru state the sec ond component is the output of the conditioning stack dimension ality of the wavegru state repeated in time to obtain the correct sampling rate of s n hz the training of the gru network and the conditioning stack is performed simultaneously using teacher forcing that is the past signal samples that are provided as input to the gru are ground truth signal samples the objective function combining log like lihood cross entropy and variance regularization is used for each subsequent signal sample for our implementation with variance regularization we found the baseline distribution not to aid perfor mance significantly and it was omitted from the experiments table 1 systems in test system label b v t vt q qv qt qvt var regularization 3 3 3 3 tasnet noise supp 3 3 3 3 3 kb s quantization 3 3 3 3 93 pruning 3 3 3 3 5 experiments our experiments had two goals the first is to show the effect of predictive variance regularization and noise suppression the second is to show that our contributions enable a practical system 5 1 system configuration we tested eight systems all variants based on a single baseline sys tem operating on 16 khz sampled signals it is conditioned using a sequence of 160 dimensional log mel spectra computed from 80 ms windows at an update rate of 50 hz the system uses four frequency bands each band sampled at 4 khz the conditioning stack consists of a single non causal input layer expanding from 160 channels to 512 channels three dilated causal convolutional layers with kernel size two and three upsampling transpose convolutional layers ker nel size two the overall algorithmic delay is 90 ms the condition ing outputs are tiled to match the gru update frequency the gru state dimensionality is 1024 and eight mixture of logistics compo nents are used for the predictive distribution per band the systems were trained from randomly initialized weights w for 7 5 million steps using a mini batch size of 256 the target signal was from a combination of clean 24 25 and noisy sources 26 including large proprietary tts datasets additional noise was added from 27 with random snr between 0 and 40 db snr table 1 shows the combinations of coder attributes that were used we briefly discuss each attribute the variance regularization included refinements that further improved its performance it was applied to the first two bands only and in 4 was made proportional to a voicing score the noise suppression system was a version of convtasnet 21 the weight pruning attribute was selected to en able implementation on consumer devices for the three gru ma trices we used block diagonal matrices with 16 blocks which uses 93 fewer weights than a fully connected model for other hidden layers we applied iterative magnitude pruning to remove 92 of the model weights 28 the pruning makes the codec with tasnet run reliably on a pixel 2 phone in single threaded mode the system was quantized with 120 bits per supervector each supervector containing two log mel spectra for an overall rate of 3 kb s the quantization was a two dimensional vector quantization of the klt coefficients 5 2 testing procedure to evaluate the absolute quality of the different systems on different snrs a mean opinion score mos listening test was performed except for data collection we followed the itu t p 800 29 acr recommendation the data was collected using a crowd sourcing platform with the requirements on listeners being native english speakers and using headphones the evaluation dataset is composed of 30 samples from the noisy vctk dataset 30 15 clean and 15 augmented with additive noise at various snrs 2 5 7 5 and 12 5 db each utterance for each system was rated about 200 times and the average and 95 confidence interval were calculated per snr fig 1 quality vs snr for baseline regularized denoised and reg ularized and denoised systems also shown are the quality of unpro cessed and denoised only signals fig 2 quality vs snr for the pruned and quantized systems 5 3 results the quality for the systems of table 1 is shown in figs 1 and 2 the mos with 95 confidence intervals are given for four snrs fig 1 displays the effect of predictive variance regularization and noise suppression tn without weight pruning and quantiza tion predictive variance regularization results in a significant quality improvement and reduces the sensitivity to noise in the input signal noise suppression aids performance when noise is present fig 2 shows the quality for pruned and quantized systems for this case the improvement due to variance regularization is partic ularly large for clean signals the effect of noise suppression tn varies in an unexpected manner with snr this likely results from an interaction between noise suppression and quantization it may be related to noise suppression reducing signal variability and quan tization reducing noise on its own as a reference fig 2 provides the performance of the opus codec 31 operating at 6 kb s and the evs codec 32 operating at 5 9 kb s for fairness with disabled dtx it is seen that the proposed fully practical 3 kb s wavegru coder performs significantly better than opus at 6 kb s and similarly to evs operating at 5 9 kb s 6 conclusion we have developed a robust speech codec using neural network based signal synthesis that encodes speech at 3 kb s our system is suitable for for example low rate video calls and fits in con sumer devices as evidenced by our implementation running on a wide range of mobile phones including the pixel 2 our experiments show that its quality is similar or better than state of the art conven tional codecs operating at double the rate our main contribution is that we addressed the impact of variability and distortion inherent in real world input to practical speech codecs we identified as causes for poor performance i the inherent emphasis of outliers by the maximum likelihood criterion and ii the difficulty of modeling a sum of multiple independent sources we resolved these problems with predictive variance regularization and noise suppression 7 references 1 a v d oord s dieleman h zen k simonyan o vinyals a graves n kalchbrenner a senior and k kavukcuoglu wavenet a generative model for raw audio arxiv preprint arxiv 1609 03499 2016 2 n kalchbrenner e elsen k simonyan s noury n casagrande e lockhart f stimberg a v d oord s dieleman and k kavukcuoglu efficient neural audio syn thesis arxiv preprint arxiv 1802 08435 2018 3 a v d oord y li i babuschkin k simonyan o vinyals k kavukcuoglu g driessche e lockhart l cobo f stim berg et al parallel wavenet fast high fidelity speech synthesis in international conference on machine learning pmlr 2018 pp 3918 3926 4 r prenger r valle and b catanzaro waveglow a flow based generative network for speech synthesis in 2019 ieee int conf acoust speech signal processing icassp ieee 2019 pp 3617 3621 5 c donahue j mcauley and m puckette adversarial audio synthesis arxiv preprint arxiv 1802 04208 2018 6 j engel k k agrawal s chen i gulrajani c donahue and a roberts gansynth adversarial neural audio synthe sis arxiv preprint arxiv 1902 08710 2019 7 w b kleijn f s lim a luebs j skoglund f stimberg q wang and t c walters wavenet based low rate speech coding in 2018 ieee int conf acoust speech signal pro cessing icassp ieee 2018 pp 676 680 8 j klejsa p hedelin c zhou r fejgin and l villemoes high quality speech coding with sample rnn in 2019 ieee int conf acoust speech signal processing icassp ieee 2019 pp 7155 7159 9 c ga rbacea a van den oord y li f s lim a luebs o vinyals and t c walters low bit rate speech coding with vq vae and a wavenet decoder in 2019 ieee int conf acoust speech signal processing icassp ieee 2019 pp 735 739 10 j m valin and j skoglund a real time wideband neural vocoder at 1 6 kb s using lpcnet in proc interspeech 2019 2019 pp 3406 3410 11 f s lim w b kleijn m chinen and j skoglund ro bust low rate speech coding based on cloned networks and wavenet in 2020 ieee int conf acoust speech signal pro cessing icassp ieee 2020 pp 6769 6773 12 r fejgin j klejsa l villemoes and c zhou source cod ing of audio signals with a generative model in 2020 ieee int conf acoust speech signal processing icassp ieee 2020 pp 341 345 13 c w j granger and m j morris time series modelling and interpretation journal of the royal statistical society series a general vol 139 no 2 pp 246 257 1976 14 i goodfellow j pouget abadie m mirza b xu d warde farley s ozair a courville and y bengio generative ad versarial nets in advances in neural information processing systems 2014 pp 2672 2680 15 s nowozin b cseke and r tomioka f gan training gen erative neural samplers using variational divergence minimiza tion in advances in neural information processing systems 2016 pp 271 279 16 m arjovsky s chintala and l bottou wasserstein gan arxiv preprint arxiv 1701 07875 2017 17 y li k swersky and r zemel generative moment match ing networks in international conference on machine learn ing 2015 pp 1718 1727 18 c l li w c chang y cheng y yang and b po czos mmd gan towards deeper understanding of moment matching network in advances in neural information pro cessing systems 2017 pp 2203 2213 19 s kim s g lee j song j kim and s yoon flowavenet a generative flow for raw audio arxiv preprint arxiv 1811 02155 2018 20 y luo and n mesgarani conv tasnet surpassing ideal time frequency magnitude masking for speech separation ieee acm transactions on audio speech and language processing vol 27 no 8 pp 1256 1266 2019 21 s sonning c schu ldt h erdogan and s wisdom per formance study of a convolutional time domain audio separa tion network for real time speech denoising in 2020 ieee int conf acoust speech signal processing icassp ieee 2020 pp 831 835 22 d o shaughnessy speech communications human and ma chine ieee universities press 1987 23 j chung c gulcehre k cho and y bengio empirical evaluation of gated recurrent neural networks on sequence modeling arxiv preprint arxiv 1412 3555 2014 24 j s garofolo d graff d paul and d pallett csr i wsj 0 other harvard dataverse tech rep 2016 online available https doi org 10 7910 dvn zvu 9 hf 25 v panayotov g chen d povey and s khudanpur lib rispeech an asr corpus based on public domain audio books in 2015 ieee international conference on acoustics speech and signal processing icassp ieee 2015 pp 5206 5210 26 r ardila et al common voice a massively multilingual speech corpus arxiv preprint arxiv 1912 06670 2019 27 e fonseca j pons x favory f font d bogdanov a fer raro s oramas a porter and x serra freesound datasets a platform for the creation of open audio datasets in proc 18 th int society music information retrieval conference is mir 2017 suzhou china 2017 pp 486 493 28 m zhu and s gupta to prune or not to prune exploring the efficacy of pruning for model compression arxiv preprint arxiv 1710 01878 2017 29 recommendation itu t p 800 methods for subjective determi nation of transmission quality itu t std aug 1996 30 c valentini botinhao noisy speech database for training speech enhancement algorithms and tts models university of edinburgh school of informatics centre for speech technol ogy research cstr tech rep 2016 31 j m valin k vos and t terriberry definition of the opus audio codec ietf std sept 2012 rfc 6717 32 m dietz et al overview of the evs codec architecture in 2015 ieee int conf acoust speech signal processing icassp ieee 2015 pp 5698 5702 https doi org 10 7910 dvn zvu 9 hf 1 introduction 2 problem formulation 3 objective functions for predictive distribution models 3 1 encouragement of low variance predictive distributions 3 1 1 predictive variance computation 3 1 2 regularization terms 3 2 baseline distribution approach 4 system architecture 5 experiments 5 1 system configuration 5 2 testing procedure 5 3 results 6 conclusion 7 references