author guidelines for icme 2002 operator formalism for optical neural network based on the parametrical four wave mixing process l b litinskii b v kryzhanovsky center of optical neural technologies scientific research institute for system analisys russian academy of sciences 44 2 vavilov street 119333 moscow e mail litin mail ru and a fonarev department of engineering science and physics the college of staten island cuny 2800 victory blvd si new york 10314 e mail afonarev gmail com abstract in this paper we develop a formalism allowing us to describe operating of a network based on the parametrical four wave mixing process that is well known in nonlinear optics the recognition power of a network using parametric neurons operating with q different frequencies is considered it is shown that the storage capacity of such a network is higher compared with the potts glass models 1 introduction in the papers 1 2 a network based on the well known in nonlinear optics parametrical four wave mixing process fwm 3 was examined such a network is capable to hold and handle information that is encoded in the form of the phase frequency modulation in the network the signals propagate along interconnections in the form of quasi monochromatic pulses at q different frequencies the model is based on a parametrical neuron that is a cubic nonlinear element capable to transform and generate frequencies in the adiabatic parametrical fwm processes 4 6 21 q q k rkji schematically this model of a neuron can be assumed as a device that is composed of a summator of input signals a set of q ideal frequency filters a block comparing the amplitudes of the signals and q generators of quasi monochromatic signals q k q k the network operates as follows the input signals are summarized the summarized signal propagates through q parallel frequency filters the output signals from the filters are compared with respect to their amplitudes the signal with the maximal amplitude activates generation of an output signal whose frequency and phase are the same as the frequency and the phase of the initiating signal for this scheme the condition of the frequencies noncommensurability is of principle only if or qrkji ij kj the fulfillment of this condition guarantees a high level of the internal noises suppression in this network the signals are characterized by a set of parameters frequencies in other words by a vector consequently the state of neurons has to be described with the aid of vector quantities analyzing the approach of refs 1 2 it can be seen that it is close to the well known potts model 4 5 as well to another vector models of neural networks 7 12 here we present a general formalism for the model 1 described above 2 operator formalism we use the hopfield network formalism when describing the optical neural network based on the processes of electromagnetic waves interaction in nonlinear medium because of the deep resemblance of the hopfield and fwm hamiltonians really let us consider the hopfield network with p stored patterns mailto afonarev gmail com 2 1 nxxxx suppose that the state of the network is described by n dimensional vector then the hamiltonian of the network is p 2 1 21 nxxxx 1 n ji jiji xtxh where the interconnections are defined as 2 p iijiij tjixxt 1 0 for the i th neuron the local field is 3 n j jiji xth 1 where the superscript denotes the hermitian conjugate in what follows it will become clear why the hermitian conjugate is necessary the analogy with fwm process becomes clear if we treat the quantities and as amplitudes of electromagnetic waves propagating along interconnections here we interpret the interconnections as a nonlinear medium where a residual perturbation is induced by the waves and in this approach we interpret the local field as a medium polarization induced under the influence of the wave propagating from the j th neuron to the i th one thus we can construct a neural network where the neurons exchange the signals in the form of quasi monochromatic pulses at q different frequencies to describe such an optical neural network we need to treat a neuron as an object which has one ground state ix ix jiij xxt ix jx jx q k 0 and q excited states k an analogy is an atom with one ground and q excited levels the state qk 1 k is excited by the wave with the corresponding frequency only the transition k 0 k is accompanied by emitting the wave of frequency k a detailed description of such optical neural network processing is given in refs 1 2 herein we will show only that a formal description of this network can be done in the framework of the hopfield model if we introduce the creation and annihilation operators of neuron states the properties of these operators are given by the commutation relations klkllk cccc the effect of the field on the neuron is described by expressions 0 klk lc kck 0 00 kc 0 lck the rule of the multiplication of the vectors is kllk in eqs 1 3 in place of the amplitudes and let us substitute the operators and where are the amplitudes of the waves and as a result of this substitution we obtain the hamiltonian operator and the local field operator that are sufficient for the description of the network processing for example to describe the dynamics of the network we have to average the hamiltonian operator with respect to the ground state of the system ix ix i kii cxx i kii cxx 1 ii xx q k i k i k ccc 0 then for the obtained quantities we use the standard analysis in the framework of the hopfield model the state of the i th neuron is obtained by the action of the local field operator on the state ih 0 q k i ki kah 1 0 4 where n ij p jjijji i k kkkkxxxa 1 5 we see that as a result of local field action the i th neuron turns out to be in a mixed excited state to determine the type of the signal this neuron emits when relaxing into the ground state in ref 1 the rule winner take all was used in the series 4 the state with the maximal amplitude is determined the frequency of this state and the phase the sign that coincide with the phase of the amplitude of this state are assigned to the emitted signal it is easy to see that this rule ensures the system energy decreasing the further analysis of the capability of the network to recognize the stored patterns is the same as for the standard network of the hopfield type in the next section we present a formalized form of such analysis 3 vector formalism let us examine a network consisting of n neurons connected with each other in order to describe the states of neurons we use the set of basic vectors in the space where the state of the i th neuron is described by a vector where ke q r 1 q kii exx 1 i x q k re then the state of the network as a whole 2 1 2 1 niqk x is determined by a set of n q dimensional vectors since in this model neurons are vectors the local field i x 21 nxxxx ih affecting the i th neuron is the vector too by analogy with the standard hopfield model eqs 1 3 we write n j jiji xh 1 t 6 the matrix describes the interconnection between the i th and the i th neurons the elements of this matrix can be calculated with the aid of the generalized hebb rule qq ijt 7 0 1 kl ii p ljik kl ij texxet where p stored patterns are 21 n xxxx with regard to eqs 6 and 7 the hamiltonian of the system takes the form p 2 1 n ji p jjii xxxxh 1 2 1 let us define the dynamics of our q dimensional neurons let be the state of the system at the time t we rewrite the local field 6 affecting the i th neuron at the time t in the form convenient for the further analysis txx q k k i ki eath 1 8 where 9 n ij p jjik i k xxxea 1 and txx jj we use the subscript max to denote amplitude in the series 8 that is maximal in modulus then the neuron dynamics is as follows due to the action of the local field the i th neuron at the time t 1 is oriented along the basic vector if and it is oriented in the opposite direction if that is i k a maxe 0 max ta i 0 max ta i max max sgn 1 etatx i i 10 the dynamics of the system consists of consequent variations of the values of neurons according to the rule 10 this dynamics corresponds to decreasing of the energy of the system during its processing a spin is oriented along a direction mostly close to the external field in conclusion we would like to note that the vector model describes completely the behavior of the operator model of neural network discussed in the previous section the expression 10 is identical to the winner take all rule of section 2 and eqs 4 and 5 can be transformed in eqs 8 and 9 with the replacement of the state vectors k by the basic vectors ke consequently the following analysis of the storage capacity of the network is valid for both models 4 storage capacity of network it can be shown that if the number of the stored patterns is equal to one or two 2 p the fixed points of the network are coinside with the stored patterns only thus if 2 p our model behaves as the standard hopfield model let us estimate the storage capacity of the network for an arbitrary value of p in the limit suppose that the network starts from a distorted pattern n 222111 mnnnmm xbaxbaxbax 11 here is one of the stored patterns and define a multiplicative noise is a random value that is equal to 1 or 1 with the probabilities a and respectively b is the probability that the operator changes the state of the vector and is the probability that vector q mnmmm xxxxx 21 n ia n ib ia a 1 ib mix b 1 mix remains unchanged let us examine to what extent the neural network recognize the vector mx correctly at first let us note that when the patterns x are uncorrelated the quantities of the types mjjmjj xbxa and with can be considered as independent random variables that are described by the probability distributions mjjjikj xbxxea m ab b ba r 1 1 1 1 0 1 2 2 2 2 1 11 2 1 1 0 1 q q q r then substituting eq 11 in eq 9 we obtain when 12 1 1 1 l r r n r rmik i k xea 0 mik xe when 13 1 l r r i ka 0 mik xe where according to the rule 10 the i th neuron finds itself in the state 1 1 pnl mix when two conditions are fulfilled i the amplitude 12 in modulus is greater than all the other amplitudes 13 ii the sign of the amplitude 12 is the same as the sign of otherwise there is an error of the vector recognition the probability of this error is mi x mix l r r n r ri 1 1 1 prpr to estimate the value of we use the well known chernov chebyshev method 15 as a result we obtain the expression for the probability of the error of the vector i pr mx recognition 22 2 1 21 2 exppr ba p nq n 14 when n increases this probability tends to zero if p as function of n increases slower than 222 1 21 ln 2 baq n n p 15 this allows us to use 15 as an asymptotically possible value of the storage capacity of our neural network 5 discussion as it follows from 15 the network is more sensitive to an error in the sign than in the number of the state when 1 q eqs 14 15 transform into well known results for the standard hopfield model when q increases the noise immunity of the considered associated memory increases noticeably in the same time the storage capacity of the network increases proportionally to moreover in contrast to the hopfield model the number of the patterns p can many times exceed the number of neurons the aforementioned properties of our model are due to the complex structure of neurons from which the network is composed 2 q let us compare our model with the potts glass model when q 2 the last also transforms into the hopfield model according to the estimates of ref 7 the storage capacity of the potts neural network is 02 1 1 qqqc where in our model for we have we see that if q is sufficiently large in our model the storage capacity is twice as big as the storage capacity of the models 7 14 we think that the gain is achieved because of the frequencies noncommensurability principle see introduction which was not used in ref 7 14 the comparison allows us to conclude that realization of the 138 0 0 1 q 0 2 qqc optical neural network based on the parametrical four wave mixing process can be more efficient than the neural network based on the potts glass model summarizing the data we can conclude the following 1 introduction of frequency characteristics for the components of the processed images leads to a significant increase in the volume of neural memory and reduce recognition errors 2 the number of interconnections in the resulting of channel multiplexing can be reduced in q 2 time without reducing the sizeof the memory and without increasing the error detection i e to a certain extent to solve the problem of n 2 3 the memory size can be varied without changing the dimensions of the processed vectors i e by fixing the value of n 4 frequencies can encode a variety of feature such as color pixel images etc of course the complexity of the neuron results in an increase in the number of local connections within itself however more importantly this decreases the number of long distance interconnections to other neurons in conclusion we would like to note that the presented approach allows us to describe not only the optical neural networks of the parametrical type but also neural networks in which information is encoded in the form of phase delays of pulses in interconnections it is much more easy to realize such a network in form of a device referencies 1 b v kryzhanovsky a l mikaelian on the recognition ability of a neural network on neurons with parametric transformation of frequencies doklady mathematics vol 65 no 2 pp 286 288 2002 2 a fonarev b v kryzhanovsky et al parametric dynamic neural network recognition power optical memory neural networks vol 10 no 4 pp 31 48 2001 3 n bloembergen nonlinear optics 1966 4 b v kryzhanovsky and a r karapetyan b a glushko theory of energy exchange and conversion via four wave mixing in a nondissipative 3 material phys rev a 44 9 6036 6042 1991 5 b kryzhanovsky and b glushko exact theory of four wave mixing process in nondissipative medium with a large rate of conversion weak field case phys rev a 45 7 4979 4989 1992 6 b kryzhanovsky and b glushko adiabatic four wave mixing in a strongly driven four level system effect of pump depletion phys rev a 46 5 2831 2839 1992 7 i kanter potts glass models of neural networks physical review a v 37 7 pp 2739 2742 1988 8 h vogt a zippelius invariant recognition in potts glass neural networks journal of physics a v 25 8 pp 2209 2226 1992 9 b v kryzhanovsky and l b litinskii vector associative memory models automation and remote control volume 64 no 11 2003 10 b v kryzhanovsky l b litinskii and a fonarev parametrical neural network based on the four wave mixing process nuclear instuments and methods in physics research a vol 502 no 2 3 pp 517 519 2003 11 d i alieva b v kryzhanovsky v m kryzhanovsky a b fonarev q valued neural network as a system of fast indentification and pattern recognition pattern recognition and image analysis vol 15 1 pp 30 33 2005 12 b kryzhanovsky v kryzhanovskiy l litinskii machine learning in vector models of neural networks advances in machine learning ii j koronacki et al eds springer issn 1860 949 x vol 263 2010 pp 427 443 2010 13 boris kryzhanovsky leonid litinskii and anatoly fonarev an effective associative memory for pattern recognition lecture notes in computer science vol 2810 2003 pp 179 186 2003 14 b v kryzhanovsky a l mikaelian and a b fonarev vector neural network identifing many strongly distorted and correlated patterns int conf on information optics and photonics technology photonics asia 2004 beijing 2004 proc of spie vol 5642 pp 124 133 15 n chernov ann math statistics vol 23 pp 493 507 1952 http www springerlink com content author b v kryzhanovskii http www springerlink com content author l b litinskii http www springerlink com content j 475747607 l 87268 p 3 c 0 ffac 190 f 14 d 7 ea 879257 e 419 f 2 b 63 pi 84 http www springerlink com content j 475747607 l 87268 p 3 c 0 ffac 190 f 14 d 7 ea 879257 e 419 f 2 b 63 pi 84 http www springerlink com content 106514 p 3 c 0 ffac 190 f 14 d 7 ea 879257 e 419 f 2 b 63 pi 0 http www springerlink com content 106514 p 3 c 0 ffac 190 f 14 d 7 ea 879257 e 419 f 2 b 63 pi 0 http www springerlink com content lm 1 p 21725255 p 3 c 0 ffac 190 f 14 d 7 ea 879257 e 419 f 2 b 63 pi 0 http www springerlink com content author boris kryzhanovsky http www springerlink com content author leonid litinskii http www springerlink com content author anatoly fonarev http www springerlink com content author anatoly fonarev http www springerlink com content mn 619 ky 06 f 70 r 8 pb p 5 cf 84 f 4122 a 643 d 9977 ad 4130 fe 2 ce 08 pi 8 http www springerlink com content mn 619 ky 06 f 70 r 8 pb p 5 cf 84 f 4122 a 643 d 9977 ad 4130 fe 2 ce 08 pi 8 http www springerlink com content 105633 p 5 cf 84 f 4122 a 643 d 9977 ad 4130 fe 2 ce 08 pi 0 http www springerlink com content 105633 p 5 cf 84 f 4122 a 643 d 9977 ad 4130 fe 2 ce 08 pi 0 operator formalism for optical neural network based on the parametrical four wave mixing process