ar x iv 1 50 6 01 61 9 v 1 m at h o c 4 j un 2 01 5 almost worst case distributions in multiple priors models imre csisza r thomas breuer 4 june 2015 abstract a worst case distribution is a minimiser of the expectation of some random payoff within a family of plausible risk factor distributions the plausibility of a risk factor distribution is quantified by a convex integral functional this includes the special cases of relative entropy bregman distance and f divergence an almost worst case dis tribution is a risk factor distribution which violates the plausibility constraint at most by the amount and for which the expected pay off is not better than the worst case by more than from a practical point of view the localisation of almost worst case distributions may be useful for efficient hedging against them we prove that the densities of almost worst case distributions cluster in the bregman neighbour hood of a specified function interpreted as worst case localiser in regular cases it coincides with the worst case density but when the latter does not exist the worst case localiser is perhaps not even a density we also discuss the calculation of the worst case localiser and its dependence on the threshold in the plausibility constraint ics acknowledges support by the hungarian national foundation for scientific re search otka grant no k 105840 tb acknowledges support by the josef ressel centre for applied scientific computing 1 http arxiv org abs 1506 01619 v 1 1 introduction let the monetary payoff or utility of some action e g of a portfolio choice be described by a function x r of a collection r of random risk factors suppose the probability distribution which governs the risk factors is not known exactly but may be assumed to belong to a set of distributions on the sample space of scenarios r multiple priors model then the worst case expected payoff inf p ep x inf p x r p dr 1 may be taken as the negative of the model risk caused by the lack of knowledge about p the same expression emerges also in the theory of preferences ambiguity averse decison makers may rank possible actions by the criterion of expected utility in the worst case over risk measures or preference criteria of a more general kind involve penalised expected payoff or utility inf p ep x p 2 where p is a suitable penalty term for details including axiomatic considerations leading to 1 or 2 we refer for example to fo llmer and schied 9 hansen and sargent 12 or gilboa 10 any risk measure satisfying some natural postulates in which case they are dubbed coherent can be represented as the negative of 1 for some convex set of distributions relaxing coherence to convexity yields 2 with some convex penalty term p for our purposes axiomatic theory serves as motivation only in that theory the infimum in 1 typically equals a minimum in models treated in this paper a worst case distribution p attaing the minimum in 1 need not exist if a best guess p 0 of the unknown risk factor distribution is available it is natural to use 1 with consisting of those distributions p that do not deviate much from p 0 in the literature many measures of deviation of distributions are available the majority are non symmetric the most versatile one in various scientific disciplines is i divergence or relative en tropy for an axiomatic approach distinguishing i divergence in the context of inference see csisza r 7 and references therein relaxing some axioms that approach leads as alternatives to other frequently used measures of deviation of distributions known as f divergences and bregman distances see section 2 for definitions in the context of risk and preferences several authors perhaps first hansen and sargent 11 have considered 1 with 2 equal to an i divergence ball around p 0 or 2 with p equal to a con stant times the i divergence of p from p 0 the preference relation based on 2 with this choice of p called multiplier preferences in 12 has been axiomatically distinguished by strzalecki 15 moreover according to ahmadi javid 1 the coherent risk measure he calls entropic value at risk obtained by taking an i divergence ball for in 1 is superior to others from the point of view of computability general f divergences have been employed in this context by maccheroni et al 13 and ben tal and teboulle 2 see also references in 2 to prior work of its authors bregman distance could be used similarly but to this we do not have references we consider problem 1 with of the following form including as special cases i divergence balls f divergence balls and bregman balls p dp pd h p k 3 where is a given measure on and h is a convex integral functional as specified in section 2 1 a corresponding choice of p in 2 is p h p 0 our main focus in this paper is the location of the infimum rather than the value of the worst case expected payoff 1 or the related infimum 2 in cases the infimum is not achieved there is no worst case distribution then it is not obvious what the location of infimum should mean we introduce the concept and prove the existence of a localiser of almost worst case distributions which in the following sense characterises the location of the infimum whether or not the minimum is achieved almost worst case distributions achieving values ever closer to the infimum are in ever smaller bregman balls around the localiser part of the results were presented in the symposium contribution 4 the problem of minimising ep x subject to h p k is related to the problem of minimising convex integral functionals subject to moment constraints this problem an extension of the celebrated information geo metric problem of i divergence minimisation has been extensively studied in the literature we rely upon those results in the form presented by csisza r and matu s 8 and we use the basic framework of breuer and csisza r 5 pre sented in section 2 the new results are presented in section 3 theorem 1 in subsection 3 1 extends a result of ahmadi javid 1 theorem 5 1 on computing the infimum in 1 for of form 3 to our framework 1 that admits also non autonomous 1 this framework does include some assumptions adopted for other purposes which were absent in 1 3 integrands and unbounded payoff functionx our main result theorem 2 in subsection 3 2 addresses the worst case and almost worst case distributions densities that attain or almost attain the minimum in 1 the almost worst case densities are shown to cluster in bregman distance around a specified function called worst case localiser a similar result is obtained also for problem 2 the worst case localiser equals the worst case density if the minimum is attained while otherwise it is perhaps not a density at all finally subsection 3 3 addresses the effect of the threshold k in 3 theorems 3 and 4 show that in many situations including the case of f divergence balls either a worst case distribution exists for all k 0 or else it does does not exist for k less larger than a critical value kcr 0 it remains open whether a similar result also holds in general apart from the possibility demonstrated by an example with bregman balls that no worst case distribution exists for any k 0 2 preliminaries 2 1 general framework let be any set equipped with a finite or finite measure on a algebra not mentioned in the sequel probability measures p will be represented by their densities p dp d the notation p will be used also for nonnegative measurable functions on which are not densities i e do not have integral 1 equality of functions on will be meant in the almost everywhere a e sense let h be a convex integral functional defined on the vector space of measurable functions 2 on by h p h p r p r dr 4 here r s is a function of r s r measurable in r for each s r strictly convex and differentiable 3 in s on 0 for each r and satisfying r 0 lim s 0 r s r s if s 0 5 2 this functional will be considered only for nonnegative functions p with no loss of generality since p 0 a e is a necessary condition for h p see 5 3 strict convexity appears essential for our main results differentiability is assumed for convenience it could be dispensed with as in 8 4 then is a convex normal integrand in the sense of rockafellar and wets 14 which ensures the measurability of r p r in 4 and of similar func tions later on let x be any measurable function interpreted as payoff function and p 0 a default distribution on with p 0 dp 0 d p 0 such that the expectation ep 0 x x r p 0 r dr b 0 exists let m and m denote the ess inf and ess sup of x and adopt as standing assumptions m b 0 m 6 h p h p 0 0 whenever pd 1 7 due to strict convexity of the inequality in 7 is strict if p 6 p 0 example 1 take p 0 thus p 0 1 and let r s f s be an autonomous convex integrand with f 1 0 to ensure 7 then h p in 4 for dp pd is the f divergence df p p 0 introduced in csisza r 6 if f is cofinite i e if lims f s s then p p 0 is a necessary condition for df p p 0 hence in that case in 3 equals the f divergence ball p df p p 0 k if f is not cofinite f divergence may be finite also in absence of absolute continuity still with some abuse of terminology the set in 3 will be called f divergence ball also in that case example 2 let f be any strictly convex and differentiable function on 0 and for s 0 let r s f s p 0 r where f s t f s f t f t s t 8 here f 0 and f 0 are defined as limits if f 0 we set f s 0 0 for s 0 and f s 0 otherwise in this example p 0 is arbitrary except that in case f 0 we assume that p 0 0 a e then h p equals the bregman distance 3 bf p p 0 f p r p 0 r dr 9 and is a bregman ball of radius k around p 0 note that here the assump tion f 1 0 is not needed to guarantee 7 but may be adopted anyhow for the function f s t is not affected by adding a constant to f 5 in the special case f s s log s both examples give the i divergence ball p d p p 0 k where d p p 0 p log p p 0 d as another special case the choice f s s 2 s 0 gives f s t s t 2 and bf p p 0 p p 0 2 d which is the squared l 2 distance between p and p 0 for of the form 3 the infimum in 1 equals v k inf p pd 1 h p k xpd 10 and for p h p 0 the infimum in 2 equals w inf p pd 1 xpd h p 11 the next lemma relates the solution of problem 10 to that of the following minimisation problem see fig 1 f b inf p pd 1 xpd b h p 12 f b is a convex function with minimum 0 attained at b b 0 a standing assumption will be in addition to 6 7 that kmax lim b m f b 0 13 this is a necessary condition for the functional h to yield a nontrivial mea sure of risk for the payoff function x since kmax 0 would imply v k m for each k 0 note that if m then kmax subject to 13 while if m is finite then kmax f m where the strict inequality is possible lemma 1 5 proposition 3 1 to each k 0 kmax there exists a unique b m b 0 with f b k and then v k b the minimum in 10 is attained if and only if that in 12 is attained for the above b and then the same p attains both minima remark 1 the assumption on k is not restrictive for if k 0 or k kmax 0 then v k trivially equals b 0 or m remark 2 by 5 theorem 2 the standing assumption 13 is equivalent to 24 below which automatically holds if m and that condition implies f b 0 for each b b 0 in particular the continuous convex function f b b m b 0 is strictly decreasing and v k k 0 kmax equals its inverse function 6 for of the form 3 the infimum in 1 equals inf pd 1 h xpd 10 and for 0 the infimum in 2 equals inf pd 1 xpd 11 by the next lemma the minimisation problem in 10 is equivalent to the information theoretical problem of generalised divergence minimisation inf pd 1 xpd 12 is a convex function with minimum 0 attained at a standing assumption will be in addition to 6 7 that max lim 13 this is a necessary condition for the functional to yield a nontrivial mea sure of risk for the payo function since max 0 would imply for each k 0 note that if then max subject to 13 while if is finite then max where the strict inequality is possible lemma 1 4 proposition 3 1 to each 0 kmax there exists a unique m b with and then v k b the minimum in 10 is attained if and only if that in 12 is attained for the above and then the same attains both minima lemma 1 relates the solution of the generalised divergence minimisa tion problem 12 to our maximum loss problem 1 it is illustrated in fig remark the assumption on is not restrictive for if 0 or max 0 then trivially equals or remark by 4 theorem 2 the standing assumption 13 is equivalent to 24 below which automatically holds if m and that condition implies 0 for each b b in particular the continuous convex function b m b is strictly decreasing and k 0 kmax equals its inverse function for of the form 3 the infimum in 1 equals inf pd 1 h xpd 10 and for 0 the infimum in 2 equals inf pd 1 xpd 11 by the next lemma the minimisation problem in 10 is equivalent to the information theoretical problem of generalised divergence minimisation inf pd 1 xpd 12 is a convex function with minimum 0 attained at a standing assumption will be in addition to 6 7 that max lim 13 this is a necessary condition for the functional to yield a nontrivial mea sure of risk for the payo function since max 0 would imply for each k 0 note that if then max subject to 13 while if is finite then max where the strict inequality is possible lemma 1 4 proposition 3 1 to each 0 kmax there exists a unique m b with and then b the minimum in 10 is attained if and only if that in 12 is attained for the above and then the same attains both minima lemma 1 relates the solution of the generalised divergence minimisa tion problem 12 to our maximum loss problem 1 it is illustrated in fig remark the assumption on is not restrictive for if k 0 or max 0 then trivially equals or remark by 4 theorem 2 the standing assumption 13 is equivalent to 24 below which automatically holds if m and that condition implies 0 for each b b in particular the continuous convex function b m b is strictly decreasing and k 0 kmax equals its inverse function for of the form 3 the infimum in 1 equals inf pd 1 h xpd 10 and for 0 the infimum in 2 equals inf pd 1 xpd 11 by the next lemma the minimisation problem in 10 is equivalent to the information theoretical problem of generalised divergence minimisation f b inf pd 1 xpd 12 is a convex function with minimum 0 attained at a standing assumption will be in addition to 6 7 that max lim 13 this is a necessary condition for the functional to yield a nontrivial mea sure of risk for the payo function since max 0 would imply for each k 0 note that if then max subject to 13 while if is finite then max where the strict inequality is possible lemma 1 4 proposition 3 1 to each 0 kmax there exists a unique m b with and then b the minimum in 10 is attained if and only if that in 12 is attained for the above and then the same attains both minima lemma 1 relates the solution of the generalised divergence minimisa tion problem 12 to our maximum loss problem 1 it is illustrated in fig remark the assumption on is not restrictive for if 0 or max 0 then trivially equals or remark by 4 theorem 2 the standing assumption 13 is equivalent to 24 below which automatically holds if m and that condition implies 0 for each b b in particular the continuous convex function b m b is strictly decreasing and k 0 kmax equals its inverse function for of the form 3 the infimum in 1 equals inf pd 1 h xpd 10 and for 0 the infimum in 2 equals inf pd 1 xpd 11 by the next lemma the minimisation problem in 10 is equivalent to the information theoretical problem of generalised divergence minimisation inf pd 1 xpd 12 b is a convex function with minimum 0 attained at a standing assumption will be in addition to 6 7 that max lim 13 this is a necessary condition for the functional to yield a nontrivial mea sure of risk for the payo function since max 0 would imply for each k 0 note that if then max subject to 13 while if is finite then max where the strict inequality is possible lemma 1 4 proposition 3 1 to each 0 kmax there exists a unique m b with and then b the minimum in 10 is attained if and only if that in 12 is attained for the above and then the same attains both minima lemma 1 relates the solution of the generalised divergence minimisa tion problem 12 to our maximum loss problem 1 it is illustrated in fig remark the assumption on is not restrictive for if 0 or max 0 then trivially equals or remark by 4 theorem 2 the standing assumption 13 is equivalent to 24 below which automatically holds if m and that condition implies 0 for each b b in particular the continuous convex function b m b is strictly decreasing and k 0 kmax equals its inverse function in 3 theorems 3 and 4 show that in many situations such as such as divergence balls the worst case distribution exists either for all k or else it does does not exist for less larger than a critical value cr it remains open whether a similar result also holds in general apart from the possibility demonstrated by an example with bregman balls that no worst case distribution exists for any k 0 2 preliminaries 2 1 general framework let be any set equipped with a finite or finite measure on a algebra not mentioned in the sequel probability measures will be represented by their densities d the notation will be used also for nonnegative measurable functions on which are not densities i e do not have integral 1 equality of functions on will be meant in the almost everywhere a e sense let be a convex integral functional defined on the vector space of measurable functions on by r p dr 4 here r s is a function of s measurable in for each strictly convex and di erentiable in on 0 for each and satisfying r 0 lim r s r s if s 5 then is a convex normal integrand in the sense of rockafellar and wets 14 which ensures the measurability of r p in 4 and of similar func tions later on let be any measurable function interpreted as payo function and a default distribution on with d such that the expectation dr b 0 this functional will be considered only for nonnegative functions with no loss of generality since 0 a e is a necessary condition for see 5 strict convexity appears essential for our main results di erentiability is assumed for convenience it could be dispensed with as in 8 for of the form 3 the infimum in 1 equals inf pd 1 h xpd 10 and for 0 the infimum in 2 equals inf pd 1 xpd 11 by the next lemma the minimisation problem in 10 is equivalent to the information theoretical problem of generalised divergence minimisation inf pd 1 xpd 12 is a convex function with minimum 0 attained at a standing assumption will be in addition to 6 7 that max lim 13 this is a necessary condition for the functional to yield a nontrivial mea sure of risk for the payo function since max 0 would imply for each k 0 note that if then max subject to 13 while if is finite then max m where the strict inequality is possible lemma 1 4 proposition 3 1 to each 0 kmax there exists a unique m b with and then b the minimum in 10 is attained if and only if that in 12 is attained for the above and then the same attains both minima lemma 1 relates the solution of the generalised divergence minimisa tion problem 12 to our maximum loss problem 1 it is illustrated in fig remark the assumption on is not restrictive for if 0 or max 0 then trivially equals or remark by 4 theorem 2 the standing assumption 13 is equivalent to 24 below which automatically holds if m and that condition implies 0 for each b b in particular the continuous convex function b m b is strictly decreasing and k 0 kmax equals its inverse function for of the form 3 the infimum in 1 equals inf pd 1 h xpd 10 and for 0 the infimum in 2 equals inf pd 1 xpd 11 by the next lemma the minimisation problem in 10 is equivalent to the information theoretical problem of generalised divergence minimisation inf pd 1 xpd 12 is a convex function with minimum 0 attained at a standing assumption will be in addition to 6 7 that max lim 13 this is a necessary condition for the functional to yield a nontrivial mea sure of risk for the payo function since max 0 would imply for each k 0 note that if then kmax subject to 13 while if is finite then max where the strict inequality is possible lemma 1 4 proposition 3 1 to each 0 kmax there exists a unique m b with and then b the minimum in 10 is attained if and only if that in 12 is attained for the above and then the same attains both minima lemma 1 relates the solution of the generalised divergence minimisa tion problem 12 to our maximum loss problem 1 it is illustrated in fig remark the assumption on is not restrictive for if 0 or max 0 then trivially equals or remark by 4 theorem 2 the standing assumption 13 is equivalent to 24 below which automatically holds if m and that condition implies 0 for each b b in particular the continuous convex function b m b is strictly decreasing and k 0 kmax equals its inverse function exists let and denote the ess inf and ess sup of and adopt as standing assumptions m b m 6 0 whenever pd 1 7 due to strict convexity of the inequality in 7 is strict if example 1 take thus 1 and let r s be an autonomous convex integrand with 1 0 to ensure 7 then in 4 for pd is the divergence introduced in csisza r 6 if is cofinite i e if lim s then is a necessary condition for hence in that case in 3 equals the divergence ball if is not cofinite divergence may be finite also in absence of absolute continuity still with some abuse of terminology the set in 3 will be called divergence ball also in that case example 2 let be any strictly convex and di erentiable function on 0 and for 0 let r s s p where s t 8 here 0 and 0 are defined as limits if 0 we set s 0 0 for 0 and s 0 otherwise in this example is arbitrary except that in case 0 we assume that a e then equals the bregman distance 3 f p p p dr 9 and is a bregman ball of radius around note that here the assump tion 1 0 is not needed to guarantee 7 but may be adopted anyhow for the function s t is not a ected by adding a constant to in the special case log both examples give the divergence ball where log d as another special case the choice s 0 gives s t and f p p d which is the squared distance between and figure 1 lemma 1 relates problem 10 to the information theoretic prob lem 12 f v k k 2 2 basic concepts and facts lemma 1 admits to treat problem 10 using known results about minimis ing convex integral functionals under moment constraints specifically with moment mapping defined by r 1 x r we will rely upon results in csisza r and matu s 8 4 specified for this moment mapping then the value function in 8 becomes j a b inf p pd a xpd b h p 14 thus f b j 1 b the function j in 14 is convex and its effective domain domj a b j a b has interior int dom j a b a 0 am b am 15 by 8 lemma 6 6 the function j is proper not identically and never equal to because it equals zero at 1 b 0 int dom j see 6 7 hence its convex conjugate j 1 2 supa b 1 a 2 b j a b is a closed i e lower semicontinuous proper convex function a crucial fact is 4 many of these results have been known earlier though typically under less general conditions 7 the instance of 8 theorem 1 1 that j 1 2 k 1 2 r 1 2 x r dr 16 where is the convex conjugate of r sup s r s r s 17 the conjugate and derivatives of are by the second variable below derivatives at 0 and are interpreted as limits of derivatives at s 0 and s for fixed r the function equals r 0 for r 0 it is strictly convex in the interval r 0 r and equals if r is finite and r this function is differentiable in the interval r its dervative r is positive and strictly increasing in r 0 r and approaches 0 or as r 0 or r since j k implies j k and j equal to the closure of j may differ from j only on the boundary of dom j f b j 1 b k 1 b sup 1 2 1 2 b k 1 2 18 except possibly for b equal to m or m see 15 this can be rewritten as f b sup 2 2 b g 2 g b 19 where g 2 inf 1 k 1 2 1 20 the function g will play a similar role as the logarithmic moment generating function does when in 3 is an i divergence ball see example 3 a consequence of 19 applied to b b 0 is the simple bound g 2 2 b 0 21 the following family of non negative functions on will play a key role like exponential families do for i divergence minimisation p 1 2 r r 1 2 x r 1 2 22 where 1 2 dom k 1 2 x r r a e 23 8 remark 3 it may happen that different parameters 1 2 give rise to equal functions 22 but only in case of functions that equal zero except for r in a set wherex r is constant a e this follows because for any fixed r the fact that r is strictly increasing for r 0 r implies that p 1 2 r in 22 if positive uniquely determines 1 2 x r in particular for positive valued functions 22 the parameters 1 2 are always unique due to the standing assumption 6 as 1 2 domk implies 1 2 for each 1 1 the sets dom k and have the same projection to the 2 axis this projection will be denoted by 2 it is a finite or infinite interval the standing assumptions 6 7 imply that 2 contains the origin and the default density p 0 belongs to the family 22 with 2 0 see 5 remark 4 the left endpoint of the interval 2 will be denoted by min by 5 theorem 2 the standing assumption kmax 0 is equivalent to min 0 24 by 8 lemma 3 6 the directional derivatives of the function k in 16 can be expressed at any 1 2 and for any 1 2 domk as lim t 0 1 t k 1 t 1 1 2 t 2 2 k 1 2 1 1 2 2 x r p 1 2 r dr 25 where the integral is well defined and is not equal to in particular k is differentiable in the interior of its effective domain with 1 k 1 2 p 1 2 d 26 2 k 1 2 xp 1 2 d 27 the same equations hold at 1 2 on the boundary of domk for those one sided partial derivatives of k which are defined there thus 26 holds for the left partial derivative at each 1 2 the following lemma gives relevant information about evaluating the function g in 20 its proof is effectively contained in the proof of 5 proposition 2 but for convenience a full proof will be given in the appendix clearly domg 2 g 2 2 9 lemma 2 given any 2 2 either i some 1 r satisfies 1 2 p 1 2 d 1 or ii 1 sup 1 1 2 domk is finite 1 2 p 1 2 d 1 in either case in 20 the minimum is attained and the unique minimiser is 1 respectively 1 2 3 generalised pythagorean identity given the convex integrand define r s t as in 8 with the convex function r s 7 r s playing the role of f the mapping r s t 7 r s t is a normal integrand 8 lemma 2 10 hence if p and q are non negative measurable functions on then so is also r p r q r denoted briefly by p q extending the concept of bregman distance 9 define b p q b p q p q d 28 like its special case in 9 it is non negative and equals 0 only if p q if f as in example 2 then b is equal to the bf of 9 the following lemma crucial for this paper is an instance of 8 lemma 4 15 combined with 8 remark 4 13 lemma 3 for each density p with xpd finite and each 1 2 h p 1 2 xpd k 1 2 b p p 1 2 r 0 1 2 x r p r dr 29 if p 1 2 is a density the special case p p 1 2 of 29 or direct calcu lation gives that h p 1 2 1 2 xp 1 2 d k 1 2 30 then 29 and 30 imply h p h p 1 2 b p p 1 2 r 0 1 2 x r p r dr 31 for each density p satisfying xpd xp 1 2 d 32 10 identities like 31 frequently occur in the literature primarily in cases when the last term vanishes it trivially does if r 0 they are referred to as pythagorean identities 5 and 29 will be called generalised pythagorean identity the above results admit a short proof of the following key lemma see 5 theorem 1 for a related result lemma 4 i let 1 2 p 1 2 d 1 then xp 1 2 d is finite if and only if h p 1 2 is in that case the density p p 1 2 uniquely 6 attains the minimum in the definition 12 of f b for b xp 1 2 d and f b h p 1 2 1 2 b k 1 2 2 b g 2 33 supposing h p 1 2 0 here b is less or larger than b 0 according as 2 is negative or positive ii for k 0 kmax a density p attains the minimum in the definition 10 of v k if and only if p p 1 2 for some 1 2 with 2 0 and h p 1 2 k or equivalently xp 1 2 d v k 34 proof the first assertion holds by 30 and the second one since 31 32 imply h p h p 1 2 for each density p with xpd b xp 1 2 d then 33 follows by 30 and the consequence k 1 2 1 g 2 of lemma 2 finally 21 and 33 yield 2 b 2 b 0 proving the last assertion of part i ii for sufficiency it is enough to verify the equivalence 34 under the given hypotheses the function v 0 kmax m b 0 is the inverse of f m b 0 0 kmax see lemma 1 and remark 2 this and the result f xp 1 2 d h p 1 2 of part i imply 34 because if 0 h p 1 2 kmax then m xp 1 2 d b 0 the upper bound follows from 2 0 due to the last assertion of part i regarding necessity a density p that attains the minimum in 10 clearly satisfies the constraint h p k with the equality we skip the proof of the remaining assertion that p has to be of form p 1 2 with 2 0 for this will be an immediate consequence of theorem 2 5 if r s f s s 2 1 s 0 then p 1 2 1 2 1 2 x r and 31 reduces to the classical pythagorean identity p 2 p 1 2 2 p p 1 2 2 provided that 32 holds for 1 2 with 1 2 x r 0 6 uniqueness is meant for the function in the a e sense see remark 3 11 3 new results 3 1 calculating v k a procedure to calculate v k in 10 is to first determine the function k in 16 then the function f via 18 this may be done in two steps first de termining the function g in 20 and finally v k as the solution b m b 0 of the equation f b k see lemma 1 in regular cases b v k is char acterised by equations involving partial derivatives of the function k see 5 corollary 1 which may facilitate its computation the following theorem combined with lemma 2 may help to reduce computational complexity even in irregular cases previously ahmadi javid 1 theorem 5 1 proved an identity equivalent to 35 for autonomous integrands and bounded payoff functions a lemma is sent forward that will be proved in the appendix lemma 5 f g note that while f g clg immediately follows from 19 it appears nontrivial that the function g is closed theorem 1 for k 0 kmax v k max 2 0 max 1 r k k 1 2 1 2 max 2 0 k g 2 2 35 a maximiser for the second maximum in 35 is equivalently a maximiser of 2 b g 2 where b v k a pair 1 2 attains the first maximum in 35 if and only if it attains the maximum in 18 for b v k such 1 2 belongs to and satisfies p 1 2 d 1 proof the conditions b v k k 0 kmax are equivalent to f b k b m b 0 see lemma 1 and remark 2 the condition that 2 is a maximiser of 2 b g 2 means by 19 that 2 b g 2 g b f b k 36 or equivalently see lemma 5 that 2 b f b f 2 this proves that 2 is a maximiser of 2 b g 2 if and only if 7 f b 2 f b in particular a perhaps non unique maximiser 2 0 does exist 7 here f and f denote one sided derivatives the differentiability of the function f is not addressed 12 by 36 the maximum of 2 b g 2 equals k hence 2 b g 2 k for each 2 r this proves the assertions that the maximum of k g 2 2 2 0 37 is equal to b v k and a maximiser of 37 is equivalently a maximiser of 2 b g 2 the remaining assertions of theorem 1 immediately follow from this and lemma 2 the calculation of w in 11 is somewhat less costly than that of v k it requires the calculation of g 2 only for a single value of 2 since for 0 we have using lemma 5 in the final step w inf b b f b sup b b f b f 1 g 1 38 closed i e lower semicontinuous proper convex function a crucial fact is the instance of 7 theorem 1 1 that equals r dr 16 where is the convex conjugate of r sup r s 17 the conjugate and derivatives of are by the second variable below derivatives at 0 and are interpreted as limits of derivatives at 0 and for fixed the function equals r 0 for r 0 it is strictly convex in the interval r 0 r and equals if r is finite and r this function is di erentiable in the interval r with non decreasing derivative r positive if r 0 and approaching 0 or as r 0 or r since implies and equal to the closure of may di er from only on the boundary of dom 1 b 1 b sup 18 except possibly for equal to or see 15 this can be rewritten as sup 19 where g 2 inf 20 the function will play a role as the logarithmic moment generating func tion does when in 3 is an divergence ball see example 3 the following family of functions will play a key role like exponential families do for divergence minimisation r 21 where dom r a e 22 as dom implies for each the sets dom and have the same projection to the axis this projection closed i e lower semicontinuous proper convex function a crucial fact is the instance of 7 theorem 1 1 that equals r dr 16 where is the convex conjugate of r sup r s 17 the conjugate and derivatives of are by the second variable below derivatives at 0 and are interpreted as limits of derivatives at 0 and for fixed the function equals r 0 for r 0 it is strictly convex in the interval r 0 r and equals if r is finite and r this function is di erentiable in the interval r with non decreasing derivative r positive if r 0 and approaching 0 or as r 0 or r since implies and equal to the closure of may di er from only on the boundary of dom 1 b 1 b sup 18 except possibly for equal to or see 15 this can be rewritten as sup 19 where inf 20 the function will play a role as the logarithmic moment generating func tion does when in 3 is an divergence ball see example 3 the following family of functions will play a key role like exponential families do for divergence minimisation r 21 where dom r a e 22 as dom implies 1 2 for each the sets dom and have the same projection to the axis this projection of the equation see lemma 1 in regular cases is char acterised by equations involving partial derivatives of the function see 4 corollary 1 which may facilitate its computation the following theorem combined with lemma 2 may help to reduce computational complexity even in irregular cases previously ahmadi javid 1 theorem 5 1 proved an identity equivalent to 35 for autonomous integrands and bounded payo functions a lemma is sent forward that will be proved in the appendix lemma 5 e that while cl immediately follows from 19 it appears nontrivial that the function is closed theorem 1 for 0 kmax maxmax max 35 a pair attains the first maximum in 35 if and only if it attains the maximum in 18 for such belongs to and satisfies d a maximiser for the second maximum in 35 is equivalently a maximiser of where proof by lemma 5 is a closed convex function with 0 0 let g 36 denote the graph of restricted to negative arguments as 0 37 equals the slope of the straigh line through 0 and g g its maximum equals the slope of the supporting line to through 0 k see fig 1 denote this maximum by then 38 the first equality by the definition of convex conjugate and the second one by 19 our framework includes assumptions about the integrand not made there which we need for other purposes and appear dispensible for the proof of 35 11 the calculation of in 11 is somewhat less costly than that of it requires the calculation of only for a single value of since for 0 we have using lemma 5 in the final step inf sup 38 closed i e lower semicontinuous proper convex function a crucial fact is the instance of 7 theorem 1 1 that equals r dr 16 where is the convex conjugate of r sup r s 17 the conjugate and derivatives of are by the second variable below derivatives at 0 and are interpreted as limits of derivatives at 0 and for fixed the function equals r 0 for r 0 it is strictly convex in the interval r 0 r and equals if r is finite and r this function is di erentiable in the interval r with non decreasing derivative r positive if r 0 and approaching 0 or as r 0 or r since implies and equal to the closure of may di er from only on the boundary of dom 1 b 1 b sup 18 except possibly for equal to or see 15 this can be rewritten as sup 19 where inf 20 the function will play a role as the logarithmic moment generating func tion does when in 3 is an divergence ball see example 3 the following family of functions will play a key role like exponential families do for divergence minimisation r 21 where dom r a e 22 as dom implies for each the sets dom and have the same projection to the axis this projection closed i e lower semicontinuous proper convex function a crucial fact is the instance of 7 theorem 1 1 that equals r dr 16 where is the convex conjugate of r sup r s 17 the conjugate and derivatives of are by the second variable below derivatives at 0 and are interpreted as limits of derivatives at 0 and for fixed the function equals r 0 for r 0 it is strictly convex in the interval r 0 r and equals if r is finite and r this function is di erentiable in the interval r with non decreasing derivative r positive if r 0 and approaching 0 or as r 0 or r since implies and equal to the closure of may di er from only on the boundary of dom 1 b 1 b sup 18 except possibly for equal to or see 15 this can be rewritten as sup 19 where inf 20 the function will play a role as the logarithmic moment generating func tion does when in 3 is an divergence ball see example 3 the following family of functions will play a key role like exponential families do for divergence minimisation r 21 where dom r a e 22 as dom implies for each the sets dom and have the same projection to the axis this projection of the equation see lemma 1 in regular cases is char acterised by equations involving partial derivatives of the function see 4 corollary 1 which may facilitate its computation the following theorem combined with lemma 2 may help to reduce computational complexity even in irregular cases previously ahmadi javid 1 theorem 5 1 proved an identity equivalent to 35 for autonomous integrands and bounded payo functions a lemma is sent forward that will be proved in the appendix lemma 5 e that while cl immediately follows from 19 it appears nontrivial that the function is closed theorem 1 for 0 kmax maxmax max 35 a pair attains the first maximum in 35 if and only if it attains the maximum in 18 for such belongs to and satisfies d a maximiser for the second maximum in 35 is equivalently a maximiser of where proof by lemma 5 is a closed convex function with 0 0 let g 36 denote the graph of restricted to negative arguments as 0 37 equals the slope of the straigh line through 0 and g its maximum equals the slope of the supporting line to through 0 see fig 1 denote this maximum by then 38 the first equality by the definition of convex conjugate and the second one by 19 our framework includes assumptions about the integrand not made there which we need for other purposes and appear dispensible for the proof of 35 11 of the equation see lemma 1 in regular cases is char acterised by equations involving partial derivatives of the function see 4 corollary 1 which may facilitate its computation the following theorem combined with lemma 2 may help to reduce computational complexity even in irregular cases previously ahmadi javid 1 theorem 5 1 proved an identity equivalent to 35 for autonomous integrands and bounded payo functions a lemma is sent forward that will be proved in the appendix lemma 5 e that while cl immediately follows from 19 it appears nontrivial that the function is closed theorem 1 for 0 kmax maxmax max 35 a pair attains the first maximum in 35 if and only if it attains the maximum in 18 for such belongs to and satisfies d a maximiser for the second maximum in 35 is equivalently a maximiser of where proof by lemma 5 is a closed convex function with 0 0 let g 36 denote the graph of restricted to negative arguments as 0 37 equals the slope of the straigh line through 0 and g its maximum equals the slope of the supporting line to through 0 see fig 1 denote this maximum by then 38 the first equality by the definition of convex conjugate and the second one by 19 our framework includes assumptions about the integrand not made there which we need for other purposes and appear dispensible for the proof of 35 11 figure 2 the supporting line has maximum slope remark the following geometric interpretation of the proof of theorem 1 deserves emphasis see fig 2 denote by g g 39 the graph of restricted to nonpositive arguments recall that by lemma 5 is a closed convex function with 0 0 then 37 is the slope of the straigh line through 0 and g which is maximised by the supporting line to through 0 the proof of theorem 1 shows that this supporting line exists and has slope the maximum of 37 is attained if and only if g is on this supporting line 13 figure 2 the supporting line has maximum slope b k g 2 2 among all lines passing through 0 k and some point of g this slope is equal to the solution v k of problem 10 remark 4 the following geometric interpretation of the proof of theorem 1 deserves emphasis see fig 2 denote by g 2 g 2 2 2 2 0 39 13 the graph ofg restricted to nonpositive arguments recall that by lemma 5 g is a closed convex function with g 0 0 then 37 is the slope of the straight line through 0 k and 2 g 2 g which is maximised by the supporting line to g through 0 k the proof of theorem 1 shows that this supporting line exists and has slope b v k the maximum b v k of 37 is attained if and only if 2 g 2 is on this supporting line 3 2 almost worst case distributions for the problem 10 call a density p an almost worst case density awcd where 0 0 if h p k and xpd v k 40 thus an awcd is a density which does not violate the constraint h p k by more than and for which the expected payoff does not exceed by more than the worst possible one subject to the constraint a worst case density wcd is a 0 0 awcd an almost worst case distribution or a worst case distribution is a distribution p whose density is an awcd or a wcd theorem 2 below establishes a clustering property of the awcds as well as a similar result for densities that almost attain the minimum in 11 from a practical point of view this may be relevant for efficient hedging against the almost worst scenarios but this issue is not entered here let us assign to each 2 2 the unique 1 attaining the minimum in the definition 20 of g 2 determined in lemma 2 and denote q 2 r p 1 2 r with 1 attaining k 1 2 1 g 2 41 given k 0 kmax we will denote by q k the function q 2 with 2 0 attaining the second maximum in theorem 1 i e q k q 2 p 1 2 with 1 2 a maximiser in 35 42 theorem 2 i for k 0 kmax each awcd p belongs to the bregman neighborhood of radius 2 of q k in 42 i e see 28 b p q k 2 if p is an awcd 43 14 ii for 0 with 1 2 set 2 1 then for each density p xpd h p w b p q 2 44 corollary 1 let pn be a sequence of n n awcds with n 0 n 0 in case i or a sequence of densities with xpnd h pn w in case ii then pn converges to q k respectively to q 2 locally in measure 8 in particular the function q k is unique proof i by the generalised pythagorean identity lemma 3 applied to 1 2 in 42 h p 1 2 xpd k 1 2 b p p 1 2 2 xpd g 2 b p q 2 45 for each density p as 2 attains the maximum in theorem 1 here q 2 q k and g 2 2 v k k hence 45 implies 43 for each density p which is an awcd thus satisfies 40 ii in this case 45 holds as before multiplying it by 1 2 and using that g 1 w see 38 we obtain 44 the corollary follows since b pn q k 0 implies convergence of pn to q 2 locally in measure 8 corollary 2 14 remark 5 corollary 1 extends the known result that q k is a generalized solution of problem 12 in the sense 8 that densities pn with xpnd b v k h pn f b k converge to q k locally in measure and also establishes its new counterpart for problem 10 the function q k in theorem 2 i will be called worst case localiser for the almost worst case densities are clustering in its bregman neighborhood this nice intuitive interpretation of the function q k p 1 2 is complemented by the additional intuitive fact that by 43 its parameter 2 controls the radius of that neighborhood most appealing is the special case 0 of 43 that all densities that satisfy h p k and yield expected payoff not excceding the worst case by more than are contained in a bregman neighborhood of q k of radius proportional to with proportionality factor 2 the essence of the corollary is that the bregman distance of pn from 8 this means that r c pn r q 2 r 0 for each c with c finite and any 0 if is a finite measure this is equivalent to standard global convergence in measure 15 q k goes to 0 for certain choices of the integrand this implies convergence even in a stronger sense than locally in measure see example 3 clearly the worst case localiser q k coincides with the wcd whenever the latter exists apply 43 to 0 a necessary and sufficient condition for the existence of a wcd is given in lemma 4 ii there we have skipped the proof that a wcd has to be of form p 1 2 with 2 0 which is obvious now as the wcd is a worst case localiser lemma 6 below will also be useful in identifying situations when the worst case localiser is actually a wcd its corollary addresses the simplest such situation when in 10 the minimum is not attained the worst case localiser may or may not be a density see the examples below though it always satisfies q kd 1 see theorem 1 note that the computation of the worst case localiser is not harder than the computation of v k along the lines of subsection 3 1 for that calculation does provide the parameters 1 2 of q k p 1 2 that attain the double maximum in theorem 1 lemma 6 a function p 1 2 in 22 with 2 0 is the worst case lo caliser q k for k 0 kmax if and only if the vector 1 p 1 2 d v k xp 1 2 d r 2 belongs to the normal cone of domk at 1 2 i e for each 1 2 domk 1 1 1 p 1 2 d 2 2 v k xp 1 2 d 0 46 corollary 2 if the worst case localiser q k p 1 2 has parameters 1 2 in the interior of domk then it is a wcd proof by theorem 1 the condition in the definition 42 is equivalent to the condition that 1 2 attains the maximum of f 1 2 1 2 b k 1 2 where b v k the latter is satisfied if and only if for each 1 2 domk the concave function f t f 1 t 1 1 2 t 2 2 0 t 1 is maximised by t 0 i e its right derivative at t 0 is nonpositive on account of 25 that condition is equivalent to 46 the corollary follows since the normal cone of domk at an interior point consists of 0 0 alone thus lemma 6 gives the conditions p 1 2 d 1 xp 1 2 d v k which mean by lemma 4 that p 1 2 is a wcd 16 example 3 let h p be the i divergence formally see example 1 let be the autonomous integrand given by f s s log s and let p 0 then f e 1 k 1 2 e 1 2 x 1 d and g 2 min 1 k 1 2 1 log e 2 xd 2 for 2 2 dom the minimum in the definition of g 2 is attained for 1 1 2 if m and x r m on a set of measure 0 0 then 9 kmax log 0 otherwise kmax assuming 24 for k 0 kmax theorem 1 gives v k max 2 0 k 2 2 and theorem 2 gives the worst case localiser q k exp 2 2 x with 2 attaining the above maximum this worst case localiser is always a density it also satisfies h q k k and hence is actually a wcd except for the case when dom contains its left endpoint min min is finite and k h q min in that case the maximiser in theorem 1 equal to the parameter of the worst case localiser is 2 min note that for this example the formula for v k appears in 1 and 5 show that in the above exceptional case the minimum in 10 is not attained the result of theorem 2 appears new even for this special case in this example the bregman distance 28 of densities coincides with i divergence hence theorem 2 givesd p q k 2 for each awcd p in the corollary of theorem 2 now the almost worst case densities converge to the worst case localiser in a much stronger sense than in measure indeed the result that their i divergence from the worst case localiser approaches 0 is stronger than l 1 convergence to the worst case localiser example 4 again in the setting of example 1 take now f s log s then h p equals reverse i divergence i e the i divergence of the default distribution p 0 from the distribution p with density p as f is not cofinite the standing assumption kmax 0 holds if and only if m take specifically 0 1 x r r and take for p 0 the distri bution with lebesgue density 2 r as f 1 log 0 then k 1 2 1 0 1 log 1 2 r 2 rdr and p 1 2 r 1 1 2 r for 1 2 domk 1 2 1 0 1 2 0 simple calculus shows that for 2 2 0 the minimum in the definition of g 2 is at tained for 1 such that p 1 2 q 2 is a density but the functions g and q 2 can not be given explicitly if 2 2 then this minimum is attained for 9 note that theorems 1 2 do not apply to k kmax here in that case the wcd equals 1 0 on the set r x r m and 0 elsewhere it does not belong to the family 22 and the almost worst case densities do not cluster in its bregman neighborhood 17 1 0 and g 2 1 2 log 2 one sees that h q 2 ranges from 0 to log 2 1 2 as 2 ranges from 0 to 2 hence in case k log 2 1 2 the wcd exists it equals that p 1 2 which is a density and satisfies h p 1 2 k in case k log 2 1 2 the worst case localiser is q k r 1 2 r with 2 2 attaining v k max 2 0 k g 2 2 by simple calculus this maximiser is 2 e k 1 2 the maximum is v k e k 1 2 and the worst case lo caliser is q k r 1 r e k 1 2 which is not a density unless k log 2 1 2 in this case the bregman distance 28 is b p q log q p p q 1 d the corollary of theorem 2 now does not admit a substantial strengthen ing for the result that this bregman distance approaches 0 does not imply convergence in a familiar sense stronger than in measure example 5 let x be as in example 4 but this time let the default distribution p 0 be the uniform distribution whose density is p 0 r 1 2 r take for h p the bregman distance b p p 0 in example 4 i e the integral functional 4 with r s f s p 0 r log s log 2 r 2 r s 1 2 r then r log 2 r log 2 r r s 1 2 r 2 r the set equal to domk of this example consists of those 1 2 for which 1 2 1 belongs to the set domk of example 4 moreover for such 1 2 the function p 1 2 r 1 1 2 2 r coincides with the function p 1 2 2 of example 4 which can not be a density if 2 0 this proves that in the present example no wcd exists for any k 0 3 3 effect of the threshold k on the existence of a wcd this subsection addresses the effect of the choice of the threshold k on the worst case localiser in particular on whether that localiser is also a wcd examples 3 4 and 5 demonstrate that a wcd may exist for all or for no k or there may exist a critical value kcr such that a wcd exists if k kcr but does not exist if k kcr it appears a plausible conjecture that these three alternatives are exhaustive i e that if a wcd exists for some k it also exists for each k k while this conjecture remains open in general it will be proved under conditions that cover many typical cases recall that min with min 0 denotes the left endpoint of the interval 2 the projection of domk to the 2 axis the condition m is necessary for kmax and sufficient for min 18 theorem 3 i if for some k 0 kmax the worst case localiser q k is a density it is a wcd for k unless 10 min 2 g min 47 and k kcr g min ming min 48 if 47 and 48 hold then q k q kcr q min and no wcd exists for k ii if domk contains the 1 axis i e r 1 dr is finite for each 1 r then the worst case localiser q k is a density and hence it is a wcd unless 47 and 48 hold proof i suppose q k p 1 2 in 42 is a density by lemma 4 it is a wcd for k if and only if xp 1 2 d v k 49 since p 1 2 is a worst case localiser and p 1 2 1 lemma 6 gives that 2 2 v k xp 1 2 d 0 for 2 2 50 this immediately implies 49 if 2 6 min or equivalently see remark 4 if the supporting line through 0 k to the curve g does not contain min g min this is always the case if 47 does not hold and also when 47 holds but g min the largest slope of supporting lines to g at min g min is less than k g min min as the last condition is equivalent to k kcr only the case k kcr remains to cover to complete the proof that q k is a wcd unless 47 and 48 hold in that remaining case q k p 1 2 with 2 min and instead of 49 only the inequality xp 1 2 d v kcr follows from 50 suppose indi rectly that it is strict then lemma 1 implies that xp 1 2 d v k for some k 0 kcr as the integral is less than b 0 by lemma 4 this means by lemma 4 that p 1 2 with 2 min is a wcd for k hence by remark 4 the supporting line through 0 k to the curve g contains min g min contradicting the fact that among the supporting lines to g at min g min the one through 0 kcr has the largest slope this contradiction proves that 49 holds and hence q k is a wcd also when k kcr the last assertions of part i are obvious indeed if 47 holds and k kcr then the supporting line through 0 k to g meets the curve at 10 here g min means the right derivative 19 min g min just as the supporting line through 0 kcr does hence q k q min qkcr as it is a wcd for kcr it can not be a wcd for k kcr with v k v kcr ii the hypothesis implies that a vector in r 2 can belong to the normal cone of domk at some 1 2 only if the first component of this vector is 0 on account of lemma 6 this proves that the worst case localiser q k p 1 2 for any k 0 kmax has to satisfy p 1 2 d 1 corollary 3 the function g is differentiable at each 2 min 0 for which q 2 in 41 is a density proof suppose indirectly that the curve g has several supporting lines at 2 g 2 say one containing 0 k 1 and another 0 k 2 where k 1 6 k 2 then q k 1 q k 2 q 2 see remark 4 hence q 2 is the wcd both for k 1 and k 2 by theorem 3 this means that v k 1 xq 2 d v k 2 contradicting k 1 6 k 2 finally we discuss for f divergence balls see example 1 the dependence on the threshold k the radius of the ball of the worst case localiser and whether it is a wcd formally let r s f s be an autonomous inte grand f strictly convex and differentiable on 0 f 0 lims 0 f s f 1 0 let be a probability measure and p 0 the case of cofinite f is covered by theorem 3 ii the integral in its hypothesis being equal to f 1 finite for each 1 r therefore we focus on the non cofinite case supposing lim s f s s c c finite 51 then the standing assumption kmax 0 equivalent to min 0 holds if and only if m with no loss of generality assume that m 0 clearly the minimisation problem 10 is not affected by adding a constant to x under the above assumptions k 1 2 f 1 2 x d with 2 0 is finite if 1 c and infinite if 1 c because 51 implies that f is finite for c but not for c it follows for any 2 0 that the associated 1 in lemma 2 is equal to c hence lemma 2 gives that the function q 2 p 1 2 in 41 is a density if and only if g 2 f c 2 x d 1 52 moreover if g 2 1 then q 2 f c 2 x pc 2 53 20 theorem 4 under the assumptions 51 and m 0 if g 2 for each 2 0 then the wcd exists for all k 0 kmax otherwise denote min inf 2 g 2 1 54 k cr ming min g min 55 then min 0 k cr 0 kmax and for k k cr the wcd exists for k k cr the worst case localiser q k is of form 53 it is not a density if k k cr while for k k cr it is a density and hence a wcd unless 11 g 2 1 for each 2 0 with g 2 proof since in the current case min theorem 3 implies that the worst case localiser q k is a wcd if and only if it is a density by the passage preceding the theorem the latter holds if and only if q k q 2 with 2 satisfying 52 this immediately proves the first assertion suppose next that g 2 is finite for some 2 0 and denote the supre mum of such parameters 2 by one verifies via monotone convergence and dominated convergence that g 2 is a continuous strictly increasing function of 2 that approaches 0 or g as 2 goes to or hence if g 1 then min is equal to the unique 2 with g 2 1 whereas if g 1 then min in both cases min 0 using that g 0 by the definition 55 of k cr the supporting line to g at min g min of slope g min intersects the vertical axis at 0 k cr hence k cr 0 unless the function g is linear in the interval min 0 the latter possibilty will be ruled out in the appendix it follows too that the supporting line to g through 0 k with k k cr or k k cr meets the curve g at a point or points with argument 2 min respectively 2 min moreover the latter inequality is strict if g min 1 equivalent to g 1 for in that case g is differentiable at min due to corollary 3 referring to remark 4 the above considerations prove that the param eter 2 in the representation q k q 2 in 42 satisfies or does not satisfy the condition 52 if k k cr respectively k k cr no matter whether g 1 or not these facts and that q k q min if k k cr imply all remaining assertions of the theorem see the first passage of the proof 11 it is left open whether that exceptional case is possible 21 appendix proof of lemma 2 proof fix 2 2 define 1 as in the lemma then 1 2 for all 1 1 and the function f 1 k 1 2 is convex closed and differentiable in its effective domain 1 with f 1 p 1 2 d 1 1 56 see 26 if 1 2 then 56 holds also for the left derivative at 1 1 hence the last assertion of the lemma immediately follows to prove that one of the alternatives i and ii indeed takes place note that the properties of stated in the passage after 17 imply by monotone convergence that f 1 in 56 goes to 0 if 1 and to if 1 and 1 hence due to continuity of f 1 alternative i fails only if p 1 2 d 1 for all 1 with 1 2 57 and 57 can hold only if 1 further 57 implies that 1 2 dom k for in the opposite case f 1 the derivative 56 of the closed convex function f 1 would go to as 1 1 the proof will be complete if we show that 57 implies 1 2 2 it has already been shown to imply 1 2 dom k in particular that 1 2 x r r a e thus it remains to verify see 23 that the set r 1 2 x r r has measure 0 on that set p 1 2 r r 1 2 x r grows to as 1 1 hence were it not a 0 measure set p 1 2 d would grow to contradicting 57 proof of lemma 5 proof fix 2 r and consider the not necessarily proper convex function l a inf b r j a b 2 b a r then f 2 sup b 2 b f b inf b f b 2 b l 1 l 1 sup 1 1 l 1 inf 1 l 1 1 22 where the third equality holds since f b j 1 b and the fourth one holds since a 1 is in the interior of dom l here l 1 sup a 1 a l a sup a 1 a sup b j a b 2 b j 1 2 k 1 2 recalling the definition 20 of g this completes the proof completion of the proof of theorem 4 it remains to rule out the possibility that the function g is linear in the interval min 0 suppose indirectly that for some b r g 2 2 b if 2 min 0 58 here necessarily b b 0 by 21 as 58 implies g b 0 which means by 19 that f b 0 it follows by remark 2 that actually b b 0 as g min 1 by the proof of theorem 4 the value 1 in 41 attaining k 1 2 1 g 2 for 2 min is equal to c thus lemma 3 applied to p p 0 and 1 2 c min gives 0 h p 0 min xp 0 d g min b p 0 q min here the integral equals b 0 by definition and b in 58 has been shown to equal b 0 hence it follows that b p 0 q min 0 which means that q min equals p 0 1 a e by remark 3 this contradicts min 6 0 proving that the indirect assumption 58 is false references 1 amir ahmadi javid entropic value at risk a new coherent risk mea sure journal of optimizaton theory and applications 155 3 1105 1123 2011 2 aharanov ben tal and marc teboulle an old new concept of con vex risk measures the optimized certainty equivalent mathematical finance 17 449 476 2007 3 lev m bregman the relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming ussr computational mathematics and mathematical physics 7 200 217 1967 23 4 thomas breuer and imre csisza r information geometry in mathemat ical finance model risk worst and almost worst scenarios in ieee international symposium on information theory proceedings isit pages 404 408 2013 5 thomas breuer and imre csisza r measuring distribution model risk mathematical finance 2013 doi 10 1111 mafi 12050 6 imre csisza r eine informationstheoretische ungleichung und ihre an wendung auf den beweis der ergodizita t von markoffschen ketten pub lications of the mathematical institute of the hungarian academy of sciences 8 85 108 1963 7 imre csisza r why least squares and maximum entropy an axiomatic approach to inference for linear inverse problems annals of statistics 19 4 2032 2066 1991 8 imre csisza r and franti sek matu s on minimization of entropy func tionals under moment constraints kybernetika 48 637 689 2012 9 hans fo llmer and alexander schied stochastic finance an introduc tion in discrete time volume 27 of de gruyter studies in mathematics walter de gruyter 2 nd edition 2004 10 itzhak gilboa theory of decision under uncertainty volume 45 of econometric society monographs cambridge university press 2009 11 lars peter hansen and thomas sargent robust control and model uncertainty american economic review 91 60 66 2001 12 lars peter hansen and thomas sargent robustness princeton uni versity press 2008 13 fabio maccheroni massimo marinacci and aldo rustichini ambiguity aversion robustness and the variational representation of preferences econometrica 74 1447 1498 2006 14 ralph tyrell rockafellar and roger j b wets variational analy sis volume 317 of grundlehren der mathematischen wissenschaften springer 1997 15 tomasz strzalecki axiomatic foundations of multiplier preferences econometrica 79 47 73 2011 24 1 introduction 2 preliminaries 2 1 general framework 2 2 basic concepts and facts 2 3 generalised pythagorean identity 3 new results 3 1 calculating v k 3 2 almost worst case distributions 3 3 effect of the threshold k on the existence of a wcd