arxiv 2204 00893 v 1 math oc 2 apr 2022 ar x iv 2 20 4 00 89 3 v 1 m at h o c 2 a pr 2 02 2 on resolution coresets for constrained clustering maximilian fiedler peter gritzmann fabian klemm april 5 2022 abstract specific data compression techniques formalized by the concept of coresets proved to be powerful for many optimization problems in fact while tightly controlling the approximation error coresets may lead to significant speed up of the computations and hence allow to extend algorithms to much larger problem sizes the present paper deals with a weight balanced clustering problem from imaging in materials science here the class of desired coresets is naturally confined to those which can be viewed as lowering the resolution of the data hence one would expect that such resolution coresets are inferior to unrestricted coreset we show however that the restrictions are more than compensated by utilizing the underlying structure of the data in particular we prove bounds for resolution coresets which improve known bounds in the relevant dimensions and also lead to significantly faster algorithms practice keywords constrained clustering coresets diagrams resolution compression grain mapping imaging mathematics subject classification 90 05 52 68 1 introduction many challenging practical problems require the solution of weight balanced clustering problems on huge data sets fields of applications include agriculture consolidation of farmland see e g 9 transportation air cargo prediction 11 medicine drug response rate analysis see 12 24 or political sciences electoral districting 14 the present paper is more specifically motivated by the goal to provide highly efficient yet provably precise mathematical tools for studying the grain growth and structure of polycrystalline materials in fact 2 applied constrained clustering formulations in order 1 http arxiv org abs 2204 00893 v 1 to compute generalized balanced power diagrams gbpd for concise representations of grain maps while according to several independent studies including 25 26 28 the gbpd algorithm seems to reflect the physical principles of forming polycrystals quite well its computational cost exceed reasonable let alone real time requirements for large but practically relevant data sets more specifically dynamic simulations of grain growth at a small temporal resolution quickly drive implementations based on the uncompressed data of the method over the limit hence currently such simulations are typically based on heuristics see in particular 22 29 and 4 as proved in 19 and experimentally verified in 5 significant acceleration can already be obtained by the use of general purpose coresets for constrained clustering such general coresets work on arbitrary finite data sets hence ignore the special pixel or voxel based structure of the data in our specific and other image processing applications the present paper addresses the question whether the resolution based image structure can be utilized to improve the known best bounds for deterministic coresets in spite of the additional requirement that such coresets should maintain a cartesian product structure and hence can simply be interpreted as lower resolution images we prove that resolution coresets are indeed smaller in the relevant dimensions phrased differently we show that even when applied at a relatively low resolution the clustering techniques will still produce near optimal weight constrained clusterings in fact our bounds on the size of resolution coresets prove that they can be used to safely thin out the given data beyond what has been shown before and hence put experimental observations of 5 on a firm theoretical ground the paper is organized as follows first section 2 formally introduces the problems and states our main results section 3 describes relevant concepts and provides some required preliminaries sections 4 5 and 6 then study resolution coresets in detail and prove our main results section 7 concludes with final remarks 2 motivation notation and main results we will now provide the relevant general notation introduce the problems and state our main results we begin with a brief nontechnical description of our running application of grain mapping formal details will follow later grain mapping mathematical models play an important role for providing tools which can be used to understand the grain growth and structure of polycrystalline materials mathematically grain maps are dissections of the occupied spatial region into monocrystalline cells here we focus on a specific approach introduced in 2 for efficiently describing such dissections in fact parameters characterizing a diagram representation of the grain map are determined as the solutions of an optimization problem in effect the points which represent the voxels of the desired grain image at a given resolution are clustered in such a way that certain characteristics of the resulting dissection coincide with available 2 tomographic measurements a natural question is how low a resolution can be chosen while still keeping a high accuracy of representation as the underlying resolution determines the dimension of the clustering problem this is of utmost importance for practical computations as it may determine whether desired clusterings can actually be computed in practice resolution in the following we focus on pixel or voxel based images while we are mainly interested in the dimensions d 2 3 we will give results in arbitrary dimensions hence we will use voxel as generic term independently of the dimension d n of space typically we consider an image in 0 1 d each axis is partitioned into intervals of equal length and each voxel is the cartesian product of one such interval on each axis formally let 1 d nd be a vector we consider the partitioning of the d dimensional cube 0 1 d into uniform boxes of size 2 1 2 d then we obtain a data set x which will later be clustered into sets which represent the gains by replacing each box by its centroid i e x consists of all points x xj 1 jd 1 2 1 1 j 1 1 2 1 1 2 d 1 jd 1 2 d 1 2 1 1 j 1 2 1 1 2 d 1 jd 2 d where using the notation 1 for n j 1 2 1 jd 2 d note that x 2 1 2 d i 1 i of course the voxels of the image are the boxes xj 1 jd 1 2 1 1 1 1 1 2 d 1 1 1 of volume 1 2 1 1 2 d the spatial resolution of such a discretization of 0 1 d in each coordinate i is 2 i for simplicity we speak of the resolution in the following we will refer to point sets of the form x as cartesian point sets or resolution set the paper addresses the error which occurs if we base the optimization on a coarser resolution 1 d we will adopt the following notation is the given bench mark resolution with which we will compare all results for coarser resolutions i e inequalities involving vectors are always meant componentwise 3 weight constrained clustering next we introduce the constrained clustering problem while we are specifically inter ested in cartesian point sets the following description is more general given x 1 xn rd and associated weights 1 n 0 let x denote the weighted data set of the tuples xj j for j n further let k n 1 a k clustering or if k is clear simply a clustering c of x is a vector c c 1 ck 11 1 n k 1 kn 0 1 kn whose components ij specify the fraction of xj that is assigned to the ith cluster ci i 1 in the set of all such clusterings will be denoted by c k x if all ij are integer i e ij 0 1 the clustering is called integer suppose that approximations 1 k of the desired volumes of the cells are given collected in the family k 1 k then we also demand that the weight ci n j 1 ij j of each cluster ci is close to i more precisely for given i 0 we require that the weak weight constraints 1 i i w ci 1 i i i k hold in the specific situation of grain mapping x x all weights are identical in fact i i and thus the cluster weights of all integer clusterings are multiples of i hence with measurement errors i 1 2 the weak weight constraints are equivalent to the strong weight constraints w ci i i k as the measurement errors are generally determined by the benchmark resolution any way we will in the following just for the simplicity of exposition always require this stronger condition the set of all such weight constrained clusterings will be denoted by ck k x note that ck k x 6 if and only if k i 1 i n j 1 j and we will assume that this is the case as it is well known see e g 10 when all weights are 1 or equivalently are identical there always exist optimal clusterings c ck k x which are integer the quality of a clustering will be measured in terms of deviations from k different sites s 1 sk with respect to ellipsoidal norms more precisely for i k let ai rd d be a positive definite symmetric matrix and let ai denote the associated norm 4 i e x ai xtaix for any x rd further we set s s 1 sk and a a 1 ak let us point out that a is a family i e repetitions are allowed the cost of a clustering c with respect to s and a is defined as costa x c s k i 1 n j 1 ij j xj si 2 ai a case of special relevance is that with all matrices being the unit matrix ed then e ed ed refers to the classic euclidean least squares objective in which each cluster norm is x 2 x e in the realm of grain mapping this is called the isotropic case while the general situation models the anisotropic case in the isotropic situation we drop the subscript and simply write cost x c s rather than coste x c s if no sites s are given we measure the cost of a clustering c with respect to the best sites see remark 2 i e the cluster centroids ci 1 ci n j 1 ij jxj and define costa x c k i 1 n j 1 ij j xj ci 2 ai grain volumes k sites s and matrices a can often be measured directly with sites approximating the centroids different norms then reflect measured knowledge about the moments of the grains in materials see 2 in the rare cases that grain scans are explicitly available the volumes k can be derived directly sites can be computed as centroids and the matrices can be estimated as the inverse of covariance matrices however both s and a may also be subject to optimization to reduce the classification error see 5 here we are dealing with the more standard situation which leads to the mathematical problems of finding optimal weight constrained anisotropic assignments or clusterings wca assignment given k x a k s find c ck k x that minimizes costa x c s wca clustering given k x a k find c ck k x that minimizes costa x c we say that k x a k s or k x a k is an instance of wca assignment or wca clustering respectively if we are in the euclidean case i e when a e ed ed we just speak of weight constrained clustering wc clustering or assig ment wc assignment and often drop a from the instance 5 coresets the notion of a coreset is used to classify the effect of data reductions on the quality of solutions of hard optimization problems as it is well known even unconstrained least squares clustering is np hard 15 1 and the np hardness persists even if the dimension is fixed to d 2 23 also by 6 no ptas in both d and k exists of course when s is fixed the computation of costa x s min c ck k x costa x c s boils down to a linear program and can hence be carried out in polynomial time but even then typical real world instances in materials science are so large that they can be solved only on very sparse grids therefore one is aiming at a significant speed up by resorting to a compressed set x without sacrificing much of the clustering quality more precisely let 0 1 2 and 1 then x is an coreset for k x k if there exists a mapping g ck k x ck k x called extension and real constants referred to as terms or offsets with 0 such that the following two conditions hold for all sets s of k sites and clusterings c ck k x 1 costa x g c s costa x c s a costa x s 1 costa x s b if the two terms coincide i e 0 we can choose 1 and we speak of an coreset then note that in this situation for c argmin costa x c s c ck k x 1 costa x s 1 costa x g c s 1 costa x s hence if the extension g can be computed efficiently solving the assignment problem on the coreset x leads to an approximate solution on the original data set x it is less obvious but still true that the above general definition of coresets indeed captures the intuition behind the concept that an approximate solution of the clustering problem on the coresets leads to an approximate solution on the original data set x as a service to the reader this result of 19 thm 3 5 will be restated in full generality as proposition 3 in section 6 where we use a special case in the proof of theorem 2 anyway coresets for unconstrained least squares clustering have been studied intensively see 21 17 16 18 27 7 in particular 21 constructed coresets of size o k 3 d 1 for uncon strained least squares clustering which live on o k pencils subsequently 19 im proved and generalized their construction leading to pencil coresets of size only o k 2 d 1 even for weight constrained anisotropic clustering in the following more precise state ment of this result a and a denote the largest and smallest eigenvalue of all matrices in the set a 6 proposition 1 19 thm 2 3 for any instance k x k a of wca cluster ing 0 1 2 and a a there is an coreset x with x o k 2 d 1 while proposition 1 gives the currently best bound for deterministic coresets for wca clustering it has still some severe drawbacks which limit its use in image segmen tation problems like grain mapping first the available coresets may still be very large for instance for k 100 d 3 and 0 001 we have k 2 d 1 104 10 12 1016 also note that the asymptotic o statement for the size of x conceals the constant which is also relevant as another crucial downside the constructed pencil coresets completely ignore the grid structure of x that is present in many image segmentation problems including our running application of grain mapping resolution coresets in the following we restrict the original data x to the resolution setting x x x 1 xn 1 n for a fixed resolution and also assume that 1 k n n k i 1 i we are interested in coresets x which carry the same structure i e for some reso lution with x x x 1 x n 1 n with 1 n 2 1 2 d also due to the cartesian product structure of x we restrict our considerations to the euclidean case i e a e e e while this is a relevant restriction to the isotropic case theorem 2 will show however that results for this setting also yield approximation results for the anisotropic case 7 for this restricted setting we will study the question how far we can reduce while maintaining the coreset properties more precisely let 0 1 2 and 1 then x is an resolution coreset if x is an coreset and there exists a resolution such that x x and note that such a cartesian structure is beneficial for many applications that directly work on images including superpixel segmentation or polycrystal representation main results on the one hand the instances of our constrained clustering problem are strongly re stricted on the other hand only resolution coresets are permitted hence it is not clear whether the drawbacks of the more general result of proposition 1 can be avoided in the decribed resolution setting at least in small dimensions those which are relevant for grain mapping and other related image segmentation problems as the following theorem shows the answer is affirmative even when we require that the offsets coincide theorem 1 let nd x x and 0 1 2 then there is an resolution coreset x for k x k of size x 28 3 d k 2 3 d note that the size of the resolution coreset in theorem 1 is independent of thus the result shows that a higher resolution is often not needed to achieve a good clustering of image voxels in fact using a grid with less than 7 k 2 3 points on each coordinate axis results in a clustering whose cost is at most off the optimum by a factor of 1 in our running application from materials science this is not only relevant for computing grain maps from few of their characteristic measurements but may also ease scanning processes used for verification in fact theorem 1 allows to scan grains at a relatively low resolution without sacrificing much of the resulting quality and can hence be used to speed up scanning based imaging series of grain growth dynamics in practice let us now compare the size of the resolution coreset to the bound given in propo sition 1 beginning with the asymptotics and ignoring multiplicative constants the bounds read k 2 d 1 proposition 1 kd 2 d 3 theorem 1 hence the former is better in k for d 3 but always worse in in dimension d 3 the latter is better by a factor of 2 k specifically for values k 100 and 0 001 and when compared to the evaluation after proposition 1 we save a factor of 104 note that the bound for resolution coresets involves an additional constant of 256 while the o notation in proposition 1 hides the constant required for constructing the required nets such constructions typically involve a large constant that is exponential in d see e g 13 lemma 5 3 let us point out that as we will see the construction of theorem 1 is explicit including the extension g hence from a constrained clustering c on the low resolution data set 8 we obtain quickly a clustering g c on the original data set let us finally point out that this is not just useful in the isotropic case but also for anisotropic clusterings theorem 2 let i a k x a k s be an instance of wca assignment i e k x e k s and 0 1 2 further let x be an 3 resolution coreset for i e g its extension c ck k x and c g c if c is a approximation for i e k x e k s then c is a 1 a a approximation for i a 3 some basics for the analysis of resolution coresets in the following we will prepare the proof of our main result we will outline its structure provide some prerequisites and set our new estimates into the more general perspective of known approaches structure of the proof of theorem 1 the construction of resolution coresets can be viewed as a uniform local neighborhood merger hence we can in principle evoke 19 theorem 2 2 to obtain an upper estimate for the required coreset size it turns out however that the general estimates are much weaker than what we are aiming at hence in order to prove theorem 1 we need to make much stronger use of the specific underlying setting the cartesian structure of the point set x the uniform weights and the fact that in the isotropic case the objective function is separable with respect to the coordinates while the former allow us to use local arguments the latter permits in effect to reduce the estimates to the 1 dimensional case in fact indicating as usual the tth coordinate of a vector y by yt we have costa x c s k i 1 n j 1 ij j xj si 22 k i 1 n j 1 ij j d t 1 xj t si t 2 d t 1 k i 1 n j 1 ij j xj t si t 2 we will in the following provide some preparatory results then in section 4 conduct the computations and derive all estimates needed for the proof of theorem 1 in the 1 dimensional case and subsequently extend the result to arbitrary dimensions in sec tion 5 the general reasoning follows in part that of 19 we can however only use very few results directly while others will have to be substantially modified yet most parts of the proofs are independent of 19 and rely heavily on the specific structure of the present setting 9 merging function let 1 d nd and 1 d nd with we regard as the fixed benchmark resolution while at this point is a parameter vector which is adjusted later to specify the resolution of the desired coreset as before x x n x x x n x with weights j 2 1 2 d j n q 2 1 2 d q n we will interpret the coarsening of the resolution from to in terms of a neighborhood merger defined by a merging function p x x here p is simply given by 1 2 1 1 j 1 1 2 1 1 2 d 1 jd 1 2 d 7 1 2 1 1 q 1 1 2 1 1 2 d 1 qd 1 2 d for jt rt qt 1 2 t t rt 2 t t qt 2 t t d whenever this seems more convenient we will regard p as a function of the index set n i e p n n note that indeed q j p 1 q j q n in effect we replace all points of x in each batch bq xj j p 1 q by x q which is carrying the total weight of all corresponding batch points observe that bq 2 1 1 2 d d q j given a clustering c ck k x the merging function p gives rise to a clustering c ck k x defined by iq 1 q j p 1 q ij j j bq ij q n i k 10 conversely given c ck k x we obtain a clustering c ck k x by setting ij ip j j n i k this defines an extension g ck k x ck k x that converts clusterings of x into clusterings of x note that the extension g assigns each point xj of x to clusters in the same way its representative xp j in x is assigned to clusters as we will prove later the induced approximation error can be tightly controlled although this is not an issue in our present context let us point out in passing that coarsening the resolution generally excludes the integrality of clusterings of course the constraint matrix is still totally unimodular but unless the prescribed cluster weights i are divisible by the set ck k x will not contain any integer clustering while by 10 cor 2 3 the number of fractional variables i q that need to be accepted in optimal coreset clusterings is at most 2 k 1 the extension g spreads any such nonintegrality to all points in the corresponding batches bq power diagrams in the proof of theorem 1 we will make use of the close relation between constrained clustering and diagrams we will specifically use a result of 10 for our isotropic setting see however 14 and the handbook article 20 for examples and properties of different classes of diagrams in the anisotropic case and additional pointers to the literature a power diagram pd p p s p 1 pk in rd is specified by k sites s s 1 sk in rd and real sizes 1 k it is a dissection of rd into polyhedral cells defined by proximity i e more precisely pi x rd x si 22 i x s 22 k i note that power diagrams generalize voronoi diagrams which result for 1 k a clustering c ck k x admits a power diagram if there exists a power diagram p such that supp ci xj x ij 0 pi i k in this situation p and c are called compatible if actually supp ci pi x i k then p and c are strongly compatible note that if in addition c is integer then supp ci lies in the interior of pi for each i i e none of the point in x lies on the boundary of any of the cells of p proposition 2 10 cor 2 2 any c ck k x which attains cost x s ad mits a power diagram further there exists an optimal clustering c ck k x and size parameters such that c and the diagram p p a s are strongly compatible 11 batch error for a nonempty subset y of x let c y 1 y x y x denote its centroid and set v y x y x c y 22 we will mostly but not exclusively apply this notion to batches bq with respect to the resolution then use the notation v bq and speak of the batch error let us point out that the centroid c bq of a batch bq lies in x and is actually the point resulting from the merging process remark 1 let q n then c bq x q proof for t d let qt 2 t such that x q x q 1 qd then using the abbreviation t 2 t t we have for the tth coordinate c bq t of the centroid c bq c bq t 1 bq x bq c bq t x bq c bq t 1 t t rt 1 1 2 t 1 rt qt 1 t 2 t 1 t t 2 t 1 t t 1 2 t 1 2 t qt 1 2 t t 2 qt 1 2 t 1 2 qt 1 2 t 1 1 2 t 1 qt 1 2 t x q t remark 1 shows that the reduction of the resolution from to can be viewed as batching accordingly the batch error v bq quantifies the effect of the merging process note that due to the uniform batching process the batch error is independent of q and we will therefore omit bq and simply write v v bq lemma 1 we have v 1 12 1 2 1 1 2 d d t 1 1 22 t 1 22 t proof let b b 1 denote the batch with centroid 2 1 1 2 d 1 then v v b x b x c b 22 d t 1 x b xt c b t 2 d t 1 d t 2 2 t t r 1 1 2 t 1 r 2 t 1 2 t 1 2 d t 1 1 2 t t 2 t t r 1 1 2 t 1 r 2 t 1 2 t 1 2 12 now for t t and with 2 r 1 1 2 1 r 2 1 2 1 2 1 22 2 r 1 2 r 1 2 1 22 2 4 1 2 1 6 4 1 1 2 1 2 2 1 3 1 22 2 2 1 1 3 1 2 2 22 1 hence v 1 3 d t 1 1 2 t t 1 2 t t 2 22 t t 1 1 3 d t 1 1 22 t 1 22 t t 1 1 12 d t 1 1 22 t 1 22 t as claimed in the next section we will also consider the error with respect to an arbitrary site this can be handled with the aid of the following remark which is folklore can be found in 8 and actually holds in greater generality than needed here see 19 lemma 3 2 a remark 2 let y be a nonempty subset of x s r then x y x s 22 v y y c y s 2 2 hence in particular the error on the left is minimized for s c y proof since x s 22 x c 22 2 x c t c s c s 22 and x y x c t c s x y x y c t c s 0 we have x y x s 22 x y x c 22 y c s 22 v y y c s 22 which also implies the second assertion 13 4 proof of theorem 1 the case d 1 while up to now all computations were exact for arbitrary dimension d we will next derive estimates for the costs of clustering in dimension 1 in section 5 these results will in effect be applied for each line parallel to any coordinate axis through points of x and we will formulate them already with a view towards their later use since in this section we are considering exclusively the 1 dimensional situation we will adopt the simplified notation d 1 and do not distinguish between the 1 dimensional vectors xj x q and their coordinates xj 1 x j 1 respectively best unconstrained least squares clustering we begin by computing the cost optk x of a best unconstrained least squares clustering of the 1 dimensional resolution set x into k 2 log k clusters the following result will later be used as a lower bound for cost x c s for any choice of c and s theorem 3 let log k k 2 and let c c 1 ck be an optimal un constrained least squares clustering of x then each cluster contains the same number 2 of points of x the centroids ci c ci of the k clusters are the equidistant points ci 1 2 k i 1 k 1 2 1 i 1 2 i k and optk x 1 3 1 22 1 22 1 1 3 1 22 1 22 k 2 1 proof since by proposition 2 c admits a power diagram the points in each cluster are consecutive without loss of generality we may also assume that c is integer so with k 1 n i ci i i 1 1 ii i 1 i i k and the standard interpretation of empty sums and 0 we have for j ii xj 1 2 1 j 1 2 i 2 1 2 1 j 1 i 2 i 2 xj i 14 hence ci 1 i j ii xj i 2 1 i i j 1 xj i 2 1 i i j 1 1 2 1 j 1 2 i 2 1 i i 2 1 1 2 i j 1 j 1 i 2 i 2 1 therefore we obtain as ci s contribution to the cost of the clustering v ci i j 1 xj ci 2 i j 1 1 2 1 j 1 2 i 2 1 2 1 23 2 i j 1 i 1 2 j 2 1 23 2 i j 1 i 1 2 4 i 1 j 4 j 2 1 23 2 i i 1 2 2 3 i i 1 2 i 1 1 3 1 23 2 i 2 i 1 since c is optimal the cluster cardinalities minimize k i 1 i 2 i 1 as a function of the i under the constraints k i 1 i n and i n 0 now note that the ith summand of the objective function is strictly increasing in i hence in the minimum we must have i n for all i k let us now discard the integrality condition and consider the constrained nonlinear program min k i 1 i 2 i 1 k i 1 i n i 1 i k in the real variables 1 k as all occurring functions are continuously differentiable and the objective function is convex in the nonnegative orthant we can apply the karush kuhn tucker theorem and obtain the conditions 3 21 1 3 2 k 1 k i 1 i 1 1 1 1 0 k i 1 i n 1 k 1 1 k 0 k i 1 i i 0 involving the lagrange parameters i and since i 1 and i 0 for all i k the complementary slackness condition yields 1 k 0 and the first condition reads 3 21 1 3 2 k 1 0 15 this implies that all i coincide thus we obtain 1 k n k 2 n i i 1 2 i k and ci i 2 i 2 1 1 2 1 i 1 2 i k as claimed also v c 1 v ck v and hence opt k x 2 v 2 1 3 1 23 2 2 22 1 1 3 1 22 1 22 1 which completes the proof note that the centroids and hence by remark 2 best choices of sites are exactly the points of the data set x and thus the cluster error is the batch error v as a corollary we obtain the desired lower bound corollary 1 let s be a set of k sites and c ck k x then cost x c s optk x 1 3 1 22 1 22 1 k 2 1 proof since k k 2 log k 2 log k 1 2 k and optk x optk x cost x c s theorem 3 implies 1 3 1 22 1 22 1 k 2 1 1 3 1 22 1 22 k 2 1 cost x c s as asserted coreset property a next we prove an even stronger version of coreset property a lemma 2 let 2 v s r a set of k sites c ck k x and c g c where g ck k x ck k x is the standard extension defined in the previous section then cost x c s cost x c s 16 proof for i k j n and q n let as before ij and iq denote the components of c and c respectively further recall that the merging function maps each point of a batch bq to its centroid then using remark 2 we have cost x c s k i 1 n j 1 ij xj si 2 k i 1 n q 1 iq xj bq xj si 2 k i 1 n q 1 iq v bq xq si 2 n v k i 1 n q 1 iq 2 2 xq si 2 2 v k i 1 n q 1 iq xq si 2 cost x c s which proves the assertion coreset property b setup and good indices not surprisingly the proof of coreset property b is much more involved and the rest of this section will be devoted to that first we observe that it follows from a more general inequality remark 3 suppose that there exists a constant such that the inequality cost x p c s 1 cost x c s b holds for every set s of k sites and any integer clustering c ck k x which admits a strongly compatible power diagram then for every such s cost x s 1 cost x s proof first note that by proposition 2 there exists an integer clustering c which admits a strongly compatible power diagram and attains cost x s then of course cost x s cost x p c s and cost x c s cost x s hence by assumption cost x s cost x p c s 1 cost x c s 1 cost x s according to remark 3 it suffices to prove b so let in the following s be a set of k sites let c c 1 ck ck k x be an integer clustering which admits the strongly compatible power diagram p p 1 pk and set c p c we split the cost of the clustering c of the resolution coreset into two components which reflect whether batches are fully contained in one of the cells of p or split by the 17 diagram more precisely for each q n let i q denote the unique index i k such that xq pi and set i q n bq pi q i n i the elements of i and i will be called good and bad indices respectively as the latter require significantly more care in terms of cost estimations note that for good indices q the clusterings c and c coincide in the sense that iq ij for all i k j n q i with xj bq also as the points of x in both the batches bq and the intervals pi are consecutive we obtain the following bound remark 4 i k 1 the cost of c can be expressed in terms of the good and bad indices in fact cost x c s k i 1 n j 1 ij xj si 2 i i where i k i 1 q i xj bq ij xj si 2 similarly we use the abbreviations i k i 1 q i iq x q si 2 to denote the parts of cost x c s associated with i the cost for the good indices relates directly to the cost incurred by the corresponding original data points in fact by remark 2 we have i k i 1 q i xj bq ci xj si 2 k i 1 q i v bq ci bq ci x q si 2 k i 1 q i iq v x q si 2 hence we obtain for the good indices remark 5 i i v i 18 bad indices now we deal with the indices in i naturally the clustering error for the bad indices depends more specifically on the batch sizes in the following we assume that is chosen appropriately in view of the extension of our results to the d dimensional case in section 5 we show slightly more than what would otherwise be needed in this section so let k n such that k k and set log 2 5 3 k 2 3 of course we also assume that as before next we deal with the total deviation dev i k i 1 q i xj bq ij x q xj 2 caused by the bad indices let us first point out how dev i relates to the batch error remark 6 dev i i v proof by remark 1 we have dev i q i xj bq k i 1 ij x q xj 2 q i xj bq x q xj 2 q i xj bq c bq xj 2 q i v bq i v the next lemma provides an upper bound for the total deviation it will employ lemma 1 in its following 1 dimensional form remark 7 v 1 12 1 2 1 22 1 22 1 3 1 2 22 1 2 2 lemma 3 dev i 2 8 optk x 6 cost x c s proof by remarks 4 6 and 7 dev i i v k 1 1 3 1 22 2 22 1 and by corollary 1 optk x 1 3 1 22 1 22 1 k 2 1 19 thus dev i 2 8 optk x 1 3 1 22 1 k 1 2 22 1 2 8 22 1 k 2 1 setting f k k 1 2 22 1 2 8 22 1 k 2 1 we have for k k f k 1 f k 1 2 22 1 2 22 1 8 2 k 1 k 2 k 1 2 0 hence f k is increasing in k and we have dev i 2 8 optk x 1 3 1 22 1 k 1 2 22 1 2 8 22 1 k 2 1 1 3 1 22 1 22 k 1 23 2 32 k 2 k 1 2 2 8 and to prove the first asserted inequality it suffices to show that k 1 23 2 32 k 2 k 1 2 2 8 to verify these inequalities simply note that k 1 k 2 23 k 1 2 25 k 2 32 and since k 2 and 0 1 2 k 1 2 k 1 4 k 2 3 2 8 k 1 k 2 4 3 2 8 finally note that 2 8 16 6 the next lemma gives the desired bound for the bad indices lemma 4 i i 5 6 cost x c s 20 proof let us remark that the assertion can also be inferred from the more general result in 19 lemma 4 3 as a service to the reader we give an explicit direct proof of what we need here in fact i k i 1 q i iq x q si 2 k i 1 q i xj bq ij x q xj xj si 2 dev i i 2 k i 1 q i xj bq ij x q xj xj si further using the cauchy schwartz inequality k i 1 q i xj bq ij x q xj xj si 2 k i 1 q i xj bq ij x q xj 2 k i 1 q i xj bq ij xj si 2 dev i i since 0 1 2 and 1 16 1 2 5 6 lemma 3 hence yields i dev i i 2 dev i i 1 2 i 2 8 optk x 2 2 8 optk x i 1 2 i 2 8 2 cost x c s i 5 6 cost x c s as claimed we can now combine the results of this subsection to show property b lemma 5 let 2 v then cost x c s 1 cost x c s proof first note that 2 i i using in this order remarks 5 and 6 and lemmas 3 and 4 we hence obtain cost x c s i i v i i v i i dev i i i 5 6 cost x c s 6 cost x c s 1 cost x c s 21 as claimed note that depends only on the resolutions and but is independent of k and k resolution coresets in 0 1 the main result of this sections follows now easily theorem 4 x is an coreset for k x k of size x 28 3 k 2 3 proof by lemmas 2 and 5 x is indeed an coreset and by the choice of we have x 2 2 log 2 k 2 3 2 2 5 3 k 2 3 2 8 3 k 2 3 this completes the proof of the theorem 5 proof of theorem 1 the general case we will now prove our main theorem by reducing it to the 1 dimensional case settled already in section 4 so suppose that d 2 the key observation is that in the isotropic case the costs of a given clustering c ij i k j n ck k x on x x in rd is a separable function with respect to the lines parallel to the coordinate axes in fact for t d let ut denote the tth standard unit vector and set x t x 1 x t 1 0 x t 1 x d note that x 1 2 t 1 2 t ut x t i e x t is the corresponding d 1 dimensional resolution set located in the coordinate hyperplane perpendicular to ut further l t x t r ut denote the set of lines through the cartesian point set and parallel to the tth coordinate axis and set l t d l t then we have cost x c s d t 1 k i 1 n j 1 ij j xj t si t 2 d t 1 k i 1 l l t xj l ij j xj t si t 2 d t 1 l l t k i 1 xj l ij xj t si t 2 22 clusterings induced on the lines in l the double sum in parenthesis can be interpreted as the cost of a certain clustering of the points of x l on the line l the reduction to the 1 dimensional case is however somewhat technical and requires some additional notation before we introduce the details let us begin with some remarks concerning our nota tional conventions obviously we need to distinguish between the 1 and d dimensional objects and also bear in mind that in the 1 dimensional setting points carry different weights than in the d dimensional situation in addition we need to consider sets in the d 1 dimensional coordinate hyperplanes in particular batches of points in x t are relevant in order to keep the notation as intelligible as possible we will use sub scripts for components as in 1 d but also for intrinsically 1 dimensional objects for instance we write xl for the point set x l naturally xl can be identified with the resolution set x t in r note however that each point in x t carries the weight t rather than as it is the case for subsets of x further we use superscripts in parenthesis to indicate the dependence of sets in rd on a param eter as e g in l t we indicate that a single component is missing by means of a minus sign and write for instances t 1 t 1 t 1 d the same notation is also used if a component is fixed in x t for example the minus sign indicates that the tth coordinate of each point in x is set to 0 in other situations again indicated by the superscript t the tth coordinate may however be fixed differently while these conventions are used to facilitate a more intuitive exposition all objects will of course be defined precisely let us now introduce the needed concepts more precisely we begin with the formal definition of clusterings on the lines l l t induced by c let nl j n xj l ic l i k j nl ij 6 0 kl ic l and set i l j nl t ij for i ic l kl i l i ic l j l t for j nl l j l j nl then of course cl ci l i ic l ij i ic l j nl ckl kl xl l note that i ic l i l i ic l j nl ij j l xj xl t t xl 1 further let si l denote the orthogonal projection of si on l and set sl si l i ic l let us point out that si l s l whenever si t s t i e sl may in general be a multiset in such a situation it is straightforward to merge the corresponding 23 cells of cl perform subsequent operations in the standard model and finally split up the assignment again we prefer however not to further complicate the notation and simply apply this additional technicality tacitly note that when c is integer and admits a strongly compatible power diagram the sites si l for i ic l are all different anyway with the introduced notation we see that k i 1 xj l ij xj t si t 2 i sc l j nl ij xj si l 2 t i sc l j nl t ij xj si l 2 t cost xl cl sl and hence have the following result remark 8 cost x c s d t 1 l l t t cost xl cl sl remark 8 gives the desired split of the costs of c into the costs of the induced clus terings cl for l l and opens up the possibility to apply results from section 4 resolution coresets as before we will use the simplified notation x x x x and generally signify objects related to x by means of put on top in particular we set l t x t r ut for t d l t d l t and let b t denote the set of batches in x t induced by x t by further for b b t let l t b b r ut for future reference the following remark collects the cardinalities of the different sets remark 9 we have t b t 1 and for b b t b t t d t 2 l t b l t l t b l t in fact the lines of l t are in 1 to 1 correspondence with the batches in b t as each line l l t intersects exactly one such batch in the following we use the notation l l b and b b l to indicate this correspondence note that the point l x t b is the centroid of b also recall from section 3 that the merging function p x x has a cartesian structure too more precisely for x x we have p x p 1 x 1 pd x d 24 hence merging takes place in each coordinate independently in the following we will use the specific values 1 d log 2 5 3 k 2 3 1 d x x and define t 2 t v t t d d t 1 l l t t t finally we set k k and note that kl k for each l l since x x d 2 8 3 k 2 3 d theorem 1 is proved once the two coreset properties are established this will be done in the next two subsections more specifically in lemmas 6 and 7 to slightly simplify the notation we will omit the asterisk and simply write 1 d for the resolution of x coreset property a let c iq i k q n ck k x be a clustering on x recall from section 3 that c can be extended to a clustering c g c ij i k q n ck k x by means of the extension g ck k x ck k x defined by ij iq for j n i k q n with p xj x q in order to apply the established 1 dimensional results we will now take a closer look at the relation between extensions in rd and on the lines of course remark 8 can be used to express cost x c s in terms of the costs of the induced clusterings on the lines l l explicitly it reads as follows remark 10 cost x c s d t 1 l l t t cost x l c l sl hence using the coreset properties on the involved lines alone we can bound cost xl cl sl only for l l indeed the underlying point set is x l l l but does not contain any point from x l l l due to the definition of the extension cost xl cl sl can however be included for all lines in l l as well in fact except of a translation of the point set the clusterings cl 1 and cl 2 for lines l 1 l 2 l t coincide whenever there is a batch b b t such that l 1 l 2 l t b hence in particular we have the following remark 25 remark 11 let l l t b b l b t and l l t b then cost xl cl sl cost xl cl sl the following lemma addresses coreset property a lemma 6 let c ck k x and c g c then cost x c s cost x c s proof by remark 9 we have t l t t l t for each t d hence d t 1 l l t t t d t 1 l l t t t thus with remarks 9 and 10 we obtain cost x c s d t 1 l l t t cost x l c l sl t d t 1 l l t 1 b l l l t b l t cost x l c l sl t d t 1 l l t l l t b l t cost x l c l sl t now with l l t b b l b t and l l t b we apply lemma 2 to the clustering c l ckl kl x l l with the aid of remark 11 obtain cost xl cl sl cost xl cl sl cost x l c l sl t hence with remark 8 we can now reverse the previous arguments to see that d t 1 l l t l l t b l t cost x l c l sl t d t 1 l l t l l t b l t cost xl cl sl d t 1 l l t cost xl cl sl cost x c s this completes the proof 26 coreset property b let us now turn to coreset property b so suppose that the clustering c ck k x is integer and admits a strongly compatible power diagram then for each line l l each clustering cl ij i ic l j nl ckl kl xl l is also integer and admits a strongly compatible power diagram we can hence apply lemma 5 to relate its cost to that of the corresponding coreset clustering note that the latter lives on l while c l is only defined on the lines l l the following proof of coreset property b will therefore explicitly address the transition from coresets on each line l l to c lemma 7 let c ck k x be integer and admit a strongly compatible power dia gram and set c p c then cost x c s 1 cost x c s proof by remark 8 cost x c s can be decomposed into the costs on the lines l l i e cost xl cl sl for each cl ckl kl xl l we derive a coreset clustering with kl k according to lemma 5 we refrain from denoting it by c l with a wide tilde but use a different notation for better distinction let t d l l t and x t l x t such that l x t l r ut then we set yl x t l xtut xt x t l t t zl l ir i ic l r 2 t ckl kl yl l note that yl x t l y 1 y 2 t ut with yr 2 t 1 r 1 2 t r 2 t by remark 8 and lemma 5 we have 1 cost x c s d t 1 l l t t 1 cost xl cl sl t d t 1 l l t t cost yl zl sl d t 1 b b t l l t b t cost yl zl sl now let q n b bq r ut x t b t l l b and r r q 2 t such that x q t yr t then as x q l we have yr si l 22 x q si l 22 for all l l t b 27 and iq 1 xj bq ij l l t b xj l bq ij l l t b t t 1 t xj l bq ij t t t l l t b ir hence with the additional setting l ir 0 for i 6 ic l we obtain l l t b t cost yl zl sl l l t b t i ic l 2 t r 1 l ir t yr si l 22 k i 1 x q x l t l l t b l ir q t x q si l 22 t k i 1 x q x l iq t x q si l 22 t cost x l c l sl thus 1 cost x c s d t 1 b b t l l b t cost yl zl sl d t 1 l l t t cost x l c l sl cost x c s which concludes the proof 6 proof of theorem 2 while theorem 1 explicitly exploits the orthogonality of the euclidean norm and hence produces resolution coresets only for the euclidean case we will now show that it can still be used for general anisotropic objective functions defined by means of families a a 1 ak of positive definite symmetry d d matrices the next well known result specifies how good constrained clusterings on coresets yield good clusterings on the original data sets in the proof of theorem 2 we will need it only in the euclidean case and for 1 we formulate it here for coresets in the anisotropic case since it provides the rationale behind the general coreset definition given in section 2 which involves two terms 28 proposition 3 19 thm 3 5 a let i k x a k s be an instance of wca assignment 0 1 2 1 and further let x be an 3 coreset for is and g its extension now suppose that c ck k x is a approximation for i k x a k s then g c is a 1 approximation for i we can now prove theorem 2 proof of theorem 2 first note that for a positive definite symmetric d d matrix a with smallest and largest eigenvalues a and a respectively we have a x 22 x 2 a a x 22 x rd recalling that a min ai i k a max ai i k we hence have costa x c s k i 1 n j 1 ij j xj si 2 ai k i 1 n j 1 ij j a xj si 22 a cost x c s and similarly a cost x c s costa x c s since c g c proposition 3 therefore yields costa x c s a cost x c s a 1 cost x s 1 a a costa x s which completes the proof 7 final remarks as pointed out before the size bounds for resolution coresets are superior to those for pencil coresets in small dimensions comparing the results of theorem 1 and proposi tion 1 we see however that the number of clusters enters as kd and k 2 respectively in fact resolution coresets utilize the grid structure of x but are themselves restricted to a coarser grid pencil coresets on the other hand produce very different point densities depending on the distance from the chosen vertices of the pencils since for resolution coresets we insist on a uniform resolution on each line we cannot employ the rationale leading to the reduced dependence on k in proposition 1 it remains however an open question that is already relevant in dimension 3 whether the term kd can be further reduced also generalizations to cartesian point sets with arbitrary positive weights would be interesting further an analysis of nonuniform tilings of 0 1 d into axis par allel boxes would be relevant this corresponds to adaptive thinning of cartesian point sets and is interesting for fast image processing see e g 3 and the papers quoted there 29 references 1 daniel aloise amit deshpande pierre hansen and preyas popat np hardness of euclidean sum of squares clustering in machine learning 75 2 may 2009 pp 245 248 issn 08856125 doi 10 1007 s 10994 009 5103 0 url http link springer com 10 1007 s 10994 009 5103 0 2 andreas alpers andreas brieden peter gritzmann allan lyckegaard and hen ning friis poulsen generalized balanced power diagrams for 3 d representations of polycrystals in phil mag 95 9 2015 pp 1016 1028 3 andreas alpers and maximilian fiedler power slic fast superpixel segmen tations by diagrams submitted 2021 url http arxiv org abs 2012 11772 v 2 4 andreas alpers maximilian fiedler peter gritzmann and fabian klemm dy namic grain models via fast diagram representations submitted 2022 5 andreas alpers maximilian fiedler peter gritzmann and fabian klemm turn ing grain scans into diagrams submitted 2022 6 pranjal awasthi moses charikar ravishankar krishnaswamy and ali kemal sinop the hardness of approximation of euclidean k means in symposium on computational geometry 2015 pp 1 14 issn 18688969 doi 10 4230 lipics socg 2015 754 url http arxiv org abs 1502 03316 7 olivier bachem mario lucic and andreas krause practical coreset construc tions for machine learning in arxiv e prints 2017 arxiv 1703 06476 url http arxiv org abs 1703 06476 8 steffen borgwardt andreas brieden and peter gritzmann an lp based k means algorithm for balancing weighted point sets in european journal of opera tional research 263 2 2017 pp 349 355 9 steffen borgwardt andreas brieden and peter gritzmann geometric clustering for the consolidation of farmland and woodland in math intelligencer 26 2014 pp 37 44 10 andreas brieden and peter gritzmann on optimal weighted balanced cluster ings gravity bodies and power diagrams in siam journal on discrete math ematics 26 2 2012 pp 415 434 url http epubs siam org doi abs 10 1137 110832707 20 http epubs siam org doi 10 1137 110832707 11 andreas brieden and peter gritzmann predicting show rates in air cargo trans port in international conference on artificial intelligence and data analytics for air transportation aida at ieee 2020 pp 1 9 doi 10 1109 aida at 48540 2020 9049209 12 andreas brieden and peter gritzmann response prediction gaining reliable and interpretable insight from small study data submitted 2021 30 13 andreas brieden peter gritzmann ravi kannan victor klee laszlo lovasz and miklos simonovits approximation of diameters randomization doesn t help in proceedings 39 th annual symposium on foundations of computer science cat no 98 cb 36280 ieee comput soc 1998 pp 244 251 isbn 0 8186 9172 7 doi 10 1109 sfcs 1998 743451 url http ieeexplore ieee org document 743451 14 andreas brieden peter gritzmann and fabian klemm constrained clustering via diagrams a unified theory and its application to electoral district design in european journal of operational research 263 1 2017 pp 18 34 15 sanjoy dasgupta the hardness of k means clustering university of california san diego cs 2008 091 2007 16 dan feldman melanie schmidt and christian sohler turning big data into tiny data constant size coresets for k means pca and projective clustering in siam j comput 49 2020 pp 601 657 17 dan feldman melanie schmidt and christian sohler turning big data into tiny data constant size coresets for k means pca and projective clustering in proceedings of the twenty fourth annual acm siam symposium on discrete al gorithms philadelphia pa society for industrial and applied mathematics jan 2013 pp 1434 1453 isbn 978 1 61197 251 1 doi 10 1137 1 9781611973105 103 url https epubs siam org page terms 20 https epubs siam org doi 10 1137 1 9781611973105 103 18 hendrik fichtenberger marc gille melanie schmidt chris schwiegelshohn and christian sohler bico birch meets coresets for k means clustering in lec ture notes in computer science including subseries lecture notes in artificial in telligence and lecture notes in bioinformatics vol 8125 lncs springer berlin heidelberg 2013 pp 481 492 isbn 9783642404498 doi 10 1007 978 3 642 40450 4 41 url http link springer com 10 1007 978 3 642 40450 4 7 b 5 c 7 d 41 19 maximilian fiedler and peter gritzmann coresets for weight constrained anisotropic assignment and clustering submitted arxiv 2203 10864 v 1 2022 20 peter gritzmann and victor klee computational convexity in crc press boca raton 2017 chap 36 of the handbook on discrete and computational geometry ed by j e goodman j o rourke and c d toth pp 937 968 21 sariel har peled and akash kushal smaller coresets for k median and k means clustering in discrete computational geometry 37 1 jan 2007 pp 3 19 issn 0179 5376 doi 10 1007 s 00454 006 1271 x url http link springer com 10 1007 s 00454 006 1271 x 22 allan lyckegaard and henning friis poulsen data from grain scan measure ments in 2011 private communication 31 23 meena mahajan prajakta nimbhorkar and kasturi varadarajan the planar k means problem is np hard in theoretical computer science vol 442 elsevier 2012 pp 13 21 isbn 3642002013 doi 10 1016 j tcs 2010 05 034 url https www sciencedirect com science article pii s 0304397510003269 24 daniel scho ttle klaus wiedemann wolfgang janetzky michael friede christoph u correll holger jahn and andreas brieden prediction of response to treat ment a biostatistical approach on the use of long acting aripiprazole in a multi center prospective uncontrolled open label cohort study in germany in sub mitted 2021 24 pp 25 ondr ej s edivy timothy brereton daniel westhoff leos pol vka viktor benes volker schmidt and ales ja ger 3 d reconstruction of grains in polycrystalline materials using a tessellation model with curved grain boundaries in phil mag 96 18 2016 pp 1926 1949 26 ondr ej s edivy jule m dake carl e krill iii volker schmidt and ales ja ger description of the 3 d morphology of grain boundaries in aluminum alloys using tessellation models generated by ellipsoids in image anal stereol 36 1 2017 pp 5 13 27 christian sohler and david p woodruff strong coresets for k median and sub space approximation goodbye dimension in 2018 ieee 59 th annual sympo sium on foundations of computer science focs ieee oct 2018 pp 802 813 isbn 978 1 5386 4230 6 doi 10 1109 focs 2018 00081 arxiv 1809 02961 url http arxiv org abs 1809 02961 20 https ieeexplore ieee org document 8555159 28 aron spettl timothy brereton quibin duan thomas werz carl e krill iii dirk p kroese and volker schmidt fitting laguerre tessellation approximations to tomographic image data in philosophical magazine 96 2 jan 2016 pp 166 189 issn 1478 6435 doi 10 1080 14786435 2015 1125540 url http www tandfonline com doi full 10 1080 14786435 2015 1125540 29 kirubel teferra and david j rowenhorts direct parameter estimation for gen eralised balanced power diagrams in phil mag lett 98 2018 pp 79 87 32