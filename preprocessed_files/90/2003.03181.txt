can ml predict the solution value for a difficult combinatorial problem 7 jun 2019 can ml predict the solution value for a difficult combinatorial problem constantine goulimis gast n simone greycon ltd 7 calico house plantation wharf london sw 11 3 tn cng gs greycon com abstract we look at whether machine learning ml can predict the final objective function value of a difficult combinatorial optimisation problem from the input our context is the pattern reduction problem one industrially important but difficult aspect of the cutting stock problem machine learning appears to have higher prediction accuracy than a na ve model reducing mean absolute percentage error mape from 12 0 to 8 7 keywords cutting stock problem pattern reduction setup minimisation paper industry plastic film industry 1 background related work the many practical applications of the one dimensional cutting stock problem 1 d csp have provided a rich source of challenges to the mathematical optimisation community in this paper we look at one such aspect namely the minimisation of cutting patterns within the universe of minimum waste solutions it is well known that the 1 d csp is quite degenerate i e multiple different solutions with the same waste often exist this can be explained by geometrical re arrangement i e it is sometimes possible for example to swap items belonging to different patterns creating new patterns in the process in some industrial settings particularly in the plastic film industry pattern minimisation is quite important because the technical characteristics of the slitter winders that cut the material are such that changing patterns can cause production bottlenecks in general depending on the machinery the actual effort in producing a particular 1 d csp solution is a multi faceted problem pattern minimisation is an important aspect but not the only one sometimes the time cost of a pattern change is not fixed but depends on the differences from the previous one for example the sequencing of a given set of patterns and the relative position of the rolls in each pattern gives rise to the knife change minimisation problem which can be modelled as a generalised travelling salesman problem nonetheless pattern count has become a common key performance indicator see 1 for an overview of different approaches for addressing the pattern minimisation problem it is known that pattern minimisation is np hard 2 in practice this might not mean much 3 however it is harder than the waste minimisation 1 d csp for which optimal integer solutions have been known since 1990 4 it is definitely not a well solved problem in the sense that in the real world we encounter instances that are not solvable to optimality with the typical computational budget that is available a few minutes for example 5 and 6 but note that 7 cast doubts on the validity of the results of the latter employed time limits of 2 and hours respectively in addition both of these methods fail to include real world constraints such as position constraints or number of knives and there are many others in our x trim commercial application we use a combination of transformation heuristics 1 complemented by semi exact methods based on integer programming using gurobi 8 1 as the 7 jun 2019 solver allowing this combined algorithm to run to completion for mid and large sized instances may take a very long time hours and therefore users tend to interrupt the search after a few minutes figure 1 solutions to an instance with 29 distinct sizes the solution on the left has 27 patterns and an equivalent solution with 16 patterns is on the right it is not known whether equivalent solutions with fewer patterns exist the problem we address in this paper is whether we can predict from the starting solution the final outcome in terms of the number of patterns in this quest we have three aims firstly we would like to provide quick guidance to the planner as to the likely outcome without such guidance she is restricted to watching the optimisation and at some point either accepting any solution found so far or cancelling secondly we can also use the ml estimate as a stopping criterion thirdly we are aiming to learn a meaningful property of this particular optimisation problem see section 3 2 2 of 8 and in particular whether machine learning ml algorithms can discern structure which is otherwise invisible even to the expert a positive outcome to the latter aim raises some obvious interesting possibilities we define some terminology suppose we have a solution to an instance of a 1 d csp where the items are of size 0 1 expressed as a fraction of the master size the solution is completely represented as a set of pairs each pair consists of two elements the number of repetitions of the pattern and its contents the latter is a multi set of the required sizes for example the first element of the solution shown to the right of figure 1 is 16 1200 5820 970 5820 970 5820 970 5820 970 5820 740 5820 to avoid trivial cases where the same pattern repeats we require that the multi sets are unique the cardinality of the solution number of elements in the set of pairs is the pattern count we call two solutions equivalent if a the solution run length sum of the pattern repetitions is the same and b the total production for each item li is the same in figure 1 the solution run length is 156 and e g both solutions contain 30 pieces of size 900 7 jun 2019 define to be the function that takes a solution and returns the minimum number of patterns for that solution this is convex in the sense that if we split a solution into two parts 1 2 1 2 1 2 then 1 2 however we cannot effectively calculate f only an upper bound this is not necessarily convex 2 data generation scaling the training set in a ml context typically should consist of several thousand instances we therefore constructed a random instance generator since the problems encountered in industry are far from uniform we created instances for three families family item width primary width comment corrugated case materials ccm typical widths of the reels are in the range 1800 2500 mm typically in multiples of 10 or 25 mm with a greater frequency for the larger widths widths both below and above this range also occur but less frequently the paper machines tend to be 5 8 m wide ccm accounts for of global total paper production output goes into producing the familiar brown cardboard boxes 1 500 instances plastic film f majority of sizes are in the 300 1000 mm size in multiples of 5 mm but there are also parent reels in the range 1800 2300 mm for metallised film primary process extruder is typically 6 8 m wide these represent the production of polypropylene film widely used in food packaging 6 800 instances fine paper fp parent reels for sheeting in the range 1500 2300 mm paper machines in the range 4 6 m these represent the production of paper sheets 1 000 instances because of the importance of pattern count in the film industry we deliberately over represented this in the instances we generated using the data generator we created 9 300 instances for each of those we had to also create the initial minimum waste solution and then using the pattern reduction algorithm with a time budget of 150 seconds the reduced solution the process took 800 hours on a dedicated 6 core computer processor intel core i 7 6800 k 4 30 ghz ram 16 gb os 64 bit windows 10 enterprise the dataset is available upon request from the authors one of the complications with this data is that we do not actually know the minimum number of patterns for an instance instead we have an upper bound obtained from our imperfect algorithm this becomes relevant when discussing the accuracy of a predictor we noticed that some of the worst absolute percentage errors difference between forecast and actual occur in the 3 of instances where the pattern reduction algorithm achieves no improvement within the time budget yet with some manual intervention using the convexity property it is possible to coax reductions in all such instances however we did not use these manually improved solutions in the comparison because we felt it would be more useful to predict the benefit the user might obtain with a time budget of 150 seconds rather than what is achievable in an ideal scenario this makes this analysis sensitive to changes to the pattern 7 jun 2019 reduction algorithm as and when algorithm improvements are made the training will have to be re done in presenting the solution to the ml algorithm one issue is how to represent solutions with a different initial number of patterns they range from 5 to 66 in our training data set this is analogous to image recognition with images of different size see 9 in our case we decided on a simple form 1 1 1 1 1 1 1 1 1 1 1 this matrix of size 1 2 represents each pattern in the solution as one of its rows containing the following information cp number of repetitions of pattern p np i number of repetitions of item i in pattern p wp i width of item i in pattern p parent reels in a pattern and patterns in the solution are ordered decreasingly by widths we use m 400 and k 12 as these dimensions are enough for all problems we have found in real scenarios smaller solutions are padded with zeros to get a matrix shaped as above so the first pattern in the solution to the right of figure 1 corresponding to the first row of the above matrix is represented as 16 1 1200 5820 4 970 5820 1 740 5820 0 0 we experimented with shrinking the values of m and k to smaller values since the vast majority of real world instances including all in the dataset we have generated have m 80 and k 6 although there was a small difference in the anyway small training time there was no statistically significant difference in the results we suspect this is because ml technology has become quite good at identifying empty information the solution representation removes some of the natural redundancy e g changing the sequence of items within a pattern in the process we found that different solution representations have a big impact on the learning speed 7 jun 2019 3 the na ve model the na ve model compares the initial and final pattern counts and fits a quadratic on 80 7 440 randomly chosen instances of the 9 300 instance collection we find it quite astonishing that this na ve model has an r 2 value of 86 8 figure 2 scatter diagram showing original vs final pattern count on the 7 440 instances in the training data set using the quadratic on the remaining 1 860 20 instances we get a mean absolute percentage error mape of 12 0 4 the ml model training for the training we used the same 80 of the data 7 440 instances randomly chosen that was used to fit the quadratic keeping 20 for validation we used the popular open source tensorflow ml framework with the following configuration layer input neurons output neurons activation 0 canonical solution 10 802 100 relu 1 100 100 relu 2 100 1 linear optimiser adam 10 with learning rate 0 001 loss function mean absolute error mae epochs 500 stopping criterion 25 epochs without improvement in the validation set 20 7 jun 2019 the model with the lowest mae for the validation set is the one saved and used for testing the training always ends before epoch 100 on the same computer as used for the data generation the training takes about 2 5 minutes figure 3 shows a training session example figure 3 training example showing how the mean absolute error evolves as part of the process of selecting the ml model we tried different optimisers the table on the right shows the different results we obtained with each optimiser in addition we tested different learning rates 0 0005 to 0 002 number of hidden layers 1 to 3 dropout layers and neurons per layer 32 64 128 and 256 the difference in the results from the chosen configuration was always a small increase of the mape 5 results discussion the ml model on the testing data set generates on average a mape of 8 7 which compares quite favourably with the average 12 0 of the na ve model the trained ml model produces an answer in a trivial time 1 ms per instance optimiser mape std dev adam 8 74 0 138 adamax 8 81 0 138 adagrad 8 97 0 131 nadam 9 05 0 118 rmsprop 9 13 0 555 adadelta 9 26 0 171 7 jun 2019 figure 4 error histogram for the predictions obtained from the ml and na ve models a negative prediction error means the predicted value was smaller than the actual figure 5 comparison of absolute errors between na ve and ml models the coefficient of determination of the ml model 91 7 shows that the prediction accuracy is better than the na ve approach 86 8 incidentally the ml model predicts 16 7 patterns when passed the solution to the left in figure 1 whereas the na ve model predicts 19 5 7 jun 2019 figure 6 scatter diagram showing final pattern count vs predicted by the ml model on the 1 860 instances of the testing set following this work there are many open questions one of them concerns the integrality of the answer both the na ve and the ml algorithm return a floating point number yet the number of patterns in a solution remains resolutely integer there are no clear arguments on whether rounding to the nearest or up down is better another question is whether we gained any insights into how the ml model was coming up with its predictions unfortunately we have no such insight at this point but there was a side benefit as mentioned earlier in 3 of instances our current algorithm failed to find an improvement in 150 s closer examination of these instances is leading to algorithmic improvements which will be the subject of a future paper in terms of future work one possible direction would be to move further up the chain namely to see how accurately can ml predict solution statistics waste pattern count from the input of the 1 d csp problem itself list of sizes master size s constraints 6 acknowledgments the authors would like to acknowledge constructive comments by ed rothberg and sophia drossopoulou on an earlier draft 7 references 1 c goulimis and a olivera kbp a new pattern reduction heuristic for the cutting stock problem in or 59 loughborough 2017 7 jun 2019 2 c mcdiarmid pattern minimisation in cutting stock problems discrete applied mathematics pp 121 130 1999 3 c n goulimis appeal to np completeness considered harmful does the fact that a problem is np complete tell us anything interfaces pp 584 586 2007 4 c n goulimis optimal solutions for the cutting stock problem european journal of operational research pp 197 208 1990 5 f vanderbeck exact algorithm for minimising the number of setups in the one dimensional cutting stock problem operations research pp 915 926 2000 6 g belov and g scheithauer the number of setups different patterns in one dimensional stock cutting department of mathematics dresden university of technology dresden 2003 7 y cui c zhong and y yao pattern set generation algorithm for the one dimensional cutting stock problem with setup cost european journal of operational research pp 540 546 2015 8 y bengio a lodi and a prouvost machine learning for combinatorial optimization a methodological tour d horizon corr 2018 9 k he x zhang s ren and j sun spatial pyramid pooling in deep convolutional networks for visual recognition corr 2014 10 d kingma and j ba adam a method for stochastic optimization in 3 rd international conference for learning representations san diego 2015 1 background related work 2 data generation scaling 3 the na ve model 4 the ml model training 5 results discussion 6 acknowledgments 7 references