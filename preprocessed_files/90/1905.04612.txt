continuous time systems for solving 0 1 integer linear programming feasibility problems a preprint chengrui li 1 wu yuzhang honors college sichuan university chengdu sichuan china 2 university of tennessee knoxville tn 37996 usa cnlichengrui foxmail com bruce j maclennan department of electrical engineering computer science university of tennessee knoxville tn 37996 usa maclennan utk edu november 11 2021 abstract the 0 1 integer linear programming feasibility problem is an important np complete problem this paper proposes a continuous time dynamical system for solving that problem without getting trapped in non solution local minima first the problem is transformed to an easier form in linear time then we propose an impulse algorithm to escape from local traps and show its performance is better than randomization for escaping traps second we present the time to solution distribution of the impulse algorithm and compare it with exhaustive search to see its advantages third we show that the fractional size of the basin of attraction of the global minimum is significantly larger than 2 n the corresponding discrete probability for exhaustive search finally we conduct a case study to show that the location of the basin is independent of different dimensions these findings reveal a better way to solve the 0 1 integer linear programming feasibility problem continuously and show that its cost could be less than discrete methods in average cases keywords 0 1 integer linear programming feasibility problem local trap impulse algorithm basin of attraction continuous time analog computation 1 introduction the 0 1 integer linear programming ilp feasibility problem is one of a relatively large class of np complete problems 1 unlike the optimization version of 0 1 ilp the feasibility version does not have an objective function that needs to be optimized and its only aim is to satisfy the given constraints although it might be easy to solve quickly in practical applications because the constraints of many real world problems have some special properties e g the coefficients are only 0 or 1 and 0 weights are far more frequent than 1 weights in general the complexity of the purely mathematical problem is 2 n in 1971 cook proved the first np complete problem boolean satisfiability sat 2 in the following year karp identified twenty one np complete problems 1 which have a kind of hierarchy that is all of these problems can be reduced to each other for instance the vertex cover problem can be reduced to the clique problem and the latter can be reduced to sat where sat is the most basic np complete problem up to now the time complexity of a reduction is polynomial 1 3 but this reduction process might give a larger sized problem also polynomially larger therefore sometimes it might be undesirable to reduce a problem to a more general np complete problem if we just want to adopt an exhaustive search method this is also why many investigators are interested in finding special methods for solving for example hamiltonian cycle problems directed or undirected even though they can be reduced to sat another reason is that perhaps more practical problems can be easily described as those higher level problems such as contact author ar x iv 1 90 5 04 61 2 v 1 cs d s 1 2 m ay 2 01 9 continuous time systems for solving 0 1 integer linear programming feasibility problems a preprint hamiltonian cycle rather than by reduction to sat the situation is similar for 0 1 ilp feasibility problem e g the facility location problem the 0 1 ilp feasibility problem is positioned at the second basic level of karp s twenty one np complete problems at the same level as the clique and 3 sat problems 1 further no other of these twenty one problems can be reduced to it but many other np complete problems have many reductions to them subsequent research has proved that many interesting problems are np complete such as minesweeper 4 on the windows operating system and some simplified domain restricted np complete problems 5 generally speaking the term 0 1 integer linear programming refers to the optimization version of the problem which is composed of an objective function max or min and a series of linear constraints compared with the feasibility version the constraints of the optimization version are relatively easier to satisfy thus a very common idea the penalty function method is widely used in many practical problems because of the easily satisfied constraints in most cases very large penalty terms are able to effectively optimize the objective function when finding the maximum or minimum in addition many optimization problems are not very strict in other words sometimes an application is able to accept a relatively good result even if it is not the best hence a trade off often happens between time cost and the extent of the optimization when it comes to the feasibility version however the constraints are more complex not a sparse matrix and the constraints have to be satisfied strictly the critical requirements of such problems increase the degree of difficulty because there are very few papers that use continuous time dynamical systems to solve the 0 1 ilp feasibility problem in a general sense this paper will introduce its continuous time method with a strategy to decrease the time complexity below exponential and to escape local traps the last few sections also analyze the advantages of this method and the possibility of reducing its time complexity 2 related work in recent years many discrete problems have been solved by continuous methods especially decision problems because continuous time dynamical systems operate similarly to our brain the state of neurons change continuously but their outputs are usually binary fire or not which depends on the real time difference between the present state and a threshold 6 most of continuous time complex systems can be treated as neural networks and particularly when the neurons are connected with feedback paths and the input is a time sequence this kind of network is called a recurrent neural network rnn rnns have been applied in many fields such as neuroscience 7 and chaotic physical system 8 9 when it comes to combinatorial problems continuous methods have been investigated but most of them are for solving incomplete versions of a specific problem under some special cases or for the optimization version of 0 1 ilp for example tagliarini et al reviewed the use of neural networks for optimization 10 if all of the coefficients are only 0 or 1 a hopfield neural network is able to solve the satisfiability problem continuously e g figure 1 a in fact following their construction we can also build a neural network for the general 0 1 ilp feasibility problem but the local traps non solution minima and the convergence conditions largely depend on the structure of the specific problem instance e g figure 1 b d impagliazzo et al solve the 0 1 ilp feasibility problem with exponential speedup over exhaustive search in the special case where the number of constraints is a multiple of the number of variables this is accomplished by a reduction to the vector domination problem 11 other research including 0 1 12 integer 13 mixed 14 16 programming have usually addressed the optimization version of the problem if one reduces the 0 1 ilp feasibility problem to sat then the new problem can be solved continuously without getting caught in local traps for example ercsey ravasz and toroczkai added exponentially growing auxiliary terms to each constraint so that the network is able to escape from traps and converge to the k sat solution possibly after a period of chaotic behavior 17 moln r and ercsey ravasz designed an asymmetric continuous time neural network to solve k sat problems 18 and this kind of network could also be used to solve the maxsat problem with good analog performance 19 these methods inspired us to design a similar continuous time dynamical system to solve the 0 1 ilp feasibility problem directly since the reduction to 3 sat would enlarge the size of the problem 3 general algorithm 3 1 problem transformation the 0 1 ilp feasibility problem can be described as given an integer matrix c and an integer vector d the goal is to find an unknown binary vector x that satisfies cx d 1 the number of rows m of c represents the number of 2 continuous time systems for solving 0 1 integer linear programming feasibility problems a preprint 0 20 40 60 80 100 t 0 0 0 2 0 4 0 6 0 8 1 0 x 1 2 3 a 0 2 4 6 8 10 12 t 0 0 0 2 0 4 0 6 0 8 1 0 x 1 2 3 4 5 6 7 8 9 10 b 0 2 4 6 8 10 t 0 0 0 2 0 4 0 6 0 8 1 0 x 1 2 3 4 5 c 0 2 4 6 8 10 t 0 5 10 15 20 en er gy k x d figure 1 examples of hopfield network a an example of solving a problem that has three variables one constraint and the coefficients are 0 or 1 the network always converges to the correct result b an example of solving a problem that has ten variables one constraint and the coefficients could be any non negative integer less than or equal to 3 the network sometimes converges to the result successfully c an example of solving a problem that has five variables one constraint and the coefficients could be any non negative integer less than or equal to 3 this time the network fails to converge to the result in spite of the state being randomized in the local trap at t 5 d the energy of the network in c constraints and the number of columns n of c represents the number of variables for example if there are three constraints and five variables 3 x 1 10 x 2 6 x 3 14 x 4 8 x 5 17 7 x 1 4 x 2 30 x 3 0 x 4 x 5 38 19 x 1 4 x 2 0 x 3 5 x 4 9 x 5 28 the problem can be written as c 3 10 6 14 8 7 4 30 0 1 19 4 0 5 9 d 17 38 28 t 1 whose solution could be x 1 0 1 0 1 t here the elements of c and d are all non negative to extend this method to c mm n z one can always substitute 1 yi for those xi whose corresponding coefficients are negative this transformation gives a new matrix and when applying the following method these transformed variables change in the opposite direction to its corresponding constraint during the continuous updating and iteration process more specifically substitute 1 ymi for xmi and keep in mind that ymi are used for computation and xi 1 yi represent the values of the original variables so that the xi across all constraints are identical with 3 continuous time systems for solving 0 1 integer linear programming feasibility problems a preprint each other for example consider the following problem 2 x 1 8 x 2 43 10 3 x 1 2 x 2 5 x 3 1 in this case we replace x 2 in the second constraint by y 22 1 x 2 and the problem is converted to 2 x 1 8 x 2 43 10 3 x 1 2 y 22 5 x 3 3 if the incremental update for x 2 is 0 3 in the first constraint and for y 22 is 0 2 in the second constraint then the final incremental update for x 2 should be the average of 0 3 and 0 2 the updated value of y 22 should be y 22 1 x 2 where x 2 is the updated value of x 2 this transformation costs only linear time o n but can decrease the logical complexity and increase the conciseness to a great extent especially in a high dimensional space the following experiments are all conducted with c mm n n and d for which solutions exist if m n and a solution exists then gaussian elimination is the optimal method to find it and its complexity is o n 3 on the other hand we are interested in the case n m m can be assumed as a constant and whether there is an algorithm with complexity less than that of exhaustive search o 2 n the exhaustive search method is very stable because it is not affected by the number of constraints m and the range of the coefficients hence exhaustive search o 2 n will be the comparison baseline in the following sections the satisfiability problem for linear equations cx d is polynomial over the reals rationals and integers but np complete over the natural numbers 20 in fact the feasibility problem for integer linear programming is equivalent to the problem of solving a system linear diophantine equations which can be solved in polynomial time by computing either the hermite or smith normal forms of c 20 however if the solutions are restricted to 0 1 n then the problem is np complete as karp showed 1 3 2 dynamical system model the first step for finding the solution is to build an energy function k x that attains its global minimum only at the solution for this purpose two criteria should be taken into consideration 1 the continuous solution to the equation and 2 the constraint that all the variables be binary thus two terms are included km x 1 2 dm n i 1 cixi n i 1 ci 2 1 2 n i 1 ci n i 1 ci xi 1 xi 2 k x 1 m m m 1 km 2 where m means the m th constraint ci in km is the coefficient of xi in the m th constraint i e cmi and dm is the m th element of d the energy function is scaled by n i 1 ci so that its magnitude is stable across different coefficient ranges the first term will be 0 for any continuous solution and the second term will be 0 for any binary vector x thus the energy function k x attains its global minimum 0 when x x gradient descent on the energy surface is then defined dxj dt k x xj 1 m m m 1 km x xj km x xj cj n i 1 ci d n i 1 cixi n i 1 ci xj 1 xj 2 xj 1 3 gradient descent is the basic iterative process to find the minimum equation 3 but k x is a 4 th order polynomial so there are two minima along each dimension therefore we cannot guarantee that only one minimum exists in the 0 1 hypercube 0 1 n given this challenge we need a mechanism embedded in the dynamics for detecting and escaping local traps non solution minima 4 continuous time systems for solving 0 1 integer linear programming feasibility problems a preprint 3 3 local trap detection mechanism one approach would be to follow the k out of n rules presented by tagliarini et al 10 to construct and to rewrite the energy function as follows km s dm n i 1 cig si 2 n i 1 c 2 i g si 1 g si d 2 m 1 2 n i 1 n j 1 j 6 i 2 cicjg si g sj n i 1 2 dm c 2 i g si 1 2 n i 1 n j 1 j 6 i 2 cicjg si g sj n i 1 2 dm c 2 i g si 4 where they eliminated the constant d 2 m to fit the form of the energy function for hopfield s continuous model 10 dsj dt sj i n i 1 tijg si ij 5 where xi g si 1 1 e si tij 2 cicj i 6 j 0 i j and ij 2 dm c 2 j we however retained the constant for detecting the global minimum because the global minimum of our k x equation 2 is 0 under all conditions that is we do not know where the global minimum is but we always know the global minimum is 0 and all other local traps are strictly positive values moreover the derivative of the energy function k x is a 3 rd degree polynomial define the detection function l k k k dk k dt 6 and according to l hospital s rule l k will approach 0 when encountering a local trap and will approach when it is the global minimum this quantitative determinant could also be used in other research fields when the problem is finding a global minimum of an energy function that is zero only at the global minimum in the following section 4 we will give a detailed iterative method and compare its performance with randomization as an escape mechanism 4 local trap escape algorithm 4 1 impulse algorithm if we regard the phase trajectory as a moving point wandering around the 0 1 hypercube the moving point sometimes will encounter local traps as mentioned before the detection function equation 6 can be used for automatically triggering an escape from a local trap by a method we call the impulse algorithm in section 4 2 we test it as a way of escaping local traps and compare it with randomization as an escape mechanism to see which performs better in finding solutions the idea of the impulse algorithm is to add another term to equation 3 so that the trajectory can escape out of a local trap when the detection function equation 6 exceeds a given threshold first the heaviside step function is applied to the detection function to trigger the impulse h l 1 l l 0 0 0 otherwise 7 where l 0 is a threshold and l is the detection function equation 6 then an impulse which is a direction vector i k with amplitude h is added to the gradient so equation 3 is amended to x t k x h i k 8 where k x is just the right hand side of equation 3 if the moving point encounters a local trap k x will be minute and h will fire the impulse i k to escape from the local trap 5 continuous time systems for solving 0 1 integer linear programming feasibility problems a preprint the method here is to make use of the current gradient of k x to indicate the escape direction to prevent a gradient explosion along certain dimensions a filter process and a scale factor are applied i k k f ins k 9 where f is the filter matrix defined as f 0 5 0 5 n 1 0 5 n 1 0 5 n 1 0 5 0 5 n 1 0 5 n 1 0 5 n 1 0 5 where n is the number of variables k is a scale factor so that 1 n i k 1 12 i e k 2 n k 1 and is an appropriate factor the minimum integer so that i k 1 the purpose of ins is to fix the sign of each element of a vector to make it point to the inside of the 0 1 hypercube ins k x sgn k sgn x 0 5 0 5 0 5 t k 10 where represents the hadamard component wise product operation for instance if x 0 1 0 9 t and k x 0 5 0 6 t then sgn k 1 1 t and sgn x 0 5 0 5 t 1 1 t so sgn k sgn x 0 5 0 5 t 1 1 t which is able to flip the direction of the second variable of k although the formula here looks complicated it is easier to implement it in the computer program in brief i k preserves the property of the gradient and scales the amplitude so that the average amplitude across all dimensions is 1 2 further the amplitude along each dimension is less than 1 and the overall direction is toward the interior of the hypercube this design is intended to make full use of the current local trap information to escape and to find the global minimum with greater probability for more details see section 4 2 to demonstrate this algorithm s effectiveness we conducted a series of comparisons on different problem sizes for the impulse algorithm and compared it to a randomization method that randomizes all variables when encountering a local trap the euler method with step length 1 was used for the gradient descent m 1 2 3 5 8 10 15 n 3 5 8 10 12 15 r 1 2 3 5 10 15 resulted in 7 6 6 252 conditions in total where r is the range of coefficients e g r 5 means all of the coefficients 0 ci 5 for each condition 200 trials were conducted and in each trial the maximum number of iterations was 1000 we used a threshold l 0 10 4 but the value is not critical 4 2 results running examples are shown in figure 2 when the trajectory has fallen into a local trap the impulse algorithm makes use of the current gradient information to fire an escape impulse directed to the interior of the hypercube in figure 2 a two pulses appear at about t 60 and t 230 whereas the randomization method generates a random vector with components uniformly distributed between 0 and 1 figure 3 compares the results in general the success rate decreases with increasing n and r and increases with increasing m except for m 1 since there may be more than one solution to a specific problem this phenomenon becomes rare when the density m n is relatively large and the coefficient range r is large in this comparison the impulse algorithm is obviously better than the randomization method in the n 5 conditions for instances under the conditions m 5 n 15 it is almost impossible for the randomization method to find the global minimum in 1000 steps but the impulse method succeeded in several trials out of 200 total trials to understand why the impulse algorithm performs better we may go back to the energy function k x equation 2 k is a concave up fourth order polynomial along each dimension there are two minima and one of them is the global minimum located exactly at 0 or 1 given that the trajectory is initiated within the 0 1 hypercube under normal circumstances it would only move around the hypercube as long as the step length is carefully controlled thus because of the effect of the first term on the second term of k the local minimum along some dimensions might be repelled far away from the hypercube so that the moving point will not drop into that local trap furthermore the energy surface along those dimensions are usually steep so its directional derivative in those dimensions approaches 0 quickly and this leads to the large escape occurring mainly along other dimensions because of the property of the iterative formula 8 of the impulse algorithm 5 time to solution distribution of the impulse algorithm since the impulse algorithm does not include any random process during the search process it is interesting to see the time to solution distribution for some particular sized problems hence we extend the maximum number of iterations 6 continuous time systems for solving 0 1 integer linear programming feasibility problems a preprint 0 50 100 150 200 250 t 0 0 0 2 0 4 0 6 0 8 1 0 x 1 2 3 4 5 a 0 50 100 150 200 250 t 0 00 0 01 0 02 0 03 0 04 0 05 0 06 0 07 en er gy k x b 0 50 100 150 200 250 300 t 0 0 0 2 0 4 0 6 0 8 1 0 x 1 2 3 4 5 c 0 50 100 150 200 250 300 t 0 00 0 01 0 02 0 03 0 04 en er gy k x d figure 2 running examples of impulse algorithm a b and randomization method c d m 3 n 5 r 10 a c represent the moving point b d represent the corresponding energy 3 5 8 10 12 15 n 1 2 3 5 8 10 15 m 1 0 1 0 1 0 0 98 0 96 0 94 1 0 1 0 1 0 0 94 0 95 0 7 1 0 1 0 0 98 0 9 0 76 0 57 1 0 0 98 0 9 0 8 0 56 0 21 1 0 0 98 0 95 0 78 0 44 0 17 1 0 1 0 0 96 0 88 0 61 0 16 1 0 1 0 1 0 0 98 0 78 0 3 3 5 8 10 12 15 n 1 2 3 5 8 10 15 m 0 98 1 0 0 97 0 95 0 9 0 83 0 98 0 91 0 81 0 66 0 44 0 36 0 99 0 92 0 68 0 46 0 23 0 12 1 0 0 98 0 74 0 44 0 18 0 02 1 0 0 98 0 89 0 6 0 37 0 08 1 0 1 0 0 96 0 7 0 4 0 12 1 0 1 0 1 0 0 86 0 62 0 24 3 5 8 10 12 15 n 1 2 3 5 8 10 15 m 1 0 0 98 0 9 0 88 0 8 0 62 0 96 0 86 0 64 0 4 0 32 0 19 0 96 0 87 0 53 0 24 0 1 0 04 1 0 0 94 0 66 0 32 0 15 0 02 1 0 0 98 0 85 0 62 0 3 0 08 1 0 0 99 0 92 0 59 0 35 0 12 1 0 1 0 0 99 0 78 0 54 0 2 3 5 8 10 12 15 n 1 2 3 5 8 10 15 m 0 93 0 96 0 8 0 68 0 54 0 4 0 96 0 78 0 4 0 26 0 12 0 08 0 95 0 86 0 42 0 14 0 06 0 08 1 0 0 96 0 66 0 32 0 11 0 02 1 0 1 0 0 82 0 48 0 26 0 08 1 0 0 99 0 9 0 62 0 28 0 1 1 0 1 0 0 94 0 69 0 48 0 16 3 5 8 10 12 15 n 1 2 3 5 8 10 15 m 0 94 0 85 0 56 0 54 0 51 0 46 0 98 0 78 0 36 0 24 0 22 0 14 0 98 0 83 0 36 0 18 0 1 0 06 1 0 0 97 0 65 0 25 0 07 0 04 1 0 0 98 0 75 0 53 0 22 0 05 1 0 1 0 0 84 0 57 0 23 0 06 1 0 1 0 0 92 0 76 0 45 0 15 3 5 8 10 12 15 n 1 2 3 5 8 10 15 m 0 94 0 8 0 64 0 51 0 48 0 48 0 97 0 79 0 4 0 28 0 19 0 19 0 96 0 87 0 46 0 18 0 06 0 05 0 98 0 94 0 55 0 28 0 12 0 02 1 0 0 98 0 75 0 4 0 2 0 04 1 0 1 0 0 84 0 56 0 28 0 06 1 0 1 0 0 9 0 68 0 37 0 16 a 3 5 8 10 12 15 n 1 2 3 5 8 10 15 m 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 97 0 88 1 0 1 0 1 0 0 94 0 82 0 59 1 0 1 0 0 93 0 65 0 42 0 13 1 0 1 0 0 92 0 6 0 22 0 02 1 0 1 0 0 94 0 6 0 21 0 02 1 0 1 0 0 96 0 74 0 26 0 03 3 5 8 10 12 15 n 1 2 3 5 8 10 15 m 1 0 1 0 1 0 1 0 0 99 0 94 1 0 1 0 0 88 0 69 0 58 0 34 1 0 0 98 0 68 0 42 0 2 0 1 1 0 1 0 0 6 0 2 0 1 0 0 1 0 1 0 0 7 0 24 0 06 0 0 1 0 1 0 0 74 0 19 0 06 0 0 1 0 1 0 0 84 0 26 0 09 0 02 3 5 8 10 12 15 n 1 2 3 5 8 10 15 m 1 0 1 0 0 96 0 94 0 91 0 8 1 0 0 98 0 64 0 47 0 25 0 16 1 0 0 98 0 49 0 16 0 09 0 04 1 0 1 0 0 52 0 14 0 02 0 0 1 0 1 0 0 57 0 18 0 04 0 0 1 0 1 0 0 62 0 18 0 04 0 0 1 0 1 0 0 68 0 18 0 06 0 0 3 5 8 10 12 15 n 1 2 3 5 8 10 15 m 1 0 0 99 0 86 0 76 0 58 0 54 1 0 0 94 0 41 0 16 0 12 0 08 1 0 0 96 0 36 0 12 0 05 0 03 1 0 1 0 0 39 0 16 0 02 0 0 1 0 1 0 0 46 0 11 0 02 0 0 1 0 1 0 0 48 0 14 0 02 0 0 1 0 1 0 0 5 0 1 0 01 0 0 3 5 8 10 12 15 n 1 2 3 5 8 10 15 m 1 0 0 92 0 64 0 64 0 64 0 5 1 0 0 9 0 39 0 2 0 16 0 1 1 0 0 98 0 3 0 08 0 07 0 02 1 0 0 98 0 35 0 1 0 02 0 0 1 0 1 0 0 37 0 08 0 0 0 0 1 0 1 0 0 46 0 1 0 02 0 0 1 0 1 0 0 44 0 1 0 02 0 0 3 5 8 10 12 15 n 1 2 3 5 8 10 15 m 1 0 0 86 0 7 0 64 0 52 0 45 1 0 0 88 0 3 0 22 0 14 0 12 1 0 0 96 0 26 0 14 0 02 0 04 1 0 0 99 0 38 0 1 0 02 0 0 1 0 0 99 0 36 0 1 0 02 0 0 1 0 1 0 0 41 0 11 0 04 0 0 1 0 1 0 0 42 0 17 0 02 0 0 b figure 3 success rates of escape algorithm a and randomization method b from left to right r 1 2 3 5 10 15 7 continuous time systems for solving 0 1 integer linear programming feasibility problems a preprint to see the time to solution distribution for m 3 5 n 5 8 10 r 10 each condition has 500 trials 0 250 500 750 1000 1250 1500 1750 2000 t 0 000 0 001 0 002 0 003 0 004 0 005 0 006 pr ob ab ili ty 0 1000 2000 3000 4000 5000 t 0 0000 0 0002 0 0004 0 0006 0 0008 0 0010 0 0012 0 0014 0 0016 pr ob ab ili ty 0 2000 4000 6000 8000 10000 t 0 0000 0 0002 0 0004 0 0006 0 0008 pr ob ab ili ty a 0 250 500 750 1000 1250 1500 1750 2000 t 0 000 0 001 0 002 0 003 0 004 0 005 0 006 0 007 pr ob ab ili ty 0 1000 2000 3000 4000 5000 t 0 00000 0 00025 0 00050 0 00075 0 00100 0 00125 0 00150 0 00175 0 00200 pr ob ab ili ty 0 2000 4000 6000 8000 10000 t 0 0000 0 0002 0 0004 0 0006 0 0008 0 0010 pr ob ab ili ty b figure 4 time to solution distribution for m 3 a and m 5 b left n 5 middle n 8 right n 10 100 bins for all conditions trials not finished in 2000 iterations for n 5 5000 iterations for n 8 and 10000 iterations for n 10 are not shown in this figure the time to solution distribution results figure 4 are consistent with the results in the previous section with increasing n the time to solution becomes longer the greater the density m n of the problem the less time it takes to find the solution an explicit comparison of these six conditions is shown in figure 5 the simpler the problem the higher the cumulative success rate and the sooner it tends to stabilize 0 250 500 750 1000 1250 1500 1750 2000 t 0 0 0 2 0 4 0 6 0 8 1 0 su cc es s ra te m 3 n 5 m 3 n 8 m 3 n 10 m 5 n 5 m 5 n 8 m 5 n 10 figure 5 comparison of cumulative success rates as a function of time t within 2000 iterations coefficient range r 10 it is worth mentioning that the time to solution distribution is not a uniform distribution at all these results indicate that most of the instances can be solved quickly by the impulse algorithm and only a few of them are difficult for example the median time of the condition m 3 n 5 is 138 5 and the average time of that condition is larger than 417 8 since there are 17 instances not yet solved by t 2000 these statistics mean that more than half of the instances can be solved below the average time when it comes to the exhaustive search method however its time to solution distribution is a uniform distribution therefore in average cases the impulse algorithm is more likely to find a solution in a short time compared with the exhaustive search method 8 continuous time systems for solving 0 1 integer linear programming feasibility problems a preprint 6 basin of attraction estimation 6 1 method as discussed in section 4 2 we speculate that the number of minima around the 0 1 hypercube is less than 2 n if we treat the randomized initial position of the moving point as casting the point on the energy surface we also hypothesize that the basin of attraction of the global minimum could be larger than 2 n the discrete probability in an exhaustive search the basin of attraction of a minimum is the high dimensional region in which all trajectories to descend to that minimum in case there could be more than one solution for some particular sized problems we conducted 100 trials for each condition of m 3 5 10 n 5 8 10 12 15 r 3 5 10 to see if the attracting area is larger than 2 n of course it is almost impossible to have multiple solutions in these conditions in each trial 100 points are randomly initialized within the 0 1 hypercube to calculate how many points are in the global minimum s basin of attraction the basin of attraction for a particular condition is estimated by averaging across the 100 trials in that condition to get an approximation of the basin of attraction of the global minimum 6 2 results the comparison is shown in table 6 2 it is obvious that all of the basins of attraction for these conditions are at least ten times larger than 2 n the discrete probability this means the basin is quite large in the 0 1 hypercube as discussed before this may happen because some local traps are far from the 0 1 hypercube or because the energy surface itself has a larger potential well for the global minimum table 1 comparison table between the global minimum s basin of attraction and the discrete probability 2 n each column represents the ratio of the basin area to 2 n for each m and n the three values represent the conditions r 3 5 10 respectively m n 5 8 10 12 15 3 14 5 12 7 10 1 26 4 16 3 14 1 38 2 25 9 20 5 79 1 27 9 34 8 268 7 258 9 203 2 5 18 1 15 0 13 7 25 3 20 2 11 9 24 6 13 5 15 9 24 2 14 8 32 8 19 7 59 0 6 6 10 24 5 20 7 16 6 41 3 26 4 17 5 35 3 26 4 15 3 18 8 28 7 16 4 16 4 16 4 3 3 when the coefficient range becomes larger from 0 3 to 0 10 the basin size shows a decreasing trend except for the conditions m 5 n 10 m 5 n 15 and n 12 this suggests that the narrower the coefficient range is the simpler the problem is and the simpler the energy function surface is therefore compared with the exhaustive search method the impulse algorithm has a greater advantage for the narrower coefficient range however with respect to m and n the ratio of the basin area does not have an obvious uniform trend besides it is interesting that for the condition of m 3 n 15 the ratios are all greater than 200 which are significantly greater than other conditions 7 locating the basin of attraction 7 1 method since the results in the previous section have proved the global minimum possesses a relatively larger basin of attraction we were curious about where these basins are and whether there is any relationship between its corresponding constraint matrix c and object vector d here we conduct a more detailed point casting experiment only for the condition m 3 n 10 r 10 it contains 200 trials 200 different problems and 5000 points are randomly cast into the 0 1 hypercube in each trial initial positions and their corresponding constraint matrix c and object vector d in each trial are recorded 7 2 results first the t test is applied to the value of each variable dimension of those initial positions within the global minimum s basin of attraction in each trial out of 200 trials the null hypothesis for the t test is that those values are not different from 0 5 in other words we want to check across all dimensions to see if the global minimum s basin is shifted to a corner of the 0 1 hypercube for example if the projection of the global minimum s basin to the dimension of x 1 is 9 continuous time systems for solving 0 1 integer linear programming feasibility problems a preprint near to 0 it means that the global minimum s basin is close to the 0 s corner of the 0 1 hypercube no matter what the values are in other dimensions see figure 6 0 0 0 2 0 4 0 6 0 8 1 0 x 1 0 0 0 2 0 4 0 6 0 8 1 0 x 3 0 0 0 2 0 4 0 6 0 8 1 0 x 5 0 0 0 2 0 4 0 6 0 8 1 0 x 7 figure 6 example of the basin of attraction of the global minimum trial 1 the left panel shows that the basin is close to the 0 s corner along the dimension of x 1 mean 0 32 sd 0 23 and the dimension of x 3 mean 0 32 sd 0 21 the right panel shows that the basin is close to the 1 s corner along the dimension of x 5 mean 0 87 sd 0 10 but does not have any deviation along the dimension of x 7 mean 0 44 sd 0 26 we count for each dimension across 200 trials to see in how many trials the global minimum s basin of attraction deviates from 0 5 in that dimension the result table 7 2 evinces that this kind of deviation is widespread this is a very important point because the deviation of the global minimum s basin implies that the energy surface determined by the particular c and d contains stable information for that dimension independent of the other dimensions hence one can just determine some of the variables one at a time and not need to go back to adjust them again when processing other dimensions this is totally different from the exhaustive search method since flipping one variable of the x would affect the value of cx and this would then impact the values of other dimensions therefore one has to adjust the values of other dimensions again to try to obtain the correct object vector d table 2 deviation of the basin of attraction of the global minimum each element in this table represents in how many trials out of 200 in total the global minimum s basin deviated from 0 5 in the corresponding dimension x 1 x 2 x 3 x 4 x 5 x 6 x 7 x 8 x 9 x 10 153 143 147 157 160 145 151 158 149 160 for testing the correlations between each pair of two dimensions we calculate the correlation matrix for those initial points within the global minimum s basin in each trial after that the number of pairs whose correlations are greater than 0 75 or less than 0 75 are counted in each trial the diagonal elements are excluded since all of them are 1 the result indicates that only 17 trials out of 200 in total contain related pairs two variables dimensions are correlated with each other this further suggests the independence of the global minimum s basin in different dimensions nevertheless it is quite hard to find a relationship between the augmented matrix c d and the location of the basin of attraction of the global minimum although the larger basin does exist at this time we are unable to determine where it is based on the problem itself the relationship might or might not exist but it needs to be studied further 8 discussion the primary objective of this paper is to find a better continuous time method to solve the 0 1 ilp feasibility problem without being trapped in a local well for this purpose we conducted four experiments sections 4 5 6 7 to analyze the solution procedure when this continuous time method is applied to a practical problem the procedure should be as follows a section 7 shows that the global minimum s basin of attraction is independent in some of the dimensions if a future study could find a relationship between the augmented matrix c d and the location of the basin of the global minimum one could just start by initializing a point for certain dimensions successively and perhaps not need to go back to adjust these basically well determined dimensions b section 6 shows that the global minimum attracting area is larger than 2 n the discrete probability corresponding to exhaustive search c section 4 suggests that our impulse algorithm is able to escape from the local minima to find the global minimum effectively and that its performance is 10 continuous time systems for solving 0 1 integer linear programming feasibility problems a preprint better than escape by randomization thus after randomly initializing a point adopt the impulse algorithm to find the global minimum d in section 5 the time to solution distribution of the impulse algorithm shows us that more than half of the solutions could be solved in a short time in other words in the average case the impulse algorithm is more likely to reach the solution compared with the exhaustive search method the peak of the time to solution distribution skews to the left hand side another point that must be observed here is that generally speaking the time complexity of the impulse algorithm is still exponential although ben hur proved the continuous time ode network has polynomial complexity 21 our impulse algorithm includes the escape process which makes the algorithm continue searching until it lands at the global minimum thus the complexity of this algorithm is still exponential 9 conclusion this paper proposed a continuous time dynamical system for solving the 0 1 integer linear programming feasibility problem first we transformed the problem to a better form and proposed the impulse algorithm to tackle the local minima then we discussed the time to solution distribution after that the basin of attraction of the global minimum and its location were investigated however if one wants to use this continuous time method to solve problems the relationship between the location of the basin of attraction and its corresponding problem still need to be found so this should be a direction of future work our results here are empirical and preliminary future work will investigate simplification and other improvements to the impulse algorithm relevant theorems continuous time complexity analysis and exploration of performance on a wider range of instances acknowledgement this project was completed during chengrui li s exchange period an international student exchange program between sichuan university and the university of tennessee knoxville references 1 richard m karp reducibility among combinatorial problems in complexity of computer computations pages 85 103 springer 1972 2 stephen a cook the complexity of theorem proving procedures in proceedings of the third annual acm symposium on theory of computing pages 151 158 acm 1971 3 richard e ladner on the structure of polynomial time reducibility journal of the acm jacm 22 1 155 171 1975 4 richard kaye minesweeper is np complete the mathematical intelligencer 22 2 9 15 2000 5 michael r garey david s johnson and larry stockmeyer some simplified np complete problems in proceedings of the sixth annual acm symposium on theory of computing pages 47 63 acm 1974 6 wojciech zaremba ilya sutskever and oriol vinyals recurrent neural network regularization technical report arxiv 1409 2329 2014 7 omri barak recurrent neural networks as versatile tools of neuroscience research current opinion in neurobiol ogy 46 1 6 2017 8 chengcheng huang and brent doiron once upon a slow time in the land of recurrent neuronal networks current opinion in neurobiology 46 31 38 2017 9 alex graves abdel rahman mohamed and geoffrey hinton speech recognition with deep recurrent neural networks in 2013 ieee international conference on acoustics speech and signal processing pages 6645 6649 ieee 2013 10 gene a tagliarini j fury christ and edward w page optimization using neural networks ieee transactions on computers 40 12 1347 1358 1991 11 russell impagliazzo shachar lovett ramamohan paturi and stefan schneider 0 1 integer linear programming with a linear number of constraints technical report arxiv 1401 5512 v 2 cs cc january 2014 12 marianna de santis and francesco rinaldi continuous reformulations for zero one programming problems journal of optimization theory and applications 153 1 75 84 2012 11 continuous time systems for solving 0 1 integer linear programming feasibility problems a preprint 13 krasimira genova and vassil guliashki linear integer programming methods and approaches a survey journal of cybernetics and information technologies 11 1 2011 14 christodoulos a floudas and xiaoxia lin mixed integer linear programming in process scheduling modeling algorithms and applications annals of operations research 139 1 131 162 2005 15 laura di giacomo giacomo patrizi and emanuele argento linear complementarity as a general solution method to combinatorial problems informs journal on computing 19 1 73 79 2007 16 pedro m castro and ignacio e grossmann new continuous time milp model for the short term scheduling of multistage batch plants industrial engineering chemistry research 44 24 9175 9190 2005 17 m ria ercsey ravasz and zolt n toroczkai optimization hardness as transient chaos in an analog approach to constraint satisfaction nature physics 7 12 966 2011 18 botond moln r and m ria ercsey ravasz asymmetric continuous time neural networks without local traps for solving constraint satisfaction problems plos one 8 9 e 73400 2013 19 botond moln r ferenc moln r melinda varga zolt n toroczkai and m ria ercsey ravasz a continuous time maxsat solver with high analog performance nature communications 9 1 4864 2018 20 alexander bockmayr and volker weispfenning solving numerical constraints in john alan robinson and andrei voronkov editors handbook of automated reasoning volume 1 chapter 12 elsevier and mit press amsterdam and cambridge 2001 21 asa ben hur hava t siegelmann and shmuel fishman a theory of complexity for continuous time systems journal of complexity 18 1 51 86 2002 12 1 introduction 2 related work 3 general algorithm 3 1 problem transformation 3 2 dynamical system model 3 3 local trap detection mechanism 4 local trap escape algorithm 4 1 impulse algorithm 4 2 results 5 time to solution distribution of the impulse algorithm 6 basin of attraction estimation 6 1 method 6 2 results 7 locating the basin of attraction 7 1 method 7 2 results 8 discussion 9 conclusion