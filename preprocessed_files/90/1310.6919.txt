arxiv 1310 6919 v 2 math oc 26 may 2014 ar x iv 1 31 0 69 19 v 2 m at h o c 2 6 m ay 2 01 4 fast implementation for semidefinite programs with positive matrix completion makoto yamashita 1 kazuhide nakata 2 november 1 2013 revised may 26 2014 abstract solving semidefinite programs sdp in a short time is the key to managing various mathe matical optimization problems the matrix completion primal dual interior point method mc pdipm extracts a sparse structure of input sdp by factorizing the variable matrices in this paper we propose a new factorization based on the inverse of the variable matrix to enhance the performance of mc pdipm we also use multithreaded parallel computing to deal with the major bottlenecks in mc pdipm numerical results show that the new factorization and multithreaded computing reduce the computation time for sdps that have structural sparsity keywords semidefinite programs interior point methods matrix completion multithreaded computing ams classification 90 operations research mathematical programming 90 c 22 semidefinite programming 90 c 51 interior point methods 97 n 80 mathematical software computer programs 1 introduction semidefinite programs sdp have become one of main topics of mathematical optimization be cause of its wide range application from combinatorial optimization 9 to quantum chemistry 7 21 and sensor network localization 4 a survey of its many applications can be found in todd s pa per 24 and the range is still expanding moreover there is no doubt that solving sdps in a short time is the key to managing such applications the primal dual interior point method pdipm 1 11 15 18 22 is often employed since it can solve sdps in a polynomial time and many solvers are based on it for example sdpa 26 csdp 5 sedumi 23 and sdpt 3 25 a recent paper 27 reports that integration with parallel computing enables one to solve large scale sdps arising in practical applications a major difficulty with pdipm is that the primal variable matrix x must be handled as a fully dense matrix even when all the input data matrices a 0 am are considerably sparse the standard form in this paper is the primal dual pair p min a 0 x subject to ak x bk k 1 m x o d max m k 1 bkzk subject to m k 1 akzk y a 0 y o let sn be the space of n n symmetric matrices the symbol x o x o indicates that x s n is a positive semidefinite definite matrix the notation u v is the inner product between 1 department of mathematical and computing sciences tokyo institute of technology 2 12 1 w 8 29 ookayama meguro ku tokyo 152 8552 japan makoto yamashita is titech ac jp the work of the first author was financially supported by the sasakawa scientific research grant from the japan science society 2 department of industrial engineering and management tokyo institute of technology 2 12 1 w 9 60 ookayama meguro ku tokyo 152 8552 japan nakata k ac m titech ac jp the work of the second author was partially supported by grant in aid for young scientists b 22710136 1 http arxiv org abs 1310 6919 v 2 u v sn defined by u v n i 1 n j 1 uijvij the input data are a 0 a 1 am s n and b 1 bm r the variable in the primal problem p is x s n while the variable in the dual problem d is y sn and z rm the sparsity of the input matrices directly affects the dual matrix y a 0 m k 1 akzk more precisely yij can be nonzero only when the aggregate sparsity pattern defined by a i j 1 i n 1 j n ak ij 6 0 for some k 0 m covers i j here ak ij is the i j th element of ak examples of aggregate sparsity patterns are illustrated in figures 3 and 4 these patterns arise from the sdps we solved in the numerical experiments on the other hand all the elements of x in the primal problem p must be stored in memory in order to check the constraints x o the matrix completion primal dual interior point method mc pdipm proposed in 8 19 enables the pdipm to be executed by factoring x into the form x lt 1 l t 2 l t 1 dl 1 l 2 l 1 1 where d is a diagonal block positive semidefinite matrix and l 1 l 2 l 1 are lower triangular matrices a remarkable feature of this factorization is that d and l 1 l 2 l 1 inherit the sparsity of a when a is considerably sparse these matrices are also sparse hence mc pdipm has considerable advantages compared with handling the fully dense matrix x it was first im plemented in a solver called sdpa c semidefinite programming algorithm with completion a variant of sdpa 26 and as reported in 8 19 it significantly reduces computation costs of solving sdps with structural sparsity the main objective of this paper is to accelerate mc pdipm the principal bottleneck is the repeated computation of the form xv for v rn the original factorization 1 can be summarized as x l tdl with a lower triangular matrix l l 1 l 2 l 1 instead of this factorization we introduce the cholesky factorization of the inverse of x x 1 l l t and show that the lower triangular matrix l directly inherits the sparsity from a another obstacle of 1 is that the presence of d means that it is not a standard form of cholesky factorization and this prevents us from using software packages that are available for sparse cholesky factorization such as cholmod 6 and mumps 2 however the removal of d by using x 1 l l t would enable us to naturally integrate these packages into mc pdipm framework in so doing we can obtain the results of xv in a more effective way and shrink the computation time of mc pdipm in this paper we also introduce multithreaded parallel computing to this new factorization most processors on modern pcs have multiple cores and we can process some tasks simultaneously on different cores a parallel computation of mc pdipm on multiple pcs connected by a local area network was already discussed in 20 in this paper we employ different parallel schemes for multithreading on a single pc because the differences between the memory accesses of parallel computing with the message passing interface mpi protocol on multiple pcs and those of multithreading on a single pc strongly affects the performance of parallel computing in addition to enhance the performance of multithreading we control the number of threads involved in our parallel schemes on the basis of the existing version sdpa c 6 2 1 we implemented a new version sdpa c 7 3 8 the version numbers reflect the versions of sdpa that sdpa c branches from we conducted numerical experiments showing that the new sdpa c 7 3 8 successfully reduces the computation time because of the effectiveness ofx 1 l l t we also show that the multithreaded computation further expands the difference in computation time between sdpa c 6 2 1 and 7 3 8 this paper is organized as follows in section 2 we introduce two preliminary concepts i e the positive matrix completion and pdipm section 3 is the main part of this paper that describes the new implementation in detail section 4 presents numerical results showing its performance in section 5 we summarize this paper and discuss future directions 2 throughout this paper we will use s to denote the number of elements of the set s for a matrix x and two sets s t 1 n we use the notation xst to denote the sub matrix of x that collects the elements of xij with i s and j t for example x 2 6 3 4 x 23 x 24 x 63 x 64 2 preliminaries here we briefly describe the basic concepts of positive matrix completion and pdipm for more details on the two and their relation please refer to 8 19 and references therein 2 1 positive matrix completion positive matrix completion is closely related to the cholesky factorization of the variable matrices x and y in the context of the pdipm framework when y a 0 m k 1 akzk is positive definite we can apply cholesky factorization to obtain a lower triangular matrix n such that y nnt however this factorization generates nonzero elements out of the aggregate sparsity pattern a and this phenomenon is called fill in although a is not enough to cover all the nonzeros in n it is known that we can prepare a set of appropriate subsets c 1 c 1 2 n so that the set e r 1 cr cr covers the nonzero positions of a and the fill in these subsets c 1 c 1 2 n are called cliques in relation with graph theory and they are obtained in three steps we permute the rows columns of y with an appropriate order like approximation minimum ordering and generate a chordal graph from a then we extract the maximal cliques there as c 1 c the set e is called the extended sparsity pattern throughout this paper we will assume that e is considerably sparse a and e are much less than the fully dense case n 2 for instance a e 10 2 n 2 for large n in addition we will assume for simplicity that c 1 c 1 2 n are sorted in an appropriate order which satisfies a nice property called the running intersection property in 8 such an order can be easily derived from the chordal graph grone et al 10 proved that if a given matrix x satisfies the positive definite conditions on all the sub matrices induced by the cliques c 1 c 2 c that is x satisfies xc 1 c 1 o xc 2 c 2 o xc c o then x can be completed to x such that xcrcr xcrcr for r 1 and the entire matrix x is positive definite furthermore it was shown in 8 that the explicit formula 2 below completes x to the max determinant completion x which satisfies det x max det x xcrcr xcrcr for r 1 x o the sparse factorization of x from x is given by x lt 1 l t 2 l t 1 dl 1 l 2 l 1 2 where l 1 l 2 l 1 are the triangular lower matrices of the form lr ij 1 i j x 1 urur xursr ij i ur j sr 0 otherwise 3 and d is the diagonal block matrix d ds 1 s 1 ds 2 s 2 ds s 4 3 with sr cr cr 1 cr 2 c r 1 2 ur cr cr 1 cr 2 c r 1 2 and dsrsr xsrsr xsrurx 1 urur xursr r 1 2 1 xs s r it can be shown that the triangular lower matrix l defined by l l 1 l 2 l 1 is usually fully dense thereby destroying the structural sparsity of e therefore when we compute w x v ltdlv for some vector v rm constructing a fully dense l is not efficient we should note that the inverse l 1 can maintain the sparsity of e that is l 1 ij 0 if i j e and l 1 is a lower triangular matrix 19 hence the two equations l 1 w 1 v and l tw dw 1 can be solved with forward backward substitutions by exploiting the structure of e and they can be used to compute w much faster in addition we do not need to compose a fully dense x via multiplication of ltdl this idea saves on the computation cost of pdipm as discussed in the next subsection 2 2 primal dual interior point method this subsection briefly describes the primal dual interior point method pdipm and the modifi cation of its computation formula by using the positive matrix completion method the basic framework of the pdipm can be summarized as follow basic framework of the primal dual interior point method step 0 choose an initial point x y z such that x o and y o choose parameters and from 0 1 and 0 1 step 1 if x y z satisfies a stopping criterion output x y z as a solution and terminate step 2 compute a search direction dx dy dz based on the modified newton method step 3 compute the maximum step length p and d such that p max 0 1 x dx o 5 d max 0 1 y dy o step 4 update x y z with x pdx y ddy z ddz go to step 1 the chief computation in the above framework is usually that of the search direction dx dy dz as pointed out in 28 if we employ the hkm direction 11 15 18 the search direction can be obtained with the following system bdz g 6 dy g m k 1 akdzk d x y 1 x xdy y 1 dx d x d x t 2 7 4 where bij xaiy 1 aj i 1 m j 1 m 8 gk ak y 1 x xgy 1 k 1 m with x y n g a 0 m k 1 akzk the linear system 6 is often called the schur complement equation sce and its coefficient matrix determined from 8 is called the schur complement matrix scm we first solve sce 6 to obtain dz and then compute dy and dx the matrix completion 1 enables us to replace the fully dense matrices x and y 1 with their sparse versions in the above computation from the properties of the inner product the change from x to x in formula 8 does not affect bij therefore its formula can be transformed into bij x aiy 1 aj m k 1 ltdlek t ai n t n 1 aj k 9 where ek and aj k are the kth columns of i and aj respectively we also modify the computation of the primal search direction dx by evaluating its auxiliary matrix d x in a column wise manner d x k y 1 ek x ek x dy y 1 ek n tn 1 ek l tdlek l tdldy n tn 1 ek 10 as pointed out in section 2 1 by solving the linear equations that involve the sparse matrices l 1 and n we can avoid the fully dense matrices x and y 1 in 9 and 10 the computation of the step length p in 5 can also be decomposed into the sub matrices p min r 1 2 max 0 1 xcrcr dxcrcr o 11 so that xcrcr pdxcrcr is positive definite for r 1 and we can complete these sub matrices to the positive definite matrix x the numerical results in 19 indicated that removal of the fully dense matrices x and y 1 makes the mc pdipm run more effectively than the standard pdipm i e a pdipm which does not use the positive matrix completion method for some types of sdp that have the structural sparsity in e 3 fast implementation of the matrix completion primal dual interior point method mc pdipm was first implemented in the solver sdpa c 5 19 along with the update of sdpa based on the standard pdipm to version 6 sdpa c was also updated to sdpa c 6 sdpa c 6 utilizes the blas basic linear algebra subprograms library 16 to accelerate the linear algebra computation involved in mc pdipm the new sdpa c version 7 3 8 described in this paper further reduces the computation time from that of version 6 2 1 in this section we describe the new features of sdpa c 7 3 8 the improvements in the factorization of x are in section 3 1 and the multithreaded parallel computing for scm b and the primal auxiliary direction d x are in section 3 2 in what follows we will abbreviate sdpa c 6 2 1 and sdpa c 7 3 8 to sdpa c 6 and sdpa c 7 respectively 5 3 1 new factorization of the completed matrix the factorization of x into x ltdl is not a standard cholesky factorization due to the diagonal block matrix d hence we could not employ software packages for the sparse cholesky factorization the completed matrix x is usually fully dense while the sparsity of its inverse x 1 inherits the structure of e i e x 1 ij 0 for i j e therefore we will focus on x 1 rather than x and introduce a new factorization of the form x 1 l l t with the lower triangular matrix l we want to emphasize here that l also inherits the structure of e in this subsection we show that we can obtain the factorized matrix l from x in an efficient way by using the structure of sr and cr r 1 algorithm 1 is used to obtain l the input is x and since x is going to be completed to a positive definite matrix x we suppose that xcrcr o r 1 the validity of the algorithm will be discussed later algorithm 1 efficient algorithm to obtain the cholesky factorization of the inverse of the completed matrix step 1 initialize the memory space for l by e r 1 cr cr step 2 for r 1 apply cholesky factorization to x 1 crcr to obtain the lower triangular matrix lr that satisfies x 1 crcr lrl t r we take the following steps to avoid computing x 1 crcr step 2 1 let p r be the permutation matrix of dimension cr cr with p r ij 1 if i j cr 1 p r ij 0 otherwise so that p rxcrcrp t r has the inverse row column order of xcrcr step 2 2 apply cholesky factorization to p rxcrcrp t r to obtain a lower triangular matrix m r that satisfies p rxcrcrp t r m rm t r step 2 3 let lr be p rm t r p t r step 3 for r 1 put the first sr columns of lr in the memory space of l crsr algorithm 1 requires neither a fully dense x nor its inverse x 1 in addition since most of the computation is devoted to the cholesky factorization of p rxcrcrp t r we can expect there will be a considerable reduction in computation time when the extended sparsity pattern e is decomposed into small c 1 c 2 c furthermore algorithm 1 assures that all nonzero elements of l appear only in e validity of algorithm 1 we will prove the validity of algorithm 1 on the basis of lemma 2 6 of 8 for simplicity we will focus on the first clique c 1 and wrap up the other cliques into c 2 r 2 cr because of the running intersection property the other cliques c 2 cr can be handled in the same way by induction on the number of cliques thanks to this property as well we can suppose that i j for i c 1 and j c 2 c 1 for x s n we decompose 1 2 n into three sets s c 1 c 2 u c 1 c 2 and t c 2 c 1 note that the extended sparsity pattern e of x is covered by s u s u u t u t hence the situation is one where x is of the form x xss xsu xus xuu xut xtu xtt 6 with unknown elements in the position s t t s and the sub matrices induced by the cliques c 1 c 2 are positive definite xc 1 c 1 xss xsu xus xuu o xc 2 c 2 xuu xut xtu xtt o note that c 1 s u and c 2 u t in this situation lemma 2 6 of 8 claims that x can be completed to the max determinant positive definite matrix x x xss xsu xsux 1 uuxut xus xuu xut xtux 1 uuxus xtu xtt o hence proving the validity of algorithm 1 reduces to proving that x l t l 1 in step 2 the inverses of the positive definite sub matrices are factorized into the lower triangular matrices by using cholesky factorization as follow xss xsu xus xuu 1 mss mus muu mtss m t us mtuu xuu xut xtu xtt 1 nuu ntu ntt ntuu n t tu nttt since the matrices on the left hand side are positive definite we can take the inverses of components on the right hand side e g m 1 ss by comparing the elements of both sides we obtain xss m t ss m 1 ss m t ss m t usm t uum 1 uumusm 1 ss xsu m t ss m t usm t uum 1 uu xuu m t uum 1 uu n t uun 1 uu n t uun t tun t ttn 1 ttntun 1 uu xut n t uun t tun t ttn 1 tt xtt n t ttn 1 tt 12 in step 3 the elements of the above factorized matrices are located in l as follows l mss mus nuu ntu ntt 13 and since l is lower triangular its inverse can be explicitly obtained as l 1 m 1 ss n 1 uumusm 1 ss n 1 uu n 1 ttntun 1 uumusm 1 ss n 1 ttntun 1 uu n 1 tt the product l t l 1 and 12 lead to x l t l 1 this completes the proof of validity of algorithm 1 as a result of this new factorization the evaluation formula of scm 9 and the primal auxiliary matrix 10 can be replaced with efficient ones i e bij m k 1 l t l 1 ek tai n tn 1 aj k 14 d x k n tn 1 ek l t l 1 ek l t l 1 dy n tn 1 ek 15 7 note that once we have the factorization x 1 l l t we can use the sdpa c 6 routine to compute x ldlt by 2 and get l l td 1 2 where the matrix d 1 2 is such that d d 1 2 d 1 2 this computation however requires one more step to obtain d 1 2 as well as memory for l t d and d 1 2 hence direct acquisition of l by algorithm 1 is more efficient than using the formula l l td 1 2 to implement this new factorization based on algorithm 1 in sdpa c 7 five types of the computation related to cholesky factorization should be employed i dense cholesky factorization for the scm b and its forward backward substitution for sce 6 if b is fully dense ii sparse cholesky factorization for the scm b and its forward backward substitution for sce 6 if b is considerable sparse iii dense cholesky factorization for the sub matrices xc 1 c 1 xc c in step 2 of algorithm 1 iv forward backward substitution of l to solve the linear systems of the form l l t w v v sparse cholesky factorization for the dual variable matrix y nnt and forward backward substitution of n to solve the linear systems of form nntw v for types i and ii the sparsity of the scm b heavily depends on the types of application that generates the input sdp as pointed out in 27 for example sdps arising from quantum chemistry 7 21 have fully dense scms in contrast the density of the scms of sdps arising from sensor network localization problems 14 is often less than 1 hence we should appro priately choose software packages for either fully dense or sparse scms we employed the dense cholesky factorization routine of lapack 3 for i and the sparse cholesky factorization routine of mumps 2 for ii one of these routines can be selected according to the criteria proposed in 27 that uses information from the input sdp the lapack routine is also applied to type iii for types iv and v we chose cholmod 6 rather than mumps 2 since we must access the internal data structure of the software package in order to locate l r in the appropriate space of l in step 3 of algorithm 1 cholmod internally uses a super nodal cholesky factorization and we noticed that the row sets and the column sets of super nodes computed in cholmod have the running intersection property hence the row sets and columns sets can be used as c 1 c 2 c and s 1 s 2 s respectively cholmod determines the size of super nodes by using heuristics like approximate minimum ordering so that the blas library can be used to process the sparse cholesky factorization and forward backward substitution in addition the structure of the memory allocated to primal l is identical to that of dual n in mc pdipm hence we first obtain the structure of n by constructing the aggregate sparsity pattern a and applying cholmod to obtain its symbolic sparse cholesky factorization then we extract its super node information c 1 c 2 c and s 1 s 2 s to prepare the memory for l table 1 shows the computation time reductions had by the new factorization of x the computation times of sdpa c 6 and sdpa c 7 were measured on a relaxation of a max clique problem on a lattice graph with the parameters p 300 and q 10 the details of this sdp and the computation environment will be described in section 4 the dimension of the variable matrices x and y was n p q 3000 and the extended sparsity pattern e was decomposed into 438 cliques c 1 c 438 since the maximum cardinality of the cliques max cr r 1 438 was only 59 the matrices were decomposed into small cliques we named three representative bottlenecks as follows s element is the time taken by 6 9 or 14 to evaluate the scm elements s cholesky is the cholesky factorization routine for the scm and p matrix is the computation 8 of the primal auxiliary matrix d x by 7 10 or 15 in addition sub s elements and sub p matrix are the times of the forward backward substitutions like ltdlek or l t l 1 ek in s elements and p cholesky table 1 reduction in computation time due to the new factorization time in seconds sdpa c 6 2 1 sdpa c 7 3 8 s elements 4837 64 2054 98 sub s elements 3798 64 1512 22 s cholesky 167 65 161 23 p matrix 346 46 242 06 sub p matrix 322 54 224 16 other 10 02 21 20 total 5361 77 2479 49 table 1 indicates that the new factorization reduced the evaluation time of scm 4837 seconds to half 2054 seconds the computation time on p matrix also shrank from 346 seconds to 242 seconds consequently the new factorization yielded a speedup of 2 16 times from the time reduction in sub s elements and sub p matrix we can see that the removal of d enabled us to utilize the efficient forward backward substitution of cholmod as we will show in section 4 the effect is even more pronounced for larger sdps 3 2 matrix completion primal dual interior point method for multithreaded parallel computing to enhance the performance of sdpa c 7 even further we take advantage of multithreaded parallel computing since the processors on modern pcs have multiple cores computation unit we can assign different threads computation tasks to the cores and run multiple tasks simultaneously for example multithreaded blas libraries are often used to reduce the time related to linear algebra computations in a variety of numerical optimization problems however the effect of multithreaded blas libraries is limited to dense linear algebra computations hence we should seek a way to apply multithreaded parallel computing to not only the dense linear algebra but also larger computation blocks of mc pdipm here we use multithreaded parallel computing to resolve the three bottlenecks of mc pdipm s elements s cholesky and p matrix a parallel computation of these bottlenecks was already performed in sdpara c 20 with the mpi message passing interface protocol on multiple pcs to apply multithreaded parallel computing on a single pc however we need a different parallel scheme for s elements the scm is evaluated with the formula 14 and we can reuse the results of l t l 1 ek and n tn 1 aj k for all the elements in b j the j th column of b on the other hand we can not reuse the results for different columns of b since the memory needed to hold l t l 1 ek for all k 1 n is equivalent to holding the fully dense matrix x and this means we lose the nice sparse structure of e since the computation of each column b j is independent from those of the other columns b i 1 i m i 6 j sdpara c simply employs a column wise distribution in which the pth thread evaluates the columns assigned by the set sp j 1 j m j 1 mod u p 1 where a mod b stands for the remainder of the division of a by b and u denotes the number of cores that participate in the multithreaded computation 9 this simple assignment was necessary for sdpara c since the memory taken up by b was assumed to be distributed over multiple pcs and we had to fix the column assignments in order not to send the evaluation result on one pc to another pc which would have entailed a lot of network communications in contrast on a single pc all of the threads of can share the memory space hence we devised more efficient parallel schemes to improve the load balance over all the threads algorithm 2 multithreaded parallel computing for the evaluation of scm step 1 initialize the scm b o set s 1 2 m step 2 generate u threads step 3 for p 1 2 u run the pth thread on the pth core to execute the following steps step 3 1 if s terminate step 3 2 take the smallest element j from s and update s s j step 3 3 evaluate b j by for k 1 n apply the forward backward substitution routine of cholmod to obtain v 1 l t l 1 ek and v 2 n tn 1 aj k for i 1 m update bij bij v t 1 aiv 2 figure 1 shows an example of thread assignment to scm b where b s 8 and u 4 note that we evaluated only the lower triangular part of b since b is symmetric we had u 4 threads thus the pth thread evaluated the pth column for p 1 2 3 and 4 at the beginning let us assume the computation cost is the same over all bij in figure 1 among the four threads the 4 th thread finishes its column evaluation in the shortest time so its evaluates the 5 th column after that the 3 rd thread finishes its first task and moves to the 6 th column on the other hand when the 4 th column requires a longer computation time than the 3 rd column does the 3 rd thread takes the 5 th column and the 4 th thread evaluates the 6 th column thread 1 thread 2 thread 3 thread 4 thread 4 thread 3 thread 2 thread 1 b figure 1 thread assignment to the schur complement matrix b the overhead in algorithm 2 is only in step 3 2 where only one thread should enter step 3 2 at a time hence we expected that algorithm 2 would balance the load better than the 10 simple column wise distribution employed in sdpara c in particular the main computation cost of each column b j is l t l ek and n tn 1 aj k therefore it is almost proportional to the number of nonzero columns of aj when only a few of a 1 am have too many nonzero columns and the others have only a few a simple column wise distribution has a hard time keeping the load balanced on the other hand algorithm 2 can naturally overcome this difficulty when we implement algorithm 2 we should pay attention to the number of threads gener ated by the blas library that cholmod internally calls for the forward backward substitution l t l ek and n tn 1 aj k for example let us suppose that four cores are available u 4 if we generate four threads in step 2 and each thread internally generates four threads for the blas library then we need to manage 16 threads in total on the four cores the overhead for this management is considerable and when we implemented the multithreaded parallel computing in this way sdpa c took at least ten times longer than single thread computing therefore we decided to turn off the multithreading of the blas library before entering the forward backward substitution routine and turn it on again after the routine finishes now let us examine s cholesky and p matrix for s cholesky our preliminary ex periments indicated that the usage of the blas library for both lapack and mumps is sufficient for delivering the performance of multithreaded parallel computing in p matrix the primal auxiliary matrix d x was evaluated by using formula 15 since this formula naturally indicates the independence of d x columns the simple column wise distribution was employed in sdpara c however in multithreading all of the threads share the memory hence we can replace the column wise distribution with the first come first served concept the same parallel concept as used in algorithm 2 we also used the above scheme to control the number of threads involved in parallel computing table 2 shows the computation time reduction due to multithreaded parallel computing it compares the results of sdpa c 7 with those of sdpa c 6 and sdpara c 1 on the same sdp in each bottleneck the upper row is the computation time and the lower row is the speed up ratio compared with a single thread table 2 reduction in computation time due to the multithreaded computing time in seconds sdpa c 6 sdpara c 1 sdpa c 7 threads 1 1 2 4 1 2 4 s elements 4873 64 3103 51 1904 64 1309 88 2054 98 1170 37 731 98 1 00 x 1 62 x 2 36 x 1 00 x 1 76 x 2 81 x s cholesky 167 65 312 41 110 76 65 12 161 23 85 24 47 77 1 00 x 2 82 x 4 79 x 1 00 x 1 89 x 3 38 x p matrix 346 46 316 04 159 32 81 34 242 06 131 03 83 54 1 00 x 1 98 x 3 88 x 1 00 x 1 85 x 2 90 x other 10 02 32 80 44 76 40 69 21 06 23 88 25 86 total 5361 77 3764 84 2219 48 1497 03 2479 49 1410 52 889 15 1 00 x 1 63 x 2 51 x 1 00 x 1 76 x 2 79 x the table shows that sdpa c 7 with four threads reduced s elements to 731 98 seconds from 2054 98 seconds on a single thread a speedup of 2 81 times the computation time of p matrix also shrank from 242 06 seconds to 83 54 seconds a speed up of 2 90 times these time reductions reduced the speed up in the total time from 2479 49 seconds to 889 15 for a speedup of 2 79 times sdpa c 7 with 4 threads was 6 03 times faster than sdpa c 6 using only a single thread the 11 result indicate the parallel schemes discussed above are effectively integrated into mc pdipm and the new factorization x 1 l l t the table also shows that sdpara c 1 took longer than sdpa c 7 this was mainly due to the overhead of the mpi protocol since the mpi protocol is designed for multiple pcs it is not appropriate for a single pc a multithreaded computation performs better in addition algorithm 2 works more effectively in a multithreaded computing environment than the simple column wise distribution of sdpara c the speedup of sdpa c 7 for s elements on four threads was 2 81 and it was higher than that of sdpara c 1 2 36 4 numerical experiments we conducted a numerical evaluation of the performance of sdpa c 7 the computing environ ment was redhat linux run on a xeon x 5365 3 0 ghz 4 cores and 48 gb of memory space we used three groups of test problems i e max clique problems over lattice graphs max cut problems over lattice graphs and spin glass problems in this section we will use the notation e to denote a vector of all ones and ei to denote a vector of all zeros except 1 at the ith element max clique problems over lattice graphs consider a graph g v e with the vertex set v 1 n and the edge set e v v a vertex subset s v is called a clique if i j e for i s j s the max clique problem is to find a clique having the maximum cardinality among all the cliques though the max clique problem itself is np hard lova sz 17 proposed an sdp relaxation method to obtain a good approximation in polynomial time the sdp problem below gives a good upper bound of the max clique cardinality for g v e max eet x subject to i x 1 eie t j eje t i x 0 for i j e x o for the numerical experiments we generated sdps of this type over lattice graphs a lattice graph g v e is determined by two parameters p and q with the vertex set being v 1 2 p q and the edge set e i j 1 p i 1 j 1 p i 1 p 1 j 1 q i j 1 p i j p i 1 p j 1 q 1 an example of lattice graphs is shown in figure 2 where the parameters are p 4 q 3 1 6 5 9 10 117 8 12 2 3 4 figure 2 lattice graph with the parameters p 4 q 3 we applied the pre processing technique proposed in 8 to convert the above sdp into an equivalent but sparser sdp figure 3 shows the aggregate sparsity pattern a for the max clique 12 sdp with p 300 q 10 we applied approximate minimum degree heuristics to a to make figure 3 and this figure shows the sparse structure embedded in this sdp the sizes of cliques c 1 c can be much smaller in comparison with n p q 300 10 3000 and this a does not incur any fill in that is e a this sdp was the example solved in section 3 0 500 1000 1500 2000 2500 3000 0 500 1000 1500 2000 2500 3000 nz 31174 figure 3 aggregate sparsity pattern a for the max clique sdp with p 300 q 10 max cut problems over lattice graphs the sdp relaxation method for solving max cut problems due to goemans and williamson 9 is well known and it marked the beginning of studies on sdp relaxation methods here one considers a graph g v e with the vertex set v 1 n and the edge set e v v each edge i j e has the corresponding non negative weight wij for simplicity wij 0 if i j e the weight of the cut c v is the total weight of the edges traversed between c and v c the max cut problem is to find a subset c which maximizes the cut weight max i c j v c wij subject to c v an sdp relaxation of this problem is given by min a 0 x subject to eie t i x 1 for i 1 n x o where a 0 is defined by a 0 diag we w w is the matrix whose i j element is wij for i 1 n j 1 n and diag w is the diagonal matrix whose diagonal elements are the elements of w when we generate sdp problems from the max cut problem over the lattice graphs its ag gregate sparsity pattern appears in the coefficient matrix of the objective function a 0 hence we can find a similar structure to the one shown in figure 3 in its aggregate sparsity pattern spin glass problems the four sdps of this type were collected as a torus set in the 7 th dimacs benchmark problems 12 these sdps arise in computations of the ground state energy of ising spin glasses in quantum chemistry more information on this energy computation can be found at the spin glass server 13 webpage and references therein 13 the ising spin glass model has a parameter p the number of samples and if we generate an sdp from a 3 d spin glass model the dimension of the variable matrices x and y is n p 3 13 figure 4 illustrates the aggregate sparsity pattern a of the spin glass sdp with p 23 and n 233 12167 figure 4 the aggregate sparsity pattern a of the spin glass sdp with p 23 table 3 summarizes the sdps of the numerical experiments the first column is sdp s name and the second p is the parameter used to generate it we fixed the parameter q to 10 for the max clique problems and the max cut problems the third column n is the dimension of the variable matrices x and y and the fourth column is the density of aggregate sparsity pattern defined by a n 2 the fifth column is the number of cliques c 1 c and the sixth and seventh columns are the average and maximum sizes of the cliques defined by r 1 cr and maxr 1 cr respectively the eighth column m is the number of input data matrices a 1 am table 3 the sizes of the sdps in the numerical experiments name p n density ave size max size m maxclique 300 300 3000 0 38 348 28 36 51 5691 maxclique 400 400 4000 0 28 439 29 89 59 7591 maxclique 500 500 5000 0 23 581 28 26 50 9491 maxcut 400 400 4000 0 15 1282 8 05 26 4000 maxcut 500 500 5000 0 12 1607 8 04 26 5000 maxcut 600 600 6000 0 097 1932 8 04 26 6000 maxcut 800 800 8000 0 072 2582 8 03 26 8000 maxcut 1000 1000 10000 0 058 3232 8 02 26 10000 maxcut 1200 1200 12000 0 048 3882 8 02 26 12000 spinglass 10 10 1000 0 80 155 25 69 294 1000 spinglass 15 15 3375 0 24 191 29 97 773 3375 spinglass 18 18 5832 0 13 1118 28 13 913 5832 spinglass 20 20 8000 0 10 1737 25 75 1080 8000 spinglass 23 23 12167 0 066 2556 27 30 1488 12167 spinglass 25 25 15625 0 051 3173 29 50 1798 15625 14 we compared the computation times of sdpa c 6 20 sdpa c 7 and sdpa 7 26 and sedumi 1 3 23 the former two implemented mc pdipm while the latter two implemented the standard pdipm here we did not conduct a numerical experiment on sdpara c since we found that the overhead due to the mpi protocol was a severe disadvantage when we ran it on a single pc as shown in section 3 2 table 4 lists the computation times of the four solvers using their default parameters we used four threads for sdpa c 7 and sdpa 7 the symbol 2 days in the table indicates that we gave up on the sedumi execution since it required at least two days table 4 computation times of four solvers for the sdps in table 3 time in seconds name sdpa c 6 sdpa c 7 sdpa 7 sedumi 1 3 maxclique 300 4792 28 889 15 11680 12 10260 15 maxclique 400 12681 16 1903 13 26159 40 24824 05 maxclique 500 19973 98 3733 41 38265 02 46168 56 maxcut 400 386 80 539 22 3686 78 16779 68 maxcut 500 683 27 876 81 6548 26 32557 13 maxcut 600 1194 89 1295 01 11098 35 60444 33 maxcut 800 2518 80 2371 43 25377 62 146235 81 maxcut 1000 4301 11 4032 80 47270 45 2 days maxcut 1200 7400 28 6030 00 75888 85 2 days spinglass 10 50 10 20 77 11 85 228 71 spinglass 15 1306 00 560 40 336 40 13789 58 spinglass 18 6734 40 2136 88 1522 60 68570 89 spinglass 20 15450 13 4552 10 3726 03 2 days spinglass 23 55942 57 13184 25 12598 14 2 days spinglass 25 107502 48 24913 20 26023 67 2 days on the max clique problems the mc pdipm solvers were faster than the standard pdipm solvers since the matrix completion method benefited from the nice properties of lattice graphs even sdpa c 6 was twice as faster as sdpa 7 the detailed breakdown of the time on max clique 400 is displayed in table 5 since sedumi does not print out its internal computation time we did not list its breakdown as shown in the sdpa 7 column the standard pdipm took a long time on p matrix 7 and other mainly the computation of the step length by 5 though these parts required an o n 3 computation cost the mc pdipm decomposed the full matrix x into the sub matrices xcrcr r 1 hence it was able to reduce the computation cost of these two parts furthermore sdpa c 7 resolved the heaviest parts of sdpa c 6 by using the new factorization x 1 l l t and multithreaded parallel computing consequently sdpa c 7 was the fastest among the four solvers in particular it was 12 36 times faster than sedumi on maxclique 500 for the max cut problems though mc pdipm was again superior to the standard pdipm sdpa c 7 was less effective than sdpa c 6 in particular it took longer on s elements and p matrix both of which utilized multithreaded computing it needed an overhead to generate the threads and the input matrices of the max cut problems were too simple to derive any benefit from multithreaded computing indeed each input matrix ai eie t i has only one nonzero element and this is reflected in the short computation time of sdpa 7 s s elements in the standard pdipm the s elements computation is an inexpensive task of 8 with ai eie t i and aj eje t j since the fully dense matrices x and y 1 are obtained with extensive memory 15 table 5 computation times on maxclique 400 time in seconds sdpa c 6 sdpa c 7 sdpa 7 sedumi 1 3 s elements 9314 34 1431 69 262 04 s cholesky 2601 04 248 55 403 28 p matrix 734 41 175 24 13151 10 other 31 37 47 65 12342 98 total 12681 16 1903 13 26159 40 24824 05 space and a heavy computation through p matrix and the inverse of the fully dense matrix for the large max cut problems however sdpa c 7 solved the sdps faster than sdpa c 6 or sdpa 7 as shown in the max 1200 result of table 6 sdpa c 7 still incurred a multithreading overhead on s elements and p matrix but the multithreaded blas library resolved the principal bottleneck s cholesky we can say that sdpa c 7 would work even better on larger sdps of this type table 6 computation times on maxcut 500 and maxcut 1200 time in seconds maxcut 500 sdpa c 6 sdpa c 7 sdpa 7 sedumi 1 3 s elements 105 25 317 56 16 17 s cholesky 502 43 226 06 214 43 p matrix 61 21 315 73 3254 05 other 14 38 17 46 3063 60 total 683 27 876 81 6548 26 32557 13 maxcut 1200 sdpa c 6 sdpa c 7 sdpa 7 sedumi 1 3 s elements 1265 02 1800 27 107 17 s cholesky 5562 48 2318 17 2674 50 p matrix 532 21 1837 53 39490 81 other 40 57 74 03 33616 37 total 7400 28 6030 00 75888 85 2 days sdpa 7 was the fastest in solving the spin glass sdps with p 10 since its standard pdipm is more effective on smaller sdps where the variable matrices are small and do not need to be decomposed when we increased p however the time difference between sdpa c 7 and sdpa 7 shrank and sdpa c 7 became faster than sdpa 7 at p 25 the reason why the growth in the computation time of sdpa c 7 was not as steep in comparison with sdpa 7 is that the average size of cliques does not grow with p as shown in table 3 in particular as seen in the breakdown of the computation times in table 7 this affects p matrix and its computation time in mc pdipm grows more gradually than in the standard pdipm spinglass 25 was the largest among the spin glass sdps in our experiments it required almost 48 gb of memory close to the capacity of our computing environment we expect that sdpa c 7 would be more effective on larger sdps of the spin glass type the ratios of spinglass 25 over spinglass 23 were 107502 48 55942 57 1 92 in sdpa c 6 24913 20 13184 25 1 89 in sdpa c 7 and 26023 67 12598 14 2 07 in sdpa 7 in addition sdpa 7 incurred a considerable computation cost on the other parts i e other this other category in sdpa 7 contained the miscellaneous parts related to the computation of the variable matrices x and y 16 since they are miscellaneous and we can not say which costs the most we did not examine them in detail but we note that the fully dense properties of x and y diminished the performance of the other parts of sdpa 7 we should emphasize that mc pdipm is not the only reason for sdpa c 7 being faster because table 7 shows that sdpa c 6 was much slower than sdpa 7 the new factorization of x 1 l l t and the multithreaded computing were the keys to solving the spin glass sdps in the shortest time table 7 computation times on spinglass 18 and spinglass 25 time in seconds spinglass 18 sdpa c 6 sdpa c 7 sdpa 7 sedumi 1 3 s elements 2738 18 1012 13 22 93 s cholesky 178 15 52 70 30 69 p matrix 2516 52 984 76 620 54 other 1301 5 87 29 848 44 total 6734 40 2136 88 1522 60 68570 89 spinglass 25 sdpa c 6 sdpa c 7 sdpa 7 sedumi 1 3 s elements 35314 96 11829 56 220 19 s cholesky 3681 44 945 49 520 39 p matrix 45238 37 11594 76 11846 59 other 23267 71 588 39 13436 50 total 107502 48 24913 20 26023 67 2 days finally table 8 shows the amount of memory required to solve the sdps in tables 5 6 and 7 the notation 31 g indicates that sedumi exceeded the time limit 2 days and used 31 gigabytes of memory during the two day execution by comparison mc pdipm saved a lot of memory by removing the fully dense matrices for example in maxclique 400 sdpa c 7 used only 1 6 times and 1 10 times the memory of sdpa 7 and sedumi respectively in addition the new factorization reduced the memory needed for the largest sdp spinglass 25 from 8 1 gigabytes in sdpa c 6 to 3 7 gigabytes in sdpa c 7 it reduced the required memory because it can reuse the memory structure of cholmod table 8 amount of memory space required for solving sdps in tables 5 6 and 7 m and g indicate megabytes and gigabytes name sdpa c 6 sdpa c 7 sdpa 7 sedumi 1 3 maxclique 400 548 m 516 m 3 0 g 5 2 g maxcut 500 249 m 236 m 4 1 g 6 1 g maxcut 1200 1 2 g 1 2 g 23 g 31 g spinglass 18 2 0 g 707 m 5 6 g 8 3 g spinglass 25 8 1 g 3 7 g 40 g 36 g 17 5 conclusions and future directions we implemented a new sdpa c 7 that uses a more effective factorization of x 1 l l t and takes advantages of multithreaded parallel computing our numerical experiments verified that these two improvements enhanced the performance of mc pdipm and reduced the computation times of the max clique and spin glass sdps sdpa c 7 is available at the sdpa web site http sdpa sourceforge net unlike sdpa c 6 sdpa c 7 has a callable library and a matlab interface it can now be embedded in other c software packages and be directly called from inside matlab the callable library and matlab interface will no doubt expand the usage of sdpa c as shown in the numerical experiments on the max cut problems if the input sdp has a very simple structure we should automatically turn off the multithreading however this would require a complex task to estimate the computation time accurately over the multiple threads from the input sdps another point is that sdpa c 7 has a tendency to be faster for large sdps this is an excellent feature but it does not extend to smaller sdps although this is mainly because mc pdipm is intended to solve large sdps with the factorization of the variable matrices we should combine it with other methods that effectively compute the forward backward substitution of small dimensions acknowledgments the authors thank professor michael ju nger of universita t zu ko ln and professor frauke liers of friedrich alexander universita t erlangen nu rnberg for providing us with the general instance gen erator of the ising spin glasses computation the authors gratefully acknowledge the constructive comments of the anonymous referee references 1 f alizadeh j p a haeberly and m l overton primal dual interior point methods for semidefinite programming convergence rates stability and numerical results siam j op tim 8 2 746 768 1998 2 p r amestoy i s duff and j y l excellent multifrontal parallel distributed symmetric and unsymmetric solvers comput methods appl mech eng 184 501 520 2000 3 e anderson z bai c bischof s blackford j demmel j dongarra j croz a green baum s hammarling a mckenney and d sorensen lapack users guide third society for industrial and applied mathematics philadelphia pa usa 1999 4 p biswas and y ye semidefinite programming for ad hoc wireless sensor network localiza tion in proceedings of the third international symposium on information processing in sensor networks berkeley california 2004 acm 5 b borchers csdp a c library for semidefinite programming optim methods softw 11 12 1 4 613 623 1999 6 y chen t a davis w w hager and s rajamanickam algorithm 887 cholmod supernodal sparse cholesky factorization and update downdate acm trans math softw 35 3 article no 22 2009 18 7 m fukuda b j braams m nakata m l overton j k percus m yamashita and z zhao large scale semidefinite programs in electronic structure calculation math program series b 109 2 3 553 580 2007 8 m fukuda m kojima k murota and k nakata exploiting sparsity in semidefinite programming via matrix completion i general framework siam j optim 11 3 647 674 2000 9 m x goemans and d p williamson improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming j assoc comput mach 42 6 1115 1145 1995 10 r grone c r johnson e m sa and h wolkowicz positive definite completions of partial hermitian matrices linear algebra appl 58 109 124 1984 11 c helmberg f rendl r j vanderbei and h wolkowicz an interior point method for semidefinite programming siam j optim 6 2 342 361 1996 12 d johnson and g pataki 2000 http dimacs rutgers edu challenges seventh 13 m ju nger 2011 http www informatik uni koeln de spinglass 14 s kim m kojima h waki and m yamashita algorithm 920 sfsdp a sparse version of full semidefinite programming relaxation for sensor network localization problems acm trans math softw 38 4 article no 27 2012 15 m kojima s shindoh and s hara interior point methods for the monotone semidefinite linear complementarity problems in symmetric matrices siam j optim 7 86 125 1997 16 c l lawson r j hanson d kincaid and f t krogh basic linear algebra subprograms for fortran usage acm trans math softw 5 308 323 1979 17 l lova sz on the shannon capacity of a graph ieee trans inf theory 25 1 7 1979 18 r d c monteiro primal dual path following algorithms for semidefinite programming siam j optim 7 3 663 678 1997 19 k nakata k fujisawa m fukuda m kojima and k murota exploiting sparsity in semidefinite programming via matrix completion ii implementation and numerical results math program ser b 95 303 327 2003 20 k nakata m yamashita k fujisawa and m kojima a parallel primal dual interior point method for semidefinite programs parallel computing 32 1 24 43 2006 21 m nakata h nakatsuji m ehara m fukuda k nakata and k fujisawa variational calculations of fermion second order reduced density matrices by semidefinite programming algorithm j chem phys 114 8282 8292 2001 22 y e nesterov and m j todd primal dual interior point methods for self scaled cones siam j optim 8 2 324 364 1998 23 j f sturm using sedumi 1 02 a matlab toolbox for optimization over symmetric cones optim methods softw 11 12 1 4 625 653 1999 24 m j todd semidefinite optimization acta numerica 10 515 560 2001 19 25 k c toh m j todd and r h tu tu ncu sdpt 3 a matlab software package for semidefinite programming version 1 3 optim methods softw 11 12 1 4 545 581 1999 26 m yamashita k fujisawa m fukuda k kobayashi k nakta and m nakata latest developments in the sdpa family for solving large scale sdps in m f anjos and j b lasserre editors handbook on semidefinite cone and polynomial optimization theory algorithms software and applications chapter 24 pages 687 714 springer ny usa 2011 27 m yamashita k fujisawa m fukuda k nakata and m nakata algorithm 925 parallel solver for semidefinite programming problem having sparse schur complement matrix acm trans math softw 39 1 2012 article no 6 28 m yamashita k fujisawa and m kojima sdpara semidefinite programming algorithm parallel version parallel comput 29 8 1053 1067 2003 20