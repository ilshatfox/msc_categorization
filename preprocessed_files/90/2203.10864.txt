coresets for weight constrained anisotropic assignment and clustering maximilian fiedler and peter gritzmann march 22 2022 abstract the present paper constructs coresets for weight constrained anisotropic assignment and clustering in contrast to the well studied unconstrained least squares clustering problem approximating the cen troids of the clusters no longer suffices in the weight constrained anisotropic case as even the assignment of the points to best sites is involved this assignment step is often the limiting factor in materials science a problem that partially motivates our work we build on a paper by har peled and kushal who constructed coresets of size o k 3 d 1 for unconstrained least squares clustering we generalize and improve on their results in various ways leading to even smaller coresets with a size of only o k 2 d 1 for weight constrained anisotropic clustering moreover we answer an open question on coreset designs in the negative by showing that the total sensitivity can become as large as the cardinality of the original data set in the constrained case consequently many techniques based on importance sampling do not apply to weight constrained clustering 1 introduction the present paper constructs coresets of size o k 2 d 1 for weight constrained anisotropic least squares assignment and clustering for short wca assignment and wca clustering our focus is twofold on the one hand we improve on known results on coresets for least squares clustering and generalize them to wca clustering in this way we provide new fast approximation algorithms for the np hard wca clustering problem on the other hand we develop our results in line with a specific problem in materials science that requires the computation of wca assignments while this problem can theoretically be solved in 1 ar x iv 2 20 3 10 86 4 v 1 cs d s 2 1 m ar 2 02 2 polynomial time our results allow for significantly reduced computation times and memory which are strictly limiting factors in practice clustering is a well established tool for unsupervised learning and has been in wide use for decades in the analysis of data a variety of more recent applications requires certain fairness conditions 24 30 and particularly bounds on the cluster sizes see e g 7 for general background on constrained clustering and the handbook article 21 for a concise overview focusing on concepts and results of particular relevance in the present context for instance when consolidating farmland 8 the individual farm sizes should remain near constant while electoral districting 13 requires balanced districts by law in conjunction with transformation techniques that extend clustering to supervised learning bounding the cluster sizes can also drive the output towards statistical significance such approaches have recently been used for air cargo prediction 11 and response prediction to clinical medication 12 while computational issues are highly relevant in all such applications the representation of grain maps a problem in material science particularly motivates the present paper 3 this problem concerns structural representations of polycrystalline materials which are only accessible through specific measurements in fact the available grain scan data allows determining the number k of grains approximations of grain centroids volumes and measures of anisotropy as demonstrated in 3 anisotropic diagrams related to extremal constrained anisotropic clusterings represent grain maps well in the introduced model each voxel corresponds to k variables one for each grain hence a reasonable 3 d resolution generally leads to a prohibitively large optimization problem particularly if grain dynamics requiring solutions for many time steps are studied with this and related applications in mind the present paper addresses basic theoretical questions including the existence of small coresets for wca clustering coresets which can be seen as compressed data have been developed for various prob lems 22 19 18 20 32 6 most notably in our context for approximate unconstrained least squares clustering unfortunately results for the unconstrained case cannot directly be generalized to the constrained situation and some concepts do not generalize at all as we will see in section 6 an apparent difference between the unconstrained and the constrained case is the following if we assume that the clustering quality is measured with respect to k given points called sites the unconstrained problem becomes trivial the points are simply assigned to a nearest site in contrast for wca assignment we need to solve a linear program in o nk variables which quickly becomes practically intractable for large data sets in fact clusterings computed on coresets can only be utilized if fast weight preserving extensions to clusterings on the full data set are available we introduce a general coreset definition that also works in the constrained case and for generalized objective functions in addition our coresets have the favorable property that their wca assignments and clusterings can be mapped very quickly to competitive wca assignments and clusterings of the original data set hence after sufficiently good approximate centroids have been determined on coresets we obtain the desired constrained 2 clustering without having to solve the remaining expensive linear program on the full data set thus our coresets allow us to avoid large scale computations that would often be virtually impossible for the applications mentioned in materials science as unconstrained least squares clustering is a special case of wca clustering our results even improve on the previously best known bounds on coreset sizes for unconstrained least squares clustering in the standard deterministic setting in probabilistic models however smaller coresets for unconstrained least squares clustering can be obtained with high probability via importance sampling 17 it is therefore only natural that 30 24 ask whether this technique can also be utilized for constrained clustering as we will show however the total sensitivity of a given instance may be as large as the size of the original data set which answers this question in the negative the present paper is organized as follows section 2 introduces the necessary notation formally specifies our main problems wca clustering and wca assignment provides relevant background information and states our main results section 3 proves the first results and explains the relation between wca clustering and geometric diagrams as required for our coreset sections 4 to 6 contain the proofs of our main results finally section 7 concludes with some remarks and open problems 2 notation background and main results in the following we formally introduce the basic problems and state our main results we begin with a brief introduction to wca assignment and wca clustering although we provide the most relevant references we also refer the reader to 21 and its sources for pointers to additional literature let d n n and for j n 1 n let xj rd and j 0 in the following we will collect the data in two families x x 1 xn and 1 n and refer to j as the weight of the data point xj we refrain from a formal description of the points and weights by means of functions on n and tacitly assume that the correspondences are always indicated by the index i e j is the weight of xj with this understanding the pair x is called weighted data set and x n j 1 j is its total weight further k n always specifies the number of clusters we want to compute then a k clustering c of x is a vector c c 1 ck 11 1 n k 1 kn where the component ij specifies the fraction of xj that is assigned to the ith cluster ci i 1 in more formally the assignment conditions are ij 0 i k j n and k i 1 ij 1 j n 3 if ci 0 the ith cluster is void the weight ci and if ci is not void the centroid ci of the cluster ci are given by ci n j 1 ij j and ci c ci 1 ci n j 1 ij jxj respectively if ci is void we set ci 0 further we use the abbreviation c c c 1 ck for the family of centroids of the clustering c and sometimes we write ci c ci the set of all k clusterings of x will be denoted by c k x whenever the ingredients are clear from the context we will speak of the elements c c k x of a clustering of x or simply of a clustering as we are particularly interested in weight constrained clusterings suppose we are fur ther given lower and upper bounds i i r with i i for each cluster again collected in a family k 1 1 k k then we require that the clusters satisfy the weight constraints i w ci n j 1 j ij i i k the set of all such weight constrained clusterings will be denoted by ck k x note that the simple and easy to verify condition k i 1 i x k i 1 i is equivalent to the existence of such clusterings i e to ck k x 6 to avoid trivial ities we will assume that this condition holds of course if i 0 and i x for an i k the ith weight constraint is redundant and can be omitted we will signify this situation by writing i i e formally allowing i r hence the choice of k 0 0 refers to the unconstrained case next we introduce measures for the quality of a given clustering they are based on distances that may utilize a different norm for each cluster so for i k let ai rd d be a positive definite symmetric matrix and let ai denote the associated ellipsoidal norm i e x ai x taix for any x rd again we collect all k such matrices in the family a a 1 ak clearly if e ed denotes the d d unit matrix e e e refers 4 to the classic euclidean least squares case in which each cluster norm is x 2 x e of course if ai and ai denote the smallest and largest eigenvalue of ai then ai x 22 x 2 ai ai x 22 such ellipsoidal norms often emerge naturally in many applications for instance the different matrices reflect measured knowledge about the moments of the grains in materials see 3 and enable the representation of anisotropic growth we can now measure the quality of a weight constrained clustering c ck k x with respect to a set of reference points s s 1 sk rd called sites in terms of costa k x c s k i 1 n j 1 ij j xj si 2 ai note that costa k x c s only indirectly depends on k as c ck k x is given we use the subscript k consistently in order to indicate that we are considering the constrained case then wca assignment is the following problem given k x a k s find c ck k x that minimizes costa k x c s in this setting the optimization involves only the nk variables ij hence in effect wca assignment is a linear program that can be solved in polynomial time by our general assumption ck k x 6 a finite minimum exists which we denote by costa k x s i e costa k x s min c ck k x costa k x c s instead of fixing the sites and optimizing over the clusterings we can also fix the clus tering optimize over the sites and set costa k x c min s costa k x c s in lemma 3 2 we will see that the optimal sites are indeed the centroids of the clusters optimizing the latter term over all clusterings results in an optimal weight constrained anisotropic clustering its objective function value will be denoted by opta k x min c ck k x costa k x c the problem of finding such optimal clustering is called wca clustering and is defined as given k x a k find c ck k x that minimizes costa k x c as the centroids depend on the clusterings the objective function of wca clustering is highly non linear in its variables ij it is this property that makes the problem np hard 5 and already renders unconstrained least squares clustering difficult 16 2 the latter is shown to be np hard even in dimension d 2 27 moreover 5 shows that no ptas exists in both the number of clusters k and the dimension of the data d these hardness results apply to wca clustering as the unconstrained least squares clus tering problem is contained as the special case a e and k k in this classic situation we drop k and a from the above cost terms and simply write c k x ck k x and cost x c s coste k x c s cost x s coste k x s cost x c coste k x c opt x opte k x if we only drop k but keep a we are in the anisotropic but unconstrained case and if we drop a but keep k we are dealing with the constrained euclidean case and write for instance costa x c s costa k x c s opta x opta k x costk x c s coste k x c s optk x opte k x a popular method of handling the high computational complexity of unconstrained least squares clustering is to approximate the data set by much smaller sets that still capture the relevant properties of the data 22 15 9 28 14 17 the decisive property of such a coreset is that the cost of a clustering of its points is comparable to the cost of clusterings of the original data set hence the time consuming computations can be performed on much smaller data sets with good approximations of optimal solutions for the original massive data still being obtained we will now define coresets more formally in a manner that is suitable for the weight constrained anisotropic case definition 2 1 let k n x and x be two weighted data sets and 0 1 2 1 the weighted data set x is an coreset for k x a k if there exists a mapping f ck k x ck k x called an extension and real constants referred to as terms or offsets with 0 such that the following two conditions hold for all sets s of k sites and clusterings c ck k x 1 costa k x f c s costa k x c s a costa k x s 1 costa k x s b if the context is clear we refer to x as an coreset if 1 we speak of an coreset and if in addition 0 of a 0 offset or linear coreset note that the conditions in definition 2 1 are required for every set s of sites hence 6 x can also be regarded as a coreset for each instance k x a k s of wca assign ment and we will sometimes refer to it this way for easy distinction we will universally signify coresets by means of a tilde or if addi tionally needed a bar as in x or x parameters or objects on the different sets will be also be marked this way for instances clusterings on x will usually be named c and their components ij while clusterings and their components on coresets x are signified by a tilde i e they are called c and ij before we state our main results let us briefly comment on the rationale behind the above coreset definition first note that costa k x s costa k x f c s hence in the special situation that c attains cost x s 0 and 1 the above condition for a linear coreset implies that 1 costa k x s costa k x s 1 costa k x s which coincides with the coreset definition of 22 for standard unconstrained least squares clustering i e a e k k hence definition 2 1 generalizes the known definition to adapt to the specific requirements of wca clustering further note that the two terms generalize the one additive offset parameter used in 19 18 they will be used to show that coresets preserve approximations up a factor finally and most importantly let us point out that while rather weak definition 2 1 still captures the main motivation for the concept that an approximate solution of the constrained clustering problem on a coreset yields an approximate solution on the original data set see theorem 3 5 for the details as the general goal is to improve the tractability of the underlying optimization prob lem we are aiming at coresets x of weighted sets x that are small enough to enhance the performance of available algorithms significantly but still preserve enough of the structure of the original instance to yield solutions which can be extended to good ap proximations for the original instance in the unconstrained least squares case clusterings of the coreset can easily be converted into clusterings of the original data set in fact it suffices to assign the points of x to on of the closest determined sites and this is efficiently facilitated by means of voronoi diagrams however such a procedure does not respect the weight constraints and hence does not produce wca clusterings of the full data set in general in definition 2 1 this conversion of a coreset clustering c ck k x into a clustering c ck k x of the full data set is facilitated by the explicitly introduced extension f ck k x ck k x in fact f captures any way of extending c to a feasible clustering c of the full data 7 set specifically condition a of definition 2 1 can be interpreted as saying that for each weight constrained coreset clustering c we obtain a weight constrained clustering f c on the full data set of cost that is not much worse condition b expresses the property that an optimal wca assignment for sites s on the full data set is not much better than one on the coreset hence good approximations of the latter lead to good approximations of the former let us now turn to the main results of the present paper first as a generalization of a result in 30 we show that small changes in position of the data points result in coresets the permitted deviation is quantified in terms of the cost opt x of an optimal unconstrained least squares clustering on the data set x let a and a denote the largest and smallest eigenvalue of all matrices in a theorem 2 2 let k x a k be an instance of wca clustering further let x be a weighted data set n x and let p n n be such that j p 1 j n for any 0 1 2 if n j 1 j xj x p j 2 2 2 a 16 a opt x then x is a linear coreset this theorem allows us to extend early coreset constructions namely those that are based on small movements see e g 23 to wca clustering more importantly theorem 2 2 is one crucial ingredient of the proof of the following main coreset result theorem 2 3 for any instance k x a k of wca clustering and for any 0 1 2 and a a there is an coreset of size n with n o k 2 d 1 with the o notation we refer to the asymptotic behavior for k and 0 the important property is that the size of the coreset does not depend on the number n of the points of x it does however depend exponentially on the dimension d of the data set and involves another factor d for some constant while our proofs allow the growth in 8 d to be specified explicitly we will refrain from doing so for the sake of simplicity so that our results are stated for arbitrary but fixed dimension d the proof of theorem 2 3 follows the basic construction of 22 but generalizes it to weight constrained anisotropic clustering and by suitably exploiting the underlying ge ometric structure even reduces the asymptotics by a factor of k in fact the uniform condition number bound a a becomes 1 for a e hence we obtain the following corollary for unconstrained least squares clustering corollary 2 4 there is an coreset of size o k 2 d 1 for least squares clustering finally we ask whether it is possible to build even smaller coresets using importance sampling this technique is based on a notion of the relevance of points in clusterings called sensitivity and will be formally defined in section 6 it is well known that the total sensitivity of unconstrained least squares clustering can be bounded by o k 26 theorem 3 1 for 2 and this results in small corests in the underlying probabilistic model in the context of fair clustering it was acknowledged in 24 as open and actually posed as an interesting open question in 30 whether this approach extends to the constrained case the following theorem shows however that in wca clustering the total sensitivity might actually depend linearly on n hence the answer is in the negative theorem 2 5 in the case of wca clustering for any n n there is a data set with n points such that the total sensitivity is n 3 initial results auxiliary lemmas and diagrams in the following we will first briefly explain the relation between coresets for wca clustering and wca assignment we will then go on to address the power of coresets for constrained clustering i e the question of what kind of error on the original data set can be expected when approximations for wca clustering are available on coresets finally we will outline the main coreset construction so as to put the individual parts of the proof of theorem 2 3 into greater perspective in particular we will prove a composition lemma for coresets and describe the relation between extremal wca assignments to diagrams let us begin with observations relating coresets for wca clustering to those for wca assignment recall that the condition in definition 2 1 is required for every set s of sites hence a coreset for x can also be regarded as a coreset for each instance k x a k s of wca assignment we show that in a certain sense the converse is also true i e a coreset according to definition 2 1 satisfies the relevant inequalities when the centroids for the full data set and the coreset respectively are used rather than identical sites in both cases before formally establishing this result we present two basic observations which are used here and in subsequent sections we emphasize that we mostly apply the following results to a cluster or batch of points using 0 as an index for the objects to distinguish them from the full data set so let 9 x 0 0 be a weighted data set let c 0 be its centroid and let a rd d be a positive definite symmetric matrix with x denoting the weight of a point x x 0 we define the cluster variation or batch error va x 0 of x 0 for a by va x 0 x x 0 x x c 0 2 a note that va x 0 is bounded by ve x 0 as follows remark 3 1 min a ve x 0 va x 0 max a ve x 0 in accordance with our previous notation x 0 x x 0 x denotes the weight of x 0 the following elementary lemma is a slight generalization of well known facts its first part expresses the weighted sum of squared distances between a site s and the points of a data set x 0 as the sum of the variation of x 0 and the squared distance between the centroid c 0 and s the second part uses this equation to relate wca clustering to wca assignment the proof uses the notion of the support supp ci of the cluster ci defined by supp ci xj j n ij 0 lemma 3 2 a let x 0 0 be a weighted data set c 0 its centroid a rd d a positive definite symmetric matrix and s rd then x x 0 x x s 2 a x 0 c 0 s 2 a va x 0 b let k x a k s be an instance of wca assignment and let c c 1 ck be a clustering then costa k x c costa k x c s with equality if and only if si c ci for every non void cluster ci proof a since x x 0 xx x 0 c 0 we have x x 0 x x s 2 a x x 0 x x c 0 2 a 2 x c 0 a c 0 s c 0 s 2 a x 0 c 0 s 2 a va x 0 2 x x 0 xx x 0 c 0 a c 0 s x 0 c 0 s 2 a va x 0 10 b let as usual c c 1 ck and define the weighted data set xi i for i k by xi supp ci and i ij j xj xi then by applying a to x 0 0 xi i and a ai for i k we obtain costa k x c s k i 1 n j 1 ij j xj si 2 ai k i 1 ci c ci si 2 ai vai xi k i 1 vai xi costa k x c thus costa k x c s costa k x c with equality if and only if for each i k either ci 0 or si c ci at first glance definition 2 1 might seem to require more all sites and yield less cost with respect to the same sites rather than individual centroids than one might expect the next corollary shows however that the conditions of definition 2 1 imply the corresponding inequalities involving the centroids corollary 3 3 with the notation of definition 2 1 let x be an coreset for k x a k then 1 costa k x f c costa k x c a opta k x 1 opta k x b proof a let s c c then by lemma 3 2 b costa k x f c costa k x f c c f c costa k x f c s and inequality a of definition 2 1 yields 1 costa k x f c costa k x c s costa k x c b let c be a clustering attaining opta k x and set s c c then by lemma 3 2 b opta k x costa k x s and hence by inequality b of definition 2 1 opta k x 1 costa k x s 1 opta k x which concludes the proof 11 next we address the implications of good coreset clusterings on the quality of derived clusterings on the full data set as usual the quality of approximation will be measured in terms of the relative error hence we speak of a approximation if the objective function value of the produced solution is bounded from above by times its minimum before we state and prove the subsequent theorem 3 5 we deal with the effect of moving sites lemma 3 4 let i k x a k be an instance of wca clustering 0 1 2 1 let x be an 3 coreset for i and let f denote its extension further let c ck k x let s 1 s 2 be two sets of k sites and r with further suppose that costa k x c s 1 costa k x s 2 then costa k x f c s 1 1 costa k x s 2 proof by the coreset properties definition 2 1 a and b we have 1 3 costa k x f c s 1 costa k x c s 1 and costa k x s 2 1 3 costa k x s 2 hence it follows from the assumption that the right hand side of the former inequality is bounded from above by times the left hand side of the latter 1 3 costa k x f c s 1 1 3 costa k x s 2 since and we have hence 1 3 costa k x f c s 1 1 3 costa k x s 2 as 1 3 1 2 3 3 1 2 3 2 3 1 1 3 we obtain costa k x f c s 1 1 costa k x s 2 as asserted next we show that approximations on coresets lead to approximations on the full data sets theorem 3 5 let i k x a k be an instance of wca clustering 0 1 2 1 let x be an 3 coreset for i and let f be its extension 12 further let c ck k x and r with a if c is a approximation for i s then f c is a 1 approximation for is b if c is a approximation for i then f c is a 1 approximation for i proof a since c is a approximation for i s we have costa k x c s costa k x s thus lemma 3 4 applied with s 1 s 2 s implies that costa k x f c s 1 costa k x s hence f c is a 1 approximation for is b let c ck k x be optimal and set s c c by lemma 3 2 b opta k x costa k x s hence together with the assumption that c is a approximation for i this yields costa k x c c c costa k x c opta k x costa k x s thus lemma 3 4 applied with s 1 c c and s 2 s implies that costa k x f c c c 1 costa k x s 1 opta k x with the aid of lemma 3 2 b we conclude that costa k x f c costa k x f c c c 1 opta k x i e f c is a 1 approximation for i let us now turn to the third and final part of this section we will first outline the overall structure of the construction on which our main theorem 2 3 is based before going on to introduce two important ingredients the construction of coresets for wca clustering follows the basic scheme of 22 for producing coresets for unconstrained least squares clustering first we compute an approximation for the least squares clustering and use the obtained cluster centroids as points called vertices from which an appropriately dense set of lines is issued the constructed coreset will live on the union of these pencils of lines second we project each point of each cluster on a nearest line issuing from the cluster s centroid third on each line we merge all points that lie in appropriately constructed batches and collect their weights to obtain a much thinner set x the details will be given in section 5 1 to show that this three step scheme works for wca assignment and wca clustering we will make use of the fact that coresets of coresets are themselves coresets this will be 13 shown in lemma 3 6 also in order to verify the coreset properties by bounding the number of costly batches in lemma 5 11 we will utilize the relation between extremal wca assignments and diagrams explained subsequently lemma 3 6 let 1 2 0 1 2 1 2 1 and set 1 2 1 2 and max 1 2 further let i k x a k be an instance of wca clustering let x be an 1 1 coreset for i and let x be an 2 2 coreset for i k x a k then x is an coreset for i proof let f 1 and f 2 denote the extensions for x and x and set f f 1 f 2 further for 2 let denote the corresponding terms and set 1 2 since 0 for 2 we have 0 1 2 1 1 2 2 also note that 1 1 1 2 1 2 1 1 2 1 2 1 1 1 2 we will now verify the two conditions of definition 2 1 a and b so let s be a set of k sites and let c ck k x then by the coreset conditions for x and x we have 1 costa k x f c s 1 1 1 2 costa k x f 1 f 2 c s 1 2 costa k x f 2 c s 1 costa k x c s 2 1 2 1 costa k x c s further costa k x s 1 2 costa k x s 1 1 2 costa k x s 1 1 2 1 1 costa k x s 1 costa k x s hence x is an coreset for i finally we explain the relation between wca assignments and diagrams we will present the relevant theory in a concise manner which is strongly focussed towards its use in section 5 for a detailed general account additional pointers to the literature and further background information see 13 21 an anisotropic diagram p p t a is specified by a set t t 1 tk rd of k different sites corresponding sizes 1 k rk and a set of positive definite 14 symmetric matrices a a 1 ak it is then defined as the collection p p 1 pk of subsets pi of rn called cells specified by pi x rd k x ti 2 ai i x t 2 a since the ellipsoidal norms ai are strictly convex cells of different sites do not have any interior points in common see 13 note that in the special case of 0 0 and a e we obtain the classic voronoi diagram of the points in t now let c c 1 ck ck k x then we can say that p and c are compatible if supp ci pi x for all i k more strongly if supp ci pi x the diagram p and the clustering c are called strongly compatible note that two cells of different sites can only have boundary points in common anyway as we will see later the bound on the coreset quality relies on the fact that for relevant clusterings the number of points that are fractionally assigned to different clusters can be further controlled more precisely c and p are called strictly compatible if p and c are strongly compatible and for each i k with i 6 the intersection pi p x contains at most one point later we will use the following result which has been shown for the euclidean case in 10 corollary 2 2 and which follows from 13 in the general case proposition 3 7 given an instance k x a k s of wca assignment there is a clus tering c that attains costa k x s and admits a strictly compatible anisotropic diagram further such a diagram can be computed in polynomial time 4 coresets based on local neighborhood mergers a well known standard method of constructing coresets for unconstrained least squares clustering is to merge neighboring data points into a single point that carries their total weight in this section we will prove theorem 2 2 which shows that a similar approach also works for wca clustering this is an essential first step for the construction of improved coresets in section 5 and in particular it allows the repositioning of points to obtain favorable structural properties generally the merging of points in a neighborhood while preserving weights can be represented by a function that assigns each point of the original data set x to a point of a new and typically much smaller set x such that each point of x carries the sum of the weights of all the original points assigned to it more formally with n x and n x as before let p n n be surjective such that j p 1 j n then we call p a merging function 15 if the total movement i e the sum of all distances between a point and its image is small enough the compressed set x is a coreset for example using such coreset constructions an approximate solution for unconstrained least squares clustering whose cost exceeds opt x by at most a constant factor can be efficiently found the existence of such approximation algorithms is stated more formally in the following proposition see 1 25 22 19 18 9 for a non exhaustive list of corresponding algorithms proposition 4 1 there exist constants 1 n and a polynomial time algorithm with the following property given an instance k x of unconstrained least squares clus tering the algorithm computes a clustering with at most k clusters with a cost alg satisfying alg opt x algorithms according to proposition 4 1 are usually called approximations as they compute an approximation but use k rather than k clusters while in terms of their approximation properties smaller constants and are preferable the running times typically increase significantly with decreasing constants hence in practice these effects require some fine tuning as pointed out by 1 the parameter does not affect the coreset s ability to approximate unconstrained least squares k clusterings next we prove various technical lemmas for handling the weight constrained anisotropic case in particular we will relate the anisotropic case to the euclidean bound the cost of an optimal wca clustering and finally define and analyze an appropriate extension function f in order to bound the cost of optimal solutions for a given instance of wca clustering recall first that a and a denote the largest and smallest eigenvalue of all matrices in a respectively and note that the optima in the euclidean and anisotropic unconstrained situation are related via opt x 1 a opta x a a opt x this shows in particular that for approximations of least squares clustering we have the estimate alg a opta x a a opt x for the corresponding anisotropic problem also the former observation implies the fol lowing simple but useful consequence lemma 4 2 let 0 and suppose that n j 1 j x p j xj 2 2 a a opt x 16 then whenever k i 1 ij 1 for each j n k i 1 n j 1 ij j x p j xj 2 ai opta x proof for the proof just note that k i 1 n j 1 ij j x p j xj 2 ai a n j 1 j x p j xj 2 2 k i 1 ij a n j 1 j x p j xj 2 2 a opt x opta x the next technical result considers the effect of merging data points on the cost of clustering so let p be a merging function for x let x be the obtained data set and let c ck k x then p gives rise to a clustering c ck k x defined by i 1 j p 1 ij j n i k slightly abusing notation we will write p x x p x x and p c c then we obtain the following inequalities lemma 4 3 let p be a merging function for x c ck k x and set d k i 1 n j 1 ij j x p j xj 2 ai then costa k x c s 2 d costa k x c s costa k p x p c s k i 1 n j 1 ij j x p j si 2 ai costa k x c s 2 d costa k x c s d 17 proof first we explain the equality using the definition of p costa k p x p c s k i 1 n 1 i x si 2 ai k i 1 n 1 1 j p 1 ij j x si 2 ai k i 1 n 1 j p 1 ij j x si 2 ai k i 1 n j 1 ij j x p j si 2 ai now note that the latter norm decomposes as follows x p j si 2 ai x p j xj xj si 2 ai x p j xj 2 ai 2 x p j xj ai xj si xj si 2 ai hence it suffices to bound the three terms d k i 1 n j 1 ij j x p j xj ai xj si k i 1 n j 1 ij j xj si 2 ai separately the first term appears in the bound from above and as it is nonnegative it can be omitted for the estimate from below also the third sum is exactly costa k x c s this suffices to suitably bound the absolute value of the second sum to do so we evoke the cauchy schwarz inequality twice and obtain k i 1 n j 1 ij j x p j xj ai xj si k i 1 n j 1 ij j x p j xj ai xj si ai k i 1 n j 1 ij j x p j xj ai ij j xj si ai d costa k x c s this concludes the proof as a final ingredient for the proof of theorem 2 2 we define a mapping f ck k x ck k x that converts clusterings of x p x to clusterings of x in fact f assigns each point xj of x to clusters exactly as its representative xp j in x is assigned more 18 precisely let c ck k x and define c f c by setting ij ip j j n i k the following simple lemma shows in particular that f preserves the cluster weights i e f c ck k x lemma 4 4 let p be a merging function for x x p x c ck k x and f as defined above then for c f c a n j 1 ij j n 1 i for each i k b p f c c proof for the proof of a simply note that n j 1 ij j n j 1 ip j j n 1 j p 1 ij j n 1 i to prove b denote the components of p f c by i then we have i 1 j p 1 ij j i j p 1 j i this concludes the proof of the lemma by lemma 4 4 a f preserves the cluster sizes and maps weight constrained clusterings onto weight constrained clusterings also note that f can be very easily computed lemma 4 4 b allows us to infer that the inequalities of lemma 4 3 not only hold as stated i e for pairs c ck k x and c p c but also for pairs c ck k x and c f c i e corollary 4 5 let p be a merging function for x x p x and c ck k x then with d as before costa k x f c s 2 d costa k x f c s costa k x c s costa k x f c s 2 d costa k x c s d we are now ready to prove theorem 2 2 proof of theorem 2 2 we show that under the assumptions of the theorem the coreset properties definition 2 1 a b hold with offset 0 i e for 0 19 since opta x costa k x c s and by assumption n j 1 j xj x p j 2 2 2 a 16 a opt x lemma 4 2 applied with 2 16 provides d k i 1 n j 1 ij j x p j xj 2 ai 2 16 opta x 2 16 costa k x c s thus corollary 4 5 yields for c ck k x and c f c 1 costa k x c s costa k x c s 2 d costa k x c s costa k x c s which shows coreset property a to verify coreset property b let c ck k x be optimal for the given sites s and set c p c then by lemma 4 3 costa k x s costa k x c s costa k x c s 2 d costa k x c s d 1 2 2 16 costa k x s 1 costa k x s this concludes the proof 5 small coresets we will now construct small coresets analyze their properties and prove theorem 2 3 in section 5 1 we will first provide the construction and bound the sizes of the resulting coresets x in section 5 2 we will derive a structural property for clusterings on x that admit a strictly compatible anisotropic diagram finally the coreset properties according to definition 2 1 will be verified in section 5 3 5 1 the construction the construction will follow the 3 step scheme already outlined in section 3 the first step is to compute an approximation for least squares clustering using proposition 4 1 this approximation results in an integer assignment of the points of x to k clusters c i with k k of cost alg x opt x 20 for i k let c i denote the centroid of c i the following exposition is written with the general situation in mind that none of the clusters is void note however that void clusters do not cause any problems but actually reduce the size of the constructed coreset we start by describing the second step in full detail in this step the obtained cluster centroids c i are used as vertices from which suitable pencils of lines are issued this follows the approach of 22 which essentially reduces the coreset construction in the least squares case to dimension one in order to construct such pencils we employ nets i e point sets q with properties explained in the following lemma proposition 5 1 see 29 22 let as usual sd 1 denote the euclidean unit sphere of rd given 0 0 1 2 there exists a point set q q 0 qd 0 with the following properties a q contains o d 1 0 points b for each p sd 1 there exists a point q q such that p q 2 0 and c q can be computed in time o d 1 0 using proposition 5 1 we compute such a set q 0 for 0 4 a a this leads to k pencils c i rq of the lines l c i q c i rq i k q q 0 note that as 0 6 q all sets l c i q are indeed lines let l denote the set of all such lines then l k q o k d 1 next for each i k we project each point of x x c i orthogonally on a closest ray l c i q ties are resolved arbitrarily the resulting point will be denoted by x note that unless q is chosen in an appropriately general position it may happen that two points of x are projected onto the same point x we do not merge such points and we treat them as separate entities in the family x the weights then remain unchanged and we obtain the weighted data set x of the same cardinality n i e the merging function is the identity as the following lemma shows x is a 0 offset coreset for wca clustering with the extension f 1 ck k x ck k x defined by ij ij j n i k lemma 5 2 x is a linear coreset for wca clustering proof let j n and i j k such that xj x c i j further let q q such that 21 x j l c i j q using the intercept theorem we see that with i i j xj x j 2 c i xj c i xj c i 2 c i q 2 xj c i 2 0 xj c i 2 since the k clusters c i constitute an approximation for least squares clustering this implies n j 1 j xj x j 22 2 0 n j 1 j xj c i j 2 2 2 0 alg x 2 0 opt x 2 a 16 a opt x hence it follows from theorem 2 2 that x is an coreset for wca clustering let us point out that together lemmas 3 6 and 5 2 imply that it suffices to design coresets that live on the union of the constructed pencils i e on l this describes the second step of the procedure in the third step we merge points on each line into batches by replacing them with their centroids and adjusting their weights this finally results in the desired data set x so let l l starting from the leftmost point we successively add from left to right points of x l to form the first batch b b 1 until ve b ve 2 32 k a l alg x and continue with the next batch the above condition can always be achieved by assigning if needed the last point fractionally to the batch in such a case we may assume that all points are completely assigned to a single batch by splitting this point into two with the appropriate weights while this assumption simplifies the exposition it does not effect on the results and can be made without loss of generality the process is continued until all points are assigned to batches note that all batches have the same error apart from the last one whose error may be smaller let b l denote the set of batches on the line l and let b signify the total set of all batches i e the union of all b l with l l finally we merge the points of each batch more precisely let b b and let b x j x b j xb 1 b x j x b j x j be the set s x b total weight and centroid respectively we replace the points of x b 22 by their centroid xb and assign b as its weight the data set that results from applying the merger to all lines of the pencils will be called xb b this is actually the desired set x and we will use both notations interchangeably depending on which one leads to the more intuitive exposition in the subsequent proofs in particular the number of points will still be denoted by n if we index the batches as b for n and set x xb the corresponding merging function p n n is again specified by p j x j b and the extension f 2 ck k x ck k x is again defined by ij ip j j n i k hence the extension f ck k x ck k x is simply given by f f 1 f 2 i e ij ip j j n i k this completes the construction before we prove that xb b is an coreset we bound its cardinality n in this section the basic idea for deriving a bound is due to 22 however we improve on their arguments to reduce the dependency of the estimate by a factor of k lemma 5 3 b o k 2 d 1 i e the number of points in xb is of the order k 2 d 1 proof to establish the asserted bound let c 1 c k be the centroids of an optimal least squares clustering of x in this unconstrained euclidean case the clustering is com patible with a voronoi diagram as we need not respect any weight constraints for the clusters we may assume that no point of x lies on the boundary of any voronoi cell since l intersects each voronoi cell in a possibly empty or one element interval all points in the same interval lie in the same cluster this implies that for at most k 1 batches in l the points are assigned to more than one cluster i e at least b l k 1 such batches are fully assigned to a single cluster now let b b l be contained in the ith cluster but let b not be the last batch on l as the error ve b is known from the construction we obtain from lemma 3 2 with x 0 b c 0 cb x 0 b and a e that x b x x c i 22 b cb c i 2 2 ve b ve b ve 23 as at most k 1 1 k of the batches b b l can contribute less than ve to the cost opt x of the clustering we see that xb opt x ve k l since x is a linear coreset for x by lemma 5 2 we can use corollary 3 3 b to obtain opt x 1 opt x 2 alg x all in all xb 2 alg x ve k l 64 a k 2 k l as the right term is in o k 2 d 1 this concludes the proof 5 2 structural properties of clusterings admitting strictly compatible diagrams we will now show that each clustering that admits a strictly compatible diagram gives rise to a partition of each line l into at most 2 k 1 intervals such that no data point in the relative interior of each interval is fractionally assigned i e belongs to more than one cluster since by proposition 3 7 it suffices to consider such diagrams on the coresets this will allow us to verify coreset condition b of definition 2 1 condition a on the other hand does not require additional assumptions on the structure of the clustering given an instance k x a k s of wca assignment suppose that c ck k x at tains cost x s and admits a strictly compatible diagram p p t a p 1 pk note that c is available by proposition 3 7 we use p to bound the structural complexity of c s intersections with lines l later we apply the results to those lines that constitute the pencils on which x lives the first lemma considers the generic situation and could be stated within the realm of davenport schinzel sequences see e g 31 we will however give a direct self contained proof of the relevant result so let c r q rd 0 l l c q and for i k gi rd r gi x x si 2 ai further let h rd r h x min gi x i k be the lower envelope of g 1 gk then of course the diagram p is induced by g 1 gk and l pi x l gi x h x 24 consists of finitely many connected components all closed intervals and proper i e 1 dimensional or possibly singletons let i denote their number then we have the following bound lemma 5 4 suppose that no two of the restrictions gi l coincide then k i 1 i 2 k 1 and this bound is the best possible proof the proof of the inequality is by induction with respect to k as the assertion is trivial for k 0 1 suppose that it has been verified for some k 0 n and let k k 0 1 suppose first that there exists an index i k such that i 1 by the induction hypothesis we have i k i i 2 k 1 1 and adding gi back in the corresponding interval can split at most one other interval by the assumption that no two of the restrictions gi l coincide hence the assertion holds so suppose in the following that i 2 for all i k we will show that this leads to a contradiction thereby proving the theorem for i k let y i j be a relative interior point in the jth component of l pi ordered such that y i 1 y i i further let i 0 m 1 argmax y i 1 i k if the index i 0 is unique set i i 0 otherwise let i m 2 argmin y i 2 i m 1 note that m 2 still does not need to be a singleton but the different choices do not affect the subsequent arguments since i 2 there must be i k i and j i such that y i 1 y i j y i 2 as i m 1 we know that j 2 and y i 1 y i 1 equality however is excluded by the choice of i m 2 hence y i 1 y i 1 y i j y i 2 this implies that gi gi has at least three roots in l i e gi l gi l which contradicts our general assumption the bound is tight for examples of suitably nested parabolas as a consequence we obtain the following result 25 corollary 5 5 suppose again that no two of the restrictions gi l coincide then there is a dissection of l into at most 2 k 1 proper intervals such that the relative interior of each interval is contained in exactly one of the cells of p proof for i k let ii i 0 i denote the set of the relative interiors of all proper intervals that occur as connected components of l pi and set i i 0 i 1 ik since by assumption all univariate polynomials gi l are different any two intervals in i are disjoint for i k let 0 i denote the number of singletons that occur as connected components of l pi and set 1 i ii then of course i 0 i 1 i i k and i k i 1 1 i starting with i 0 we will now construct refinements i by successively splitting intervals that violate the assertion as long as such intervals exist so let i 0 k let i 0 ii 0 and suppose that there exists a point y i 0 that also belongs to some other cell pi 1 then y is the unique point of pi 0 pi 1 in relint i 0 since otherwise gi 0 l gi 1 l would have at least three local extrema contradicting the assumption that gi 0 l 6 gi 1 l for the same reason no other interval i ii 0 can contain a point of pi 1 in its relative interior now we split i 0 at y into two intervals and obtain a new set i 1 by replacing i 0 in ii 0 by the corresponding two relatively open intervals i 0 and i 0 note that neither i 0 nor i 0 contains points of pi 1 anymore and that y is a connected component of l pi 1 which counted towards 0 i 1 we continue by successively splitting intervals that violate the assertion and finally obtain a set i of intervals that have the desired property the total number of necessary splits is bounded by i 1 0 i hence with the aid of lemma 5 4 i i 1 1 i i 1 0 i i 1 i 2 k 1 which is the asserted bound as an immediate consequence the corollary yields a structural result when c and p are compatible corollary 5 6 let g 1 l gk l be all different and let c and p be compatible then there exists a partition of l into at most 2 k 1 proper intervals such that all data points in the relative interior of each interval are fully assigned to the same cluster note however that the genericity assumption that the k univariate polynomials g 1 l g 2 l are all different for each line l can generally not be guaranteed even if 26 figure 1 a clustering and diagram that are strongly compatible sites are depicted as squares the clustering assigns all points on the boundary between the green and pink cluster with a proportion of 1 2 1 2 to the two clusters consequently a dissection of the horizontal line l with the required properties of corollary 5 6 requires more than 2 k 1 2 4 1 7 intervals all multivariate polynomials g 1 gk are different for instance when a e the cells of p are polyhedra that might all have a line or even some higher dimensional affine sub space in common in this case it may happen that all points of x in the relative interior of a component i of l pi are fractionally assigned to ci see figure 1 for an illustration in this case the number of splitting operations will generally not be bounded by a function of k only and corollary 5 6 does not hold anymore in fact a clustering that admits a feasible or even strongly compatible diagram does not suffice to provide the bound required later and we need the additional properties of strictly compatible diagrams to prove the following theorem theorem 5 7 let c ck k x admit a strictly compatible diagram and let l be a line in rd then there is a partition of l into at most 2 k 1 proper intervals such that all data points in the relative interior of each interval are fully assigned to the same cluster proof let p p t a p 1 pk be a diagram such that c and p are strictly compatible suppose that exactly k of the univariate polynomials gi l are different let n k be the set of the corresponding indices and set n k n if k k the assertion follows from corollary 5 6 so suppose that k k by corollary 5 6 we obtain a partition i of l into at most 2 k 1 proper intervals with the following property if i i and x x relint i then x 6 pi for any i n since c and p are strictly compatible pi p contains at most one point for i 6 since for each i n each gi l coincides with one of the g l with n splitting intervals for each such points will result in an additional number of at most n intervals in order to guarantee the desired property hence we obtain a total of at most 2 n 1 n 2 k 1 k k k k 1 2 k 1 27 intervals that have the required properties 5 3 proof of the coreset properties we will now prove various lemmas that will subsequently be used to show that x xb b is a coreset for k x a k thereby proving theorem 2 3 note that by lemma 5 2 and lemma 3 6 it suffices to show that xb b is a coreset for k x a k we begin with a simple observation that relates the batch errors in the anisotropic case to those in the euclidean case remark 5 8 let b b then a b b ve b k i 1 b b ibvai b a b b ve b proof by remark 3 1 and since k i 1 ib 1 for each b b we obtain k i 1 b b ibvai b a k i 1 b b ibve b a b b ve b and k i 1 b b ibvai b a k i 1 b b ibve b a b b ve b the following lemma essentially handles condition a of definition 2 1 for each set s of sites lemma 5 9 let cb ck k xb b then costa k x f 2 cb s costa k xb cb s a b b ve b proof first note that for each b b we have by lemma 3 2 a with x 0 b c 0 cb b xb si 2 ai x j b j x j si 2 ai vai b 28 therefore costa k xb cb s k i 1 b b ib b xb si 2 ai k i 1 b b x j b ib j x j si 2 ai ibvai b since ij ib for each x j b we have k i 1 b b x j b ib j x j si 2 ai k i 1 b b x j b ij j xj si 2 ai k i 1 n j 1 ij j x j si 2 ai costa k x f 2 cb s hence costa k xb cb s costa k x f 2 cb s k i 1 b b ibvai b and by remark 5 8 this yields costa k x f 2 cb s costa k xb cb s a b b ve b which is the asserted inequality in order to verify that xb b is a coreset for k x a k we address condition b of definition 2 1 with the aid of proposition 3 7 let c ck k x such that it attains costa k x s and admits a strictly compatible anisotropic diagram p p t a we will show that for cb p c costa k xb cb s 1 costa k x c s 1 as this yields the inequality costa k xb s costa k xb cb s 1 costa k x c s 1 costa k x s needed for condition b the next two lemmas will prepare the ground to verify the former inequality 29 the clustering c partitions b into the set b of those batches b whose points are all assigned to the same cluster and its complement b b b similarly x and xb b are split into x and xb b and the clusterings c and cb are split into their restrictions c and cb of x and xb b respectively as the clustering cost is additive in the contributions of each point we can consider the costs costa k x c s k i 1 b b x j b ij j x j si 2 ai costa k xb cb s k i 1 b b ib b xb si 2 ai incurred by points in batches from b and b separately let us point out that the subscript k is used here to indicate that we are still in the constrained case the bounds in k however only apply to the clusterings on the unsplit data sets note that for each b b ib ij 1 for x j b 0 for x j 6 b this property is used in the next lemma to show that batches in b behave nicely lemma 5 10 costa k xb cb s a b b ve b costa k x c s proof since all points of each batch b b have been assigned to the same cluster we see with the aid of lemma 3 2 a with x 0 b c 0 xb that costa k x c s k i 1 b b x j b ij j x j si 2 ai k i 1 b b ib x j b j x j si 2 ai k i 1 b b ib b xb si 2 ai vai b costa k xb cb s k i 1 b b ibvai b now the asserted inequality follows with the aid of remark 5 8 30 while the proof of lemma 5 10 did not make any use of the special choice of the optimal clustering the next lemma which deals with b relies heavily on the property that c admits a strictly compatible diagram note that lemma 5 11 can also be used to fix a flaw in the proof 22 theorem 3 7 lemma 5 11 costa k xb cb s a b b ve b costa k x c s costa k x c s proof first by the definition of the merging function p ib b x j b ij j hence costa k xb cb s k i 1 b b x j b ij j xb si 2 ai next we apply lemma 4 3 to x and c in this setting the number d becomes d k i 1 b b x j b ij j xb x j 2 ai and lemma 4 3 yields costa k xb cb s costa k x c s 2 d costa k x c s d we will now apply lemma 4 2 to derive an upper bound for d in order to do so let us first consider the lemma s assumption in fact we have b b xj b j xb x j 22 b b ve b b ve b 2 32 k a l alg x hence we need to bound the number of batches in b we consider the different lines of the pencils separately so let l l since c admits a strictly compatible diagram we can apply theorem 5 7 there is thus a partition of l into at most 2 k 1 proper intervals such that all data points in the relative interior of each interval are fully assigned to the same cluster consequently only batches that contain a boundary point of at least one such interval can contain points 31 of more than one cluster therefore b b l 2 k 1 and thus b 2 k 1 l since x is a linear coreset for x and 0 1 2 we obtain b b xj b j xb x j 22 2 16 a opt x 2 8 a opt x lemma 4 2 applied to x with 2 8 yields d k i 1 b b x j b ij j xb x j 2 ai 2 8 opta x 2 8 costa k x c s since costa k x c s costa k x c s we obtain costa k xb cb s costa k x c s 2 2 8 2 8 costa k x c s finally with the aid of remark 5 8 the previous estimates also imply that k i 1 b b ibvai b a b b ve b a b b xj b j xb x j 22 2 8 costa k x c s therefore using remark 5 8 again costa k xb cb s a b b ve b costa k xb cb s k i 1 b b ibvai b costa k x c s 2 2 8 2 2 8 costa k x c s costa k x c s costa k x c s as claimed 32 now we are ready to verify that xb b is a coreset for k x a k theorem 5 12 xb b is an a a coreset for k x a k proof let a b b ve b and a b b ve b then a a further lemma 5 9 immediately yields costa k x f 2 cb s costa k xb cb s which shows condition a of definition 2 1 in order to verify b remember that it suffices to show costa k xb cb s 1 costa k x c s we combine lemmas 5 10 and 5 11 using costa k xb cb s costa k xb cb s a b b ve b costa k xb cb s a b b ve b this yields costa k xb cb s costa k x c s costa k x c s costa k x c s 1 costa k x c s which implies condition b of definition 2 1 finally we have all the ingredients with which to prove our main theorem proof of theorem 2 3 the assertion that x xb b is a coreset for the instance k x a k follows immediately from our previous results by means of lemma 3 6 more precisely we apply lemma 5 2 for 1 3 to obtain a linear coreset i e 1 1 and theorem 5 12 for 2 3 and 2 a a since 1 2 1 2 and max 1 2 a a 33 the assertion follows from lemma 3 6 6 large sensitivity while theorem 2 3 provides what is currently the best known bound for the size of deter ministic coresets for wca clustering we will round off the paper by addressing the question of whether it is possible to design even smaller coresets for wca clustering by applying techniques based on the notion of sensitivity within a probabilistic setting in importance sampling as applied to unconstrained least squares clustering points are sampled from the data set according to their relevance for the clustering cost the impor tance t xj of a point xj x called the sensitivity of xj is assessed as its contribution in the worst case to the cost of any feasible clustering more formally for the unconstrained case t xj sup s 1 cost x s k i 1 ij j xj si 22 where c s 11 kn argmin cost x c s c c k x and the supremum is taken over all sets s of k sites moreover t t k x n j 1 t xj is called total sensitivity note that t xj 1 hence t n a key requirement for successfully employing the concept of importance sampling for obtaining small coresets is however that t does not depend on the cardinality n of the data set but only on k see 6 lemma 2 2 in fact 26 theorem 3 1 showed that t o k and derived coresets of size nearly quadratic in k with high probability subsequently 9 theorem 6 7 used an improved bound to obtain coresets of size nearly linear in k via importance sampling to our knowledge this is the smallest dependence on k currently known while such techniques have been successfully applied to unconstrained least squares clustering it is not clear and it is indeed posed as an open question in 24 30 whether they still work in the presence of constraints for the cluster weights of course the notion of sensitivity can easily be generalized to the constrained case yielding tk xj by simply replacing cost x s by costk x s and taking the argmin of costk x c s for all c ck k x as the following example 6 1 shows the total sensitivity tk might however be as large as n hence approaches based on sampling require too large samples to be of any use for wca clustering example 6 1 for r 0 1 2 let x 1 xn r 2 be equally spaced on the circle with 34 radius r 1 centered at the origin and set x x 1 xn r 2 and 1 1 further let k 2 1 1 n 1 2 2 1 and k n 1 n 1 1 1 now we choose a point xj 0 x and set s 1 0 s 2 1 r xj 0 and s 0 s 1 s 2 see fig 2 we are interested in finding a clustering c c 1 c 2 ck 2 x that minimizes costk x c s 0 figure 2 construction of example 6 1 the data points are represented as black circles and the two sites s 1 0 and s 2 1 0 are indicated as pink squares the encircled data point xj 0 contributes most of the cost since xj 0 is the unique closest point of xj 0 to s 2 and c 2 requires exactly weight one the optimal clustering is unique each point is fully assigned to one cluster and c 1 contains the points of x xj 0 while c 2 consists of xj 0 more formally ij 1 for i j 1 j j n j 0 2 j 0 0 else and we obtain costk x s 0 2 i 1 n i 1 ij xj si 22 n 1 r 2 1 r 2 35 hence for the sensitivity tk xj 0 of xj 0 we know tk xj 0 sup s 1 costk x s 2 i 1 ij 0 xj 0 si 2 2 1 r 2 n 1 r 2 1 r 2 since lim r 0 tk xj 0 lim r 0 1 r 2 n 1 r 2 1 r 2 1 and xj 0 x was chosen arbitrarily we see that n tk tk k x n j 1 tk xj n proving theorem 2 5 note that in the instance of wca clustering of example 6 1 the weight constraints for the clusters prevent the assignment of each point to its nearest site in fact for sufficiently small r the one point assigned to the far away site s 2 contributes the most to the clustering cost since the sensitivity measures the worst case contribution over all choices of sites each of the n points can be decisive for the cost consequently importance sampling based on the above notion of sensitivity does not help in designing smaller coresets for wca clustering 7 final remarks our results provide small coresets for wca clustering i e in the weight constrained anisotropic case this allows to compute good clusterings for the generally much smaller sets x and to subsequently convert them to clusterings of the original data sets x while the corresponding extensions f preserve the cluster weights and guarantee the feasibility of the obtained clusterings we might however loose the favorable properties of compati bility with diagrams hence the question arises as to whether any efficiently computable extensions exist that map clusterings that admit a strictly strongly or just compatible diagram on the coresets to clusterings of the same type on the full data set alternatively we might ask if we can find a diagram on the coreset such that the induced clustering on the original data set only slightly violates the cluster size constraints this leads to a number of relevant stability issues one may also wonder whether other techniques might enable the design of even smaller coresets for wca clustering while theorem 2 5 gives a negative answer for techniques based on importance sampling even for weight constrained least squares clustering there might be other techniques that are better suited for constrained clustering recall that the high sensitivity of each data point in example 6 1 is a consequence of its definition as worst case 36 behavior of course the average or expected contribution of each point to the clustering cost taken over all choices of k sites is much smaller in this example therefore it might be reasonable to investigate whether such a weaker notion of average case sensitivity can be utilized to design improved coresets for the applications in materials science referred to in the introduction it might be worthwhile to point out that our construction and analysis in the proof of theorem 2 3 which has now reduced the dependency on k from cubic in 22 to quadratic does not come with a larger constant hidden in the big o notation in fact by representing each batch of points by its centroid the number of coreset points is actually reduced by half note that 22 states explicitly that this does not work in their setup reductions in the multiplicative constant play a decisive role in the grain map application in practice in terms of memory requirements and computation times we refer to 4 for a computational study of grain map reconstructions based on coresets finally let us point out that our approach is not limited to wca clustering in fact it will work as long as the extensions f preserve the feasibility for the constraint set this is the case for instance with fair clustering where points belong to different groups and clusters should not over or under represent any of the groups using a result by 24 that designing coresets for this problem with l groups can be reduced to the design of l coresets for points from a single group our results can be extended by creating batches per group references 1 ankit aggarwal amit deshpande and ravi kannan adaptive sampling for k means clustering in lecture notes in computer science vol 5687 lncs springer berlin heidelberg 2009 pp 15 28 doi 10 1007 978 3 642 03685 9 2 2 daniel aloise amit deshpande pierre hansen and preyas popat np hardness of euclidean sum of squares clustering in machine learning 75 2 may 2009 pp 245 248 doi 10 1007 s 10994 009 5103 0 3 andreas alpers andreas brieden peter gritzmann allan lyckegaard and hen ning friis poulsen generalized balanced power diagrams for 3 d representations of polycrystals in philosophical magazine 95 9 mar 2015 pp 1016 1028 doi 10 1080 14786435 2015 1015469 4 andreas alpers maximilian fiedler peter gritzmann and fabian klemm turning grain scans into diagrams tech rep 2022 5 pranjal awasthi moses charikar ravishankar krishnaswamy and ali kemal sinop the hardness of approximation of euclidean k means in symposium on compu tational geometry 2015 pp 1 14 6 olivier bachem mario lucic and andreas krause practical coreset constructions for machine learning in arxiv e prints 2017 37 7 sugato basu ian davidson and kiri lou wagstaff constrained clustering ad vances in algorithms theory and applications crc press 2009 p 472 8 steffen borgwardt andreas brieden and peter gritzmann geometric clustering for the consolidation of farmland and woodland in math intelligencer 26 2014 pp 37 44 9 vladimir braverman dan feldman and harry lang new frameworks for offline and streaming coreset constructions tech rep 2016 10 andreas brieden and peter gritzmann on optimal weighted balanced clusterings gravity bodies and power diagrams in siam journal on discrete mathematics 26 2 jan 2012 pp 415 434 doi 10 1137 110832707 11 andreas brieden and peter gritzmann predicting show rates in air cargo trans port in international conference on artificial intelligence and data analytics for air transportation aida at doi 10 1109 aida at 48540 2020 9049209 ieee singapore singapore february 3 4 2020 12 andreas brieden and peter gritzmann response prediction gaining reliable and interpretable insight from small study data in submitted 2021 13 p 13 andreas brieden peter gritzmann and fabian klemm constrained clustering via diagrams a unified theory and its application to electoral district design in euro pean journal of operational research 263 1 nov 2017 pp 18 34 doi 10 1016 j ejor 2017 04 018 14 ke chen on coresets for k median and k means clustering in metric and euclidean spaces and their applications in siam journal on computing 39 3 jan 2009 pp 923 947 doi 10 1137 070699007 15 michael b cohen sam elder cameron musco christopher musco and madalina persu dimensionality reduction for k means clustering and low rank approxima tion in proceedings of the forty seventh annual acm on symposium on theory of computing stoc 15 new york new york usa acm press 2015 pp 163 172 doi 10 1145 2746539 2746569 16 sanjoy dasgupta the hardness of k means clustering in technical report cs 2007 0890 university of california san diego cs 2008 091 2007 p 6 17 dan feldman and michael langberg a unified framework for approximating and clustering data in proceedings of the 43 rd annual acm symposium on theory of computing stoc 11 new york new york usa acm press 2011 p 569 doi 10 1145 1993636 1993712 18 dan feldman melanie schmidt and christian sohler turning big data into tiny data constant size coresets for k means pca and projective clustering in siam j comput 49 2020 pp 601 657 38 19 dan feldman melanie schmidt and christian sohler turning big data into tiny data constant size coresets for k means pca and projective clustering in pro ceedings of the twenty fourth annual acm siam symposium on discrete algo rithms philadelphia pa society for industrial and applied mathematics jan 2013 pp 1434 1453 doi 10 1137 1 9781611973105 103 20 hendrik fichtenberger marc gille melanie schmidt chris schwiegelshohn and christian sohler bico birch meets coresets for k means clustering in lec ture notes in computer science including subseries lecture notes in artificial in telligence and lecture notes in bioinformatics vol 8125 lncs springer berlin heidelberg 2013 pp 481 492 doi 10 1007 978 3 642 40450 4 41 21 peter gritzmann and victor klee computational convexity in crc handbook on discrete and computational geometry 3 rd extended ed eds j e goodman j o rourke and c d toth crc press boca raton florida 2017 pp 937 968 22 sariel har peled and akash kushal smaller coresets for k median and k means clustering in discrete computational geometry 37 1 jan 2007 pp 3 19 doi 10 1007 s 00454 006 1271 x 23 sariel har peled and soham mazumdar on coresets for k means and k median clustering in proceedings of the thirty sixth annual acm symposium on theory of computing stoc 04 new york new york usa acm press 2004 p 291 doi 10 1145 1007352 1007400 24 lingxiao huang shaofeng h c jiang and nisheeth k vishnoi coresets for clus tering with fairness constraints in advances in neural information processing systems 32 2019 25 tapas kanungo david m mount nathan s netanyahu christine d piatko ruth silverman and angela y wu a local search approximation algorithm for k means clustering in computational geometry theory and applications 28 2 3 spec iss june 2004 pp 89 112 doi 10 1016 j comgeo 2004 03 003 26 michael langberg and leonard j schulman universal approximators for inte grals in proceedings of the twenty first annual acm siam symposium on dis crete algorithms philadelphia pa society for industrial and applied mathematics jan 2010 pp 598 607 doi 10 1137 1 9781611973075 50 27 meena mahajan prajakta nimbhorkar and kasturi varadarajan the planar k means problem is np hard in theoretical computer science vol 442 elsevier 2012 pp 13 21 doi 10 1016 j tcs 2010 05 034 28 konstantin makarychev yury makarychev and ilya razenshteyn performance of johnson lindenstrauss transform for k means and k medians clustering in pro ceedings of the 51 st annual acm sigact symposium on theory of computing 39 stoc 2019 new york new york usa acm press nov 2019 pp 1027 1038 doi 10 1145 3313276 3316350 29 ji r matous ek lectures on discrete geometry ed by ji r matous ek vol 212 grad uate texts in mathematics new york ny springer new york 2002 doi 10 1007 978 1 4613 0039 7 30 melanie schmidt chris schwiegelshohn and christian sohler fair coresets and streaming algorithms for fair k means in lecture notes in computer science in cluding subseries lecture notes in artificial intelligence and lecture notes in bioin formatics 11926 lncs sept 2020 pp 232 251 doi 10 1007 978 3 030 39479 0 16 31 micha sharir and pankaj k agarwal davenport schinzel sequences and their ge ometric applications cambridge university press 1995 32 christian sohler and david p woodruff strong coresets for k median and subspace approximation goodbye dimension in 2018 ieee 59 th annual symposium on foundations of computer science focs ieee oct 2018 pp 802 813 doi 10 1109 focs 2018 00081 40