ar x iv 1 90 5 08 29 3 v 1 cs l g 2 0 m ay 2 01 9 issues concerning the realizability of blackwell optimal policies in reinforcement learning nicholas denis nick denis 1983 gmail com abstract n discount optimality was introduced as a hi erarchical form of policy and value function optimality with blackwell optimality lying at the top level of the hierarchy 17 3 we formalize notions of myopic discount factors value functions and policies in terms of black well optimality in mdps and we provide a novel concept of regret called blackwell re gret which measures the regret compared to a blackwell optimal policy our main analy sis focuses on long horizon mdps with sparse rewards we show that selecting the discount factor under which zero blackwell regret can be achieved becomes arbitrarily hard more over even with oracle knowledge of such a discount factor that can realize a blackwell regret free value function an blackwell op timal value function may not even be gain op timal difficulties associated with this class of problems is discussed and the notion of a pol icy gap is defined as the difference in expected return between a given policy and any other policy that differs at that state we prove cer tain properties related to this gap finally we provide experimental results that further sup port our theoretical results 1 1 introduction when is one policy better than another and how does one arrive at the best policy additionally is there a dif ference between the theoretical answers to these ques tions and how they are addressed in practice within the reinforcement learning and markov decision process community these questions are fundamental and nothing 1 work in progress new indeed though these questions have been well de fined and well studied this paper reconsiders important issues with solutions to mdps and rl problems specif ically we explore the role of the discount factor in find ing an optimal policy and value function v once is chosen though an approximately optimal solution may be returned by some algorithmic solution it may still be unsatisfactory in some regards as demonstrated by openai with the coastrunners domain in this paper we explore the relationship between in arriving at an optimal policy as well as a researchers preference or evaluation of such a policy we discuss issues surround ing selecting and without any domain knowledge of the problem and how even theoretically sound algo rithms such as pac mdp solution methods can produce policies that though satisfy being pac are still not even gain optimal especially difficult are long horizon problems lhps with sparse rewards motived by such problems we introduce a novel concept of regret called blackwell regret rb which compares the expected re turn of a given policy to that of a blackwell optimal pol icy evaluated at an appropriate value of 0 1 we believe blackwell regret is more akin to how humans ex perience regret when comparing oneself to the highest of standards we formalize the notion of myopic dis count factors and policies and introduce a notion of being blackwell realizable we discuss how policies that minimize blackwell regret are fundamentally difficult to solve for as recent literature has hinted at for long hori zon problems lhp s 10 this is due to the existence of pivot states where discovering the blackwell optimal policy hinges on discerning the values of a blackwell op timal policy and a non blackwell optimal policy which can be arbitrarily close at a given state even with oracle knowledge of the infimum that can induce a black well optimal and blackwell regret free policy 0 an accurate blackwell optimal policy may not be black well optimal and in fact may not even be gain optimal we provide experimental results using pac mdp algo rithms that demonstrate this phenomenon motivated by http arxiv org abs 1905 08293 v 1 these findings we argue the need for progress within three areas of theoretical research 1 analytical solution methods for blackwell optimal policies 2 provable con vergent algorithms for solving n discount optimal poli cies 3 goal based and human preference based rl our focus is on the latter 2 background 2 1 markov decision process recall that an mdp m is an n tuple s a p r where s is a finite state space a is a finite action space p p s s a is the transition kernel r s a s 0 rmax is the reward function and 0 1 is the discounted current value associated to one unit of reward to be received one unit of time into the future this work focusses on deterministic markovian policies 2 2 notation this work considers how plays a role both in learn ing a policy as well as how it is used in evaluating the value function associated to a policy perhaps learned with a different discout factor for this reason it is im portant to clearly separate used to learn a policy and 6 used to evaluate that policy hence by we refer to a policy learned using whereas v 1 2 s refers to the value of a state when following policy learned using 1 as defined just previously however the value function is computed using 2 symbolically v 1 2 st e 1 k t k t 2 rk 2 3 optimality in mdp s if 1 then we are considering undiscounted re wards and for any infinite stream of rewards gt e rt rt 1 rt 2 e k t rk since gt is often infinite the gain of a policy is defined where lim t 1 t e t t 1 rt using the gain of a policy an ordering is defined on some policy class where 1 2 1 2 1 2 with the strict inequality defined similarily it is worth noting that if we define r r 1 r 2 as the sequence of expected rewards from following pol icy then for any permutation n n any pol icy whose sequence of expected rewards is r r 1 r 2 r 1 r 2 r then hence the temporal ordering of rewards has no bearing on the value or gain of a policy when 1 this is certainly not true for 1 most commonly 0 1 in this setting we can deal with infinite series of expected rewards as the partial sums converge geometrically fast in the value of a state when considering discount factor is v st e k t k trk since most frameworks assume rewards are bounded in some interval 0 rmax then s v s v max rmax 1 such assumptions and the use of vmax are in tegral to theoretical bounds for algorithms and solution methods in rl and mdp s similar to the ordering on policies in the undiscounted setting an ordering of poli cies for fixed 0 1 is used to order policies 1 2 unlike the undiscounted setting under 1 two policies are not equivalent under permuta tion of the temporal sequence of rewards interestingly a value of 0 is rarely used in the literature and is of ten called myopic with 0 the induced policy does not sufficiently account for the future horizon and in do ing so is generally viewed to only lead to sub optimal behaviour 0 1 we say a policy is optimal if v s v s s where v 1 1 despite these notions of optimality being the most common in rl there are other notions of optimality 13 2 3 1 bias optimality bias optimality was introduced to supplement the use of gain optimal policies when 1 since the gain of a policy only considers the asymptotic behaviour of a policy two policies that have the same gain may expe rience different reward trajectories before arriving at the stationary distribution of the policy for this reason the bias of a policy defined as b s lim t e t t 1 rt s and was introduced by 3 for any finite state and action space mdp a bias optimal policy always exists 2 3 2 n discount optimality n discount optimality 17 introduces a hierarchical view of policy optimality in mdps a policy is n discount optimal for n 1 0 1 2 3 if s s and lim 1 1 n v s v s 0 s 0 s 1 sh 1 0 0 0 0 0 0 1 figure 1 distracting long horizon mdp example for h 1 and initial state s 0 it has been shown 17 that a policy is 1 discount op timal it is gain optimal and a policy is 0 discount optimal it is bias optimal moreover if a pol icy is n discount optimal then it is m discount optimal m 1 0 n the strongest and most selective notion of optimality is that of being n discount op timal n 1 such a policy is referred to as being discount optimal 2 3 3 blackwell optimality a policy is blackwell optimal if 0 1 such that v s v s 1 s s for finite state spaces such a is attained 13 intu itively a blackwell optimal policy is one that upon con sidering sufficiently far into the future as encoded as a planning horizon via no other policy has a higher expected cumulative reward 17 showed that a policy is discount optimal it is blackwell optimal hence blackwell optimality implies all other forms of optimal ity and for this reason is the focus of this work finally 3 shows that for finite state and action space mdps there always exists a stationary and deterministic black well optimal policy 3 motivation for blackwell regret consider the infinite horizon mdp in figure 1 with initial state s 0 before proceeding consider what you would do if you were in this mdp what do you think is the best policy what sort of solution would you hope that an rl algorithm return to you and how did you come to this conclusion in wanting to maximize cumulative reward it is hard to argue with any other action selection policy for the provided example than to always move right towards the state sh and upon doing so remain there why might someone consider any other policy why might a rational agent with full oracle knowledge of the mdp consider staying in s 0 to receive a reward of 1 at every time step for perpetuity it is hard to account for why such a policy would be preferred over the policy that takes the agent to sh aside from laziness compu tationally v stay v right h hence depending on h rmax the policy induced by can be set appropriately in order to induce the desired policy behaviour returning to 0 it is widely accepted within the literature that is myopic we ask if it is possible for to be myopic for 6 0 is 10 1000 myopic 10 999 if we abstract what makes myopic it is the fact that is not sufficiently large so as to provide the agent with the possibility of properly assessing the op timal value of states and actions where this optimality is in some sense not defined with respect to learn the used during learning but rather with respect to some ideal policy or behaviour just as a child might seek to maximize immediate gratification rewards by eating candy before bed which may be optimal given 0 the role of a parent will be to convey the non optimality of such a policy by noting that the yet to be experienced consequences poor sleep fussy behavior the following day which can only be taken into consideration with 0 this is paradoxical for the child as they op erate under 0 eatcandy and hence v eatcandy 0 is optimal from the perspective of 0 the lesson the parent tries to impart to the child is to use 0 so that the child can learn in this way we intu itively compare v to v it is this intuition that we seek to formalize by noting that eating candy before bed does not sufficiently value the future and for this reason we attempt to resist this myopic behaviour in order to do so sufficiently valuing the future means selecting a suitable 0 1 we argue that this sufficiency is rep resented by the 0 1 as found in the definition of a blackwell optimal policy and value function we argue that the myopic behaviour intuitively is de fined with respect to the strongest sense of optimality blackwell optimality note that 0 1 13 so why then is dismissed as myopic for 0 it is still after all an optimal policy we believe that this oc curs since we intuitively understand that not all optimal policies are equal it appears that all optimal policies are optimal but some are more optimal than others that is though is optimal under not all s induce the policies or behaviours that a researcher prefers this clearly highlights a common issue in machine learning that of using a given objective function as a surrogate representation for what we want the algorithm to do the hierarchical nature of policy optimality as expressed by n discount optimality naturally captures this phe nomenon and we revisit this body of literature to help motivate why our sense of being myopic has nothing to do with not being capable of finding but rather not finding the that characterizes a blackwell optimal policy we introduce a novel notion of regret called blackwell regret and relate the concept of a my opic and policy to blackwell regret our work looks at a simple class of mdps called distracting long hori zon mdps and show that even for such a simple class of environments it is arbitrarily hard to select a so as to arrive at a blackwell optimal policy and value function that achieves zero blackwell regret 4 myopic blackwell realizable and blackwell regret looking at the mdp s in figure 1 we intuitively get a sense of what the right policy is and we agree that 0 is myopic and will not produce the optimal policy more over we can check that 0 h will suffer the same drawback since no formal definition of a myopic can be found in the literature we provide a definition definition myopic and blackwell regret let denote a blackwell optimal policy let be as defined above as for blackwell optimality such that v s v s 1 s s then for 0 we say is myopic similarly a policy is myopic if it is learned using a myopic simi larly we say for that is blackwell realizable for learn 0 1 we define blackwell regret rb let max learn then for a given policy learn rb learn e v s v learn s where the expectation is taken over initial state distri bution hence blackwell regret is the regret accrued for using a given policy learned with learn when com pared to a blackwell optimal policy since it may be that learn to ensure commensurability we require that max learn in the definition since under non negative rewards fixed 0 1 s s we see that v s v s it immediately follows that if is myopic and is the optimal policy induced by then rb 0 we see in the following lemma that blackwell regret captures the very notion exempli fied in the child parent example previously given in that for the blackwell regret is simply the regret computed using the regret of a given policy with value evaluated at is defined as r e v s v s lemma 1 let 0 as defined in blackwell optimality then rb r previous definitions of regret measure the difference in value of a given policy learn and the optimal value function each evaluated with respect to a fixed learn since v learn s v learn s s s as well the learn used to learn the policy is typically also used to evaluate the value of that policy and thus the regret blackwell regret differs in that it measures the difference in value of a given policy learn and a blackwell opti mal policy evaluated at max learn that fa vors a blackwell optimal policy and value function in doing so a policy that achieves zero blackwell regret is either itself blackwell optimal or when considering a sufficiently long time horizon as encoded by has the same value as a blackwell optimal policy 5 difficulty in selecting blackwell realizable when implementing an rl algorithm that incorpo rates discounting typically no reasoning is provided to explain the choice of used though most often val ues of are set around 0 9 may be treated as a hy perparameter and a grid search over values may be per formed however even under these settings the prob ability measure of non myopic s can become vanish ingly small for various types of problems such as lhp s and sparse reward problems hence any randomized selection approach can have a vanishingly small proba bility of achieving non zero blackwell regret as for the example in figure 1 as h grows we show that selecting a non myopic that is selecting a blackwell realizable is quite difficult without oracle knowledge of the prob lem moreover even using a blackwell realizable an optimal policy may not even be gain optimal let alone blackwell optimal ultimately we would like to consider mdp environ ments of a particular nature conducive to multi task rl problems the environments problems we are inter ested in are those such that for every task assigned to the agent the optimal policy for that task induces a parti tion of the state space into non empty subsets of transient and recurrent states st sa this is equivalent to say ing that for each task the optimal policy associated to the task induces a markov chain on s which is unichain or that the environment is multichain 13 the intuition is that the environment is sufficiently controllable in the sense that the agent can direct the environment towards some preferable subset of the state space and stay there indefinitely if needed as encoded by the task mdp for this paper we will consider a particular subset of such environments where there are only two regions of the state space that produce non zero rewards and these two regions are maximally separated from one another we demonstrate that even for such a simple class of mdps selecting blackwell realizable discount factors can be ar bitrarily hard more formally we consider the class of mdps with finite diameters that is d such that d max s 6 s s min e s s where s s is the first hitting time of s when start ing in state s under hence within the class of en vironments considered it is possible to reach any state from any other starting state and do so in a finite num ber of actions in expectation under some policy fur thermore denote sd s and sh s two states that realize the diameter d suppose 0 rd rmax and a a a such that r sd a sd rd and r sh a sh rmax and all other rewards are zero e g r 0 rd rmax moreover p sd a sd p sh a sh 1 though this structure is quite spe cific it abstractly represents two regions of the state space where actions exist that allow the agent to remain in those respective regions and while remaining in that such region receive on average a positive rewards rd and rmax respectively an example of such an mdp can be seen in figure 1 we call these particular envi ronments distracting long horizon problems in the sense that due to the nature of the long horizon problem the high reward region of the environment is many time steps away from an arbitrarily low reward region of the envi ronment with the rest of the environment producing no rewards given a state such as s 0 in figure 1 under a blackwell optimal policy the agent will not be dis tracted by the nearby yet miniscule rewards and will traverse to the high reward region sh this setting is a slight step up in complexity from a simple goal based mdp where only a single state produces a positive re ward we show that with oracle domain knowledge of features of the mdp which we state below one can select a blackwell realizable and solve for in such distracting mdps however even with oracle knowledge of 0 selected one may receive an optimal policy and value function that is not gain optimal since for lhps the value of a gain optimal policy and black well optimal policy may differ by less than interest ingly these results suggest a multi step learning process for distracting mdp problems may be possible which we leave for future work we start with a proposition that shows that for this class of mdps being blackwell optimal are exactly those policies that are not distracted in the sense that they are those policies that act solely to minimize the hit ting time of the high reward state sh proposition 2 let m be a distracting long horizon mdp as described above then is blackwell optimal argmin e s sh s s we now provide results that show with oracle knowl edge of d rd rmax we may select for and thus for a blackwell realizable discount factor corollary 3 for any distracting long horizon mdp m as described above if d rd rmax is known then an rl algorithm can select and hence select a black well realizable discount factor the following corollary shows that with oracle knowl edge of only two of the following properties d rd and rmax then after committing to particular 0 1 there exists a distracting long horizon mdp that is con sistent with those mdp properties wherein is not gain optimal but 2 2 is blackwell optimal corollary 4 suppose for every distracting long horizon mdp m as described above only two of d rd rmax is known let k d rd rmax k 2 denote the mdp features known with oracle knowledge then 0 1 m consistent with k such that is not gain optimal but 1 is blackwell optimal these corollaries demonstrate that there exists sufficient domain knowledge for distracting long horizon mdps to allow for the computation and use of a blackwell real izable however without complete domain knowledge of m any selected may be myopic and may not even lead to a gain optimal policyl these results suggest for distracting long horizon mdps that a multi step learning approach may be best where in the first phase the agent learns the d rd rmax and then in the second phase uses this knowledge to select for a non myopic to solve the task however we leave such results for future work the next results show that even under with access to a blackwell realizable for distracting long horizon prob lems then the value of a policy that is not gain optimal and that of a blackwell optimal policy may be arbitrarily close e g within hence any learning algorithm that returns a policy that is accurate to a blackwell optimal policy may not even be gain optimal further we provide empirical results that mirror our theoretical results corollary 5 let 0 a distracting mdp m with blackwell optimal 0 1 and associated blackwell optimal policy such that v v where is not gain optimal 5 1 policy gaps and pivot states prior work has been done in putting forward measure ments that can act as indicators of when learning an op timal policy may be difficult 10 2 2 discuss the notion of an action gap at a given state s that is the dif ference in expected value at that state between the opti mal action and the second best action more formally let a s a s then ag s v s max a a s q s a 10 introduce the notion of the maximal action gap mag of a policy as mag s max s s max a a q s a min a a q s a both studies argue that if their respective measurement is small then learning the optimal policy can be hard as it is hard to discern the value of the optimal action from one that is sub optimal while each may be useful we argue that since the action gap measures the difference in value associated with abstaining just once from taking the optimal action it doesn t truly measure the differ ence in value between two policies nor the associated difficulty in discerning the value of one policy over an other the maximal action gap suffers from this as well moreover 10 that under certain conditions the maximal action gap collapses to zero making learning arbitrarily hard however in the appendix section we prove that this condition only occurs in environments where the set of states that receive non zero rewards must be transient we introduce a novel measurement the policy gap which is motivated by the action gap discussed above for s a p r fixed policy and s s we define the policy gap pg s pg s min s 6 s v s v s the policy gap at state s is the smallest difference in value at that state between the query policy and any other policy that differs at s intuitively if s pg s is large then the ability to discern the optimal action and thereby learn a blackwell optimal policy becomes eas ier conversely if s s sucht that pg s 0 then at state s called a pivot state the ability to discern the value of a blackwell optimal policy and another policy becomes increasingly hard for an mdp where blackwell optimal policies are non trivial that is not all policies are blackwell optimal and therefore 0 then there exists such a pivot state for the theorem be low we use for a blackwell optimal policy and for any we use v to represent the value function computed follow the blackwell optimal policy and with discount factor theorem 6 pivot state existence let be a non trivial blackwell optimal policy with 0 1 where as defined above such that v s v s s 1 if a pivot state s s where s 6 s and v s v s v s v s moreover lim pg s 0 theorem 7 shows that for values close to there exists a pivot state such that the value of a blackwell op timal policy at that state when computed with is ar bitrarily close to the value of a different non blackwell optimal policy at the same state when computed with intuitively if the policy gap is arbitrarily close to zero an rl algorithm is expected to have a greater difficulty evaluating the difference in value associated to such poli cies and therefore have a greater difficulty in determin ing which is optimal these results may suggest that without oracle knowledge of an algorithm that at tempts to search for by increasing iteratively would have increasing difficulty as 6 experimental results in this section we provide experimental results that further illustrate the phenomena discussed in previous sections we investigate the difficulty of solving for blackwell optimal policies in distracting long horizon mdps similar to those in figure 1 for these exper iments we use the mdp in figure 2 with initial state sd we analytically solve for and implement the de layed q learning pac mdp algorithm 16 for our ex periments we use 0 85 2 0 84724541 1 in two sets of experiments with 1 10 10 for our experiments we use 0 1 and error toler ance 1 0 05 2 0 1 where indices for coin cide for experiments we run each set of experiments with a different random seed for 5 runs the delayed q learning algorithms terminates when algorithm either finds itself in state sd greedily selecting a 1 and the learn s a boolean flag is false or the algorithm finds itself in state sh greedily selecting a 2 and the learn s a boolean flag is false both situations indicate no further learning is possible and the algorithm has converged on the optimal policy for the first set of experiments with 1 and 1 0 05 the mean sample complexity required for convergence was 2 604679 x 1010 11219 2 in each of the five experiments the policy learned 1 sd a 1 1 sh a 2 which is not blackwell optimal more sd sh a 1 rd 0 1 a 2 p 1 500 a 2 p 499 500 a 1 a 2 rmax 1 figure 2 distracting lhp actions are deterministic ex cept for a 2 from state sd only non zero rewards are rd and rmax over the algorithms terminates in state sd hence the pol icy is not even gain optimal the policy gap at sd was also measured and the mean policy gap pg sd 4 56 x 10 4 1 39 x 10 5 in the second set of experi ments using 2 2 the mean sample complexity across 5 runs was 1 00433 x 1011 7 223 x 108 in each of the five experiments the policy learned was the blackwell optimal policy and the mean policy gap pg sd 2 5395 x 10 3 1 6865 x 10 5 these results corroborate the theoretical results ob tained first we see that for the experiments with 1 which is much closer to we find that the policy gap at sd is much smaller than when compared to under 2 1 as predicted by the theoretical results stated more importantly despite having oracle knowledge of and selecting a blackwell realizable 1 and imple menting a pac mdp algorithm with commonly used values of no implementation returned a blackwell optimal policy and in fact did not even return a gain optimal policy these results further support the diffi culty in arriving at blackwell optimal policies for dis tracting long horizon mdps however for 2 such that 2 0 015275 the blackwell optimal policy was returned in all experiments though a positive result in some regards it is also suggests that for distracting long horizon mdps where 1 where the lebesque mea sure 1 0 having the luxury of randomly se lecting 0 1 such that gamma becomes ar bitrarily hard finally these results corroborate the theo retical results showing the existence of a state where the policy gap approaches zero and that even with with a commonly used error tolerance value the optimal policy returned by a pac mdp algorithm was not even gain optimal 7 discussion and related work the topic of effects of selection on policy quality has been of interest for several decades 17 3 13 with n discount optimality and blackwell optimality providing a global perspective on this relationship these works rec ognize that for discounting an optimal policy may not be blackwell optimal and recognize that this problem is alleviated for 1 however there do not exist any known convergent algorithms with theoretical guarantees for the undiscounted setting 3 also showed that for fi nite mdps as 1 the discounted value function can be written as a laurent series expansion where each of the terms in this series is a scaled notion of optimal ity with the first term being the gain the second the bias and so on using this construction 13 17 show there is both a sequence of nested equations for solving for the laurent series coefficients as well as a policy iteration method that is provably convergent for such a policy sat isfying these equations for any finite term approximation of the laurent series more recently 11 utilized an ex citing approach in function approximation by construct ing value functions using basis functions comprised of terms found within the laurent series expansion 9 studies the relationship of and reward functions with policy quality for goal based mdps they argue that 1 an agent is not risk averse and prove that in the undiscounted setting and r 1 s a s an agent is guaranteed to arrive at the goal state however with 1 this is not so as a shorter yet riskier path that may lead to non goal absorbing state can have higher value than a longer safer path to the goal 12 6 7 are motivated by showing that using smaller values may be advantageous besides having faster convergence rates they argue smaller values may also have better error by decomposing the error or value difference between policies induced with different values these decompo sitions have error terms dependant on the smaller term and another term that goes to zero as the two values approach each other these works argue that the best strategy is to find an intermediate value that trades off the two terms however as is often the case in theoretical analysis of rl problems the bounds are stated in terms of vmax and for various values of are vacuous as the bounds are higher than the absolute max error of vmax e g one policy only receiving zero rewards and another always receiving rmax however when the bounds are meaningful without knowledge of the blackwell opti mal policy and associated value function as shown in this study even an optimal policy may not even be gain optimal for 0 1 10 define a hypothesis class h and show that h v rs s rmax 1 under this framework for a family of hypothesis classes h indexed by 0 1 we see that as increases h is a monotonically increasing sequence of hy pothesis classes 10 6 formalize that this also corre sponds to an increase in measure of complexity via the generalized rademacher complexity r h depends only on vmax that is r h rmax 2 1 as examined in 10 long horizon mdps suffer in that may be arbitrarily close to 1 which implies the com plexity of realizable hypothesis classes for lhps grows non linearly with the horizon size and 6 argue that using learn is therefore a mechanism akin to regularization by selecting for a lower complexity hypothesis class one can prevent overfitting their re sults suggest using smaller earlier in learning however as discussed here the quality of being small or large is problem dependant and without oracle knowledge of the problem is meaningless though 6 does not con sider blackwell optimality an interesting result theo rem 2 can easily be adapted here which shows that for learn the loss as measured by for an approx imately optimal policy using and n samples from each s a s a follows with probabiliy 1 v v 1 1 rmax 2 rmax 1 2 1 2 n log 2 s a 6 argue that the tradoff between the two terms involves controlling the complexity of the policy class using a smaller versus the error induced in the first time when using a smaller our results show that even as n and the second error term goes to zero the first error term is fixed for any fixed and that without strong do main knowledge even an optimal approximate policy may not even be gain optimal 15 recently suggested nets a function approximation architecture that trains us ing a set of discount factors to learn value functions with respect to several timescales the idea being that the ap proximation architecture can generalize and approximate the value of a state for any if sufficiently trained the work presented here suggests that without consid ering blackwell optimality and related concepts theoret ical bounds on value functions in rl may not provide meaningful and interpretable semantics with respect to the optimality of the resulting policy an apt metaphor is that for a daredevil jumping across a canyon coming close to being successful is arbitrarily bad in that vein our results show that for lhps an blackwell optimal policy may not even be gain optimal in contrast in the supervised learning setting one may search over a par ticular hypothesis class and and arrive at some locally or globally optimal hypothesis h x y which obtains empirical accuracy of ptrain ptest 0 1 on the train ing and test datasets respectively once a classifier is obtained though one may not know that the bayes op timal classifier risk may be one does know that it can at most achieve 100 accuracy and hence in absolute terms one can obtain meaning from the test and training accuracy of a classifier returned by some sl algorithm however the rl setting is not similar in these regards without oracle knowledge of the rl problem the policy and value function returned by an rl algorithm param eterized by and any other parameters it is hard to say just how optimal such a policy in fact is thereby leaving a researcher in the same boat as the fictitious rl agent with results that are evaluative not instructive given that discounting has such a strong effect on the induced hypothesis class one may ask why discount ing is even used authors often cite concepts from util ity theory such as inflation and interest to motivate the use of discounting such concepts for temporal valua tion may be useful for agents such as humans with fi nite time horizons however such intuitions may not nec essarily be commensurable for infinite horizon agents the use of discounting in economic models is also of contention 18 for economic and environmental poli cies how should we discount the value of having a clean environment is discounting the future ethical in such settings might discounting the future lead us to an arbi trarily bad absorbing state utility theory has considered several qualities two utilitiy streams rt t 1 r t t 1 may posses in forming binary relations used as order ings on value functions utility streams 8 including that of anonymity which essentially states that two util ity streams are equal under an ordering if they are per mutations of one another hence anonymity can only be realized in the rl setting if 1 these works introduce and argue for the use of blackwell optimal ity in economics research 14 answers the question why discounting is used because it turns an infinite sum into a finite one that is it allows us to consider con vergent series and therefore algorithms it then follows that we are not selecting for but rather for policies that are representable by convergent algorithms if rl algorithms are to be used and incor porated in real world processes and products we raise the rhetorical question what are the moral and ethical implications of purposefully running a sub optimal infi nite horizon algorithm in perpetuity the results provided in this paper suggest that itera tive methods at arriving at are problematic suggest ing a need for analytical methods of computing how ever even with an approximately optimal policy may not even produce a gain optimal policy for lhps as 1 since even using shares this unfortunate result as demonstrated empirically in our experiments what can be done to ensure solving for the blackwell op timal policy recent advances in pac mdp algorithms 4 introduce pac uniform learning pac algorithms that are optimal simultaneously such algorithms must never explore then commit 4 but rather must never stop learning as it has been shown that such approaches are necessarily sub optimal 5 an interesting direction would be to consider the use of such algorithms for ar riving at blackwell optimal policies though blackwell optimality is an ideal for non trivial lhps it is possible that blackwell optimal poli cies are hard to discern from policies that may not even be gain optimal with such results being so dire we sug gest three main areas of focus for future research within the rl community 1 development of convergent algo rithms for solving for n discount optimal policies with theoretical bounds and efficient solution methods for ar riving at the laurent series expansion of a discounted value function as 1 2 analytical solutions to 3 human preference and goal based rl our main focus is on the third area of focus mentioned above for any applied rl solution for example a com mercial product that relies on rl we argue that ulti mately the quality of a policy is judged by human prefer ences those implementing an rl solution method will receive a policy and a value function and must evaluate if it is a sufficient solution to the given problem or not if not the researcher will experiment with other param eters including and repeat until a policy is found that is sufficient we call such an aproach based on human preference and may be separate from the value function itself and solely dependent on the behaviour of the pol icy this can be seen by the works and discussions made recently 1 based on results on the coastrunners do main coastrunners is a video game where the policy controls a boat in a racing game the policy solved for by openai resulted in the boat driving in circles collecting rewards rather than racing to the finish line and complet ing the race though openai uses this as an example of a pathological behaviour induced by a faulty reward func tion it can viewed as the induced behaviour by using a myopic in a distracting lhp openai and most others would agree that the behaviour observed was pathologi cal however what makes it pathological in fact it was the optimal policy solved for given the encoding of the mdp we argue that what makes this pathological is sim ply that the policy didn t do what the researchers wanted it to do which was to win the race for this reason at this current state in rl research we claim the ultimately the quality of policies solved for are measured by their being deemed sufficient as subjectively defined by the researcher we claim that this is equivalent to the re searcher ultimately desiring something from the solved policy and hence if this can be encoded as an indicator function then goal based rl problems should be used being some of the simplest classes of mdp problems acknowledgments the authors would like to thank maia fraser for dis cussions and thoughtful edits of prior versions of this manuscript 8 references 1 openai blog https blog openai com faulty reward functions 2 bellemare m ostrovski g guez a thomas p munos r increasing the action gap new op erators for reinforcement learning in aaai pp 14761483 2016 3 blackwell d discrete dynammic programming annals of mathematical stastics 33 719726 1962 4 dann c lattimore t brunskill e unifying pac and regret uniform pac bounds for episodic re inforce ment learning in neural information pro cessing sys tems 2016 5 garivier a kaufmann e on explore then com mit strategies in neural information process ing sys tems 2017 6 jiang n kulesza a singh s lewis r the de pendence of effective planning horizon on model accu racy in aamas vol 14 2015 7 jiang n singh s tewari a on structural prop erties of mdps that bound loss due to shallow plan ning in ijcai 2016 8 jonsson a voorneveld m the limit of dis counted utilitarianism theoretical economics 13 1937 2018 9 koenig s liu y the interaction of representa tions and planning objectives for decision theoretic plan ning tasks journal of experimental and theo retical artificial intelligence 14 303326 2002 10 lehnert l laroche r van seijen h on value function representation of long horizon problems in 32 nd aaai conference on artificial intelli gence pp 34573465 2018 11 mahadevan s liu b basis construction from power series expansions of value functions in lafferty j d williams c k i shawe taylor j zemel r s culotta a eds advances in neu ral information pro cessing systems 23 pp 1540 1548 curran associates inc 2010 12 petrik m scherrer b biasing approximate dy namic programming with a lower discount factor in in advances in neural information processing systems pp 12651272 2009 13 puterman m markov decision processes dis crete stochastic dynammic programming john wi ley and sons inc 1994 14 schwartz a a reinforcement learning method for maximizing undiscounted rewards in icml 1993 15 sherstan c macglashan j pilarski p gener alizing value estimation over timescale in pre dic tion and generative modeling in reinforcement learn ing workshop faim 2018 16 strehl a li l wiewiora e langford j littman m pac model free reinforcement learn ing in icml pp 881888 2006 17 veinott a discrete dynammic programming with sensitive discount optimality criteria annals of mathe matical stastics 40 1635166 1969 18 weitzman m gamma discounting american economic review 91 260271 2001 9 appendix a comment on bounds related to the maximal ac tion gap 10 define sc s as a fully connected subset of the state space despite the use of the term fully connected which was intended to describe a subet of the state space that is reachable from anywhere within that subset a more appropriate term is communicating as fully connected has connotations that s s sc a a such that p s s a 0 for this reason we will use the term communicating to describe sc from this they define vmax max s sc v s lemma 2 of 10 states mag sc 1 dsc 1 vmax where dsc is the diameter of sc from this it is stated that if vmax is bounded as 1 then 1 implies that mag 0 though this implication is true we show that vmax is bounded as 1 if and only if under all policies the expected number of times a non zero reward is obtained under is finite this means that under all policies all non zero rewards are transient hence such a result applies to a rather vacuous subset of mdps proposition 7 let m s a p r be an mdp such that s and a then m lim 1 vmax m e t 1 1 rt 6 0 proof suppose m such that vmax m as lim 1 wlog since r 0 rmax we may assume rmax 0 since otherwise this statement is trivial clearly s 6 sc since otherwise as there exists at least one transition that induces a non zero reward r 0 then at worst a policy may traverse the entire diameter of sc to receive a reward r and do so for perpetuity that is vmax max s sc v s r dsc r 2 dsc t 1 r tdsc r dsc t 0 tdsc r dsc 1 dsc but clearly m r dsc 1 dsc m r m dsc r dsc m r dsc 1 m r m r m dsc m r m 1 dsc so for m r m 1 dsc we have m vmax this shows that s 6 sc hence for sc s it must be that sc is transient under since otherwise if t n such that t t st sc then again by the same argument above 0 1 such that vmax m hence sc must be transient under now since s then s sc hence for sa s sc such that sa is irreducible and positive recurrent e g absorbing we claim that there must not be any possible non zero rewards within sa let t max s s e s sa be the maximum ex pected first hitting time of reaching the absorbing subset of the state space sa under by a similar argument as above there cannot be any positive rewards in sa since otherwise sa sa such that v sa as 1 if this is true then s sc such that vmax max s sc v s tv sa and therefore vmax as 1 hence as 1 obtains non zero rewards for only a finite number of time steps due to the optimality of then this must be true for any policy hence it must be that all rewards in m are transient for the reverse implication suppose that e t 1 1 rt 6 0 let t be defined as above as the maximum expected hitting time of the ab sorbing subset sa which contains no non zero rewards sa must exist by a similar argument as above then we have s s v s t t 1 rmax t 1 trmax hence vmax is bounded as 1 the maximum action gap bounds collapse to zero for an infinite horizon problem as 1 but only for en vironments where all the rewards are transient 10 ar gue that representing the value function for such class of mdps is quite difficult as 1 however such a class of environments are best solved using episodic mdp ap proaches with 1 since for any policy the number of time steps where a positive reward is possible is finite then finding an optimal policy is only relevant for the first t time steps since afterwards the behaviour be comes irrelevant 13 shows such domains can be con verted to undiscounted episodic tasks in doing so the hypothesis space is completely different as only value functions v 0 rmaxt s need be considered which have no dependancy on hence the radamacher com plexity results stated previously do not apply here a multi step learning approach of first learning t then ap plying an episodic rl algorithmic approach is ideal for such environments proof of lemma 1 proof let be a blackwell optimal policy with associ ated note that follows from the hypothesis and definition of for blackwell regret then rb e v s v s e v s v s v s v s e v s v s e v s v s 0 e v s v s r proof of proposition 2 proof let be a blackwell optimal policy then it is bias optimal which clearly must minimize the expected hitting time of sh for the reverse implication let be the policy that minimizes the expected hitting time of sh let d rd rmax with d rd rmax defined in the text then it follows that since otherwise such that v sd v sd it clearly follows that under any policy v v and therefore is a blackwell optimal policy proof of corollary 3 proof let m be a distracting mdp as described above with d rd rmax known to the algorithm let be the blackwell optimal policy learned and evaluated with by the previous proposition then is the policy that takes the shortest path from any state to sh and as given in the proof of said proposition d rd rmax moreover from proposition 2 6 v sd v sd this follows for any policy that does not minimize the expected first hitting time of sh hence d rd rmax hence with knowledge of d rd rmax can be computed and therefore a realiz able discount factor may be selected proof of corollary 4 proof first given rd rmax and let 0 1 for sd sh as defined above it suffices to show as in the pre vious proposition d 0 for the induced optimal policies and the blackwell optimal pol icy v sd rd 1 rmax d 1 v sd but v sd rd 1 rmax d 1 v sd hence it suffices to show d v sd rd 1 rmax d 1 v sd rmax d 1 v sd let d sup d d log rd rmax log and set d rd rmax then d satisfy the claim and with initial state distribution being a point mass at sd we have is not gain optimal as rd but it follows that is blackwell optimal without loss of generality the same proof technique can be applied when either rd d are known and 0 1 is fixed as well as if rmax d are known and 0 1 is fixed proof of corollary 5 proof this follows as a corollary from theorem 6 and proposition 2 since a pivot state s where the policy gap vanishes it is easy to see that under proposition 2 and the previous two corollaries that followed sd is a pivot state let equal the blackwell optimal pol icy at every state except sd astay noting that r sd astay sd rd then is not gain optimal as rd rmax yet s s sd v s v s and for sd we see that v sd rd 1 while v sd d rmax 1 then rd d rmax can be set such that 0 v v proof of theorem 6 proof let by definition of blackwell opti mality then s s such that v s v s moreover since all rewards are non negative s s 1 2 it follows that v 1 s v 2 s that is in creasing while keeping the policy constant can only increase the magnitude of the value function hence we have as well v s v s and v s v s together we see that v s v s v s v s it remains to show that s 6 s and lim pg s 0 we may assume the former since if infact s s then v s e r s s v s e r s s v s v s e r s s v s e r s s v s e v s e v s e v s e v s since the expectation is taken over mdp dynamics and both policies selected the same action at s then distri bution over successor states are the same if there are no successor states s where s 6 s then this inequality continues to the successors of the successor states however this process cannot continue indefi nitely since otherwise the two markov chains induced by and beginning at s are therefore coupled and with the same dyamics and must have the same value therefore the two policies must differ at atleast one state where the preceeding value function inequality is true for this reason wlog we assume this state is s finally to show lim pg s 0 this directly fol lows as we have v s v s v s v s 0 v s v s v s v s 0 pg s v s v s v s v s 0 lim pg s lim v s v s 0 lim pg s lim e t 1 t 1 rt t 1 rt 0 lim pg s lim e t 1 t 1 t 1 rt since lim e t 1 t 1 t 1 rt 0 it follows that lim pg s 0 1 introduction 2 background 2 1 markov decision process 2 2 notation 2 3 optimality in mdp s 2 3 1 bias optimality 2 3 2 n discount optimality 2 3 3 blackwell optimality 3 motivation for blackwell regret 4 myopic blackwell realizable and blackwell regret 5 difficulty in selecting blackwell realizable 5 1 policy gaps and pivot states 6 experimental results 7 discussion and related work 8 references 9 appendix