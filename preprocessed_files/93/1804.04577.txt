arxiv 1804 04577 v 3 cs lg 21 aug 2018 ar x iv 1 80 4 04 57 7 v 3 cs l g 2 1 a ug 2 01 8 april 2018 revised august 2018 mit lids report feature based aggregation and deep reinforcement learning a survey and some new implementations dimitri p bertsekas abstract in this paper we discuss policy iteration methods for approximate solution of a finite state discounted markov decision problem with a focus on feature based aggregation methods and their connection with deep reinforcement learning schemes we introduce features of the states of the original problem and we formulate a smaller aggregate markov decision problem whose states relate to the features we discuss properties and possible implementations of this type of aggregation including a new approach to approximate policy iteration in this approach the policy improvement operation combines feature based aggregation with feature construction using deep neural networks or other calculations we argue that the cost function of a policy may be approximated much more accurately by the nonlinear function of the features provided by aggregation than by the linear function of the features provided by neural network based reinforcement learning thereby potentially leading to more effective policy improvement dimitri bertsekas is with the dept of electr engineering and comp science and the laboratory for infor mation and decision systems m i t cambridge mass 02139 a version of this paper will appear in ieee caa journal of automatica sinica 1 http arxiv org abs 1804 04577 v 3 contents 1 introduction 1 1 alternative approximate policy iteration methods 1 2 terminology 2 approximate policy iteration an overview 2 1 direct and indirect approximation approaches for policy evaluation 2 2 indirect methods based on projected equations 2 3 indirect methods based on aggregation 2 4 implementation issues 3 approximate policy evaluation based on neural networks 4 feature based aggregation framework 4 1 the aggregate problem 4 2 solving the aggregate problem with simulation based methods 4 3 feature formation by using scoring functions 4 4 using heuristics to generate features deterministic optimization and rollout 4 5 stochastic shortest path problems illustrative examples 4 6 multistep aggregation 5 policy iteration with feature based aggregation and a neural network 6 concluding remarks 7 references 2 1 introduction we consider a discounted infinite horizon dynamic programming dp problem with n states which we denote by i 1 n state transitions i j under control u occur at discrete times according to transition probabilities pij u and generate a cost kg i u j at time k where 0 1 is the discount factor we consider deterministic stationary policies such that for each i i is a control that belongs to a constraint set u i we denote by j i the total discounted expected cost of over an infinite number of stages starting from state i and by j i the minimal value of j i over all we denote by j and j the n dimensional vectors that have components j i and j i i 1 n respectively as is well known j is the unique solution of the bellman equation for policy j i n j 1 pij i g i i j j j i 1 n 1 1 while j is the unique solution of the bellman equation j i min u u i n j 1 pij u g i u j j j i 1 n 1 2 in this paper we survey several ideas from aggregation based approximate dp and deep reinforcement learning all of which have been essentially known for some time but are combined here in a new way we will focus on methods of approximate policy iteration pi for short whereby we evaluate approximately the cost vector j of each generated policy our cost approximations use a feature vector f i of each state i and replace j i with a function that depends on i through f i i e a function of the form j f i j i i 1 n we refer to such j as a feature based approximation architecture at the typical iteration of our approximate pi methodology the cost vector j of the current policy is approximated using a feature based architecture j and a new policy is then generated using a policy improvement procedure see fig 1 1 the salient characteristics of our approach are two a the feature vector f i may be obtained using a neural network or other calculation that automatically constructs features b the policy improvement which generates is based on a dp problem that involves feature based aggregation by contrast the standard policy improvement method is based on the one step lookahead minimization i arg min u u i n j 1 pij u g i u j j f j i 1 n 1 3 3 approximate policy evaluation policy improvement initial policy controlled system cost per stage vector tion matrix evaluate approximate cost vector of current policy evaluate approximate cost vector j f i of current policy generate improved policy may involve a neural network may involve aggregation may involve a neural network may involve aggregation figure 1 1 schematic view of feature based approximate pi the cost j i of the current policy starting from state i is replaced by an approximation j f i that depends on i through its feature vector f i the feature vector is assumed independent of the current policy in this figure but in general could depend on or alternatively on multistep lookahead possibly combined with monte carlo tree search we will argue that our feature based aggregation approach has the potential to generate far better policies at the expense of a more computation intensive policy improvement phase 1 1 alternative approximate policy iteration methods a survey of approximate pi methods was given in 2011 by the author ber 11 a and focused on linear feature based architectures these are architectures where f i is an s dimensional vector f i f 1 i fs i and j depends linearly on f i e j f i s 1 f i r i 1 n for some scalar weights r 1 rs we considered in ber 11 a two types of methods a projected equation methods including temporal difference methods where policy evaluation is based on simulation based matrix inversion methods such as lstd or stochastic iterative methods such as td or variants of policy iteration such as lspe b general aggregation methods not just the feature based type considered here these methods will be briefly discussed in section 2 the present paper is complementary to the survey ber 11 a and deals with approximate pi with nonlinear feature based architectures including some where features are generated with the aid of neural networks or some other heuristic calculations 4 an important advantage of linear feature based architectures is that given the form of the feature vector f they can be trained with linear least squares type methods however determining good features may be a challenge in general neural networks resolve this challenge through training that constructs automatically features and simultaneously combines the components of the features linearly with weights this is commonly done by cost fitting nonlinear regression using a large number of state cost sample pairs which are processed through a sequence of alternately linear and nonlinear layers see section 3 the outputs of the final nonlinear layer are the features which are then processed by a final linear layer that provides a linear combination of the features as a cost function approximation the idea of representing cost functions in terms of features of the state in a context that we may now call approximation in value space or approximate dp goes back to the work of shannon on chess sha 50 the work of samuel sam 59 sam 67 on checkers extended some of shannon s algorithmic schemes and introduced temporal difference ideas that motivated much subsequent research the use of neural networks to simultaneously extract features of the optimal or the policy cost functions and construct an approximation to these cost functions was also investigated in the early days of reinforcement learning some of the original contributions that served as motivation for much subsequent work are werbos wer 77 barto sutton and anderson bsa 83 christensen and korf chk 86 holland hol 86 and sutton sut 88 the use of a neural network as a cost function approximator for a challenging dp problem was first demonstrated impressively in the context of the game of backgammon by tesauro tes 92 tes 94 tes 95 tes 02 in tesauro s work the parameters of the network were trained by using a form of temporal differences td learning and the features constructed by the neural network were supplemented by some handcrafted features following tesauro s work the synergistic potential of approximations using neural network or other architectures and dp techniques had become apparent and it was laid out in an influential survey paper by barto bradtke and singh bbs 95 it was then systematically developed in the neuro dynamic programming book by bertsekas and tsitsiklis bet 96 and the reinforcement learning book by sutton and barto sub 98 subsequent books on approximate dp and reinforcement learning which discuss approximate pi among tesauro also constructed a different backgammon player trained by a neural network but with a supervised learning approach which used examples from human expert play tes 89 a tes 89 b he called this approach compar ison learning however his td based algorithm performed substantially better and its success has been replicated by others in both research and commercial programs tesauro and galperin teg 96 proposed still another approach to backgammon based on a rollout strategy which resulted in an even better playing program see ber 17 for an extensive discussion of rollout as a general approximate dp approach at present rollout based backgammon pro grams are viewed as the most powerful in terms of performance but are too time consuming for real time play they have been used in a limited diagnostic way to assess the quality of neural network based programs a list of articles on computer backgammon may be found at http www bkgm com articles page 07 html 5 other techniques include cao cao 07 busoniu et al bbd 10 szepesvari sze 10 powell pow 11 chang fu hu and marcus cfh 13 vrabie vamvoudakis and lewis vvl 13 and gosavi gos 15 to these we may add the edited collections by si barto powell and wunsch sbp 04 lewis liu and lendaris lll 08 and lewis and liu lel 12 which contain several survey papers the original ideas on approximate pi were enriched by further research ideas such as rollout abramson abr 90 tesauro and galperin teg 96 bertsekas tsitsiklis and wu btw 97 bertsekas and castanon bec 99 see the surveys in ber 13 ber 17 adaptive simulation and monte carlo tree search chang hu fu and marcus cfh 05 cfh 13 coulom cou 06 see the survey by browne et al bpw 12 and deep neural networks which are neural networks with many and suitably specialized layers see for the example the book by goodfellow bengio and courville gbc 16 the textbook discussion in ber 17 ch 6 and the recent surveys by schmidhuber sch 15 arulkumaran et al adb 17 liu et al lwl 17 and li li 17 a recent impressive success of the deep neural network based approximate pi methodology is the alphazero program which attained a superhuman level of play for the games of chess go and others see silver et al shs 17 a noteworthy characteristic of this program is that it does not use domain specific knowledge i e handcrafted features but rather relies entirely on the deep neural network to construct features for cost function approximation at least as reported in shs 17 whether it is advisable to rely exclusively on the neural network to provide features is an open question as other investigations including the ones by tesauro noted earlier suggest that using additional problem specific hand crafted features can be very helpful in the context of approximate dp except for the use of deep rather than shallow neural networks which are used in backgammon the alphazero algorithm is similar to several other algorithms that have been proposed in the literature and or have been developed in the past it can be viewed as a conceptually straightforward implementation of approximate pi using monte carlo tree search and a single neural network to construct a cost and policy approximation and does not rely on any fundamentally new ideas or insightful theoretical analysis conceptually it bears considerable similarity to tesauro s td gammon program its spectacular success may be attributed to the skillful implementation of an effective mix of known ideas coupled with great computational power we note that the ability to simultaneously extract features and optimize their linear combination is not unique to neural networks other approaches that use a multilayer architecture have been proposed see the survey by schmidhuber sch 15 including the group method for data handling gmdh which is principally based on the use of polynomial rather than sigmoidal nonlinearities the gmdh method was investigated extensively in the soviet union starting with the work of ivakhnenko in the late 60 s see e g iva 68 it has been used in a large variety of applications and its similarities with the neural network methodology have been noted see the survey by ivakhnenko iva 71 and the large literature summary at the web site http www gmdh net most of the gmdh research relates to inference type problems we 6 are unaware of any application of gmdh in the context of approximate dp but we believe this to be a fruitful area of investigation in any case the feature based pi ideas of the present paper apply equally well in conjunction with gmdh networks as with the neural networks described in section 3 while automatic feature extraction is a critically important aspect of neural network architectures the linearity of the combination of the feature components at the final layer may be a limitation a nonlinear alternative is based on aggregation a dimensionality reduction approach to address large scale problems this approach has a long history in scientific computation and operations research see for example bean birge and smith bbs 87 chatelin and miranker chm 82 douglas and douglas dod 93 mendelssohn men 82 and rogers et al rpw 91 it was introduced in the simulation based approximate dp context mostly in the form of value iteration see singh jaakkola and jordan sjj 95 gordon gor 95 tsitsiklis and van roy tsv 96 see also the book bet 96 sections 3 1 2 and 6 7 more recently aggregation was discussed in a reinforcement learning context involving the notion of options by ciosek and silver cis 15 and the notion of bottleneck simulator by serban et al ssp 18 in both cases encouraging computational results were presented aggregation architectures based on features were discussed in section 3 1 2 of the neuro dynamic programming book bet 96 and in section 6 5 of the author s dp book ber 12 and earlier editions including the feature based architecture that is the focus of the present paper they have the capability to produce policy cost function approximations that are nonlinear functions of the feature components thus yielding potentially more accurate approximations basically in feature based aggregation the original problem is approximated by a problem that involves a relatively small number of feature states feature based aggregation assumes a given form of feature vector so for problems where good features are not apparent it needs to be modified or to be supplemented by a method that can construct features from training data motivated by the reported successes of deep reinforcement learning with neural networks we propose a two stage process first use a neural network or other scheme to construct good features for cost approximation and then use there features to construct a nonlinear feature based aggregation architecture in effect we are proposing a new way to implement approximate pi retain the policy evaluation phase which uses a neural network or alternative scheme but replace the policy improvement phase with the solution of an aggregate dp problem this dp problem involves the features that are generated by a neural network or other scheme possibly together with other handcrafted features its dimension may be reduced to a manageable level by sampling while its cost function values are generalized to the entire feature space by linear interpolation in summary our suggested policy improvement phase may be more complicated but may be far more powerful as it relies on the potentially more accurate function approximation provided by a nonlinear combination of features aside from the power brought to bear by nonlinearly combining features let us also note some other advantages that are generic to aggregation in particular 7 a aggregation aims to solve an aggregate dp problem itself an approximation of the original dp problem in the spirit of coarse grid discretization of large state space problems as a result aggrega tion methods enjoy the stability and policy convergence guarantee of exact pi by contrast temporal difference based and other pi methods can suffer from convergence difficulties such as policy oscil lations and chattering see e g bet 96 ber 11 a ber 12 a corollary to this is that when an aggregation scheme performs poorly it is easy to identify the cause it is the quantization error due to approximating a large state space with a smaller aggregate space the possible directions for improvement at a computational cost of course are then clear introduce additional aggregate states and increase improve these features b aggregation methods are characterized by error bounds which are generic to pi methods that guarantee the convergence of the generated policies these error bounds are better by a factor 1 compared to the corresponding error bounds for methods where policies need not converge such as generic temporal difference methods with linear cost function approximation see eqs 2 2 and 2 3 in the next section let us finally note that the idea of using a deep neural network to extract features for use in another approximation architecture has been used earlier in particular it is central in the deepchess program by david netanyahu and wolf dnw 16 which was estimated to perform at the level of a strong grandmaster and at the level of some of the strongest computer chess programs in this work the features were used in conjunction with supervised learning and human grandmaster play selections to train a deep neural network to compare any pair of legal moves in a given chess position in the spirit of tesauro s comparison training approach tes 89 b by contrast in our proposal the features are used to formulate an aggregate dp problem which can be solved by exact methods including some that are based on simulation the paper is organized as follows in section 2 we provide context for the subsequent developments and summarize some of the implementation issues in approximate pi methods in section 3 we review some of the central ideas of approximate pi based on neural networks in section 4 we discuss pi ideas based on feature based aggregation assuming good features are known in this section we also discuss how features may be constructed on one or more scoring functions which are estimates of the cost function of a policy provided by a neural network or a heuristic we also pay special attention to deterministic discrete optimization problems finally in section 5 we describe some of the ways to combine the feature extraction capability of deep neural networks with the nonlinear approximation possibilities offered by aggregation 1 2 terminology the success of approximate dp in addressing challenging large scale applications owes much to an enormously beneficial cross fertilization of ideas from decision and control and from artificial intelligence the boundaries between these fields are now diminished thanks to a deeper understanding of the foundational issues and the 8 associated methods and core applications unfortunately however there have been substantial discrepancies of notation and terminology between the artificial intelligence and the optimization decision control fields including the typical use of maximization value function reward in the former field and the use of mini mization cost function cost per stage in the latter field the notation and terminology used in this paper is standard in dp and optimal control and in an effort to forestall confusion of readers that are accustomed to either the reinforcement learning or the optimal control terminology we provide a list of selected terms commonly used in reinforcement learning for example in the popular book by sutton and barto sub 98 and its 2018 on line 2 nd edition and their optimal control counterparts a agent controller or decision maker b action control c environment system d reward of a stage opposite of cost of a stage e state value opposite of cost of a state f value or state value function opposite of cost function g maximizing the value function minimizing the cost function h action or state action value q factor of a state control pair i planning solving a dp problem with a known mathematical model j learning solving a dp problem in model free fashion k self learning or self play in the context of games solving a dp problem using policy iteration l deep reinforcement learning approximate dp using value and or policy approximation with deep neural networks m prediction policy evaluation n generalized policy iteration optimistic policy iteration o state abstraction aggregation p episodic task or episode finite step system trajectory q continuing task infinite step system trajectory r afterstate post decision state 9 2 approximate policy iteration an overview many approximate dp algorithms are based on the principles of pi the policy evaluation policy improve ment structure of pi is maintained but the policy evaluation is done approximately using simulation and some approximation architecture in the standard form of the method at each iteration we compute an approximation j r to the cost function j of the current policy and we generate an improved policy using i arg min u u i n j 1 pij u g i u j j j r i 1 n 2 1 here j is a function of some chosen form the approximation architecture which depends on the state and on a parameter vector r r 1 rs of relatively small dimension s the theoretical basis for the method was discussed in the neuro dynamic programming book bet 96 prop 6 2 see also ber 12 section 2 5 6 or ber 18 a sections 2 4 1 and 2 4 2 it was shown there that if the policy evaluation is accurate to within in the sup norm sense then for an discounted problem the method while not convergent is stable in the sense that it will yield in the limit after infinitely many policy evaluations stationary policies that are optimal to within 2 1 2 2 2 where is the discount factor moreover if the generated sequence of policies actually converges to some then is optimal to within 2 1 2 3 see bet 96 section 6 4 1 this is a significantly improved error bound in general policy convergence may not be guaranteed although it is guaranteed for the aggregation methods of this paper experimental evidence indicates that these bounds are often conservative with just a few policy iterations needed before most of the eventual cost improvement is achieved 2 1 direct and indirect approximation approaches for policy evaluation given a class of functions j that defines an approximation architecture there are two general approaches for approximating the cost function j of a fixed policy within j the most straightforward approach the minimization in the policy improvement phase may alternatively involve multistep lookahead possibly combined with monte carlo tree search it may also be done approximately through q factor approximations our discussion extends straightfowardly to schemes that include multistep lookahead or approximate policy improvement 10 referred to as direct or cost fitting is to find a j j that matches j in some least squares error sense i e j argmin j j j j 2 2 4 typically is some weighted euclidean norm with positive weights i i 1 n while j consists of a parametrized class of functions j i r where r r 1 rs s is the parameter vector i e j j r r s then the minimization problem in eq 2 4 is written as min r s n i 1 i j i r j i 2 2 5 and can be viewed as an instance of nonlinear regression in simulation based methods the preceding minimization is usually approximated by a least squares minimization of the form min r s m m 1 j im r m 2 2 6 where im m m 1 m are a large number of state cost sample pairs i e for each m im is a sample state and m is equal to j im plus some simulation noise under mild statistical assumptions on the sample collection process the sample based minimization 2 6 is equivalent in the limit to the exact minimization 2 5 neural network based approximation as described in section 3 is an important example of direct approximation that uses state cost training pairs a common choice is to take j to be the subspace r r s that is spanned by the columns of an n s matrix which can be viewed as basis functions see the left side of fig 2 1 then the approximation problem 2 6 becomes the linear least squares problem min r 1 rs s m m 1 s 1 im r m 2 2 7 where i is the i th entry of the matrix and r is the th component of r the solution of this problem can be obtained analytically and can be written in closed form see e g bet 96 section 3 2 2 note that the ith row of may be viewed as a feature vector of state i and r may be viewed as a linear feature based architecture nonquadratic optimization criteria may also be used although in practice the simple quadratic cost function has been adopted most frequently we use standard vector notation in particular s denotes the euclidean space of s dimensional real vectors and denotes the real line 11 0 0 direct method projection of cost vector j j indirect method solving a projected form of bellman s equation projection onindirect method solving a projected form of bellman s equationdirect method projection of cost vector j state space feature space subspace j r s s state space feature space subspace j r s s t r r r t r j direct method projecting the figure 2 1 two methods for approximating the cost function j as a linear combination of basis functions the approximation architecture is the subspace j r r s where is matrix whose columns are the basis functions in the direct method see the figure on the left j is projected on j in an example of the indirect method the approximation is obtained by solving the projected form of bellman s equation r t r where t r is the vector with components t r i n j 1 pij i g i i j r j i 1 n and r j is the jth component of the vector r see the figure on the right in section 3 we will see that neural network based policy evaluation combines elements of both a linear and a nonlinear architecture the nonlinearity is embodied in the features that the neural network constructs through training but once the features are given the neural network can be viewed as a linear feature based architecture an often cited weakness of simulation based direct approximation is excessive simulation noise in the cost samples m that are used in the least squares minimization 2 7 or 2 7 this has motivated alternative approaches for policy evaluation that inherently involve less noise a major approach of this type referred to as indirect or equation fitting is to approximate bellman s equation for the policy j i n j 1 pij i g i i j j j i 1 n 2 8 with another equation that is defined on the set j the solution of the approximate equation is then used as an approximation of the solution of the original the most common indirect methods assume a linear approximation architecture i e j is the subspace j r r s and approximate bellman s equation with another equation with fewer variables the s parameters r 1 rs two major examples of this approach are projected equation methods and aggregation methods which we proceed to discuss 12 2 2 indirect methods based on projected equations approximation using projected equations has a long history in numerical computation e g partial differ ential equations where it is known as galerkin approximation see e g kvz 72 fle 84 saa 03 kir 11 the projected equation approach is a special case of the so called bubnov galerkin method as noted in the papers ber 11 a ber 11 b and yub 10 in the context of approximate dp it is connected with tem poral difference methods and it is discussed in detail in many sources see e g bet 96 bbd 10 ber 12 gos 15 to state the projected equation let us introduce the transformation t which is defined by the right hand side of the bellman equation 2 8 i e for any j n t j is the vector of n with components t j i n j 1 pij i g i i j j j i 1 n 2 9 note that t is a linear transformation from n to n and in fact in compact vector matrix notation it is written as t j g p j j n 2 10 where p is the transition probability matrix of and g is the expected cost vector of i e the vector with components n j 1 pij i g i i j i 1 n moreover the bellman equation 2 8 is written as the fixed point equation j t j let us denote by j the projection of a vector j n onto j with respect to some weighted euclidean norm and consider t r the projection of t r here t r is viewed as a vector in n and is viewed as an n n matrix multiplying this vector the projected equation takes the form r t r 2 11 see the right hand side of fig 2 1 with this equation we want to find a vector r of j which when transformed by t and then projected back onto j yields itself this is an overdetermined system of linear equations n equations in the s unknowns r 1 rs which is equivalently written as s 1 i r n m 1 im n j 1 pmj m g m m j s 1 j r i 1 n 2 12 here i is the i th component of the matrix and im is the imth component of the projection matrix the system can be shown to have a unique solution under conditions that can be somewhat restrictive e g 13 assuming that the markov chain corresponding to the policy has a unique steady state distribution with positive components that the projection norm involves this distribution and that has linearly independent columns see e g ber 12 section 6 3 an important extension is to replace the projected equation 2 11 with the equation r t r 2 13 where is a scalar with 0 1 and the transformation t is defined by t j i 1 0 t 1 j i i 1 n j n 2 14 and t j is the fold composition of t applied to the vector j this approach to the approximate solution of bellman s equation is supported by extensive theory and practical experience see the textbooks noted earlier in particular the td algorithm and other related temporal difference methods such as lstd and lspe aim to solve by simulation the projected equation 2 13 the choice of embodies the important bias variance tradeoff larger values of lead to better approximation of j but require a larger number of simulation samples because of increased simulation noise see the discussion in section 6 3 6 of ber 12 an important insight is that the operator t is closely related to the proximal operator of convex analysis with corresponding to the penalty parameter of the proximal operator as shown in the author s paper ber 16 a see also the monograph ber 18 a section 1 2 5 and the paper ber 18 b in particular td can be viewed as a stochastic simulation based version of the proximal algorithm a major issue in projected equation methods is whether the linear transformation t or t is a contraction mapping in which case eq 2 11 or eq 2 13 respectively has a unique solution which may be obtained by iterative fixed point algorithms this depends on the projection norm and it turns out that there are special norms for which t is a contraction these are related to the steady state distribution of the system s markov chain under see the discussion of ber 11 a or section 6 3 of ber 12 an important fact is that for any projection norm t is a contraction provided is sufficiently close to 1 still the contraction issue regarding t is significant and affects materially the implementation of the corresponding approximate pi methods another important concern is that the projection matrix may have some negative entries i e some of the components im in eq 2 12 may be negative and as a result the linear transformations t and t may lack the monotonicity property that is essential for the convergence of the corresponding approximate pi method indeed the lack of monotonicity the possibility that we may not have t j t j for two vectors j j with j j is the fundamental mathematical reason for policy oscillations in pi methods that are based on temporal differences see ber 11 a ber 12 we refer to the literature for further details and analysis regarding the projected equations 2 11 and 2 13 as our focus will be on aggregation methods which we discuss next 14 2 3 indirect methods based on aggregation aggregation is another major indirect approach which has originated in numerical linear algebra simple examples of aggregation involve finite dimensional approximations of infinite dimensional equations coarse grid approximations of linear systems of equations defined over a dense grid and other related methods for dimensionality reduction of high dimensional systems in the context of dp the aggregation idea is implemented by replacing the bellman equation j t j cf eq 2 8 with a lower dimensional aggregate equation which is defined on an approximation subspace j r r s the aggregation counterpart of the projected equation r t r is r dt r 2 15 where and d are some matrices and t is the linear transformation given by eq 2 9 this is a vector matrix notation for the linear system of n equations in the s variables r 1 rs s k 1 ikrk s k 1 ik n m 1 dkm n j 1 pmj m g m m j s 1 j r i 1 n where i is the i th component of the matrix and dkm is the kmth component of the matrix d a key restriction for aggregation methods as applied to dp is that the rows of d and should be probability distributions these distributions usually have intuitive interpretations in the context of specific aggregation schemes see ber 12 section 6 5 for a discussion assuming that has linearly independent columns which is true for the most common types of aggregation schemes eq 2 15 can be seen to be equivalent to r dt r 2 16 or rk n m 1 dkm n j 1 pmj m g m m j s 1 j r k 1 s 2 17 in most of the important aggregation methods including the one of section 4 d and are chosen so that the product d is the identity d i it turns out that under some widely applicable conditions including the assumptions of section 4 the projected and aggregation equations are closely related in particular it can be proved under these conditions that the matrix d that appears in the aggregation equation 2 15 is a projection with respect to a suitable weighted euclidean seminorm see yub 12 section 4 or the book ber 12 it is a norm projection in the case of hard aggregation aside from establishing the relation between the two major indirect approximation methods projected equation and aggregation this result provides the basis for transferring the rich methodology of temporal differences methods such as td to the aggregation context 15 assuming that this is true the operator i dt of the aggregation equation 2 16 is obtained by pre multiplying and post multiplying the operator i t of the bellman equation with d and respectively mathematically this can be interpreted as follows a post multiplying with we replace the n variables j j of the bellman equation j t j with convex combinations of the s variables r of the system 2 15 using the rows j 1 js of j j s 1 j r b pre multiplying with d we form the s equations of the aggregate system by taking convex combinations of the n components of the n n bellman equation using the rows of d we will now describe how the aggregate system of eq 2 17 can be associated with a discounted dp problem that has s states called the aggregate states in what follows at an abstract level the aggregate states may be viewed as entities associated with the s rows of d or the s columns of indeed since t has the form t j g p j cf eq 2 10 the aggregate system 2 17 becomes r g p r 2 18 where g dg p dp 2 19 it is straightforward to verify that p is a transition probability matrix since the rows of d and are probability distributions this means that the aggregation equation 2 18 or equivalently eq 2 17 represents a policy evaluation bellman equation for the discounted problem with transition matrix p and cost vector g this problem will be called the aggregate dp problem associated with policy in what follows the corresponding aggregate state costs are r 1 rs some important consequences of this are a the aggregation equation 2 18 2 19 inherits the favorable characteristics of the bellman equation j t j namely its monotonicity and contraction properties and its uniqueness of solution b exact dp methods may be used to solve the aggregate dp problem these methods often have more regular behavior than their counterparts based on projected equations c approximate dp methods such as variants of simulation based pi may also be used to solve approx imately the aggregate dp problem the preceding characteristics of the aggregation approach may be turned to significant advantage and may counterbalance the restriction on the structure of d and their rows must be probability distributions as stated earlier 16 2 4 implementation issues the implementation of approximate pi methods involves several delicate issues which have been extensively investigated but have not been fully resolved and are the subject of continuing research we will discuss briefly some of these issues in what follows in this section we preface this discussion by noting that all of these issues are addressed more easily and effectively within the direct approximation and the aggregation frameworks than within the temporal difference projected equation framework because of the deficiencies relating to the lack of monotonicity and contraction of the operator t which we noted in section 2 2 the issue of exploration an important generic difficulty with simulation based pi is that in order to evaluate a policy we may need to generate cost samples using that policy but this may bias the simulation by underrepresenting states that are unlikely to occur under as a result the cost to go estimates of these underrepresented states may be highly inaccurate causing potentially serious errors in the calculation of the improved control policy via the policy improvement equation 2 1 the situation just described is known as inadequate exploration of the system s dynamics it is a particularly acute difficulty when the system is deterministic i e pij u is equal to 1 for a single successor state j or when the randomness embodied in the transition probabilities of the current policy is relatively small since then few states may be reached from a given initial state when the current policy is simulated one possibility to guarantee adequate exploration of the state space is to break down the simulation to multiple short trajectories see ber 11 c ber 12 yub 12 and to ensure that the initial states employed form a rich and representative subset this is naturally done within the direct approximation and the aggregation frameworks but less so in the temporal difference framework where the theoretical convergence analysis relies on the generation of a single long trajectory another possibility for exploration is to artificially introduce some extra randomization in the simulation of the current policy by occasionally generating random transitions using some policy other than this is called an off policy approach and its implementation has been the subject of considerable discussion see the books sub 98 ber 12 a monte carlo tree search implementation may naturally provide some degree of such randomization and has worked well in game playing contexts such as the alphazero architecture for playing chess go and other games silver et al shs 17 other related approaches to improve exploration based on generating multiple short trajectories are discussed in sections 6 4 1 and 6 4 2 of ber 12 limited sampling optimistic policy iteration in the approximate pi approach discussed so far the evaluation of the current policy must be fully carried out an alternative is optimistic pi where relatively few simulation samples are processed between successive 17 policy changes and corresponding parameter updates optimistic pi with cost function approximation is frequently used in practical applications in particu lar extreme optimistic schemes including nonlinear architecture versions and involving a single or very few q factor updates between parameter updates have been widely recommended see e g the books bet 96 sub 98 bbd 10 where they are referred to as sarsa a shorthand for state action reward state action the behavior of such schemes is very complex and their theoretical convergence properties are unclear in particular they can exhibit fascinating and counterintuitive behavior including a natural tendency for policy oscillations this tendency is common to both optimistic and nonoptimistic pi as we will discuss shortly but in extreme optimistic pi schemes oscillations tend to manifest themselves in an unusual form whereby we may have convergence in parameter space and oscillation in policy space see bet 96 section 6 4 2 or ber 12 section 6 4 3 on the other hand optimistic pi may in some cases deal better with the problem of exploration discussed earlier the reason is that with rapid changes of policy there may be less tendency to bias the simulation towards particular states that are favored by any single policy policy oscillations and chattering contrary to exact pi which converges to an optimal policy in a fairly regular manner approximate pi may oscillate by this we mean that after a few iterations policies tend to repeat in cycles the parameter vectors r that correspond to the oscillating policies may also tend to oscillate although it is possible in optimistic approximate pi methods that there is convergence in parameter space and oscillation in policy space a peculiar phenomenon known as chattering oscillations and chattering have been explained with the use of the so called greedy partition of the parameter space into subsets that correspond to the same improved policy see bet 96 section 6 4 2 or ber 12 section 6 4 3 policy oscillations occur when the generated parameter sequence straddles the boundaries that separate sets of the partition oscillations can be potentially very damaging because there is no guarantee that the policies involved in the oscillation are good policies and there is often no way to verify how well they compare to the optimal we note that oscillations are avoided and approximate pi can be shown to converge to a single policy under special conditions that arise in particular when aggregation is used for policy evaluation these conditions involve certain monotonicity assumptions e g the nonnegativity of the components im of the projection matrix in eq 2 12 which are fulfilled in the case of aggregation see ber 11 a however for temporal difference methods policy oscillations tend to occur generically and often for very simple problems involving few states a two state example is given in ber 11 a and in ber 12 section 6 4 3 this is a potentially important advantage of the aggregation approach 18 model free implementations in many problems a mathematical model the transition probabilities pij u and the cost vector g is unavail able or hard to construct but instead the system and cost structure can be simulated far more easily in particular let us assume that there is a computer program that for any given state i and control u simulates sample transitions to a successor state j according to pij u and generates the transition cost g i u j as noted earlier the direct and indirect approaches to approximate evaluation of a single policy may be implemented in model free fashion simply by generating the needed cost samples for the current policy by simulation however given the result j of the approximate policy evaluation the policy improvement minimization i arg min u u i n j 1 pij u g i u j j j i 1 n 2 20 still requires the transition probabilities pij u so it is not model free to provide a model free version we may use a parametric regression approach in particular suppose that for any state i and control u state transitions i j and corresponding transition costs g i u j and values of j j can be generated in a model free fashion when needed by using a simulator of the true system then we can introduce a parametric family approximation architecture of q factor functions q i u where is the parameter vector and use a regularized least squares fit regression to approximate the expected value that is minimized in eq 2 20 the steps are as follows a use the simulator to collect a large number of representative sample state control pairs im um and successor states jm m 1 m and corresponding sample q factors m g im um jm j jm m 1 m 2 21 b determine the parameter vector with the least squares minimization argmin m m 1 q im um m 2 2 22 or a regularized minimization whereby a quadratic regularization term is added to the above quadratic objective c use the policy i arg min u u i q i u i 1 n 2 23 this policy may be generated on line when the control constraint set u i contains a reasonably small number of elements otherwise an approximation in policy space is needed to represent the policy using a policy approximation architecture such an architecture could be based on a neural network 19 in which case it is commonly called an action network or actor network to distinguish from its cost function approximation counterpart which is called a value network or critic network note some important points about the preceding approximation procedure 1 it does not need the transition probabilities pij u to generate the policy through the minimization 2 23 the simulator to collect the samples 2 21 suffices 2 the policy obtained through the minimization 2 23 is not the same as the one obtained through the minimization 2 20 there are two reasons for this one is the approximation error introduced by the q factor architecture q and the other is the simulation error introduced by the finite sample regression 2 22 we have to accept these sources of error as the price to pay for the convenience of not requiring a mathematical model 3 two approximations are potentially required one to compute j which is needed for the samples m cf eq 2 21 and another to compute q through the least squares minimization 2 22 and the subsequent policy generation formula 2 23 the approximation methods to obtain j and q may not be the same and in fact may be unrelated for example j need not involve a parametric approximation e g it may be obtained by some type of problem approximation approach an alternative to first computing j and then computing subsequently q via the procedure 2 21 2 23 is to forgo the computation of j and use just the parametric approximation architecture for the policy q factor q i u we may then train this q factor architecture using state control q factor samples and either the direct or the indirect approach generally algorithms for approximating policy cost functions can be adapted to approximating policy q factor functions as an example a direct model free approximate pi scheme can be defined by eqs 2 22 2 23 using m state control samples im um corresponding successor states jm generated according to the probabilities pimj um and sample costs m equal to the sum of a the first stage cost g im um jm b a discounted simulated sample of the infinite horizon cost of starting at jm and using in place of the term j jm in eq 2 21 a pi scheme of this type was suggested by fern yoon and givan fyg 06 and has been discussed by several other authors see ber 17 section 6 3 4 in particular a variant of the method was used to train a tetris playing computer program that performs impressively better than programs that are based on other variants of approximate pi and various other methods see scherrer sch 13 scherrer et al sgg 15 and gabillon ghavamzadeh and scherrer ggs 13 who also provide an analysis 20 3 approximate policy evaluation based on neural networks in this section we will describe some of the basic ideas of the neural network methodology as it applies to the approximation of the cost vector j of a fixed policy since is fixed throughout this section we drop the subscript is what follows a neural network provides an architecture of the form j i v r s 1 f i v r 3 1 that depends on a parameter vector v and a parameter vector r r 1 rs here for each state i j i v r approximates j i while the vector f i v f 1 i v fs i v may be viewed as a feature vector of the state i notice the different roles of the two parameter vectors v parametrizes f i v and r is a vector of weights that combine linearly the components of f i v the idea is to use training to obtain simultaneously both the features and the linear weights consistent with the direct approximation framework of section 2 1 to train a neural network we generate a training set that consists of a large number of state cost pairs im m m 1 m and we find v r that minimizes m m 1 j im v r m 2 3 2 the training pairs im m are generated by some kind of calculation or simulation and they may contain noise i e m is the cost of the policy starting from state im plus some error the simplest type of neural network is the single layer perceptron see fig 3 1 here the state i is encoded as a vector of numerical values y i with components y 1 i yk i which is then transformed linearly as ay i b where a is an m k matrix and b is a vector in m some of the components of y i may be known interesting features of i that can be designed based on problem specific knowledge or prior training experience this transformation will be referred to as the linear layer of the neural network we view the components of a and b as parameters to be determined and we group them together into the parameter vector v a b there are also neural network implementations of the indirect projected equation approximation approach which make use of temporal differences such as for example nonlinear versions of td we refer to the textbook literature on the subject e g sub 98 in this paper we will focus on neural network training that is based on minimization of the quadratic cost function 3 2 21 1 0 1 encoding cost approximationlinear layer parameter cost approximation sigmoidal layer linear weighting sigmoidal layer linear weighting sigmoidal layer linear weighting sigmoidal layer linear weighting sigmoidal layer linear weighting linear layer parameter linear layer parameter linear layer parameter v a b sigmoidal layer linear weighting state nonlinear ay i y i ay i b i ay i b f 1 i v j f 2 i v s 1 f i v r state i y state b fs i v r r 1 rs figure 3 1 a perceptron consisting of a linear layer and a nonlinear layer it provides a way to compute features of the state which can be used for approximation of the cost function of a given policy the state i is encoded as a vector of numerical values y i which is then transformed linearly as ay i b in the linear layer the scalar output components of the linear layer become the inputs to single input single output nonlinear functions that produce the s scalars f i v ay i b which can be viewed as feature components that are in turn linearly weighted with parameters r each of the s scalar output components of the linear layer ay i b 1 s becomes the input to a nonlinear differentiable function that maps scalars to scalars typically is monotonically increasing a simple and popular possibility is the rectified linear unit which is simply the function max 0 rectified to a differentiable function by some form of smoothing operation for example ln 1 e other functions used since the early days of neural networks have the property lim lim such functions are referred to as sigmoids and some common choices are the hyperbolic tangent function tanh e e e e and the logistic function 1 1 e in what follows we will ignore the character of the function except for the differentiability requirement and simply refer to it as a nonlinear unit and to the corresponding layer as a nonlinear layer at the outputs of the nonlinear units we obtain the scalars f i v ay i b 1 s 22 one possible interpretation is to view these scalars as features of state i which are linearly combined using weights r 1 s to produce the final output s 1 f i v r s 1 ay i b r 3 3 note that each value f i v depends on just the th row of a and the th component of b not on the entire vector v in some cases this motivates placing some constraints on individual components of a and b to achieve special problem dependent handcrafted effects given a set of state cost training pairs im m m 1 m the parameters of the neural network a b and r are obtained by solving the training problem 3 2 i e min a b r m m 1 s 1 ay im b r m 2 3 4 the cost function of this problem is generally nonconvex so there may exist multiple local minima it is common to augment the cost function of this problem with a regularization function such as a quadratic in the parameters a b and r this is customary in least squares problems in order to make the problem easier to solve algorithmically however in the context of neural network training regularization is primarily important for a different reason it helps to avoid overfitting which refers to a situation where a neural network model matches the training data very well but does not do as well on new data this is a well known difficulty in machine learning which may occur when the number of parameters of the neural network is relatively large roughly comparable to the size of the training set we refer to machine learning and neural network textbooks for a discussion of algorithmic questions regarding regularization and other issues that relate to the practical implementation of the training process in any case the training problem 3 4 is an unconstrained nonconvex differentiable optimization problem that can in principle be addressed with standard gradient type methods let us now discuss briefly two issues regarding the neural network formulation and training process just described a a major question is how to solve the training problem 3 4 the salient characteristic of the cost function of this problem is its form as the sum of a potentially very large number m of component functions this structure can be exploited with a variant of the gradient method called incremental which computes just the gradient of a single squared error component s 1 ay im b r m 2 sometimes the more recent name stochastic gradient descent is used in reference to this method however once the training set has been generated possibly by some deterministic process the method need not have a stochastic character and it also does not guarantee cost function descent at each iteration 23 of the sum in eq 3 4 at each iteration and then changes the current iterate in the opposite direc tion of this gradient using some stepsize the books ber 15 ber 16 b provide extensive accounts and theoretical analyses including the connection with stochastic gradient methods are given in the book bet 96 and the paper bet 00 experience has shown that the incremental gradient method can be vastly superior to the ordinary nonincremental gradient method in the context of neural network training and in fact the methods most commonly used in practice are incremental b another important question is how well we can approximate the cost function of the policy with a neural network architecture assuming we can choose the number of the nonlinear units s to be as large as we want the answer to this question is quite favorable and is provided by the so called universal approximation theorem roughly the theorem says that assuming that i is an element of a euclidean space x and y i i a neural network of the form described can approximate arbitrarily closely in an appropriate mathematical sense over a closed and bounded subset s x any piecewise continuous function j s 7 provided the number s of nonlinear units is sufficiently large for proofs of the theorem at different levels of generality we refer to cybenko cyb 89 funahashi fun 89 hornik stinchcombe and white hsw 89 and leshno et al llp 93 for intuitive explanations we refer to bishop bis 95 pp 129 130 and jones jon 90 while the universal approximation theorem provides some assurance about the adequacy of the neural network structure it does not predict the number of nonlinear units that we may need for good perfor mance in a given problem unfortunately this is a difficult question to even pose precisely let alone to answer adequately in practice one is reduced to trying increasingly larger numbers of units until one is convinced that satisfactory performance has been obtained for the task at hand experience has shown that in many cases the number of required nonlinear units and corresponding dimension of a can be very large adding significantly to the difficulty of solving the training problem this has motivated various suggestions for modifications of the neural network structure one possibility is to concatenate multiple single layer perceptrons so that the output of the nonlinear layer of one perceptron becomes the input to the linear layer of the next as we will now discuss multilayer and deep neural networks an important generalization of the single layer perceptron architecture is deep neural networks which involve multiple layers of linear and nonlinear functions the number of layers can be quite large hence the deep characterization the outputs of each nonlinear layer become the inputs of the next linear layer see fig 3 2 in some cases it may make sense to add as additional inputs some of the components of the state i or the state encoding y i 24 1 0 1 encoding state sigmoidal layer linear weighting sigmoidal layer linear weighting sigmoidal layer linear weighting sigmoidal layer linear weighting sigmoidal layer linear weighting sigmoidal layer linear weighting sigmoidal layer linear weightingnonlinear ay nonlinear ay s 1 f i v r state i y cost approximationlinear layer parameter cost approximation state linear weighting ofstate features figure 3 2 a neural network with multiple layers each nonlinear layer constructs a set of features as inputs of the next linear layer the features are obtained at the output of the final nonlinear layer are linearly combined to yield a cost function approximation the training problem for multilayer networks has the form min v r m m 1 s 1 f i v r m 2 where v represents the collection of all the parameters of the linear layers and f i v is the th feature component produced at the output of the final nonlinear layer various types of incremental gradient methods can also be applied here specially adapted to the multi layer structure and they are the methods most commonly used in practice in combination with techniques for finding good starting points etc an important fact is that the gradient with respect to v of each feature component f i v can be efficiently calculated using a special procedure known as backpropagation which is just a computationally efficient way to apply the chain rule of differentiation we refer to the specialized literature for various accounts see e g bis 95 bet 96 hot 06 hay 08 lzx 17 in view of the universal approximation property the reason for having multiple nonlinear layers is not immediately apparent a commonly given explanation is that a multilayer network provides a hierarchical sequence of features where each set of features in the sequence is a function of the preceding set of features in the sequence in the context of specific applications this hierarchical structure can be exploited in order to specialize the role of some of the layers and to enhance particular characteristics of the state another reason commonly given is that with multiple linear layers one may consider the possibility of using matrices a with a particular sparsity pattern or other structure that embodies special linear operations such as convolution when such structures are used the training problem often becomes easier because the number of parameters in the linear layers may be drastically decreased deep neural networks also have another advantage which is important for our aggregation related purposes in this paper the final features obtained as output of the last nonlinear layer tend to be more complex so their number can be made smaller as the number of nonlinear layers increases this tends to facilitate the implementation of the feature based aggregation schemes that we will discuss in what follows 25 1 10 20 30 40 50 i 1 1 i 2 i 2 i 3 3 i 1 10 20 30 40 50 i 1 1 i 2 i 2 i 3 3 i 1 10 20 30 40 50 i 1 1 i 2 i 2 i 3 3 i 1 10 20 30 40 50 i 1 1 i 2 i 2 i 3 3 i feature vector f i approximate cost aggregate states aggregate problem approximation aggregate problem approximation figure 4 1 illustration of aggregate states and a corresponding cost approximation which is constant over each disaggregation set here there are three aggregate states with disaggregation sets denoted i 1 i 2 i 3 4 feature based aggregation framework in this section we will specialize the general aggregation framework of section 2 3 by introducing features in the definition of the matrices d and the starting point is a given feature mapping i e a function f that maps a state i into its feature vector f i we assume that f is constructed in some way including hand crafted or neural network based but we leave its construction unspecified for the moment we will form a lower dimensional dp approximation of the original problem and to this end we introduce disjoint subsets s 1 sq of state feature pairs i f i which we call aggregate states the subset of original system states i that corresponds to s i i i f i s 1 q 4 1 is called the disaggregation set of s an alternative and equivalent definition given f is to start with disjoint subsets of states i 1 q and define the aggregates states s by s i f i i i 1 q 4 2 mathematically the aggregate states are the restrictions of the feature mapping on the disaggregation sets i in simple terms we may view the aggregate states s as some pieces of the graph of the feature mapping f see fig 4 1 to preview our framework we will aim to construct an aggregate dp problem whose states will be the aggregate states s 1 sq and whose optimal costs denoted r 1 r q will be used to construct a function approximation j to the optimal cost function j this approximation will be constant over each disaggregation set see fig 4 1 our ultimate objective is that j approximates closely j which suggests as a general guideline that the aggregate states should be selected so that j is nearly constant over each of the disaggregation sets i 1 iq this will also be brought out by our subsequent analysis 26 to formulate an aggregation model that falls within the framework of section 2 3 we need to specify the matrices and d we refer to the row of d that corresponds to aggregate state s as the disaggregation distribution of s and to its elements d 1 d n as the disaggregation probabilities of s similarly we refer to the row of that corresponds to state j j 1 q as the aggregation distribution of j and to its elements as the aggregation probabilities of j we impose some restrictions on the components of d and which we describe next definition of a feature based aggregation architecture given the collection of aggregate states s 1 sq and the corresponding disaggregation sets i 1 iq the aggregation and disaggregation probabilities satisfy the following a the disaggregation probabilities map each aggregate state onto its disaggregation set by this we mean that the row of the matrix d that corresponds to an aggregate state s is a probability distribution d 1 d n over the original system states that assigns zero probabilities to states that are outside the disaggregation set i d i 0 i i 1 q 4 3 for example in the absence of special problem specific considerations a reasonable and conve nient choice would be to assign equal probability to all states in i and zero probability to all other states b the aggregation probabilities map each original system state that belongs to a disaggregation set onto the aggregate state of that set by this we mean that the row j 1 q of the matrix that corresponds to an original system state j is specified as follows i if j belongs to some disaggregation set say i then j 1 4 4 and j 0 for all 6 ii if j does not belong to any disaggregation set the row j 1 q is an arbitrary probability distribution there are several possible methods to choose the aggregate states generally as noted earlier the idea will be to form disaggregation sets over which the cost function values j i or j i depending on 27 the situation vary as little as possible we list three general approaches below and we illustrate these approaches later with examples a state and feature based approach sample in some way the set of original system states i compute the corresponding feature vectors f i and divide the pairs i f i thus obtained into subsets s 1 sq some problem specific knowledge may be used to organize the state sampling with proper consideration given to issues of sufficient exploration and adequate representation of what is viewed as important parts of the state space this scheme is suitable for problems where states with similar feature vectors have similar cost function values and is ordinarily the type of scheme that we would use in conjunction with neural network constructed features see section 5 b feature based approach start with a collection of disjoint subsets f 1 q of the set of all possible feature values f f i i 1 n compute in some way disjoint state subsets i 1 iq such that f i f i i 1 q and obtain the aggregate states s i f i i i 1 q with corresponding disaggregation sets i 1 iq this scheme is appropriate for problems where it can be implemented so that each disaggregation set i consists of states with similar cost function values examples will be given in section 4 3 c state based approach start with a collection of disjoint subsets of states i 1 iq and introduce an artificial feature vector f i that is equal to the index for the states i i 1 q and to some default index say 0 for the states that do not belong to q 1 i then use as aggregate states the subsets s i i i 1 q with i 1 iq as the corresponding disaggregation sets in this scheme the feature vector plays a subsidiary role but the idea of using disaggregation subsets with similar cost function values is still central as we will discuss shortly the scheme where the aggregate states are identified with subsets i 1 iq of original system states has been called aggregation with representative features in ber 12 section 6 5 where its connection with feature based aggregation has been discussed the approaches of forming aggregate states just described cover most of the aggregation schemes that have been used in practice two classical examples of the state based approach are the following 28 hard aggregation the starting point here is a partition of the state space that consists of disjoint subsets i 1 iq of states with i 1 iq 1 n the feature vector f i of a state i identifies the set of the partition that i belongs to f i i i 1 q 4 5 the aggregate states are the subsets s i i i 1 q and their disaggregation sets are the subsets i 1 iq the disaggregation probabilities di are positive only for states i i cf eq 4 3 the aggregation probabilities are equal to either 0 or 1 according to j 1 if j i 0 otherwise j 1 n 1 q 4 6 cf eq 4 4 the following aggregation example is typical of a variety of schemes arising in discretization or coarse grid schemes where a smaller problem is obtained by discarding some of the original system states the essence of this scheme is to solve a reduced dp problem obtained by approximating the discarded state costs by interpolation using the nondiscarded state costs aggregation with representative states the starting point here is a collection of states i 1 iq that we view as representative the costs of the nonrepresentative states are approximated by interpolation of the costs of the representative states using the aggregation probabilities the feature mapping is f i if i i 1 q 0 otherwise 4 7 the aggregate states are s i 1 q the disaggregation sets are i i 1 q and the disaggregation probabilities are equal to either 0 or 1 according to d i 1 if i i 0 otherwise i 1 n 1 q 29 cf eq 4 3 the aggregation probabilities must satisfy the constraint j 1 if j i 1 q cf eq 4 4 and can be arbitrary for states j i 1 iq an important class of aggregation frameworks with representative states arises in partially observed markovian decision problems pomdp where observations from a controlled markov chain become avail able sequentially over time here the states of the original high dimensional dp problem are either infor mation vectors groups of past measurements or belief states conditional probability distributions of the state of the markov chain given the available information features may be state estimates given the information and possibly their variances or a relatively small number of representative belief states see e g section 5 1 of ber 12 or the paper by yu and bertsekas yub 04 and the references quoted there choice of disaggregation probabilities in both of the preceding aggregation schemes the requirement d i 0 for all i i cf eq 4 3 leaves a lot of room for choice of the disaggregation probabilities simple examples show that the values of these probabilities can affect significantly the quality of aggregation based approximations the paper by van roy van 06 provides a relevant discussion thus finding a good set of disaggregation probabilities is an interesting issue generally problem specific knowledge and intuition can be helpful in designing aggregation schemes but more systematic methods may be desirable based on some kind of gradient or random search optimiza tion in particular for a given set of aggregate states and matrix we may introduce a parameter vector and a parametrized disaggregation matrix d which is differentiable with respect to then for a given policy we may try to find that minimizes some cost function f r where r is defined as the unique solution of the corresponding aggregation equation r d t r for example we may use as cost function f the squared bellman equation residual f r r d t r 2 the key point here is that we can calculate the gradient of r with respect to each component of by using simulation and low dimensional calculations based on aggregation we can then use the chain rule to compute the gradient of f with respect to for use in some gradient based optimization method this methodology has been developed for a related projected equation context by menache mannor and shimkin mms 06 yu and bertsekas yub 09 and di castro and mannor dim 10 but has never been tried in the context of aggregation the paper mms 06 also suggests the use of random search algorithms such as the cross entropy method in the context of basis function optimization a further discussion of parametric 30 according to pij u with cost j 1 i original system states aggregate states original system states aggregate states aggregation probabilities disaggregation probabilities aggregation probabilities disaggregation probabilities aggregation probabilities disaggregation probabilities g i u j matrix matrixwith aggregation aggregate states aggregate states s s d i 0 if i i j 1 if j i figure 4 2 illustration of the transition mechanism and the costs per stage of the aggregate problem optimization of the disaggregation probabilities or other structural elements of the aggregation framework is beyond the scope of the present paper but may be an interesting subject for investigation 4 1 the aggregate problem given a feature based aggregation framework i e the aggregate states s 1 sq the corresponding disag gregation sets i 1 iq and the aggregation and disaggregation distributions we can consider an aggregate dp problem that involves transitions between aggregate states in particular the transition probabilities pij u and the disaggregation and aggregation probabilities specify a controlled dynamic system involving both the original system states and the aggregate states cf fig 4 2 i from aggregate state s we generate a transition to original system state i according to d i note that i must belong to the disaggregation set i because of the requirement that d i 0 only if i i ii from original system state i we generate a transition to original system state j according to pij u with cost g i u j iii from original system state j we generate a transition to aggregate state s according to j note here the requirement that j 1 if j i cf eq 4 4 we will consider the aggregate problem for the case where there are multiple possible controls at each state however it is also possible to consider the aggregate problem for the purpose of finding an approximation to the cost function j of a given policy this is the special case where the control constraint set u i consists of the single control i for every state i 31 this is a dp problem with an enlarged state space that consists of two copies of the original state space 1 n plus the q aggregate states we introduce the corresponding optimal vectors j 0 j 1 and r r 1 r q where r is the optimal cost to go from aggregate state s j 0 i is the optimal cost to go from original system state i that has just been generated from an aggregate state left side of fig 4 3 j 1 j is the optimal cost to go from original system state j that has just been generated from an original system state right side of fig 4 3 note that because of the intermediate transitions to aggregate states j 0 and j 1 are different these three vectors satisfy the following three bellman s equations r n i 1 d ij 0 i 1 q 4 8 j 0 i min u u i n j 1 pij u g i u j j 1 j i 1 n 4 9 j 1 j q m 1 j r m j 1 n 4 10 by combining these equations we see that r satisfies r n i 1 d i min u u i n j 1 pij u g i u j q m 1 jm r m 1 q 4 11 or equivalently r hr where h is the mapping that maps the vector r to the vector hr with components hr n i 1 d i min u u i n j 1 pij u g i u j q m 1 jm rm 1 q 4 12 it can be shown that h is a contraction mapping with respect to the sup norm and thus has r as its unique fixed point this follows from standard contraction arguments and the fact that d i pij u and j are probabilities note the nature of r it is the optimal cost of the aggregate state s which is the restriction of the feature mapping f on the disaggregation set i thus roughly r is an approximate optimal cost associated with states in i solution of the aggregate problem while the aggregate problem involves more states than the original dp problem it is in fact easier in some important ways the reason is that it can be solved with algorithms that execute over the smaller space 32 according to pij u with cost j 1 i original system states aggregate states original system states aggregate states g i u j matrix matrix aggregate states aggregate states s s self learning policy iteration constraint relaxation d i j aggregate costs r aggregate costs r cost function j 0 i cost function cost function j 1 j figure 4 3 the transition mechanism and the cost functions of the aggregate problem of aggregate states in particular exact and approximate simulation based algorithms can be used to find the lower dimensional vector r without computing the higher dimensional vectors j 0 and j 1 we describe some of these methods in section 4 2 and we refer to chapter 6 of ber 12 for a more detailed discussion of simulation based methods for computing the vector r of the costs of the aggregate states that correspond to a given policy the simulator used for these methods is based on figs 4 2 and 4 3 transitions to and from the aggregate states are generated using the aggregation and disaggregation probabilities respectively while transitions i j between original system states are generated using a simulator of the original system which is assumed to be available once r is found the optimal cost to go of the original problem may be approximated by the vector j 1 of eq 4 10 note that j 1 is a piecewise linear cost approximation of j it is constant over each of the disaggregation sets i 1 q and equal to the optimal cost r of the aggregate state s cf eqs 4 4 and 4 10 and it is interpolated linear outside the disaggregation sets cf eq 4 10 in the case where q 1 i 1 n e g in hard aggregation the disaggregation sets i form a partition of the original system state space and j 1 is piecewise constant figure 4 4 illustrates a simple example of approximate cost function j 1 let us also note that for the purposes of using feature based aggregation to improve a given policy it is not essential to solve the aggregate problem to completion instead we may perform one or just a few pis and adopt the final policy obtained as a new improved policy the quality of such a policy depends on how well the aggregate problem approximates the original dp problem while it is not easy to quantify the relevant approximation error generally a small error can be achieved if a the feature mapping f conforms to the optimal cost function j in the sense that f varies little in regions of the state space where j also varies little b the aggregate states are selected so that f varies little over each of the disaggregation sets i 1 iq 33 1 10 20 30 40 501 10 20 30 40 501 10 20 30 40 501 10 20 30 40 501 10 20 30 40 501 10 20 30 40 50 1 10 20 30 40 50 i 1 1 i 2 i 2 i 3 3 i i j 1 i figure 4 4 schematic illustration of the approximate cost function j 1 here the original states are the integers between 1 and 50 in this figure there are three aggregate states numbered 1 2 3 the corresponding disaggregation sets are i 1 1 10 i 2 20 30 i 3 40 50 are shown in the figure the values of the approximate cost function j 1 i are constant within each disaggregation set i 1 2 3 and are obtained by linear interpolation for states i that do not belong to any one of the sets i if the sets i 1 2 3 include all the states 1 50 we have a case of hard aggregation if each of the sets i 1 2 3 consist of a single state we have a case of aggregation with representative states this is intuitive and is supported by the subsequent discussion and analysis given the optimal aggregate costs r 1 q the corresponding optimal policy is defined implicitly using the one step lookahead minimization i arg min u u i n j 1 pij u g i u j q 1 j r i 1 n 4 13 cf eq 4 9 or a multistep lookahead variant it is also possible to use a model free implementation as we describe next model free implementation of the optimal policy of the aggregate problem the computation of the optimal policy of the aggregate problem via eq 4 13 requires knowledge of the transition probabilities pij u and the cost function g alternatively this policy may be implemented in model free fashion using a q factor architecture q i u as described in section 2 4 i e compute sample approximate q factors m g im um jm q 1 jm r m 1 m 4 14 cf eq 2 21 compute via a least squares regression argmin m m 1 q im um m 2 4 15 34 or a regularized version thereof cf eq 2 22 and approximate the optimal policy of the aggregate problem via i arg min u u i q i u i 1 n 4 16 cf eq 2 23 error bounds intuitively if the disaggregation sets nearly cover the entire state space in the sense that 1 qi contains most of the states 1 n and j is nearly constant over each disaggregation set then j 0 and j 1 should be close to j in particular in the case of hard aggregation we have the following error bound due to tsitsiklis and vanroy tsv 96 we adapt their proof to the notation and terminology of this paper proposition 4 1 in the case of hard aggregation where q 1 i 1 n and eqs 4 5 4 6 hold we have j i r 1 i such that i i 1 q 4 17 where max 1 q max i j i j i j j 4 18 proof consider the mapping h defined by eq 4 12 and consider the vector r with components defined by r min i i j i 1 1 q denoting by j the index of the disaggregation set to which j belongs i e j i j we have for all hr n i 1 d i min u u i n j 1 pij u g i u j r j n i 1 d i min u u i n j 1 pij u g i u j j j 1 n i 1 d i j i 1 min i i j i 1 min i i j i 1 r 35 where for the second equality we used the bellman equation for the original system which is satisfied by j and for the second inequality we used eq 4 18 thus we have hr r from which it follows that r r since h is monotone which implies that the sequence hkr is monotonically nonincreasing and we have r lim k hkr since h is a contraction this proves one side of the desired error bound the other side follows similarly q e d the scalar of eq 4 18 is the maximum variation of optimal cost within the sets of the partition of the hard aggregation scheme thus the meaning of the preceding proposition is that if the optimal cost function j varies by at most within each set of the partition the hard aggregation scheme yields a piecewise constant approximation to the optimal cost function that is within 1 of the optimal we know that for every approximation j of j that is constant within each disaggregation set the error max i 1 n j i j i is at least equal to 2 based on the bound 4 17 the actual value of this error for the case where j is obtained by hard aggregation involves an additional multiplicative factor that is at most equal to 2 1 and depends on the disaggregation probabilities in practice the bound 4 17 is typically conservative and no examples are known where it is tight moreover even for hard aggregation the manner in which the error j j 1 depends on the disaggregation distributions is complicated and is an interesting subject for research the following proposition extends the result of the preceding proposition to the case where the aggre gation probabilities are all either 0 or 1 in which case the cost function j 1 obtained by aggregation is a piecewise constant function but the disaggregation sets need not form a partition of the state space ex amples of this type of scheme include cases where the aggregation probabilities are generated by a nearest neighbor scheme and the cost j 1 j of a state j q 1 i is taken to be equal to the cost of the nearest state within q 1 i proposition 4 2 assume that each aggregation probability j j 1 n 1 q is equal to either 0 or 1 and consider the sets i j j 1 1 q 36 then we have j i r 1 i i 1 q where max 1 q max i j i j i j j proof we first note that by the definition of a feature based aggregation scheme we have i i for all 1 q while the sets i 1 q form a partition of the original state space in view of our assumption on the aggregation probabilities let us replace the feature vector f with another feature vector f of the form f i i i 1 q since the aggregation probabilities are all either 0 or 1 the resulting aggregation scheme with i replaced by i and with the aggregation and disaggregation probabilities remaining unchanged is a hard aggregation scheme when the result of prop 4 1 is applied to this hard aggregation scheme the result of the present proposition follows q e d the preceding propositions suggest the principal guideline for a feature based aggregation scheme it should be designed so that states that belong to the same disaggregation set have nearly equal optimal costs in section 4 3 we will elaborate on schemes that are based on this idea in the next section we discuss the solution of the aggregate problem by simulation based methods 4 2 solving the aggregate problem with simulation based methods we will now focus on methods to compute the optimal cost vector r of the aggregate problem that corre sponds to the aggregate states this is the unique solution of eq 4 11 we first note that since r together with the cost functions j 0 and j 1 form the solution of the bellman equations 4 8 4 10 they can all be computed with the classical exact methods of policy and value iteration pi and vi for short respectively however in this section we will discuss specialized versions of pi and vi that compute just r which has relatively low dimension but not j 0 and j 1 which may have astronomical dimension these methods are based on stochastic simulation as they involve the aggregate problem which is stochastic because of the disaggregation and aggregation probabilities even if the original problem is deterministic we start with simulation based versions of pi where policy evaluation is done with lookup table versions of classical methods such as lstd 0 lspe 0 and td 0 applied to a reduced size dp problem whose states are just the aggregate states 37 simulation based policy iteration one possible way to compute r is a pi like algorithm which generates sequences of policies k for the original problem and vectors rk which converge to an optimal policy and r respectively the algorithm does not compute any intermediate estimates of the high dimensional vectors j 0 and j 1 it starts with a stationary policy 0 for the original problem and given k it performs a policy evaluation step by finding the unique fixed point of the contraction mapping h k dt k that maps the vector r to the vector h kr with components h kr n i 1 d i n j 1 pij k i g i k i j q m 1 jm rm 1 q cf eq 4 12 thus the policy evaluation step finds rk rk 1 q satisfying rk h kr k dt k r k d g k p k r k 4 19 where p k is the transition probability matrix corresponding to k g k is the expected cost vector of k i e the vector whose ith component is n j 1 pij k i g i k i j i 1 n and d and are the matrices with rows the disaggregation and aggregation distributions respectively following the policy evaluation step the algorithm generates k 1 by k 1 i arg min u u i n j 1 pij u g i u j q m 1 jmr k m i 1 n 4 20 this is the policy improvement step in the preceding minimization we use one step lookahead but a multistep lookahead or monte carlo tree search can also be used it can be shown that this algorithm converges finitely to the unique solution of eq 4 11 equivalently the unique fixed point of the mapping h of eq 4 12 an indirect way to show this is to use the convergence of pi applied to the aggregate problem to generate a sequence k rk j k 0 j k 1 we provide a more direct proof which is essentially a special case of a more general convergence proof for pi type methods given in prop 3 1 of ber 11 a the key fact here is that the linear mappings dt and dt are sup norm contractions and also have the monotonicity property of dp mappings which is used in an essential way in the standard convergence proof of ordinary pi proposition 4 3 let 0 be any policy and let k rk be a sequence generated by the pi algorithm 4 19 4 20 then the sequence rk is monotonically nonincreasing i e we have rk rk 1 for all and k and there exists an index k such that rk is equal to r the unique solution of eq 4 11 38 proof for each policy we consider the linear mapping dt n 7 n given by dt j d g p j j n this mapping is monotone in the sense that for all vectors j and j with j j we have dt j dt j since the matrix dp of the mapping has nonnegative components moreover the mapping is a con traction of modulus with respect to the sup norm the reason is that the matrix dp is a transition probability matrix i e it has nonnegative components and its row sums are all equal to 1 this can be verified by a straightforward calculation using the fact that the rows of and d are probability distribu tions while p is a transition probability matrix it can also be intuitively verified from the structure of the aggregate problem dp is the matrix of transition probabilities under policy for the markov chain whose n states are the states depicted in the top righthand side of fig 4 2 since the mapping dt k has r k as its unique fixed point cf eq 4 19 we have rk dt k r k so that the vector j k r k satisfies j k dt k j k it follows that j k is the unique fixed point of the contraction mapping dt k by using the definition t k 1 j k t j k of k 1 cf eq 4 20 we have j k dt k j k dt j k dt k 1 j k 4 21 applying repeatedly the monotone mapping dt k 1 to this relation we have for all m 1 j k dt k 1 mj k lim m dt k 1 mj k j k 1 4 22 where the equality follows from the fact that j k 1 is the fixed point of the contraction mapping dt k 1 it follows that j k j k 1 or equivalently rk rk 1 k 0 1 by the definition of the feature based aggregation architecture cf eq 4 4 each column of has at least one component that is equal to 1 therefore we have for all k rk rk 1 39 and moreover the equality rk rk 1 holds if and only if j k j k 1 the inequality j k j k 1 implies that as long as j k 6 j k 1 a policy k cannot be repeated since there is only a finite number of policies it follows that we must eventually have j k j k 1 in view of eqs 4 21 4 22 we see that j k dt j k or rk dt rk since each column of has at least one component that is equal to 1 it follows that rk is a fixed point of the mapping h dt of eq 4 12 which is r by eq 4 11 q e d to avoid the n dimensional calculations of the policy evaluation step in the pi algorithm 4 19 4 20 one may use simulation in particular the policy evaluation equation r h r is linear of the form r dg dp r 4 23 cf eq 4 19 let us write this equation as cr b where c i dp b dg and note that it is bellman s equation for a policy with cost per stage vector equal to dg and transition probability matrix equal to dp this is the transition matrix under policy for the markov chain whose states are the aggregate states the solution r of the policy evaluation eq 4 23 is the cost vector corresponding to this markov chain and can be found by using simulation based methods with lookup table representation in particular we may use model free simulation to approximate c and b and then solve the system cr b approximately to this end we obtain a sequence of sample transitions i 1 j 1 i 2 j 2 by first generating a sequence of states i 1 i 2 according to some distribution i i 1 n with i 0 for all i and then generate for each m 1 a sample transition im jm according to the distribution pimj j 1 n given the first m samples we form the matrix c m and vector b m given by c m i m m m 1 1 im d im jm b m 1 m m m 1 1 im d im g im im jm 4 24 where d i is the ith column of d and j is the jth row of we can then show that c m c and b m d by using law of large numbers arguments i e writing c i n i 1 n j 1 pij i d i j b n i 1 n j 1 pij i d i g i i j 40 multiplying and dividing pij i by i in order to properly view these expressions as expected values and using the relation lim m number of occurrences of the i to j transition from time m 1 to m m m i pij i the corresponding estimates r m c 1 m b m converge to the unique solution of the policy evaluation eq 4 23 as m and provide the estimates r m of the cost vector j of j r m this is the aggregation counterpart of the lstd 0 method one may also use an iterative simulation based lspe 0 type method or a td 0 type method to solve the equation cr b see ber 12 note that instead of using the probabilities i to sample directly original system states we may alterna tively sample the aggregate states s according to some distribution 1 q generate a sequence of aggregate states s 1 s 2 and then generate a state sequence i 1 i 2 using the disaggregation probabilities in this case eq 4 24 should be modified as follows c m i m m m 1 1 md mim d im jm b m 1 m m m 1 1 md mim d im g im im jm the main difficulty with the policy improvement step at a given state i is the need to compute the expected value in the q factor expression n j 1 pij u g i u j q 1 j r k that is minimized over u u i if the transition probabilities pij u are available and the number of successor states the states j such that pij u 0 is small this expected value may be easily calculated an important case where this is so is when the system is deterministic otherwise one may consider approximating this expected value using one of the model free schemes described in section 2 4 simulation based value iteration and q learning an exact vi algorithm for obtaining r is the fixed point iteration rk 1 hrk starting from some initial guess r 0 where h is the contraction mapping of eq 4 12 a stochastic approximation type algorithm based on this fixed point iteration generates a sequence of aggregate states 41 s 0 s 1 by some probabilistic mechanism which ensures that all aggregate states are generated in finitely often given rk and s k it independently generates an original system state ik according to the probabilities d i and updates the component r k according to rk 1 k 1 k r k k k min u u i n j 1 pikj u g ik u j q 1 j r k where k is a diminishing positive stepsize and leaves all the other components unchanged rk 1 rk if 6 k this algorithm can be viewed as an asynchronous stochastic approximation version of vi the stepsize k should be diminishing typically at the rate of 1 k and its justification and convergence mechanism are very similar to the ones for the q learning algorithm we refer to the paper by tsitsiklis and vanroy tsv 96 for further discussion and analysis see also bet 96 section 3 1 2 and 6 7 a somewhat different algorithm is possible in the case of hard aggregation assuming that for every the set u i is the same for all states i in the disaggregation set i then as discussed in bet 96 section 6 7 7 we can introduce q factors that are constant within each set i and have the form q i u q u i i u u i we then obtain an algorithm that updates the q factors q u one at a time using a q learning type iteration of the form q u 1 q u g i u j min v u j q m j v where i is a state within i that is chosen with probability d i j is the outcome of a transition simulated according to the transition probabilities pij u the index m j corresponds to the aggregate state sm j to which j belongs and is the stepsize it can be seen that this algorithm coincides with q learning with lookup table representation applied to a lower dimensional aggregate dp problem that involves just the aggregate states with a suitably decreasing stepsize and assuming that each pair u is simulated an infinite number of times the standard convergence results for q learning tsi 94 apply we note however that the q learning algorithm just described has a substantial drawback it solves an aggregate problem that differs from the aggregate problem described in section 4 1 because implicit in the algorithm is the restriction that the same control is applied at all states i that belong to the same disaggregation set in effect we are assigning controls to subsets of states the disaggregation sets and not to individual states of the original problem clearly this is a coarser form of control which is inferior in terms of performance however the q learning algorithm may find some use in the context of initialization of another algorithm that aspires to better performance 42 4 3 feature formation by using scoring functions the choice of the feature mapping f and the method to obtain aggregate states are clearly critical for the success of feature based aggregation in the subsequent section 5 we will discuss how deep neural network architectures can be used for this purpose in what follows in this section we consider some simple forms of feature mappings that can be used when we already have a reasonable estimate of the optimal cost function j or the cost function j of some policy which we can use to group together states with similar estimated optimal cost then the aggregation approach can provide an improved piecewise constant or piecewise linear cost approximation we provide some simple illustrative examples of this approach in section 4 5 in particular suppose that we have obtained in some way a real valued scoring function v i of the state i which serves as an index of undesirability of state i as a starting state smaller values of v are assigned to more desirable states consistent with the view of v as some form of cost function one possibility is to use as v an approximation of the cost function of some good e g near optimal policy another possibility is to obtain v as the cost function of some reasonable policy applied to an approximation of the original problem e g a related problem that can be solved more easily either analytically or computationally see ber 17 section 6 2 still another possibility is to obtain v by training a neural network or other architecture using samples of state cost pairs obtained by using a software or human expert and some supervised learning technique such as for example tesauro s comparison learning scheme tes 89 b tes 01 finally one may compute v using some form of policy evaluation algorithm like td given the scoring function v we will construct a feature mapping that groups together states i with roughly equal scores v i in particular we let r 1 rq be q disjoint intervals that form a partition of the set of possible values of v i e are such that for any state i there is a unique interval r such that v i r we define a feature vector f i of the state i according to f i i such that v i r 1 q 4 25 this feature vector in turn defines a partition of the state space into the sets i i f i i v i r 1 q 4 26 assuming that all the sets i are nonempty we thus obtain a hard aggregation scheme with aggregation probabilities defined by eq 4 6 see fig 4 5 a related scoring function scheme may be based on representative states here the aggregate states and the disaggregation probabilities are obtained by forming a fairly large sample set of states im m 1 m by computing their corresponding scores v im m 1 m 43 3 i aggregate problem approximation aggregate problem approximation aggregate states scoring function v i j i i i r 1 r q r r 1 r 2 r 2 r 3 cost i 1 i 2 i 3 i i iq 1 iq piecewise constant aggregate problem approximation rq r q 1 r figure 4 5 hard aggregation scheme based on a single scoring function we introduce q disjoint intervals r 1 rq that form a partition of the set of possible values of v and we define a feature vector f i of the state i according to f i i such that v i r 1 q this feature vector in turn defines a partition of the state space into the sets i i f i i v i r 1 q the solution of the aggregate problem provides a piecewise constant approximation of the optimal cost function of the original problem and by suitably dividing the range of these scores into disjoint intervals r 1 rq to form the aggregate states similar to eqs 4 25 4 26 simultaneously we obtain subsets of sampled states i i to which we can assign positive disaggregation probabilities figure 4 6 illustrates this idea for the case where each subset i consists of a single representative state this is a form of discretization of the original state space based on the score values of the states as the figure indicates the role of the scoring function is to assist in forming a set of states that is small to keep the aggregate dp problem computations manageable but representative to provide sufficient detail in the approximation of j i e be dense in the parts of the state space where j varies a lot and sparse in other parts the following proposition illustrates the important role of the quantization error defined as max 1 q max i j i v i v j 4 27 44 3 i aggregate problem approximation aggregate problem approximation aggregate states scoring function v i j i i 11 i 2 2 i iq i i r 1 r q r figure 4 6 schematic illustration of aggregation based on sampling states and using a scoring function v to form a representative set i 1 iq a piecewise linear approximation of j is obtained by using the corresponding aggregate costs r 1 r q and the aggregation probabilities it represents the maximum error that can be incurred by approximating v within each set i with a single value from its range within the subset proposition 4 4 consider the hard aggregation scheme defined by a scoring function v as described above assume that the variations of j and v over the sets i 1 iq are within a factor 0 of each other i e that j i j j v i v j i j i 1 q a we have j i r 1 i i 1 q where is the quantization error of eq 4 27 b assume that there is no quantization error i e v and j are constant within each set i then the aggregation scheme yields the optimal cost function j exactly i e j i r i i 1 q 45 proof a since we are dealing with a hard aggregation scheme the result of prop 4 1 applies by our assumptions the maximum variation of j over the disaggregation sets i is bounded by and the result of part a follows from prop 4 1 b this is a special case of part a with 0 q e d examples of scoring functions that may be useful in various settings are cost functions of nearly optimal policies or approximations to such cost functions provided for example by a neural network or other approximation schemes another example arising in the adaptive aggregation scheme proposed by bertsekas and castanon bec 89 is to use as v i the residual vector tj i j i where j is some approximation to the optimal cost function j or the residual vector t j i j i where j is some approximation to the cost function of a policy see also keller mannor and precup kmp 06 note that it is not essential that v approximates well j or j what is important is that states with similar values of j or j also have similar values of v scoring function scheme with a state space partition another useful scheme is based on a scoring function v which is defined separately on each one of a collection of disjoint subsets c 1 cm that form a partition of the state space we define a feature vector f i that depends not only on the value of v i but also on the membership of i in the subsets of the partition in particular for each 1 m let r 1 rq be q disjoint intervals that form a partition of the set of possible values of v over the set c we then define f i i c such that v i r 4 28 this feature vector in turn defines a partition of the state space into the qm sets i i f i i c v i r 1 q 1 m which represent the disaggregation sets of the resulting hard aggregation scheme in this scheme the aggregate states depend not only on the values of v but also on the subset c of the partition using multiple scoring functions the approach of forming features using a single scoring function can be extended to the case where we have a vector of scoring functions v i v 1 i vs i then we can partition the set of possible values of v i into q disjoint subsets r 1 rq of the s dimensional space s define a feature vector f i according to f i i such that v i r 1 q 4 29 46 and proceed as in the case of a scalar scoring function i e construct a hard aggregation scheme with disaggregation sets given by i i f i i v i r 1 q one possibility to obtain multiple scoring functions is to start with a single fairly simple scoring function obtain aggregate states as described earlier solve the corresponding aggregate problem and use the optimal cost function of that problem as an additional scoring function this is reminiscent of feature iteration an idea that has been suggested in several approximate dp works a related and complementary possibility is to somehow construct multiple policies evaluate each of these policies perhaps approximately using a neural network and use the policy cost function evaluations as scoring functions this possibility may be particularly interesting in the case of a deterministic discrete optimization problem the reason is that the deterministic character of the problem may obviate the need for expensive simulation and neural network training as we discuss in the next section 4 4 using heuristics to generate features deterministic optimization and rollout an important context where it is natural to use multiple scoring functions is general deterministic optimiza tion problems with a finite search space for such problems simple heuristics are often available to obtain suboptimal solutions from various starting conditions e g greedy algorithms of various kinds the cost of each heuristic can then be used as a scoring function after the problem is converted to a finite horizon dp problem the formulation that we will use in this section is very general and for this reason the number of states of the dp problem may be very large alternative dp reformulations with fewer states may be obtained by exploiting the structure of the problem for example shortest path type problems and discrete time finite state deterministic optimal control control problems can be naturally posed as dp problems with a simpler and more economical formulation than the one given here in such cases the methodology to be described can be suitably adapted to exploit the problem specific structural characteristics the general discrete optimization problem that we consider in this section is minimize g u subject to u u 4 30 where u is a finite set of feasible solutions and g u is a cost function we assume that each solution u has n components i e it has the form u u 1 un where n is a positive integer we can then view the problem as a sequential decision problem where the components u 1 un are selected one at a time an m tuple u 1 um consisting of the first m components of a solution is called an m solution we associate 47 artificial start state end state artificial start state end state artificial start state end state artificial start state end state i i i set of states set of states set of states set of states set of states u 1 set of states u set of states u 1 u 2 set of states set of states u 1 u 2 u 3 set cost g u set of states u u 1 un figure 4 7 formulation of a discrete optimization problem as a dp problem there is a cost g u only at the terminal stage on the arc connecting an n solution u u 1 un to the artificial terminal state alternative formulations may use fewer states by taking advantage of the problem s structure m solutions with the mth stage of a finite horizon dp problem in particular for m 1 n the states of the mth stage are of the form u 1 um the initial state is a dummy artificial state from this state we may move to any state u 1 with u 1 belonging to the set u 1 u 1 there exists a solution of the form u 1 u 2 u n u thus u 1 is the set of choices of u 1 that are consistent with feasibility more generally from a state u 1 um we may move to any state of the form u 1 um um 1 with um 1 belonging to the set um 1 u 1 um u m 1 there exists a solution of the form u 1 um u m 1 u n u 4 31 the choices available at state u 1 um are um 1 um 1 u 1 um these are the choices of um 1 that are consistent with the preceding choices u 1 um and are also consistent with feasibility the terminal states correspond to the n solutions u u 1 un and the only nonzero cost is the terminal cost g u this terminal cost is incurred upon transition from u to an artificial termination state see fig 4 7 let j u 1 um denote the optimal cost starting from the m solution u 1 um i e the optimal cost of the problem over solutions whose first m components are constrained to be equal to ui i 1 m respectively if we knew the optimal cost to go functions j u 1 um we could construct an optimal solution by a sequence of n single component minimizations in particular an optimal solution u 1 u n our aggregation framework of section 4 1 extends in a straightforward manner to finite state finite horizon problems the main difference is that optimal cost functions feature vectors and scoring functions are not only state dependent but also stage dependent in effect the states are the m solutions for all values of m 48 could be obtained sequentially starting with u 1 and proceeding forward to u n through the algorithm u m 1 arg min um 1 um 1 u 1 u m j u 1 u m um 1 m 0 n 1 unfortunately this is seldom viable because of the prohibitive computation required to obtain the functions j u 1 um suppose that we have s different heuristic algorithms which we can apply for suboptimal solution we assume that each of these algorithms can start from any m solution u 1 um and produce an n solution u 1 um um 1 un the costs thus generated by the s heuristic algorithms are denoted by v 1 u 1 um vs u 1 um respectively and the corresponding vector of heuristic costs is denoted by v u 1 um v 1 u 1 um vs u 1 um note that the heuristic algorithms can be quite sophisticated and at a given partial solution u 1 um may involve multiple component choices from um 1 un and or suboptimizations that may depend on the previous choices u 1 um in complicated ways in fact the heuristic algorithms may require some preliminary experimentation and training using for example among others neural networks the main idea now is to use the heuristic cost functions as scoring functions to construct a feature based hard aggregation framework in particular for each m 1 n 1 we partition the set of possible values of v u 1 um into q disjoint subsets r m 1 r m q we define a feature vector f u 1 um according to f u 1 um u 1 um such that v u 1 um r m 1 q 4 32 and we construct a hard aggregation scheme with disaggregation sets for each m 1 n 1 given by im u 1 um v u 1 um r m 1 q note that the number of aggregate states is roughly similar for each of the n 1 stages by contrast the number of states of the original problem may increase very fast exponentially as n increases cf fig 4 7 the aggregation scheme is illustrated in fig 4 8 it involves n 1 successive transitions between m solutions to m 1 solutions m 1 n 1 interleaved with transitions to and from the corre sponding aggregate states the aggregate problem is completely defined once the aggregate states and the disaggregation probabilities have been chosen the transition mechanism of stage m involves the following steps there are several variants of this scheme involving for example a state space partition as in section 4 3 moreover the method of partitioning the decision vector u into its components u 1 un may be critically important in specific applications 49 artificial start state end state artificial start state end state i i i i set of states set of states set of states set of states set of states set of states u 1 set of states u set of states u 1 u 2 set of states set of states u 1 u 2 u 3 set cost g u set of states u u 1 un solutions u u 1 un 1 aggr states stage 1 stage 2 stage 3 stageaggr states stage 1 stage 2 stage 3 stageaggr states stage 1 stage 2 stage 3 stageaggr states stage 1 stage 2 stage 3 stage aggr states stage 1 stage 2 stage 3 stageaggr states stage 1 stage 2 stage 3 stageaggr states stage 1 stage 2 stage 3 stageaggr states stage 1 stage 2 stage 3 stage n 1 figure 4 8 schematic illustration of the heuristics based aggregation scheme for discrete optimization the aggregate states are defined by the scoring functions heuristics and the optimal aggregate costs are obtained by dp stating from the last stage and proceeding backwards 1 from an aggregate state at stage m we generate some state u 1 um i m according to the disaggregation probabilities 2 we transition to the next state u 1 um um 1 by selecting the control um 1 3 we run the s heuristics from the m 1 solution u 1 um um 1 to determine the next aggregate state which is the index of the set of the partition of stage m 1 to which the vector v u 1 um um 1 v 1 u 1 um um 1 vs u 1 um um 1 belongs a key issue is the selection of the disaggregation probabilities for each stage this requires for each value of m the construction of a suitable sample of m solutions where the disaggregation sets im are adequately represented the solution of the aggregate problem by dp starts at the last stage to compute the corresponding aggregate costs r n 1 for each of the aggregate states using g u as terminal cost function then it proceeds with the next to last stage to compute the corresponding aggregate costs r n 2 using the previously computed aggregate costs r n 1 etc the optimal cost function j u 1 um for stage m is approximated by a piecewise constant function 50 artificial start state end state artificial start state end state artificial start state end state artificial start state end state i i set of states u 1 set of states set of states u 1 u 2 solution u 1 u m candidate candidate m 1 solutions 1 solutions u 1 u m um 1 run the heuristics from each candidate run the heuristics from each candidate run the heuristics from each candidate m 1 solution figure 4 9 sequential construction of a suboptimal n solution u 1 u n for the original problem after the aggregate problem has been solved given the m solution u 1 u m we run the s heuristics from each of the candidate m 1 solution u 1 u m um 1 and compute the aggregate state and aggregate cost of this candidate m 1 solution we then select as u m 1 the one that corresponds to the candidate m 1 solution with minimal aggregate cost which is derived by solving the aggregate problem this is the function j u 1 um r m u 1 um with v u 1 um r m 4 33 where r m is the optimal cost of aggregate state at stage m of the aggregate problem once the aggregate problem has been solved for the costs r m a suboptimal n solution u 1 u n for the original problem is obtained sequentially starting from stage 1 and proceeding to stage n through the minimizations u 1 argmin u 1 j u 1 4 34 u m 1 arg min um 1 um 1 u 1 u m j u 1 u m um 1 m 1 n 1 4 35 note that to evaluate each of the costs j u 1 u m um 1 needed for this minimization we need to do the following see fig 4 9 1 run the s heuristics from the m 1 solution u 1 u m um 1 to evaluate the scoring vector of heuristic costs v u 1 u m um 1 v 1 u 1 u m um 1 vs u 1 u m um 1 2 set j u 1 u m um 1 to the aggregate cost r m 1 of the aggregate state s m 1 corresponding to this scoring vector i e to the set r m 1 such that v u 1 u m um 1 r m 1 51 once j u 1 u m um 1 has been computed for all um 1 um 1 u 1 u m we select u m 1 via the minimization 4 35 and repeat starting from the m 1 solution u 1 u m u m 1 note that even if there is only one heuristic u m 1 minimizes the aggregate cost r m 1 which is not the same as the cost corresponding to the heuristic we finally mention a simple improvement of the scheme just described for constructing an n solution in the course of the algorithm many other n solutions are obtained during the training and final solution selection processes it is possible that some of these solutions are actually better have lower cost g u than the final n solution u 1 u n that is constructed by using the aggregate problem formulation this can happen because the aggregation scheme is subject to quantization error thus it makes sense to maintain the best of the n solutions generated in the course of the algorithm and compare it at the end with the n solution obtained through the aggregation scheme this is similar to the so called fortified version of the rollout algorithm see btw 97 or ber 17 relation to the rollout algorithm the idea of using one or more heuristic algorithms as a starting point for generating an improved solution of a discrete optimization problem is shared by other suboptimal control approaches a prime example is the rollout algorithm which in some contexts can be viewed as a single policy iteration see btw 97 for an analysis of rollout for discrete optimization problems and the textbook ber 17 for an extensive discussion and many references to applications including the important model predictive control methodology for control system design basically the rollout algorithm uses the scheme of fig 4 9 to construct a suboptimal solution u 1 u n in n steps one component at a time but adds a new decision u m 1 to the current m solution u 1 u m in a simpler way it runs the s heuristics from each candidate m 1 solution u 1 u m um 1 and computes the corresponding heuristic costs v 1 u 1 u m um 1 vs u 1 u m um 1 it then selects as the next decision u m 1 the one that minimizes over um 1 um 1 u 1 u m the best heuristic cost v u 1 u m um 1 min v 1 u 1 u m um 1 vs u 1 u m um 1 i e it uses v in place of j in eqs 4 34 4 35 in practice the rollout algorithm s heuristics may involve sophisticated suboptimizations that may make sense in the context of the problem at hand note that the construction of the final n solution is similar and equally complicated in the rollout and the scoring vector based aggregation approach however the aggregation approach requires an extra layer 52 artificial start state end state artificial start state end state artificial start state end state artificial start state end state i i set of states u 1 set of states solution u 1 u m candidate run the heuristics from each candidate run the heuristics from each candidate candidate m 2 solutions 2 solutions u 1 u m um 1 um 2 run the heuristics from each candidate m 2 solution figure 4 10 sequential construction of a suboptimal n solution u 1 u n by using two step lookahead after the aggregate problem has been solved given the m solution u 1 u m we run the s heuristics from all the candidate m 2 solutions u 1 u m um 1 um 2 and select as u m 1 the first component of the two step sequence that corresponds to minimal aggregate cost of computation prior to constructing the n solution namely the solution of an aggregate problem this may be a formidable problem because it is stochastic due to the use of disaggregation probabilities and must be solved exactly at least in principle still the number of states of the aggregate problem may be quite reasonable and its solution is well suited for parallel computation on the other hand setting aside the issue of computational solution of the aggregate problem the heuristics based aggregation algorithm has the potential of being far superior to the rollout algorithm for the same reason that approximate policy improvement based on aggregation can be far superior to policy improvement based on one step lookahead in particular with sufficiently large number of aggregate states to eliminate the effects of the quantization error feature based aggregation will find an optimal solution regardless of the quality of the heuristics used by contrast policy iteration and rollout can only aspire to produce a solution that is better than the one produced by the heuristics using multistep lookahead and monte carlo tree search once the aggregate problem that is based on multiple scoring functions has been solved the final n solution can be constructed in more sophisticated ways than the one described in fig 4 9 it can be seen that the scheme of eqs 4 34 4 35 and fig 4 9 is based on one step lookahead it is possible instead to use multistep lookahead or randomized versions such as monte carlo tree search as an example in a two step lookahead scheme we again obtain a suboptimal solution u 1 u n for the original problem in n stages starting from stage 1 and proceeding to stage n at stage 1 we carry 53 out the two step minimization u 1 u 2 arg min u 1 u 2 j u 1 u 2 4 36 and fix the first component u 1 of the result cf fig 4 10 we then proceed sequentially for m 1 n 2 given the current m solution u 1 u m we carry out the two step minimization u m 1 u m 2 arg min um 1 um 2 j u 1 u m um 1 um 2 m 1 n 2 4 37 and fix the first component u m 1 of the result cf fig 4 10 at the final stage given the n 1 solution u 1 u n 1 we carry out the one step minimization u n argmin un j u 1 u n 1 un 4 38 and obtain the final n solution u 1 u n multistep lookahead generates a tree of fixed depth that is rooted at the last node u m of the current m solution and then runs the heuristics from each of the leaf nodes of the tree we can instead select only a subset of these leaf nodes from which to run the heuristics thereby economizing on computation the selection may be based on some heuristic criterion monte carlo tree search similarly uses multistep lookahead but selects only a random sample of leaf nodes to search based on some criterion in a more general version of monte carlo tree search instead of a single partial solution we maintain multiple partial solutions possibly of varying length at each step a one step or multistep lookahead tree is generated from the most promising of the current partial solutions selected by using a randomization mechanism the heuristics are run from the leafs of the lookahead trees similar to fig 4 10 then some of the current partial solutions are expanded with an additional component based on the results produced by the heuristics this type of monte carlo tree search has been suggested for use in conjunction with rollout see the paper rss 12 and it can be similarly used with feature based aggregation 4 5 stochastic shortest path problems illustrative examples our aggregation framework extends straightforwardly to stochastic shortest path ssp for short problems where there is no discounting and in addition to the states 1 n there is an additional cost free and absorbing termination state denoted 0 the text references given earlier discuss in detail such problems the principal change needed is to account for the termination state by introducing an additional aggregate state with corresponding disaggregation set 0 as before there are also other aggregate states s 1 sq whose disaggregation sets i 1 iq are subsets of 1 n with this special handling of the termination state the aggregate problem becomes a standard ssp problem whose termination state is the aggregate state corresponding to 0 the bellman equation of the aggregate problem is given by r n i 1 d i min u u i n j 1 pij u g i u j q m 1 jm rm 1 q 4 39 54 cf eq 4 12 it has a unique solution under some well known conditions that date to the paper by bertsekas and tsitsiklis bet 91 there exists at least one proper policy i e a stationary policy that guarantees eventual termination from each initial state with probability 1 moreover all stationary policies that are not proper have infinite cost starting from some initial state in particular these conditions are satisfied if all stationary policies are proper we will now provide two simple illustrative ssp examples which were presented in the author s paper ber 95 as instances of poor performance of td and other methods that are based on projected equations and temporal differences see also the book bet 96 section 6 3 2 in these examples the cost function of a policy will be approximated by using feature based aggregation and a scoring function obtained using either the td 1 or the td 0 algorithms the approximate cost function computed by aggregation will be compared with the results of td 1 and td 0 we will show that aggregation provides a much better approximation suggesting better policy improvement results in a pi context our examples involve a problem with a single policy where the corresponding markov chain is deterministic with n states plus a termination state 0 under when at state i 1 n we move to state i 1 at a cost gi thus starting at state i we traverse each of the states i 1 1 and terminate at state 0 at costs gi gi 1 g 1 respectively while accumulating the total cost j i gi g 1 i 1 n with j 0 0 we consider a linear approximation to this cost function which we denote by v v i ri i 1 n where r is a scalar parameter this parameter may be obtained by using any of the simulation based methods that are available for training linear architectures including td in the subsequent discussion we will assume that td is applied in an idealized form where the simulation samples contain no noise the td 1 algorithm is based on minimizing the sum of the squares of the differences between j and v over all states yielding the approximation v 1 i r 1 i i 0 1 n where r 1 argmin r n i 1 j i ri 2 4 40 here consistent with our idealized setting of noise free simulation we assume that j i is computed exactly for all i the td 0 algorithm is based on minimizing the sum of the squares of the errors in satisfying the bellman equation v i gi v i 1 or temporal differences over all states yielding the approximation v 0 i r 0 i i 0 1 n 55 0 1 10 20 30 40 50 1 10 20 30 40 50 0 n n state i i cost function j i td 1 approximation td 0 approximation v 1 i and and v 0 i td 1 approximation td 0 approximation td 1 approximation td 0 approximation figure 4 11 form of j i and the linear approximations v 1 i and v 0 i for case a g 1 1 and gi 0 for all i 2 n where r 0 argmin r n i 1 gi r i 1 ri 2 4 41 again we assume that the temporal differences gi r i 1 ri are computed exactly for all i the straightforward solution of the minimization problems in eqs 4 40 and 4 41 yields r 1 n g 1 gn n 1 g 1 gn 1 g 1 n 2 n 1 2 1 and r 0 ngn n 1 gn 1 g 1 n n 1 1 consider now two different choices of the one stage costs gi a g 1 1 and gi 0 for all i 6 1 b gn n 1 and gi 1 for all i 6 n figures 4 11 and 4 12 provide plots of j i and the approximations v 1 i and v 0 i for these two cases these plots come from ber 95 where the number of states used was n 50 it can be seen that v 1 i and particularly v 0 i are poor approximations of j i suggesting that if used for policy improvement they may yield a poor successor policy we will now consider a hard aggregation scheme based on using v 1 and v 0 as scoring functions the aggregate states of such a scheme in effect consist of disaggregation subsets i 1 iq with q 1 i 1 n 56 0 0 nn state i i cost function j i td 1 approximation td 0 approximation v 1 i and and v 0 i td 1 approximation td 0 approximation td 1 approximation td 0 approximation n n 1 state figure 4 12 form of j i and the linear approximations v 1 i and v 0 i for case b gn 1 n and gi 1 for all i 1 n 1 plus the subset 0 that serves as the termination state of the aggregate problem with either v 1 or v 0 as the scoring function the subsets i 1 iq consist of contiguous states in order to guarantee that the termination state is eventually reached in the aggregate problem we assume that the disaggregation probability of the smallest state within each of the subsets i 1 iq is strictly positive this is a mild restriction which is naturally satisfied in typical schemes that assign equal probability to all the states in a disaggregation set consider first case a cf fig 4 11 then because the policy cost function j is constant within each of the subsets i 1 iq the scalar in prop 4 1 is equal to 0 implying that the hard aggregation scheme yields the optimal cost function i e r j i for all i i to summarize in case a the td 0 approach yields a very poor linear cost function approximation the td 1 approach yields a poor linear cost function approximation but the aggregation scheme yields the nonlinear policy cost function j exactly consider next case b cf fig 4 12 then the hard aggregation scheme yields a piecewise constant approximation to the optimal cost function the quality of the approximation is degraded by quantization effects in particular as the variations of j and v 1 or v 0 increase over the disaggregation sets i 1 iq the quality of the approximation deteriorates as predicted by prop 4 4 similarly as the number of states in the disaggregation sets i 1 iq is reduced the quality of the approximation improves as illustrated in fig 57 0 0 nn state i i cost function j i n n 1 state 1 10 20 30 40 50 i i 1 iqq i 2 i 1 i 1 aggregate problem approximation aggregate problem approximation figure 4 13 schematic illustration of the piecewise constant approximation of j that is provided by hard aggregation based on the scoring functions v 1 and v 0 in case b 4 13 in the extreme case where there is only one state in each of the disaggregation sets the aggregation scheme yields exactly j to summarize in case b the td 0 approach yields a very poor linear cost function approximation the td 1 approach yields a reasonably good linear cost function approximation while the aggregation scheme yields a piecewise constant approximation whose quality depends on the coarseness of the quantization that is implicit in the selection of the number q of disaggregation subsets the example of case b also illustrates how the quality of the scoring function affects the quality of the approximation provided by the aggregation scheme here both v 1 and v 0 work well as scoring functions despite their very different form because states with similar values of j also have similar values of v 1 as well as v 0 cf prop 4 4 4 6 multistep aggregation the aggregation methodology discussed so far is based on the aggregate problem markov chain of fig 4 2 which returns to an aggregate state after a single transition of the original chain we may obtain alternative aggregation frameworks by considering a different markov chain which starting from an aggregate state involves multiple original system state transitions before return to an aggregate state we discuss two possibilities a k step aggregation here we require a fixed number k of transitions between original system states before returning to an aggregate state 58 b aggregation here the number k of transitions prior to returning to an aggregate state is controlled by some randomization mechanism in the case where k is geometrically distributed with parameter 0 1 this method involves multistep mappings that arise in temporal difference contexts and facilitate the use of temporal differences methodology another related possibility which we do not discuss in this paper is to introduce temporal abstractions faster multistep macro actions and transitions between selected states with suitably computed transition costs into the upper original system portion of the aggregate problem markov chain of fig 4 2 there have been many proposals of this type in the reinforcement learning literature under various names for some representative works see hauskrecht et al hmk 98 sutton precup and singh sps 99 parr and russell par 98 dietterich die 00 konidaris and barto kob 09 ciosek and silver cis 15 mann mannor and precup mmp 15 serban et al ssp 18 and the references cited there it is likely that some of these proposals can be fruitfully adapted to our feature based aggregation context and this is an interesting subject for further research k step aggregation this scheme suggested in ber 11 a and illustrated in fig 4 14 is specified by disaggregation and aggregation probabilities as before but involves k 1 transitions between original system states in between transitions from and to aggregate states the aggregate dp problem for this scheme involves k 1 copies of the original state space in addition to the aggregate states we accordingly introduce vectors j 0 j 1 j k and r r 1 r q where r is the optimal cost to go from aggregate state s j 0 i is the optimal cost to go from original system state i that has just been generated from an aggregate state left side of fig 4 14 j 1 j 1 is the optimal cost to go from original system state j 1 that has just been generated from an original system state i j m jm m 2 k is the optimal cost to go from original system state jm that has just been generated from an original system state jm 1 these vectors satisfy the following set of bellman equations r n i 1 d ij 0 i 1 q j 0 i min u u i n j 1 1 pij 1 u g i u j 1 j 1 j 1 i 1 n 4 42 59 according to pij u with cost according to pij u with costaccording to pij u with cost i original system states aggregate states original system states aggregate states aggregation probabilities disaggregation probabilities aggregation probabilities disaggregation probabilities aggregation probabilities disaggregation probabilities with aggregation aggregate states aggregate states s s d i 0 if i i j 1 if j i k stages stages j 1 1 j 2 2 jk 2 jk 1 2 g i u j 2 g i u j 2 g i u j 1 figure 4 14 the transition mechanism for multistep aggregation it is based on a dynamical system involving k transitions between original system states interleaved between transitions from and to aggregate states j m jm min u u jm n jm 1 1 pjmjm 1 u g jm u jm 1 j m 1 jm 1 jm 1 n m 1 k 1 4 43 j k jk q 1 jk r jk 1 n 4 44 by combining these equations we obtain an equation for r r dt k r where t is the usual dp mapping of the original problem the case k 1 corresponds to eqs 4 11 4 12 as earlier it can be seen that the associated mapping dt k is a contraction mapping with respect to the sup norm but its contraction modulus is k rather than there is a similar mapping corresponding to a fixed policy and it can be used to implement a pi algorithm which evaluates a policy through calculation of a corresponding parameter vector r and then improves it however there is a major difference from the single step aggregation case a policy involves a set of k control functions 0 k 1 and while a known policy can be easily simulated its improvement involves multistep lookahead using the minimizations of eqs 4 42 4 44 and may be costly thus the preceding implementation of multistep aggregation based pi is a useful idea only for problems where the cost of this multistep lookahead minimization for a single given starting state is not prohibitive on the other hand from a theoretical point of view a multistep aggregation scheme provides a means of better approximation of the true optimal cost vector j independent of the use of a large number of 60 aggregate states this can be seen from eqs 4 42 4 44 which by classical value iteration convergence results show that j 0 i j i as k regardless of the choice of aggregate states moreover because the modulus of the underlying contraction is k we can verify an improved error bound in place of the bound 4 17 of prop 4 1 which corresponds to k 1 j i r 1 k i such that i i 1 q where is given by eq 4 18 the proof is very similar to the one of prop 4 1 aggregation multistep aggregation need not involve sequences of a fixed number of transitions between original system states the number of transitions may be state dependent or may be controlled by some randomized mech anism in one such possibility called aggregation we introduce a parameter 0 1 and consider a markov chain that makes a transition with probability 1 from an original system state to an aggregate state at each step rather than with certainty after k steps as in fig 4 14 then it can be shown that the cost vector of a given stationary policy may be evaluated approximately by r where r is the solution of the equation r dt r 4 45 where t is the mapping given by eq 2 14 this equation has a unique solution because the mapping dt can be shown to be a contraction mapping with respect to the sup norm as noted earlier the aggregation equation r dt r is a projected equation because d is a projection mapping with respect to a suitable weighted euclidean seminorm see yub 12 section 4 it is a norm projection in the case of hard aggregation similarly the aggregation equation r dt r is a projected equation which is related to the proximal algorithm ber 16 a ber 18 b and may be solved by using temporal differences thus we may use exploration enhanced versions of the lstd and lspe methods in an approximate pi scheme to solve the aggregation equation we refer to ber 12 for further discussion 5 policy iteration with feature based aggregation and a neural network we noted in section 3 that neural networks can be used to construct features at the output of the last nonlinear layer the neural network training process also yields linear weighting parameters for the feature 61 neural network features approximate cost policy improvement policy improvement approximately improved policy approximately improved policy current policy current policy improved policy neural network features approximate cost neural network features approximate cost approximately improved policy feature vector f approximate cost j f i plays much better than all computer programs f i figure 5 1 schematic illustration of pi using a neural network based cost approximation starting with a training set of state cost pairs generated using the current policy the neural network yields a set of features and an approximate cost evaluation j using a linear combination of the features this is followed by policy improvement using j to generate the new policy vector f i at the output of the last layer thus obtaining an approximation j f i to the cost function of a given policy thus given the current policy the typical pi produces the new policy using the approximate policy improvement operation 1 3 or a multistep variant as illustrated in fig 5 1 a similar pi scheme can be constructed based on feature based aggregation with features supplied by the same neural network see fig 5 2 the main idea is to replace the approximate policy improvement operation with the solution of an aggregate problem which provides the approximately improved policy this is a more complicated policy improvement operation but computes the new policy based on a more accurate cost function approximation one that is a nonlinear function of the features rather than linear moreover not only aspires to be an improved policy relative to but also to be an optimal policy based on the aggregate problem an approximation itself of the original dp problem in particular suppose that the neural network approximates j perfectly then the scheme of fig 5 1 will replicate a single step of the pi algorithm starting from while the aggregation scheme of fig 5 2 with sufficient number of aggregate states will produce a policy that is arbitrarily close to optimal let us now explain each of the steps of the aggregation based pi procedure of fig 5 2 starting with the current policy a feature mapping construction we train the neural network using a training set of state cost pairs that are generated using the current policy this provides a feature vector f i as described in section 3 b sampling to obtain the disaggregation sets we sample the state space generating a subset of states i 1 n we partition the corresponding set of state feature pairs i f i i i into a collection of subsets s 1 sq we then consider the aggregation framework with s 1 sq as the aggregate states and the corresponding aggregate problem as described in section 4 the sampling to obtain the set of states i may be combined with exploration to ensure that a sufficiently representative set of states is included 62 current policy current policy improved policy approximately improved policy approximately improved policy neural network features approximate cost sampling optimization neural network features approximate cost neural network features approximate cost aggregate optimization approximately improved policy with aggregation problem aggregate states aggregate states s feature vector feature vector figure 5 2 illustration of pi using feature based aggregation with features supplied by a neural network starting with a training set of state cost pairs generated using the current policy the neural network yields a set of features which are used to construct a feature based aggregation framework the optimal policy of the corresponding aggregate problem is used as the new policy c aggregate problem solution the aggregate dp problem is solved by using a simulation based method to yield perhaps approximately the aggregate state optimal costs r 1 q cf section 4 2 d definition of the improved policy the improved policy is simply the optimal policy of the aggregate problem or an approximation thereof obtained for example after a few iterations of approximate simulation based pi this policy is defined implicitly by the aggregate costs r 1 q using the one step lookahead minimization i arg min u u i n j 1 pij u g i u j q 1 j r i 1 n cf eq 4 9 or a multistep lookahead variant alternatively the improved policy can implemented in model free fashion using a q factor architecture q i u as described in sections 2 4 and 4 1 cf eqs 4 14 4 16 let us also note that there are several options for implementing the algorithmic ideas of this section 1 the neural network based feature construction process may be performed any number of times each time followed by an aggregate problem solution that constructs a new policy which is then used to generate new training data for the neural network alternatively the neural network training and feature construction process may be done only once followed by the solution of the corresponding feature based aggregate problem 2 several deep neural network based pi cycles may be performed a subset of the features thus generated may be selected and the corresponding aggregate problem is solved just once as a way of improving the final policy generated by the deep reinforcement learning process 3 following each cycle of neural network based feature evaluation the generated features may be sup plemented with additional problem specific handcrafted features and or features from previous cycles this is a form of feature iteration that was noted in the preceding section finally let us mention a potential weakness of using the features obtained at the output of the last 63 nonlinear layer of the neural network in the context of aggregation the sheer number of these features may be so large that the resulting number of aggregate states may become excessive to address this situation one may consider pruning some of the features or reducing their number using some form of regression at the potential loss of some approximation accuracy in this connection let us also emphasize a point made earlier in connection with an advantage of deep rather than shallow neural networks because with each additional layer the generated features tend to be more complex their number at the output of the final nonlinear layer of the network can be made smaller as the number of layers increases an extreme case is to use the cost function approximation obtained at the output of the neural network as a single feature scoring function in the spirit of section 4 3 using neural networks in conjunction with heuristics we noted at the end of section 4 3 another use of neural networks in conjunction with aggregation somehow construct multiple policies evaluate each of these policies using a neural network and use the policy cost function evaluations as multiple scoring functions in a feature based aggregation scheme in section 4 4 we elaborated on this idea for the case of the deterministic discrete optimization problem minimize g u 1 un subject to u 1 un u where u is a finite set of feasible solutions and g is a cost function cf eq 4 30 we described the use of multiple heuristics to construct corresponding scoring functions at any given m solution the scoring function values are computed by running each of the heuristics a potential time saving alternative is to approximate these scoring functions using neural networks in particular for each of the heuristics we may train a separate neural network by using a training set consisting of pairs of m solutions and corresponding heuristic costs in this way we can obtain approximate scoring functions v 1 u 1 um 1 v s u 1 um s where 1 s are the corresponding neural network weight vectors we may then use the approximate scoring functions as features in place of the exact heuristic cost functions to construct an aggregate problem similar to the one described in section 4 4 the solution of the aggregate problem can be used in turn to define a new policy which may optionally be added to the current set of heuristics as discussed earlier note that a separate neural network is needed for each heuristic and stage so assembling the training data together with the training itself can be quite time consuming however both the data collection and the training processes can benefit greatly from parallelization finally let us note that the approach of using a neural network to obtain approximate scoring functions may also be used in conjunction with a rollout scheme that uses a limited horizon in such a scheme starting 64 from an m solution we may evaluate all possible subsequent m 1 solutions by running each of the s heuristics up to a certain horizon depth of d steps rather than the full depth of n m 1 steps and then approximate the subsequent heuristic cost from stage m 1 d to stage n by using the neural network estimates 6 concluding remarks we have surveyed some aspects of approximate pi methods with a focus on a new idea for policy improvement feature based aggregation that uses features provided by a neural network or a heuristic scheme perhaps in combination with additional handcrafted features we have argued that this type of policy improvement while more time consuming may yield more effective policies owing to the dp character of the aggregate problem and the use of a nonlinear feature based architecture the algorithmic idea of this paper seems to work well on small examples however tests with challenging problems are needed to fully evaluate its merits particularly since solving the aggregate dp problem is more time consuming than the standard one step lookahead policy improvement scheme of eq 2 20 or its multistep lookahead variants in this paper we have focused on finite state discounted markov decision problems but our approach clearly extends to other types of finite state dp involving stochastic uncertainty including finite horizon stochastic shortest path and semi markov decision problems it is also worth considering extensions to infinite state problems including those arising in the context of continuous spaces optimal control shortest path and partially observed markov decision problems generally the construction of aggregation frame works for continuous spaces problems is conceptually straightforward and follows the pattern discussed in this paper for finite state problems for example a hard aggregation scheme involves a partition of the continuous state space into a finite number of subsets aggregate states while a representative states scheme involves discretization of the continuous state space using a finite number of states note however that from a mathematical point of view there may be a substantial issue of consistency i e whether the solution of the aggregate problem converges to the solution of the continuous spaces problem as the number of aggregate states increases part of the reason has to do with the fact that the bellman equation of continuous spaces problems need not have a unique solution the author s monograph ber 18 a sections 4 5 and 4 6 provides an analysis of this question for shortest path and optimal control problems with a continuous state space and identifies classes of problems that are more amenable to approximate dp solution approaches finally we note that the key issue of feature construction can be addressed in a number of ways in this paper we have focused on the use of deep neural networks and heuristics for approximating the optimal cost function or the cost functions of policies however we may use instead any methodology that automatically constructs good features at reasonable computational cost 65 7 references adb 17 arulkumaran k deisenroth m p brundage m and bharath a a 2017 a brief survey of deep reinforcement learning arxiv preprint arxiv 1708 05866 abr 90 abramson b 1990 expected outcome a general model of static evaluation ieee transac tions on pattern analysis and machine intelligence vol 12 pp 182 193 bbd 10 busoniu l babuska r de schutter b and ernst d 2010 reinforcement learning and dynamic programming using function approximators crc press n y bbs 87 bean j c birge j r and smith r l 1987 aggregation in dynamic programming oper ations research vol 35 pp 215 220 bbs 95 barto a g bradtke s j and singh s p 1995 real time learning and control using asynchronous dynamic programming artificial intelligence vol 72 pp 81 138 bpw 12 browne c powley e whitehouse d lucas l cowling p i rohlfshagen p tavener s perez d samothrakis s and colton s 2012 a survey of monte carlo tree search methods ieee trans on computational intelligence and ai in games vol 4 pp 1 43 bsa 83 barto a g sutton r s and anderson c w 1983 neuronlike elements that can solve difficult learning control problems ieee trans on systems man and cybernetics vol 13 pp 835 846 btw 97 bertsekas d p tsitsiklis j n and wu c 1997 rollout algorithms for combinatorial optimization heuristics vol 3 pp 245 262 bec 89 bertsekas d p and castanon d a 1989 adaptive aggregation methods for infinite horizon dynamic programming ieee trans on aut control vol ac 34 pp 589 598 bec 99 bertsekas d p and castanon d a 1999 rollout algorithms for stochastic scheduling prob lems heuristics vol 5 pp 89 108 bet 91 bertsekas d p and tsitsiklis j n 1991 an analysis of stochastic shortest path problems math operations research vol 16 pp 580 595 bet 96 bertsekas d p and tsitsiklis j n 1996 neuro dynamic programming athena scientific bel mont ma bet 00 bertsekas d p and tsitsiklis j n 2000 gradient convergence in gradient methods siam j on optimization vol 10 pp 627 642 ber 95 bertsekas d p 1995 a counterexample to temporal differences learning neural computation vol 7 pp 270 279 ber 11 a bertsekas d p 2011 approximate policy iteration a survey and some new methods j of 66 control theory and applications vol 9 pp 310 335 ber 11 b bertsekas d p 2011 temporal difference methods for general projected equations ieee trans on aut control vol 56 pp 2128 2139 ber 11 c bertsekas d p 2011 policy iteration a review and a new implementation lab for in formation and decision systems report lids p 2874 mit in reinforcement learning and approximate dynamic programming for feedback control by f lewis and d liu eds ieee press computational intelligence series 2012 ber 12 bertsekas d p 2012 dynamic programming and optimal control vol ii approximate dynamic programming 4 th edition athena scientific belmont ma ber 13 bertsekas d p 2013 rollout algorithms for discrete optimization a survey handbook of combinatorial optimization springer ber 15 bertsekas d p 2015 convex optimization algorithms athena scientific belmont ma ber 16 a bertsekas d p 2016 proximal algorithms and temporal differences for large linear systems extrapolation approximation and simulation report lids p 3205 mit arxiv preprint arxiv 1610 05427 ber 16 b bertsekas d p 2016 nonlinear programming 3 rd edition athena scientific belmont ma ber 17 bertsekas d p 2017 dynamic programming and optimal control vol i 4 th edition athena scientific belmont ma ber 18 a bertsekas d p 2018 abstract dynamic programming athena scientific belmont ma ber 18 b bertsekas d p 2018 proximal algorithms and temporal difference methods for solving fixed point problems computational optimization and applications j vol 70 pp 709 736 bis 95 bishop c m 1995 neural networks for pattern recognition oxford university press n y cfh 05 chang h s hu j fu m c and marcus s i 2005 an adaptive sampling algorithm for solving markov decision processes operations research vol 53 pp 126 139 cfh 13 chang h s hu j fu m c and marcus s i 2013 simulation based algorithms for markov decision processes 2 nd ed springer n y cao 07 cao x r 2007 stochastic learning and optimization a sensitivity based approach springer n y chk 86 christensen j and korf r e 1986 a unified theory of heuristic evaluation functions and its application to learning in proceedings aaai 86 pp 148 152 chm 82 chatelin f and miranker w l 1982 acceleration by aggregation of successive approximation methods linear algebra and its applications vol 43 pp 17 47 67 cis 15 ciosek k and silver d 2015 value iteration with options and state aggregation report centre for computational statistics and machine learning university college london cou 06 coulom r 2006 efficient selectivity and backup operators in monte carlo tree search inter national conference on computers and games springer pp 72 83 cyb 89 cybenko 1989 approximation by superpositions of a sigmoidal function math of control signals and systems vol 2 pp 303 314 dnw 16 david o e netanyahu n s and wolf l 2016 deepchess end to end deep neural network for automatic learning in chess in international conference on artificial neural networks pp 88 96 springer dim 10 di castro d and mannor s 2010 adaptive bases for reinforcement learning machine learning and knowledge discovery in databases vol 6321 pp 312 327 die 00 dietterich t 2000 hierarchical reinforcement learning with the maxq value function decom position j of artificial intelligence research vol 13 pp 227 303 fyg 06 fern a yoon s and givan r 2006 approximate policy iteration with a policy language bias solving relational markov decision processes j of artificial intelligence research vol 25 pp 75 118 dod 93 douglas c c and douglas j 1993 a unified convergence theory for abstract multigrid or multilevel algorithms serial and parallel siam j num anal vol 30 pp 136 158 fle 84 fletcher c a j 1984 computational galerkin methods springer n y fun 89 funahashi k 1989 on the approximate realization of continuous mappings by neural net works neural networks vol 2 pp 183 192 gbc 16 goodfellow i bengio j and courville a deep learning mit press cambridge ma ggs 13 gabillon v ghavamzadeh m and scherrer b 2013 approximate dynamic programming finally performs well in the game of tetris in advances in neural information processing systems pp 1754 1762 gor 95 gordon g j 1995 stable function approximation in dynamic programming in machine learn ing proceedings of the 12 th international conference morgan kaufmann san francisco ca gos 15 gosavi a 2015 simulation based optimization parametric optimization techniques and rein forcement learning 2 nd edition springer n y hmk 98 hauskrecht m meuleau n kaelbling l p dean t and boutilier c 1998 hierarchical solution of markov decision processes using macro actions in proceedings of the 14 th conference on uncertainty in artificial intelligence pp 220 229 hot 06 hinton g e osindero s and teh y w 2006 a fast learning algorithm for deep belief 68 nets neural computation vol 18 pp 1527 1554 hsw 89 hornick k stinchcombe m and white h 1989 multilayer feedforward networks are uni versal approximators neural networks vol 2 pp 359 159 hay 08 haykin s 2008 neural networks and learning machines 3 rd edition prentice hall englewood cliffs n j hol 86 holland j h 1986 escaping brittleness the possibility of general purpose learning algorithms applied to rule based systems in machine learning an artificial intelligence approach michalski r s carbonell j g and mitchell t m eds morgan kaufmann san mateo ca pp 593 623 iva 68 ivakhnenko a g 1968 the group method of data handling a rival of the method of stochastic approximation soviet automatic control vol 13 pp 43 55 iva 71 ivakhnenko a g 1971 polynomial theory of complex systems ieee transactions on systems man and cybernetics vol 4 pp 364 378 jon 90 jones l k 1990 constructive approximations for neural networks by sigmoidal functions proceedings of the ieee vol 78 pp 1586 1589 kmp 06 keller p w mannor s and precup d 2006 automatic basis function construction for approximate dynamic programming and reinforcement learning in proc of the 23 rd international con ference on machine learning acm pp 449 456 kvz 72 krasnoselskii m a vainikko g m zabreyko r p and ruticki ya b 1972 approximate solution of operator equations translated by d louvish wolters noordhoff pub groningen kir 11 kirsch a 2011 an introduction to the mathematical theory of inverse problems 2 nd edition springer n y kob 09 konidaris g and barto a 2009 efficient skill learning using abstraction selection in 21 st international joint conference on artificial intelligence lll 08 lewis f l liu d and lendaris g g 2008 special issue on adaptive dynamic programming and reinforcement learning in feedback control ieee trans on systems man and cybernetics part b vol 38 no 4 llp 93 leshno m lin v y pinkus a and schocken s 1993 multilayer feedforward networks with a nonpolynomial activation function can approximate any function neural networks vol 6 pp 861 867 lwl 17 liu w wang z liu x zeng n liu y and alsaadi f e 2017 a survey of deep neural network architectures and their applications neurocomputing vol 234 pp 11 26 lel 12 lewis f l and liu d 2012 reinforcement learning and approximate dynamic programming 69 for feedback control ieee press computational intelligence series n y li 17 li y 2017 deep reinforcement learning an overview arxiv preprint arxiv 1701 07274 v 5 mmp 15 mann t a mannor s and precup d 2015 approximate value iteration with temporally extended actions j of artificial intelligence research vol 53 pp 375 438 mms 06 menache i mannor s and shimkin n 2005 basis function adaptation in temporal differ ence reinforcement learning ann oper res vol 134 pp 215 238 men 82 mendelssohn r 1982 an iterative aggregation procedure for markov decision processes op erations research vol 30 pp 62 73 par 98 parr r and russell s j 1998 reinforcement learning with hierarchies of machines in advances in neural information processing systems pp 1043 1049 pow 11 powell w b 2011 approximate dynamic programming solving the curses of dimensionality 2 nd edition j wiley and sons hoboken n j rpw 91 rogers d f plante r d wong r t and evans j r 1991 aggregation and disaggregation techniques and methodology in optimization operations research vol 39 pp 553 582 sbp 04 si j barto a powell w and wunsch d eds 2004 learning and approximate dynamic programming ieee press n y sgg 15 scherrer b ghavamzadeh m gabillon v lesner b and geist m 2015 approximate modified policy iteration and its application to the game of tetris j of machine learning research vol 16 pp 1629 1676 shs 17 silver d hubert t schrittwieser j antonoglou i lai m guez a lanctot m sifre l kumaran d graepel t and lillicrap t 2017 mastering chess and shogi by self play with a general reinforcement learning algorithm arxiv preprint arxiv 1712 01815 sjj 95 singh s p jaakkola t and jordan m i 1995 reinforcement learning with soft state aggre gation in advances in neural information processing systems 7 mit press cambridge ma sps 99 sutton r precup d and singh s 1999 between mdps and semi mdps a framework for temporal abstraction in reinforcement learning artificial intelligence vol 112 pp 181 211 ssp 18 serban i v sankar c pieper m pineau j bengio j 2018 the bottleneck simulator a model based deep reinforcement learning approach arxiv preprint arxiv 1807 04723 v 1 saa 03 saad y 2003 iterative methods for sparse linear systems siam phila pa sam 59 samuel a l 1959 some studies in machine learning using the game of checkers ibm j of research and development pp 210 229 70 sam 67 samuel a l 1967 some studies in machine learning using the game of checkers ii recent progress ibm j of research and development pp 601 617 sch 13 scherrer b 2013 performance bounds for lambda policy iteration and application to the game of tetris j of machine learning research vol 14 pp 1181 1227 sch 15 schmidhuber j 2015 deep learning in neural networks an overview neural networks vol 61 pp 85 117 sha 50 shannon c 1950 programming a digital computer for playing chess phil mag vol 41 pp 356 375 sub 98 sutton r s and barto a g 1998 reinforcement learning mit press cambridge ma a draft 2 nd edition is available on line sut 88 sutton r s 1988 learning to predict by the methods of temporal differences machine learn ing vol 3 pp 9 44 sze 10 szepesvari c 2010 algorithms for reinforcement learning morgan and claypool publishers san franscisco ca teg 96 tesauro g and galperin g r 1996 on line policy improvement using monte carlo search presented at the 1996 neural information processing systems conference denver co also in m mozer et al eds advances in neural information processing systems 9 mit press 1997 tes 89 a tesauro g j 1989 neurogammon wins computer olympiad neural computation vol 1 pp 321 323 tes 89 b tesauro g j 1989 connectionist learning of expert preferences by comparison training in advances in neural information processing systems pp 99 106 tes 92 tesauro g j 1992 practical issues in temporal difference learning machine learning vol 8 pp 257 277 tes 94 tesauro g j 1994 td gammon a self teaching backgammon program achieves master level play neural computation vol 6 pp 215 219 tes 95 tesauro g j 1995 temporal difference learning and td gammon communications of the acm vol 38 pp 58 68 tes 01 tesauro g j 2001 comparison training of chess evaluation functions in machines that learn to play games nova science publishers pp 117 130 tes 02 tesauro g j 2002 programming backgammon using self teaching neural nets artificial in telligence vol 134 pp 181 199 tsv 96 tsitsiklis j n and van roy b 1996 feature based methods for large scale dynamic pro 71 gramming machine learning vol 22 pp 59 94 tsi 94 tsitsiklis j n 1994 asynchronous stochastic approximation and q learning machine learning vol 16 pp 185 202 vvl 13 vrabie v vamvoudakis k g and lewis f l 2013 optimal adaptive control and differential games by reinforcement learning principles the institution of engineering and technology london van 06 van roy b 2006 performance loss bounds for approximate value iteration with state aggre gation mathematics of operations research vol 31 pp 234 244 wer 77 werbo s p j 1977 advanced forecasting methods for global crisis warning and models of intelligence general systems yearbook vol 22 pp 25 38 yub 04 yu h and bertsekas d p 2004 discretized approximations for pomdp with average cost proc of the 20 th conference on uncertainty in artificial intelligence banff canada yub 09 yu h and bertsekas d p 2009 basis function adaptation methods for cost approximation in mdp proceedings of 2009 ieee symposium on approximate dynamic programming and reinforcement learning adprl 2009 nashville tenn yub 10 yu h and bertsekas d p 2010 error bounds for approximations from projected linear equations mathematics of operations research vol 35 pp 306 329 yub 12 yu h and bertsekas d p 2012 weighted bellman equations and their applications in dynamic programming lab for information and decision systems report lids p 2876 mit 72