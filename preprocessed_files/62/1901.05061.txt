spectrogram feature losses for music source separation abhimanyu sahai eth zu rich asahai ethz ch romann weber disney research zu rich romann weber disneyresearch com brian mcwilliams disney research zu rich brian mcwilliams disneyresearch com abstract in this paper we study deep learning based music source separation and explore using an alternative loss to the standard spectrogram pixel level l 2 loss for model training our main contribution is in demonstrating that adding a high level feature loss term extracted from the spectrograms using a vgg net can improve separation quality vis a vis a pure pixel level loss we show this improvement in the context of the mmdensenet a state of the art deep learning model for this task for the extraction of drums and vocal sounds from songs in the musdb 18 database covering a broad range of western music genres we believe that this finding can be generalized and applied to broader machine learning based systems in the audio domain i introduction music source separation is a problem that has been studied for a few decades now given an audio track with several instruments mixed together a regular mp 3 file for example how can it be separated into its component instruments the obvious application of this problem is in music production creating karaoke tracks highlighting select instruments in an audio playback etc there is another reason why this is a useful problem to study it acts as a powerful enabler for several other applications in music informatics this is because complex multi instrument music tracks are not easily pro cessed by such algorithms in their raw audio form however once individual instruments have been isolated from such a track they can relatively easily be transcribed by contemporary algorithms up until the early 2010 s the most common approaches to this problem were not data driven but rooted in exploiting known statistical properties of music signals or in signal pro cessing theory however as with many fields that has changed in the last few years with the advent of cheaper computing power and proliferation of research in machine learning the best performance on this problem is currently achieved by deep learning based methods these methods feed the mixture at the input of the network and the source s as targets or rather typically the spectrograms of the input output since many of the patterns to be discovered are in the frequency domain to learn a function mapping between the two these deep learning approaches use a pixel level loss as the cost function averaging the l 2 losses between corresponding pixels in the output and target spectrograms the term pixel here and in the rest of the paper is used to denote time frequency bins in the spectrogram because the spectrogram is treated as an image for the purpose of our work however we believe that this is not the ideal loss function for this problem this is because it does not explicitly give weight to higher level patterns in spectrograms which could exhibit similarity between similar pieces of audio for example non pitched instruments like drums have signal present across frequencies and therefore exhibit vertical lines in their spectrograms on the other hand vocal spectrograms display harmonicity i e horizontal lines thus we propose that pairing the pixel level l 2 loss with a loss between higher level patterns extracted from the spectrogram could lead to improved performance for the latter we port the loss terms developed by the authors in 1 for the visual domain treating spectrograms as images for this purpose this is not an ideal treatment and better alternatives will be discussed in section vii on future work the rest of this paper is organized as follows in the following sections we introduce the core deep learning model for music source separation that we have utilized in our work and briefly summarize the learning from 1 in using vgg feature maps to compute the spectrogram feature losses after laying down related work we describe in detail our experiments and their results we summarize the implications of these results and finally discuss ideas to build further on this work ii related work to the best of our knowledge there is no existing work on the application of such spectrogram feature losses to music source separation the general idea of applying feature style reconstruction losses as proposed in 1 for the visual domain to an audio domain problem has been explored by some researchers with mixed results in 2 the authors propose an audio style transfer using as one of the approaches style reconstruction losses extracted using the vgg network similar to 1 in their case the vgg does not yield results of acceptable quality as per subjective tests but using a shallow cnn does in 3 the authors explore audio generation as an audio style transfer problem using similar loss terms more generally the idea of perceptual losses for audio is still an open area of research where the task is to find loss measures that correlate better with subjective measures of audio quality however while the feature losses we explore in our work are derived from perceptual losses in the image domain they are more directly a descriptor of visual patterns ar x iv 1 90 1 05 06 1 v 3 cs s d 2 6 ju n 20 19 in audio spectrograms than being a perceptual descriptor of the underlying audio iii mmdensenets for music source separation multi scale multi band densenets or mmdensenets are a cnn based deep network model for music source separation they were proposed in 4 and variations of this model achieve the current state of the art performance on the music source separation task based on the sisec the signal sepa ration evaluation campaign this is a benchmark competition for this task that we discuss in greater detail in section v a in this section we provide a brief overview of this model at the input of the mmdensenet is the spectrogram of the mixed up song in its stft short time fourier transform representation each source to be separated has its own net work and set of weights and for each network the training targets comprise the corresponding pure source spectrograms since this is a real valued neural network the phase of the mixture spectrogram is isolated and only the magnitude is fed into the network similarly during training the target consists only of the magnitude of the source spectrogram in order to recover the estimated time domain source signal during inference the phase of the input mixture spectrogram is directly applied to each source spectrogram and an inverse stft taken of the result in case the data is stereophonic i e contains more than one channel this information is fed into the mmdensenet as a multi channel spectrogram image the network architecture itself is based on the densenet which is a deep cnn where every layer s output is directly fed to every other layer succeeding it for greater detail on the densenet architecture the reader is referred to the original paper 5 furthermore while the original densenet is a classifier and periodically downsamples the original image in the current application an image needs to be created at the output for this purpose the mmdensenet includes an upsampling path also comprised of densenet blocks resulting in an autoencoder style architecture what makes the mm densenet especially unique is its use of sub band networks in simple language rather than sharing the convolution kernel across the spectrogram image it trains separate convolutional layers and therefore kernels for different frequency bands it achieves this in practice by splitting the input spectrogram along the frequency axis into two or more sub images each of which can be thought of as representing a sub frequency band image each of these sub band images is propagated through its own densenet autoencoder as described above towards the output feature maps from these sub band densenets are joined back along the frequency axis the mmdensenet architecture is illustrated in figure 1 as a post processing step during inference the predictions of the network for each source are scaled for each time frequency bin so that their sum is equal to the original mixture at the corresponding time frequency bin this is akin to single channel wiener filtering and is also part of the procedure established in 4 fig 1 illustration of complete mmdensenet architecture reproduced with permission from 4 iv spectrogram feature loss in this section we explain how a high level spectrogram feature loss can be computed using the vgg network this network refers to a deep convolutional neural network devel oped by oxford s visual geometry group vgg hence the name of the network for the purpose of image classification 6 the network uses a succession of convolution relu activation and max pooling layers to extract image features plugging in a fully connected layer followed by a softmax layer in the end for performing the classification task this network was among the winners in the imagenet challenge in 2014 while the purpose of the vgg network as it was developed was image classification it is of interest to us for our problem because it can also be viewed as a feature extractor successive hidden layers of the network compute higher level image features like shapes and forms so instead of comparing two images only on their pixel values we could use the vgg as a feature extractor to obtain high level features and compare the images on these features as well it was this insight that was used by the authors in 1 to do a style transfer between two images for the purpose of this work we treat the high level spectro gram feature loss calculation as a black box computed exactly as in 1 i e using the vgg network and computing two related loss terms the feature and style reconstruction losses we use the same layers of the vgg network for computing these loss terms as in 1 throughout our experiments which we shall describe in section v c we give a weight of 0 5 to the regular pixel level l 2 loss and 0 25 to each of these two high level feature losses in the rest of this paper we use the term composite spectrogram loss for the weighted combination we arrived at these values for the weights empirically in particular we also tried using only the high level feature losses but found the performance to be inferior for this setting ideally we would use an analog to the vgg network for the audio or music domain to optimize for extracting audio specific features however no such publicly available and rigorously tested network exists in section vii on future work we discuss how this black box calculation can be better customized for this application v experiments a dataset benchmarks and metrics the sisec is a biennial forum where researchers in signal separation across a variety of signal domains eg bio medical music etc compare the performance of their algo rithms on a standardized task the music source separation sub task currently involves separation of 50 professionally recorded stereo tracks across varying genres like pop rock rap etc into vocals drums bass and other i e the collection of remaining instruments as one track since the researchers report detailed standardized metrics and also discuss their approach at varying lengths this is a good resource to glean the state of the art for this problem for this sub task sisec provides a dataset called musdb 18 7 it consists of 150 professionally recorded tracks across genres of which the actual testing is to be done on 50 tracks while the other 100 can be used for training in supervised approaches for each track the true isolated vocals drums bass and other tracks are provided along with the main mixed track performance is evaluated on a collection of specialized met rics developed and widely used by the research community in blind source separation called bss eval 8 these measures are somewhat akin to an snr measure in the following sections of this paper we will compare performances on the signal to distortion ratio sdr as it is the overarching metric that encompasses the other metrics b baseline model implementation since the mmdensenet model is not open sourced we created our own implementation following the general guide lines listed in 4 and applied it to the sisec 2018 task the parameters for the mmdensenet architecture in our implementation are the same as those given in table 1 of 4 other important implementation details are as follows we use 2048 samples for the fft with a hop rate of 1024 each spectrogram contains 128 time frames we use rmsprop for optimization starting with a learning rate of 0 001 and dropping it to 0 0001 when learning saturates finally we use a bottleneck compressed version of the densenet as explained in 6 with a factor of 4 for the bottleneck and a factor of 0 2 for the compression as described in the sisec 2018 paper 9 we calculate the median value of the sdr for each source over all time windows figure 2 shows the boxplot of the sdr thus obtained over all songs in the musdb 18 test database for each method submitted to sisec 2018 for the vocals source as an illustration of our relative performance our relative performance is similar for the other sources our method is labelled ours while our focus was more to get a reasonably performant working implementation of a deep learning music source separation system to be able to compare the pixel level loss with composite spectrogram losses we do come close to fig 2 boxplots over all the test songs of our baseline model s performance compared to other sisec 2018 submissions for the vocals source the sdr should be viewed as the overall summary metric with a higher sdr implying better performance the state of the art as well it should be noted that among the submissions in figure 2 tak 1 tak 2 and tak 3 are based on the mmdensenet the gap in performance between our model and these submissions can be explained by a mix of reasons chiefly their use of data augmentation the use of an lstm layer in addition to the densenet cnns and the use of specialized architectures for different sources for eg increasing complexity of the lower frequency sub band for the bass source c pixel level vs composite spectrogram loss comparison methodology with the baseline model implemented as above we con ducted a series of experiments to compare its performance with pixel loss with the same model tuned with the composite spec trogram loss as defined in section iv below we describe the settings for each experiment in all the experiments training was done with the development set of the musdb 18 database and the reported sdr is on its test set our experiments cover the sources vocals drums and bass experiment a in this experiment we compared the performance of the vocals source isolation obtained by the pixel loss tuned model with that of the composite spectrogram loss tuned model the parameter settings of the model in both the cases were identical and the same as those described in section v b we repeated this experiment four times to reduce false inferences due to experimental randomness and thus to be able to comment on the statistical significance of the observed difference in performance between the two models if any machine learning optimization is a random process with some of the randomness introduced by the optimization algorithm and some introduced by the parallel computing typically used for the optimization eg gpus we used keras as our implementation framework and while it can control for the former source of randomness through the use of random seeds there is currently no way to control for the latter experiment b this was same as the above experiment conducted for the drums source instead of vocals experiment c this was also same as the above exper iment conducted for the bass source experiment d in this experiment we once again com pared the vocals source however we did this with a single channel model in place of the stereo two channel model used in the above experiments the musdb 18 songs are available as two channel recordings a single channel version can be created by averaging the two channels the motivation to do this experiment was to test the composite spectrogram loss under more diverse use cases and settings like the above experiments this was conducted four times as well vi results and discussion we discuss the results for each of the experiments described in the previous section experiment a in table i we display 1 the pixel level l 2 loss value obtained for the vali dation set upon convergence for both the models for the vocals source in four independent runs it should be noted for the composite spectrogram loss tuned model that the pixel level l 2 loss is one of the components of the overall loss as explained in section iv for this model we chose the epoch with the minimum composite validation loss as one would usually do but report in this table only the pixel level l 2 loss component for a like to like comparison 2 the sdr value obtained over the musdb 18 test dataset by both the models for the vocals source in the above four independent runs the figure reported here is the median over the test dataset as explained in section v b while there seems to be a visible difference in per formance between the two models with the composite spectrogram loss tuned model outperforming the pixel loss tuned model we run the sdr results through a t test for statistical rigor the output from these tests conducted in r is also displayed in table i the differences are significant at a 5 significance level on this sample the composite spectrogram loss tuned model delivers a 0 27 db improvement in performance experiment b similar to the above experiment table ii shows the validation pixel level l 2 loss upon conver gence and the median sdr obtained over the musdb 18 test set for both the models for the source drums the table also gives the results of the t test to check if the sdr results are significantly different we can see once table i comparison of source separation performance for the vocals source between the pixel loss tuned model model 1 and the composite spectrogram loss tuned model model 2 lower val loss and higher sdr are better run min pixel val loss l 2 sdr db model 1 model 2 model 1 model 2 1 0 59 0 47 3 70 3 98 2 0 59 0 50 3 72 3 93 3 0 59 0 50 3 83 4 06 4 0 60 0 48 3 73 3 84 welch two sample t test t statistic 4 52 df 4 47 p value 0 008 mean sdr with pixel loss 4 32 mean sdr with composite spectrogram loss 4 59 95 confidence interval difference of means 0 43 0 11 table ii comparison of source separation performance for the drums source between the pixel loss tuned model model 1 and the composite spectrogram loss tuned model model 2 lower val loss and higher sdr are better run min pixel val loss l 2 sdr db model 1 model 2 model 1 model 2 1 0 46 0 37 4 70 4 88 2 0 46 0 37 4 53 4 65 3 0 48 0 37 4 52 4 71 4 0 46 0 38 4 64 4 88 welch two sample t test t statistic 2 49 df 5 53 p value 0 051 mean sdr with pixel loss 4 60 mean sdr with composite spectrogram loss 4 78 95 confidence interval difference of means 0 37 0 00 again that the composite spectrogram loss tuned model outperforms the pixel loss tuned model however the 5 significance is more borderline for drums on this sample the vgg loss model delivers a 0 18 db improvement in performance experiment c similar to the above experiment table iii shows the validation pixel level l 2 loss upon conver gence and the median sdr obtained over the musdb 18 test set for both the models for the source bass while the composite spectrogram loss tuned model consistently converges to a lower validation l 2 loss in terms of sdr performance the two models seem to be nearly identical at least based on these samples we do not run these sdrs through a t test experiment d table iv shows the validation pixel level l 2 loss upon convergence and the median sdr obtained over the musdb 18 test set for both the models for the source vocals for a single channel model the table also gives the results of the t test to check if the sdr results are significantly different we can see that the composite spectrogram loss tuned model outperforms the pixel loss table iii comparison of source separation performance for the bass source between the pixel loss tuned model model 1 and the composite spectrogram loss tuned model model 2 lower val loss and higher sdr are better run min pixel val loss l 2 sdr db model 1 model 2 model 1 model 2 1 0 64 0 49 4 10 4 03 2 0 61 0 50 4 00 4 01 3 0 61 0 51 4 09 4 14 4 0 63 0 51 4 04 4 06 table iv comparison of source separation performance for the vocals source for a single channel model between the pixel loss tuned model model 1 and the composite spectrogram loss tuned model model 2 lower val loss and higher sdr are better run min pixel val loss l 2 sdr db model 1 model 2 model 1 model 2 1 0 59 0 47 3 70 3 98 2 0 59 0 50 3 72 3 93 3 0 59 0 50 3 83 4 06 4 0 60 0 48 3 73 3 84 welch two sample t test t statistic 3 81 df 5 06 p value 0 012 mean sdr with pixel loss 3 75 mean sdr with composite spectrogram loss 3 95 95 confidence interval difference of means 0 35 0 07 tuned model at a 5 significance level for the vocals source under these settings as well on this sample the former delivers a 0 2 db improvement in performance we also show in figure 3 the validation pixel level l 2 loss trajectory for both the models averaged across the four runs for experiment a two channel vocals the trajectories for the other sources are similar the difference in performance between the two models is once again evident from this plot with the composite spectrogram loss tuned model converging to a lower pixel level validation loss the results from our above experiments demonstrate that using a loss derived from high level spectrogram patterns to tune the model does indeed improve performance over using only a pixel level loss for the vocals and drums sources by 0 3 db and 0 2 db respectively for the multi channel model over the samples in our study while in itself this is a valuable result and an improvement over the baseline model it also lays down the case for further exploration of loss functions appropriate for music or more generally audio data vii conclusion and future work in this paper we have demonstrated how using a high level spectrogram feature loss in addition to the standard pixel level loss can improve performance of a machine learning based music source separation system we believe that this is an improvement that could be generalized to related systems fig 3 trajectory of validation pixel level l 2 losses for the composite spectrogram loss tuned model vs pixel level loss tuned model when training for 24 epochs for the vocals source dealing with audio data one area of improvement to the current work could be to explore spectrogram feature losses more customized to the audio music domain for eg an audio classifier could be built and used in place of the vgg net for the current application it could be for example a network for discriminating between different musical instrument sounds secondly to study the generalizability of our observation within deep learning based music source separation we could explore implementing alternative models described in litera ture for this task with spectrogram feature losses or their analog for models that process the audio as a 1 d signal references 1 j johnson a alahi and l fei fei perceptual losses for real time style transfer and super resolution in eccv 2016 2 e grinstein n duong a ozerov and p perez audio style transfer in icassp 2018 3 p verma and j o smith neural style transfer for audio spectro grams in nips workshop on machine learning for creativity and design 2017 4 n takahashi and y mitsufiji multi scale multi band densenets for audio source separation in ieee workshop on applications of signal processing to audio and acoustics waspaa 2017 5 g huang z liu l van der maaten and k q weinberger densely connected convolutional networks in cvpr 2017 6 k simonyan and a zisserman very deep convolutional networks for large scale image recognition in iclr 2015 7 z rafii a liutkus f r stoter s i mimilakis and r bittner the musdb 18 corpus for music separation 8 e vincent r gribonval and c fevotte performance measurement in blind audio source separation in ieee trans audio speech and language processing 14 4 pp 1462 1469 2006 9 f stter a liutkus and n ito the 2018 signal separation evaluation campaign in latent variable analysis and signal separation pp 293 305 june 2018 i introduction ii related work iii mmdensenets for music source separation iv spectrogram feature loss v experiments v a dataset benchmarks and metrics v b baseline model implementation v c pixel level vs composite spectrogram loss comparison methodology vi results and discussion vii conclusion and future work references