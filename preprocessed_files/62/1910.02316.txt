superiority of bayes estimators over the mle in high dimensional multinomial models and its implication for nonparametric bayes theory rabi bhattacharya department of mathematics university of arizona and rachel oliver department of mathematics university of arizona october 8 2019 abstract this article focuses on the performance of bayes estimators in comparison with the mle in multinomial models with a relatively large number of cells the prior for the bayes estimator is taken to be the conjugate dirichlet i e the multivariate beta with exchangeable distributions over the coordinates including the non informative uniform distribution the choice of the multinomial is motivated by its many applica tions in business and industry but also by its use in providing a simple nonparametric estimator of an unknown distribution it is striking that the bayes procedure outper forms the asymptotically efficient mle over most of the parameter spaces for even moderately large dimensional parameter space and rather large sample sizes keywords high dimensional multinomials bayes estimators versus the mle nonparamet ric bayes the authors gratefully acknowledge nsf grant dms 1811317 1 ar x iv 1 91 0 02 31 6 v 1 m at h s t 5 o ct 2 01 9 1 introduction the present article shows by analytical computations and simulations that bayes estimators even in moderately high dimensional multinomial models outperform the mle on most of the parameter space high dimensional multinomial models are useful for industrial planning for example for planning its manufacturing process a big departmental store may try to estimate the proportions of a certain type of clothing by sizes and or colors demanded by its customers see the example in section 6 for another important motivation for pursuing this study of multinomials note that an unknown distribution on a state space may be approximated nonparametrically by proba bilities of members of a fine partition sampling from this approximate distribution is the same as sampling from a multinomial bayes estimation of the approximating multinomial with a conjugate beta i e dirichlet prior was a motivation for fergusons path breaking development of nonparametric bayes theory of estimation of a probability distribution on the state space with the so called dirichlet process prior ferguson 1973 it may be pointed out that the venerable bernstein von mises theorem see e g bickel and doksum 2001 p 339 or bhattacharya et al 2016 p 190 is designed to show how bayes estimators with reasonable priors are asymptotically as efficient as the mle and approach it as the sample size increases it turns out from our study that in high dimensional multinomial models it is the mle which tries to catch up with bayes as the sample size increases the mle of a multinomial with parameter 1 2 k is given by 1 2 k where i ni n is the proportion of i i d observations x 1 xn in the i th cell estimating 2 its true proportion i the bayes estimator db uses the conjugate prior beta 1 2 k also called a dirichlet prior and denoted dir 1 2 k we take 1 2 k for purposes of ease in computation and assuming no a priori preference for some categories over others indeed when the common value is 1 one has the so called non informative prior assigning the uniform density on the parameter space ek 1 2 k i 0 i 1 i k i 1 1 1 the posterior distribution of is dir 1 n 1 k nk and its mean is the bayes estimator db of as given in 2 4 for the most part we take the loss function to be squared error under which the risk functions r and r db are given by 2 3 and 2 5 both analytical calculations and simulations are carried out for cases with 1 2 k for an approximation of the nonparametric estimation of an unknown probability on some state space the bayes procedure with a dirichlet prior and a base measure with total mass c we consider for each k i i c k i 1 i k such an approximation with large k named the tree construction of the dirichlet process may be found in ghosh and ramamoorthi 2003 chapter 3 and ghosal and van der vaart 2017 chapter 4 one may think of c k as the measure of each of k members of a partition which can be ensured if is absolutely continuous with respect to lebesgue measure on an euclidean state space or a volume measure on a manifold section 2 introduces the multinomial distribution and the estimators under consideration section 3 explores the volume of the parameter space where the bayes estimators have 3 lower risk than the mle to build intuition we begin in sections 3 2 and 3 3 with analytical computations for the binomial model i e a multinomial model with k 2 and then a multinomial with k 3 for k 4 the geometry of the region of the simplex 1 1 where r r db is complex extensive simulations show that under the uniform prior dir 1 1 1 for moderate and large vaules of k and small as well as large values of the sample size n the bayes estimator has a smaller expected squared error than that of the mle on most of the parameter space see figure 8 next consider the multivariate beta prior with parameters 1 2 k 1 k with a large k suitable for a simple nonparametric estimation of an unknown distribution as alluded to above in this case for sufficiently large k the region where the mle has a smaller expected squared error than bayes is identifiable as the union of k regions each being a cone shaped structure minus a cap at the base see figure 5 in section 3 for the case k 3 its area i e its volume measure in the simplex ek in 1 1 relative to the volume measure of ek or equivalently of its complement is estimated analytically in section 3 see lemma 3 2 this conservative estimate is compared with the true value obtained by simulation in table 1 once again showing that the region where the mle has a smaller expected risk that that of the bayes is rather negligible in section 4 the average difference over the parameter space of expected squared errors r r db is computed in proportion to the average of r and compared see table 2 and figure 11 this scaled difference is shown to be maximum under the uniform priordir 1 1 1 this may come as a suprise because the proportion of the volume of ek in which the bayes risk is smaller than that of the mle is generally larger under the prior dir 1 k 1 k than under the uniform 4 section 5 briefly illustrates the approximations in l 1 or total variation distance between a true distribution and its nonparametric estimators based on the mle and those of the nonparametric bayes estimators section 6 presents a data example to illustrate an indus trial application of a high dimensional multinomial a final section 7 lays down some final remarks 2 the multinomial distribution consider the estimation of the parameter 1 2 k in the multinomial distri bution where j is the proportion of the j th class in a population with k 2 classes j 1 2 k based on a simple random sample of size n from the population let n 1 n 2 nk be the numbers in the sample belonging to each of the k classes since n 1 n 2 nk is a sufficient statistic for it is enough to consider the distribution of n 1 n 2 nk for the estimation of namely f n 1 n 2 nk n n 1 n 2 nk n 11 n 2 2 nk k 1 2 k rk j 0 j k j 1 j 1 2 1 the maximuim likelihood estimator mle is n 1 n n 2 n nk n the multivariate beta or dirichlet prior dir 1 2 k has density with respect to lebesgue measure on where is given by 1 2 k 1 rk 1 j 0 j k 1 j 1 j 1 5 the dirichlet density is 1 1 k 1 k 1 2 k 1 11 2 1 2 k 1 k for 2 2 where k 1 1 2 k 1 it is well known and easy to prove that if the prior is dir 1 k the posterior distribution of is dirichlet dir 1 n 1 2 n 2 k nk see e g bhattacharya et al 2016 under squared error loss l 2 1 i k i i 2 then the risk function of the mle 1 k with i ni n is given by r 1 i k i 1 i n 1 1 i k 2 i n 2 3 we wish to choose an exchangeable prior invariant under permutation of coordinates thus we choose 1 2 k ck n where ck n is some constant which may depend on k and n the choices of ck n that lead to better estimators in terms of risk under squared error loss will be investigated under the dirichlet prior dir 1 k with 1 2 k ck n and squared error loss the bayes estimator is db db 1 dbk with dbi ni ck n n kck n i 1 2 k 2 4 and its risk function is see e g bhattacharya et al 2016 6 r db 1 i k n i 1 i ck n k ick n 2 n kck n 2 1 i k i 2 i n n kck n 2 c 2 k n k 2 k k 2 1 i k 2 i n kck n 2 1 1 i k 2 i n n kck n 2 kc 2 k n n kck n 2 k 2 c 2 k n 1 i k 2 i n kck n 2 n kc 2 k n n kck n 2 k 2 c 2 k n n 1 i k 2 i n kck n 2 2 5 hence r r db and thus the mle has lower risk than the bayes estimator only on the set 1 2 k i 0 i 1 i k i 1 k 2 c 2 k n n n kc 2 k n 2 1 n 1 i k 2 i 1 n n kc 2 k n n kck n 2 2 6 this can be written more compactly as 1 2 k i 0 i 1 i k i 1 1 i k 2 i 2 n n k ck n 2 n k kn ck n 2 7 recall that the parameter space is the simplex ek ek 1 2 k i 0 i 1 i k i 1 2 8 7 we will calculate simulate the volume of the region 2 7 in ek for various choices of ck n this gives the proportion of the parameter space that is better estimated with regard to risk by the mle it is seen see section 4 that even for fairly large sample sizes the bayes estimator outper forms the mle after k is large such as k 10 3 the proportion of the parameter space favoring the bayes estimator volume calculations before calculating the volume of the region 2 7 we will consider more generally the simplex ek defined in 2 8 and the region k r ek 2 r where r is a known constant note that the region 2 7 is the complement of k r for a specific choice of r thus the region k r when applied to the this problem represents the region of the parameter space where the bayes estimator has lower risk than the mle define the point e 0 by e 0 1 k 1 k 1 k 3 1 the point in ek that is closest to the origin with e 0 2 1 k we can see then that if r 1 k then k r similarly if r 1 then k r ek since 2 1 ek for r 1 k define k r to be the distance between e 0 and the sphere ek 2 r then 8 k r r 1 k 3 2 define j to be the distance between e 0 and the k 1 j dimensional boundary of ek this is the distance between e 0 and 0 0 1 k j 1 k j which has the first j coordinates equal to 0 and the remaining k j coordinates equal to 1 k j we have j j k k j 3 3 note that we take j 1 k 1 and that 1 2 k 1 thus for any r 1 k 1 we can find j such that j k r j 1 we conjecture that the precise shape of k r and thus the formula for calculating its surface area should depend on which j satifies this condition we use the term surface area since k r and also ek is a k 1 dimensional subspace of rk 3 1 the surface area of ek consider in general the simplex sk r defined sk r 1 2 k i 0 i 1 i k i r r 0 let ek r be the boundary of sk r namely ek r 1 2 k i 0 i 1 i k i r 9 and write ek ek 1 lemma 3 1 i the volume vk r of sk r is rk k and ii the surface area ak r of ek r is rk 1 k k 1 in particular the surface area of the simplex ek ek 1 is ak ak 1 k k 1 3 4 proof i vk r sk r d 1 d 2 d k sk 1 r r 1 i k 1 i d 1 d 2 d k 1 sk 2 r r 1 i k 2 i 2 2 d 1 d 2 d k 2 s 1 r r 1 k 1 k 1 d 1 rk k ii the difference in volume between sk r and sk r r is a slab around ek r note that 10 vk r r vk r r d dr rk k o r r rk 1 k 1 as r 0 the unit normal to the surface ek r at every point on it is grad 1 i k i grad 1 i k i 1 k 1 k 1 k hence at every point the thickness of the slab sk r r sk r is r k one may also see this by computing the distance between ek r and ek r r along the normal through the origin i e r k r k r k r r k r r k r r k the surface area ak r then is given by ak r lim r 0 vk r r vk r r k rk 1 k k 1 3 2 the surface area of 2 r let us calculate the k 2 case this corresponds to the binomial distribution a special case of the multinomial distribution with k 2 the simplex e 2 r 2 1 2 11 0 1 2 1 is the line between 0 1 and 1 0 the region of interest is 2 r e 2 2 1 2 2 r see figure 1 for an illustration of this region 1 2 0 1 0 1 r p 1 p 2 figure 1 an illustration of e 2 the region 2 r is the line sement between p 1 and p 2 we find the intersection points by solving 21 1 1 2 r to obtain the points p 1 1 2 1 2 2 r 1 1 2 1 2 2 r 1 and p 2 1 2 1 2 2 r 1 1 2 1 2 2 r 1 the surface area of 2 r is the length of the line segment between these two points which is 4 r 2 we can also find 2 r r 1 2 and 1 1 2 using equations 3 2 and 3 3 12 respectively these distances are pictured in figure 2 note that we also have that the 1 dimensional volume of 2 r is equal to 2 2 r 2 r 1 2 4 r 1 1 2 0 1 0 1 e 0 2 r 1 figure 2 an illustration of 2 r with the distances 2 r and 1 labeled the length of the line e 2 is 2 giving that the proportion of the 1 dimensional volume of e 2 made up by 2 r is vol 2 r vol e 2 4 r 2 2 2 r 1 3 3 the surface area of 3 r for k 3 we can also calculate this volume exactly the simplex e 3 r 3 1 2 3 0 1 2 3 1 is an equilateral triangle between the points 1 0 0 0 1 0 and 13 0 0 1 see figure 3 for an illustration of the space for r 1 3 1 2 and figure 4 for an illustration of the space for r 1 2 1 figure 3 an illustration of e 3 with the region 3 r in gray for r 1 3 1 2 we calculate 1 and 2 using equation 3 3 and 3 r using equation 3 2 these are illustrated in figure 5 1 1 3 2 2 1 3 1 2 2 1 6 2 2 1 3 2 1 3 1 2 2 3 and 14 figure 4 an illustration of e 3 with the region 3 r in gray for r 1 2 1 3 r r 1 3 if 3 r 1 then 3 r is just a circle with radius 3 r its surface area is then 3 r 2 r 1 3 the surface area of e 3 is using equation 3 4 a 3 3 2 this gives that the proportion of the 2 dimensional volume of e 3 made up by 3 r is vol 3 r vol e 3 r 1 3 3 2 2 3 3 r 1 3 if 1 3 r 2 then we can divide up the region as in figure 6 to determine the surface area 15 1 0 0 0 1 0 0 0 1 e 0 3 r 1 2 figure 5 an illustration of 3 r with 1 3 r 2 1 0 0 0 1 0 0 0 1 e 0 1 3 1 3 1 3 e 0 1 3 1 3 1 3 1 2 1 2 0 4 r 2 e 0 1 3 1 3 1 3 r 1 3 figure 6 a diagram of how to divide 3 r to determine its surface area the triangles each have area 1 2 1 6 4 r 2 1 2 2 r 1 3 the circular sectors have radius 3 r r 1 3 the angle is the angle between the vectors 1 6 1 2 2 r 1 1 3 1 6 1 2 2 r 1 and 1 6 1 2 2 r 1 1 6 1 2 2 r 1 1 3 we have cos 1 4 2 r 1 1 2 2 r 1 1 12 r 1 3 the surface area of 3 r is then 16 vol 3 r 3 2 2 r 1 3 3 2 r 1 3 arccos 1 4 2 r 1 1 2 2 r 1 1 12 r 1 3 the proportion of the 2 dimensional volume of e 3 made up by 3 r is then vol 3 r vol e 3 2 r 1 3 r 1 3 arccos 1 4 2 r 1 1 2 2 r 1 1 12 r 1 3 we can bound the surface area of 3 r from below by cutting out triangles in the corners this is done by drawing a straight line between the intersections of the sphere and the edges with the same value in one of the coordinates there are six places where the sphere intersects the edges of e 3 if we let 1 2 1 2 2 r 1 these solutions are a 1 0 b 0 1 c 1 0 d 0 1 e 0 1 and f 1 0 draw lines between a and b c and d and e and f see figure 7 a b c d e f 1 0 0 0 1 0 0 0 1 figure 7 how to bound the surface area of 3 r from below using similar triangles 3 r is shaded as before and the lower bound area is crosshatched due to the convexity of the sphere the lines drawn are entirely inside the sphere thus the set 3 r which is ek without the three triangles is entirely contained in 3 r this gives 17 vol 3 r vol 3 r 3 5 the triangles in the corners that are removed are equilateral triangles with side length r 2 r 1 they are thus similar to e 3 which is equilateral with side length 2 the ratio of the areas of the small triangles to e 3 is the ratio of the squared side lengths which is r 2 r 1 2 this gives finally vol 3 r vol e 3 1 3 r 3 2 r 1 2 3 6 3 4 the surface area of k r for k 4 calculating this surface area explicitly appears to be an open problem we have not found a formula but can approximate the area with a fairly sharp lower bound for certain choices of r we will generalize the cutting off corners method in the k 3 case which is valid for r such that k 2 k r k 1 lemma 3 2 the surface area of k r for r 1 2 1 let ek be the standard k simplex defined in equation 2 8 and k r be the region ek 2 r assume that r 1 2 1 then vol k r vol ek 1 k r 2 r 1 2 k 1 2 3 7 additionally the proportion of ek made up by k r approaches 1 as k 18 proof since 1 2 r 1 we have 1 2 r 1 1 2 1 k r 1 k 1 1 k k 2 2 k r 1 k k 1 k 2 k 2 k r 2 2 k 1 see equation 3 3 k 2 k r k 1 in this case where k 2 k r k 1 there are intersections along the 1 dimensional edges of ek such that only two of its components are nonzero with the sphere rk 2 r due to the convexity of the sphere hyperplanes between these points will be contained inside the sphere as in the k 3 case we can form k k 1 dimensional equilateral simplices the j th simplex will have as one of its vertices a single vertex from ek of the form j 1 and i 0 for i 6 j its remaining k 1 vertices will be of the form j and i 1 with i 1 2 j 1 j 1 k as in the k 3 case we define 1 2 1 2 2 r 1 these have edge length r 2 r 1 and are similar to ek which has edge length 2 the ratio of the areas of the small simplices to ek is r 2 r 1 2 k 1 2 this gives the lower bound 3 7 vol k r vol ek 1 k r 2 r 1 2 k 1 2 we see that vol k r vol ek approaches 1 as k as long as r 2 r 1 2 1 this is in particular true when r 1 2 1 19 3 5 applying the volume calculations to bayes estimators when using the prior dir ck n ck n we obtained the region 2 7 where the mle has lower risk than the bayes estimator 1 2 k i 0 i 1 i k i 1 1 i k 2 i 2 n n k ck n 2 n k kn ck n the region where the bayes estimator has lower risk than the mle the complement of region 2 7 is k r with r defined by r 2 n n k ck n 2 n k kn ck n 3 8 we can then determine which choice of ck n will yield the exponential convergence in lemma 3 2 this gives the following theorem theorem 3 3 consider estimating the k dimensional k 3 parameter in the multinomial distribution based on a simple random sample of size n under the prior dir ck n ck n the proportion of the parameter space where the bayes estimator has lower risk than the mle is greater than 1 k 1 4 k 1 2 for all n k for ck n satisfying ck n 2 n n k 2 k 3 9 20 proof as noted above the region where the bayes estimator has lower risk than the mle is k r with r defined by equation 3 8 r 2 n n k ck n 2 n k kn ck n we have r 1 2 2 n n k ck n 2 n k kn ck n 1 2 ck n k 2 n kn 2 n ck n 2 n n k 2 k we can thus apply lemma 3 2 since r 1 2 we have r 1 2 r 2 r 1 1 2 2 2 1 k r 2 r 1 2 k 1 2 k 1 4 k 1 2 1 k r 2 r 1 2 k 1 2 1 k 1 4 k 1 2 since 1 k 1 4 k 1 2 1 as k we have proved that vol k r vol ek approaches 1 as n k we can make this convergence tighter for a given ck n with r 2 n n k ck n 2 n k kn ck n 1 2 then 21 we have that the proportion of the parameter space where this bayes estimator has lower risk than the mle is greater than 1 k r 2 r 1 2 k 1 2 3 10 one may ask if there is a best choice of ck n note that the squared radius r of the region where the bayes estimator has lower risk k r defined in equation 3 8 is a decreasing function of ck n we have that r approaches 1 as ck n 0 that is the proportion of the parameter space where the bayes estimator has lower risk approaches 1 the whole space as ck n approaches 0 note that we cannot take ck n 0 as the dirichlet prior requires that i 0 for all i indeed if we could take ck n to be identically zero the bayes estimator would be equal to the mle one could however use the formula for r and the lower bound in 3 10 to select ck n small enough to satisfy a desired level of coverage of the parameter space for a choice of k and any larger k example 3 4 one such choice of prior is dir 1 k 1 k this can be thought of as relating to using a base measure that is a probability measure since 1 i k i 1 the region where the bayes estimator has lower risk than the mle is k r with r defined by r 2 n 1 n k 3 n 1 2 3 thus the proportion of the parameter space where the bayes estimator has lower risk is greater than 1 k 2 3 4 3 1 2 k 1 2 table 1 contains estimates using 3 8 and 1 3 10 22 giving an upper bound of the proportion of the parameter space where the mle has lower risk for various values of k and n it also contains simulated proportions using similar methods as in section 3 6 the simulation used sample sizes of 10 000 000 and thus the small proportions for k 20 could not be detected k n prop upper bound prop simulated k 5 n 10 2 68 10 3 2 12 10 3 k 5 n 25 2 95 10 3 2 32 10 3 k 10 n 20 1 97 10 6 7 00 10 7 k 10 n 100 2 30 10 6 8 00 10 7 k 20 n 40 6 53 10 13 0 k 20 n 400 7 88 10 13 0 table 1 estimates of the proportion of the parameter space where the mle has lower risk for various values of k and n 2 k k 2 note that it is nearly 0 for even the moderate k 10 3 6 simulation results for other priors the requirement that ck n 2 n n k 2 k precludes some priors that may be of interest these correspond to cases with a region of interest k r such that k r k 2 we have not found a suitable volume lower bound for such cases however we have found simulation examples of a slower convergence 23 3 6 1 uniform prior a common choice of prior is the uniform prior which is the prior dir 1 1 under our notation this corresponds to ck n 1 which clearly does not satisfy ck n 2 n n k 2 k 3 9 for k 4 here the mle has lower risk than the bayes estimator only on the set 1 2 k i 0 i 1 i k i 1 1 i k 2 i 3 n k nk 2 n k 3 11 we used a simulation study to better understand the regions where the mle has lower risk than the bayes estimator under this prior rearranging the inequality in the region 3 11 define the function g g 3 n k nk 2 n k 22 3 12 the mle has lower risk than the bayes estimator for ek where g 0 to estimate the percent of the volume of ek where the mle has lower risk than the bayes estimator we fixed k and took a uniform sample of size 500 000 from ek using the r package hitandrun van valkenhoef and tervonen 2016 we then calculated g for n k 2 k 3 k 4 k k 2 2 k 2 3 k 2 4 k 2 k 3 2 k 3 3 k 3 4 k 3 k 4 2 k 4 3 k 4 4 k 4 and found the percentage of the samples where g is positive for each n this gives a numeric estimate of the percent of the volume of ek where the mle has lower risk than the bayes estimator the results are summarized in figure 8 note that eventually we see that the mle has lower risk 24 in essentially none of the parameter space but it is a much slower process taking until k 200 3 6 2 dir c k c k prior note that the dirichlet distribution on the space of all probabilities on the borel sigma field of a polish space s is just the dirichlet or multivariate beta distribution when s is a finite set see section 5 by studying the limiting behavior in n and k of bayes estimators in the multinomial distribution we can hopefully gain insight into the difference between the risks of density estimation via nonparametric bayes and using the mle for a parametric model however since ferguson s construction of the dirichlet process prior relies on a finite base measure s we may want to consider the sum of the prior parameters 1 i k i c a constant rather than 1 i k i k which is the case for the uniform prior if we choose ck n c k r r db and thus the mle has lower risk than the bayes estimator only on the set 1 2 k i 0 i 1 i k i 1 1 i k 2 i 2 n c cn k 2 n c nc 3 13 note that if c 1 we obtain the example in subsection 3 5 if c 2 then ck n c k does not satisfy 3 9 for moderately sized k and n we again used simulation to study the regions in question rearranging the inequality in the region 3 13 define the function g 25 0 2 4 6 k k 2 k 3 k 4 n p e rc e n t o f sa m p le s w h e re m l e h a d s m a lle r ri sk k 10 20 30 40 50 100 200 500 with the prior dir 1 1 1 percent of samples where mle had lower risk than bayes as a function of sample size n and colored by number of classes k figure 8 we see that although theorem 3 3 can t be applied to the uniform prior the mle still has lower risk in a proportion of the parameter that decreases to 0 as k increases note that the n axis is plotted in log scale and in terms of k to make the samples comparable 26 g cn k 2 n c 2 n c nc 22 3 14 the mle has lower risk than the bayes estimator for ek where g 0 in the simulations the limiting behavior is similar in shape to the uniform prior case but seems to converge faster for fixed k near c as n increases the percent of the volume of the parameter space where the mle has lower risk increases to some limiting value as k increases this limiting value decreases to zero for k c however the bayes estimator had lower risk in 100 of the samples indicating that the volume of the region where the mle has lower risk is very small for example with c 30 and k 10 20 30 the results can be found in figure 9 a relatively large c was chosen so that there would be several k smaller than c to graph for k 40 the maximum percentage was 0 12 and for k 50 0 0076 for k 100 200 500 the three largest values used the mle had lower risk in 0 of the samples again we see that for large enough k the bayes estimator has lower risk than the mle for almost all of the parameter space we would need to find a suitable volume bound to properly describe this phenomenon 3 6 3 dir c c c prior for c 1 since we have already considered the uniform prior which is dir 1 1 1 it is natural to consider priors dir c c c with c 1 that is ck n is a constant rather than depending on k or n priors of this type are unimodal focusing most of their mass on the 27 0 20 40 60 k k 2 k 3 k 4 n p e rc e n t o f sa m p le s w h e re m l e h a d s m a lle r ri sk k 10 20 30 with the prior dir 30 k 30 k 30 k percent of samples where mle had lower risk than bayes as a function of sample size n and colored by number of classes k figure 9 sampling scheme using g with c 30 for k 10 20 30 the maximum percentages were 69 32 17 73 and 1 77 respectively again we see that although theorem 3 3 does not apply the mle has smaller risk for almost none of the space for k large enough 28 center of the parameter space 1 k 1 k 1 k with smaller component variances as c increases if we choose ck n c r r db and thus the mle has lower risk than the bayes estimator only on the set 1 2 k i 0 i 1 i k i 1 1 i k 2 i 2 n c k n 2 n c k kn 3 15 for simulation purposes define the function h which is positive when the mle has lower risk than the bayes estimator by rearranging the inequality 3 15 h 2 n c k n 2 n c k kn 22 3 16 in simulations a change of behavior is observed at c 2 for c 1 2 the limiting behavior is similar to the uniform prior case although with slower convergence however for c 2 the opposite limiting behavior is observed as k increases the proportion of the parameter space where the mle has lower risk increases to 1 see figure 10 for illustrative examples with c 1 9 c 2 and c 3 4 average risk across the parameter space in section 3 we considered whether the bayes estimator had smaller risk than the mle and where this occurred within the parameter space we did not consider the magnitude 29 0 10 20 30 k k 2 k 3 k 4 n p e rc e n t o f sa m p le s w h e re m l e h a d s m a lle r ri sk k 10 20 30 40 50 100 200 500 with the prior dir 1 9 1 9 1 9 percent of samples where mle had lower risk than bayes as a function of sample size n and colored by number of classes k 0 10 20 30 40 k k 2 k 3 k 4 n p e rc e n t o f sa m p le s w h e re m l e h a d s m a lle r ri sk k 10 20 30 40 50 100 200 500 with the prior dir 2 2 2 percent of samples where mle had lower risk than bayes as a function of sample size n and colored by number of classes k 0 25 50 75 100 k k 2 k 3 k 4 n p e rc e n t o f sa m p le s w h e re m l e h a d s m a lle r ri sk k 10 20 30 40 50 100 200 500 with the prior dir 3 3 3 percent of samples where mle had lower risk than bayes as a function of sample size n and colored by number of classes k figure 10 simulation results for the prior dir c c c with c 1 9 2 3 note the change in limiting behavior for c 2 where the final percent is increasing as k increases rather than decreasing as in the earlier examples 30 of the decrease in risk in this section we look at the average risk with respect to the volume measure of the various estimators to understand the magnitude of decrease recall that the risk of the mle by 2 3 is r 1 1 i k 2 i n and the risk of the bayes estimator with dirichlet prior with i ck n i is by 2 5 r db n kc 2 k n n kck n 2 k 2 c 2 k n n 1 i k 2 i n kck n 2 note that in each case for fixed k and prior choice of ck n the risk is decreasing to 0 as n thus any decrease in risk becomes negligible for large enough n integrating the above over ek we obtain ek r d 1 n ak 1 n ek 2 d and ek r db d n kc 2 k n n kck n 2 ak k 2 c 2 k n n n kck n 2 ek 2 d by 3 4 ak k k 1 it can be shown that ek 2 d 2 k k k 1 thus we obtain that the average risks r and r db are respectively 31 r ek r d ak k n k 1 2 k k n k 1 k k 1 1 n 2 k nk k 1 k 1 n k 1 4 1 and r db ek r db d ak n kc 2 k n k n kck n 2 k 1 2 k k 2 c 2 k n n k n kck n 2 k 1 k k 1 n kc 2 k n n kck n 2 2 k 2 c 2 k n n n kck n 2 k 1 k 1 kc 2 k n n kck n n 2 k 1 4 2 then the average decrease in risk for the bayes estimator in proportion to the average risk of the mle is r r db r k 1 n k 1 k 1 kc 2 k n n kck n n 2 k 1 k 1 n k 1 1 n kc 2 k n n kck n n 2 4 3 32 for fixed k and n this function has a global maximum at ck n 1 as illustrated in figure 11 when ck n 1 we obtain by plugging in to 4 3 r r db ck n 1 r k k n 1 1 n k 4 4 0 k k n 0 1 ck n d e cr e a se in a vg r is k p ro p o rt io n a l t o m l e r is k with the prior dir ck n ck n ck n decrease in average risk by using bayes estimator instead of mle figure 11 graph of the average decrease in risk for the bayes estimator in proportion to the average risk of the mle as a function of ck n with fixed k and n the function has a global maximum at ck n 1 it is postive for the region shown here but can become negative for large enough ck n this graph was made using the values k 10 and n 30 however a similar shape will result from any fixed k and n with a maximum at ck n 1 and maximum value k k n we see from 4 4 that for all k there is some positive decrease in proportional average risk that depends on the relationship between k and n for example when n k the decrease is 50 when n 2 k the decrease is 33 3 and when n k 2 the decrease is 100 1 k 1 the bayes estimator with the uniform prior is the estimator that has the smallest average risk with respect to the lebesgue measure by definition see e g 33 bhattacharya et al 2016 pp 22 23 we see here that this effect in comparison to the mle is most pronounced for n on the order of k on the other hand we can consider ck n 1 k this prior gave rise to an estimator that had smaller risk that the mle for nearly the entire parameter space for even small to moderate k see example 3 4 plugging in to 4 3 r r db ck n 1 k r 1 kn 2 n k n 1 2 1 n n 1 k n 1 2 4 5 this decreases to 0 as n become large but depends less on k for its limiting behavior for the nonparametric estimation of a distribution based on a given sample size n it is useful to consider the behavior of 4 5 as k for fixed n 4 5 1 n 2 n 1 2 comparing the behavior for moderate to large k and n in 4 4 and 4 5 shows an opposite kind of optimality than in section 3 where the dir 1 k 1 k was favored over the uniform prior balancing the smaller radius of the region with lower risk for the estimator under the uniform prior with its optimal decrease in average risk indicates that for moder ate to large k and n this estimator db ck n 1 is preferable over other bayes estimators for small k k 10 or when the true distribution is believed to have an unknown domi nating class and thus requires an exchangeable prior with a large radius for a decrease in risk the estimator with the dir 1 k 1 k prior may be preferable for illustration some comparison values are in table 2 34 mle uniform prior 1 k prior k n avg risk decrease vol prop decrease vol prop k 5 n 5 1 33 10 1 50 00 0 9443 27 77 0 9977 k 5 n 25 2 67 10 2 16 67 0 8926 6 80 0 9970 k 10 n 10 8 18 10 2 50 00 0 9823 16 53 1 k 10 n 100 8 18 10 3 9 09 0 9385 1 87 1 k 50 n 50 1 92 10 2 50 00 0 9998 3 84 1 k 50 n 2500 3 84 10 4 1 96 0 9939 0 08 1 k 100 n 100 9 80 10 3 50 00 1 1 96 1 k 100 n 10000 9 80 10 5 0 99 0 9993 0 02 1 table 2 table comparing the average risks for the mle and the bayes estimators under the uniform prior and the dir 1 k 1 k priors for k 5 10 50 100 and n k k 2 listed as well are estimated proportions of the parameter space where the estimators have lower risk than the mle for the uniform prior these are estimated using the simulation in section 3 6 1 for the dir 1 k 1 k prior the estimates use 3 8 and 3 10 5 on simulation of total variation distances between the true distribution and the distributions of 1 mle based frequentist estimators and 2 nonpara metric bayes estimators let q be a probability measure on some measurable state space s s which is partitioned into k measurable subsets a 1 ak if one wishes to estimate the probabilities q aj j 0 j 1 k based on the numbers n 1 nk of a random sample of size n from 35 q falling into these classes the mle 1 n 1 n k nk n is the time honored estimate and n 1 nk has the multinomial distribution m n 1 2 k one may instead use the bayes estimator with the conjugate prior dir 1 2 k namely db n 1 1 n j nk k n j where i 0 i we have seen that under squared error loss for large or moderately large k db outperforms on most of the parameter space in cases 1 i c 0 i for c 2 and 2 i 1 k i for large k these estimators provide approximations to q we now consider a way of providing approximations to q in variation norm for classes of s with a finite volume measure and q absolutely continuous with respect to it consider a closed bounded region s such as a ball or rectangular region in an euclidean space or a compact riemannian manifold such as the sphere sd kendall s planar shape space m 2 which is the same as the complex projective space cp m 2 etc each equipped with a volume measure as above we consider a partition of s into k subsets aj k j 1 k such that aj k 0 j k and aj k 0 as k and let q aj k j k let 1 k n 1 n k k nk n be the mle of 1 k k k consider the bayes estimator under the dirichlet prior with i 1 k i and under the absolute error loss function say 1 1 k k where j k is the median of the posterior distribution of j k namely the median of beta 1 k nj n 1 nj 1 k j 1 k the risk function of is r 1 j k e j k j k j k k j 1 n r 0 cnr r j k 1 j k n r f 1 r 1 k n r 1 1 k 1 2 j k 5 1 36 where f is the distribution function of beta f 1 is its inverse and f 1 1 2 is the median of beta the risk function of is r 1 j k e j k j k j k k j 1 n r 0 cnr r j k 1 j k n r r n j k 5 2 suppose q has a continuous density f with respect to the volume measure we now consider the problem of estimating the approximate density of q as fk where fk x j k aj k for x aj k j 1 k 5 3 assume that max diam aj k j 1 k 0 as k then fk x f x dx 0 as k we consider now the estimates of fk given by f k x j k aj k for x aj k j 1 k 5 4 f k x j k aj k for x aj k j 1 k 5 5 the l 1 and thus the total variation distances between fk and its estimates above are given by 5 1 and 5 2 in particular the bayes estimator f k x basically provides the nonparametric estimator of fk under the dirichlet prior with base measure k on s with density k x 1 k aj k for x aj k j 1 k note that one may consider the above type of approximation for an unbounded state space by requiring that the density decays fast outside a bounded region 37 similarly the l 2 distance between fk and its estimators under the squared error loss are related to the corresponding risk functions if the aj k are chosen such that aj k s k j and j k nj n and dj k nj 1 k n 1 the bayes estimator under squared error loss with dirichlet prior dir 1 k 1 k then fk f k 2 k s r 5 6 fk f k 2 k s r db 5 7 where r and r db are as defined in 2 3 and 2 5 respectively 5 1 simulated l 1 distances we simulated these l 1 distances using the case where s 1 by uniformly sampling 10 000 probability vectors from the standard k simplex for each of k 10 20 30 40 50 100 200 500 and calculating the risk using equations 5 1 and 5 2 we then averaged the 100 risk calcuations to obtain an estimate of the average l 1 distance across the parameter space note that the sample size of 10 000 is smaller than that used in the l 2 case 1 000 000 due to increased computational complexity the results can be found in tables 3 through 10 note that for large n the estimated average l 1 distance for the bayes estimator is often slightly larger that that for the mle it is unknown whether this is because the two are too close to distinguish with sample means or that the l 1 distance is truly larger 38 k 10 n mle bayes n 20 0 4689 0 4541 n 30 0 3827 0 3765 n 40 0 3314 0 3283 n 50 0 2964 0 2947 n 100 0 2095 0 2095 n 200 0 1481 0 1483 n 300 0 1209 0 1211 n 400 0 1047 0 1048 n 500 0 09363 0 09374 n 600 0 08547 0 08556 n 700 0 07913 0 0792 n 800 0 07402 0 07408 n 900 0 06978 0 06984 n 1000 0 0662 0 06625 table 3 simulated l 1 distances for k 10 k 20 n mle bayes n 20 0 682 0 6333 n 30 0 5583 0 5343 n 40 0 4839 0 4706 n 50 0 433 0 425 n 100 0 3063 0 3057 n 200 0 2166 0 2172 n 300 0 1768 0 1774 n 400 0 1531 0 1536 n 500 0 137 0 1373 n 600 0 125 0 1253 n 700 0 1157 0 116 n 800 0 1083 0 1085 n 900 0 1021 0 1023 n 1000 0 09683 0 09702 table 4 simulated l 1 distances for k 20 k 30 n mle bayes n 20 0 8373 0 75 n 30 0 6885 0 6404 n 40 0 5978 0 5685 n 50 0 5353 0 5163 n 100 0 3791 0 376 n 200 0 2682 0 2687 n 300 0 219 0 2198 n 400 0 1896 0 1904 n 500 0 1696 0 1703 n 600 0 1548 0 1554 n 700 0 1433 0 1438 n 800 0 1341 0 1345 n 900 0 1264 0 1268 n 1000 0 1199 0 1203 table 5 simulated l 1 distances for k 30 k 40 n mle bayes n 20 0 9611 0 8375 n 30 0 7948 0 7208 n 40 0 6916 0 6436 n 50 0 62 0 5872 n 100 0 4397 0 4325 n 200 0 3112 0 3111 n 300 0 2541 0 2549 n 400 0 2201 0 221 n 500 0 1968 0 1977 n 600 0 1797 0 1805 n 700 0 1664 0 1671 n 800 0 1556 0 1562 n 900 0 1467 0 1473 n 1000 0 1392 0 1397 table 6 simulated l 1 distances for k 40 39 k 50 n mle bayes n 20 1 063 0 9081 n 30 0 8851 0 786 n 40 0 7723 0 7051 n 50 0 6931 0 6455 n 100 0 4926 0 4803 n 200 0 3488 0 3477 n 300 0 2849 0 2855 n 400 0 2468 0 2477 n 500 0 2207 0 2217 n 600 0 2015 0 2024 n 700 0 1865 0 1874 n 800 0 1745 0 1753 n 900 0 1645 0 1652 n 1000 0 1561 0 1567 table 7 simulated l 1 distances for k 50 k 100 n mle bayes n 20 1 391 1 144 n 30 1 203 1 008 n 40 1 068 0 9143 n 50 0 9676 0 8454 n 100 0 6973 0 6499 n 200 0 4958 0 4838 n 300 0 4053 0 4016 n 400 0 3511 0 3503 n 500 0 3141 0 3144 n 600 0 2868 0 2876 n 700 0 2655 0 2665 n 800 0 2484 0 2495 n 900 0 2342 0 2353 n 1000 0 2222 0 2233 table 8 simulated l 1 distances for k 100 k 200 n mle bayes n 20 1 652 1 364 n 30 1 512 1 245 n 40 1 392 1 149 n 50 1 291 1 073 n 100 0 9698 0 8479 n 200 0 6991 0 652 n 300 0 5732 0 5508 n 400 0 4972 0 4854 n 500 0 4451 0 4386 n 600 0 4065 0 4029 n 700 0 3764 0 3746 n 800 0 3522 0 3515 n 900 0 3321 0 332 n 1000 0 3151 0 3155 table 9 simulated l 1 distances for k 200 k 500 n mle bayes n 20 1 849 1 543 n 30 1 78 1 483 n 40 1 714 1 425 n 50 1 653 1 369 n 100 1 393 1 152 n 200 1 071 0 9189 n 300 0 8932 0 7951 n 400 0 7798 0 7133 n 500 0 7003 0 6532 n 600 0 6407 0 6064 n 700 0 5941 0 5684 n 800 0 5562 0 5367 n 900 0 5248 0 5097 n 1000 0 4981 0 4864 table 10 simulated l 1 distances for k 500 40 6 data example stocking jeans in recent years the lack of sizing representation in clothing stores has been decried by many groups see for example the article women s clothing retailers are still ignoring the reality of size in the us from quartzy shendruk 2018 a large component of this problem is that stores do not tend to stock sizes in proportion to the distribution of clothing sizes reflected in the general population rather there is a notion of stocking clothing based on the typical customer for the store this becomes a self fulfilling prophecy however since choosing not to stock for parts of the population not deemed typical customers ensures that they cannot ever be customers by definition let us focus on denim jeans which are widely considered a staple in the american woman s wardrobe a brick and mortar retailer will largely only sell sizes that are currently in stock while employees may offer to special order sizes not in stock the majority of patrons will simply leave the store without purchasing if their size is not in stock since the purchasing of stock represents a risk by the retailer it is important to accurately guess which sizes to stock however when taking into account both waist size and inseam as several denim brands do this can result in a large number of size options for stock for example using the levi s online size chart and their online catalog we calculated 59 different sizes levi s 2019 the retailer could use past sales as a guide for how much of each size to stock however this has the effect of perpetuating errors in representation since patrons who desired to purchase jeans but were unable since their sizes were not in stock can not be represented in the sales data instead the retailer could sample the desired sizes of anyone who enters the store regardless of whether they make a purchase this would potentially reflect the 41 distribution of potential customers more accuately than sales data the retailer most likely would need to take a small to moderate sample initially since too much time with an inaccurate stock distribution may cause unrepresented segments of the population to stop coming altogether to simulate such a sampling scheme we used the national health and nutrition exam ination survey from 2015 16 cdc 2018 and the levi s size chart to estimate the true levi s jean size distribution of adult women in the united states after restricting to adult women and excluding those in the sample that were pregnant as this temporarily skews waist size there were 2697 adult women surveyed in the nhanes with sample weight ing to properly reflect the uninstitutionalized population of the united states using the levi s website we calculated 59 different jean sizes as well as a category for those whose waist size is too high to fit into any of levi s listed sizes we estimated that 8 39 of adult women in the united states fit into this category there was one jean size that was not sampled in the nhanes we decided that it is unlikely that this jean size does not exist in the entire population of the us so we gave this size a proprotion equal to one half of the minimum nonzero proportion in the other sizes and then renormalized we then simulated random samples of size 100 from the multinomial distribution with 60 categories using the calculated size distribution for the us adult women population this simulates the following scenario the retailer hopes to estimate the distribution of jean sizes his clientele desire by recording the desired jean size of a sample of 100 potential customers and his potential custormers reflect the size distribution of the us adult female population as a whole rather than a potentially smaller waisted subpopulation we included the 60 th category of no size since under this scenario in which the potential customers reflect the true distribution it is possible that customers may arrive at the store hoping to 42 buy jeans before learning that the sizes are not large enough we then calculated the mle for the size distribution by taking the sample counts and dividing by 100 we also used a uniform prior and calculated two different bayes estimators the estimator under squared error l 2 loss which is the posterior mean and the estimator under absolute error l 1 loss which is the posterior median in some sense estimating under a uniform prior tries to balance between two ideas of fairness representing all sizes a uniform prior and representing the size distribution the posterior mean or median given the sample we repeated this simulation 1000 times in each case at least 18 of the size categories were unrepresented in the sample of 100 thus the mle estimated zero probabilities for almost one third of the sizes on the other hand the bayes estimator are never zero and thus leans toward being more inclusive of sizes in stocking while still taking into account the sample data we also calculated the l 1 l 2 and infinity maximum distance between the estimators and the calculated size distribution the results are in table 11 note that the bayes estimators tended to be closer that the mle to the true size distribution by all three distance measures despite being a biased estimator the l 2 bayes estimator was closer in l 1 in 85 8 of the simulations closer in l 2 in 94 9 of the simulations and even had a smaller maximum distance i e the largest absolute difference among all 60 categories in 67 8 of the simulations the l 1 bayes estimator was closer in l 1 in 95 6 of the simulations closer in l 2 in 93 5 of the simulations and had a smaller maximum distance in 62 5 of the simulations in table 12 are the estimated true size distribution based on the nhanes the numbers of a stock of 1000 jeans in a levi s store this would represent as well as stock based on the 43 l 1 l 2 infinity maximum bayes estimator l 2 loss 0 4599 0 0816 0 0377 bayes estimator l 1 loss 0 4368 0 0830 0 0395 mle 0 5081 0 0969 0 0447 table 11 the mean l 1 l 2 and infinity maximum distances between the estimators and the true size distribution in 1000 simulations mle and bayes estimators from a sample of 100 customers note that in three sizes the probabilities are so small that the true size distribution still recommends to stock zero jeans in those sizes also note that in this particular sample 35 of the sizes were unrepresented and thus the mle recommended stocking less than half of the sizes due to rounding the stocks are not exactly 1000 the mle based stock has 993 the bayes l 2 has 1006 and the bayes l 1 has 985 the l 2 distances between the true size distribution and the mle bayes l 2 and bayes l 1 estimators were respectively 0 122 0 077 and 0 079 the absolute errors in the stock the sum of all absolute differences between stock numbers in each size for the mle bayes l 2 and bayes l 1 were respectively 687 486 and 487 the maximum possible absolute error would be around 2000 which would occur if all 1000 pairs of jeans were stocked in the three sizes where zero stock was recommended by the true distribution additional values are possible due to rounding 7 final remarks in this article it is shown that in a multinomial model with a moderately large number of k cells and even a reasonable large sample size the bayes estimator with a multivariate beta 44 size waist inseam true p stock true stock mle stock bayes l 2 loss stock bayes l 1 loss 24 28 1 386 10 04 0 0 7 5 24 30 8 657 10 04 1 0 7 5 24 32 2 020 10 04 0 0 7 5 25 28 6 930 10 05 0 0 7 5 25 30 1 949 10 03 2 0 7 5 25 32 1 000 10 03 1 0 7 5 26 28 6 218 10 04 1 0 7 5 26 30 5 900 10 03 6 11 14 13 26 32 4 262 10 03 5 0 7 5 27 28 6 986 10 04 1 0 7 5 27 30 1 268 10 02 14 0 7 5 27 32 8 139 10 03 9 11 14 13 27 34 1 214 10 03 1 0 7 5 28 28 1 356 10 03 1 0 7 5 28 30 5 628 10 03 6 0 7 5 28 32 1 305 10 02 14 0 7 5 28 34 2 675 10 03 3 0 7 5 29 28 1 847 10 03 2 0 7 5 29 30 1 181 10 02 13 46 34 37 29 32 1 471 10 02 16 0 7 5 29 34 3 006 10 03 3 0 7 5 30 28 2 008 10 03 2 0 7 5 30 30 1 203 10 02 13 23 21 21 30 32 1 350 10 02 15 0 7 5 30 34 4 348 10 03 5 0 7 5 31 28 4 102 10 03 4 0 7 5 31 30 2 713 10 02 30 11 14 13 31 32 3 483 10 02 38 0 7 5 31 34 9 686 10 03 11 0 7 5 32 28 3 122 10 03 3 0 7 5 32 30 3 314 10 02 36 46 34 37 32 32 4 211 10 02 46 69 48 52 32 34 1 271 10 02 14 34 27 29 33 28 3 128 10 03 3 0 7 5 33 30 2 157 10 02 24 11 14 13 33 32 3 553 10 02 39 23 21 21 33 34 3 100 10 03 3 11 14 13 34 28 9 037 10 03 10 0 7 5 34 30 4 477 10 02 49 46 34 37 34 32 7 274 10 02 79 103 68 76 34 34 1 153 10 02 13 0 7 5 16 w s 8 729 10 03 10 0 7 5 16 w m 9 149 10 03 10 0 7 5 16 w l 1 929 10 03 2 0 7 5 18 w s 4 694 10 02 51 80 55 60 18 w m 5 803 10 02 63 92 62 68 18 w l 8 997 10 03 10 11 14 13 20 w s 4 353 10 02 48 0 7 5 20 w m 4 290 10 02 47 46 34 37 20 w l 8 351 10 03 9 0 7 5 22 w s 3 617 10 02 39 69 48 52 22 w m 3 962 10 02 43 57 41 45 22 w l 8 016 10 03 9 34 27 29 24 w s 2 187 10 02 24 0 7 5 24 w m 3 911 10 02 43 103 68 76 24 w l 7 272 10 03 8 11 14 13 26 w s 1 851 10 02 20 34 27 29 26 w m 2 248 10 02 25 11 14 13 26 w l 2 546 10 03 3 0 7 5 no size 8 393 10 02 0 0 0 0 table 12 the true size distribution based on nhanes 15 16 as well as the stock of about 1000 based on the truth and estimators from a sample of size 100 the total absolute errors of the three estimated stocks are 687 486 and 487 respectively 45 or dirichlet dir ck n ck n prior has a smaller risk under squared error loss than that of the mle on most of the parameter space for the cases ck n 1 and ck n 1 k the volume of domination is larger for the case ck n 1 k than ck n 1 when compared by average performance this domination over the mle persistes but is more pronounced for the uniform prior than for the case ck n 1 k simulation studies also show the surprising fact that for ck n 2 the performance of the bayes estimator rapidly declines the choice ck n 1 k is motivated by the fact that it provides a simple approximation of the nonparametric estimation of an unknown distribution with a dirichlet process prior a la ferguson 1973 in this context there have been some simulation studies where nonparametric bayes procedures have been found to outperform frequentist ones a dra matic example may be found in bhattacharya and dunson 2010 here a random sample is drawn from a parametric distribution on kendall s planar shape space with density f 0 f 0 with a given parameter value 0 and three estimates of f 0 are compared f f with as the mle of the parameter the standard nonparametric kernel density estimator g of f and a nonparametric bayes estimator h of f one would expect that the asymptotically efficient mle of a correctly specified parametric model would per form better than its nonparametric competitors but surprisingly a set of 20 simulations each with a fresh random sample of size 200 show the following average l 1 distances d 1 d h f 0 0 44 2 d g f 0 1 03 3 d f f 0 0 75 although the present study points to the superiority of the bayes procedure compared to frequentist ones such as the histogram method the differences do not appear to be that dramatic perhaps the method of representing an unknown density as a mixture of an appropriate parametric family and estimating the mixture by fergusons dirichlet process as used by bhattacharya and dun son 2010 should be preferred also see ghosh and ramamoorthi 2003 and ghosal and 46 van der vaart 2017 still the present article provides a simple and widely applicable bayes estimation of a nonparametric distribution which perhaps may be sharpened to be more effective 47 references a bhattacharya and d dunson nonparametric bayesian density estimation on manifolds with applications to planar shapes biometrika 97 851 865 2010 r n bhattacharya l lin and v patrangenaru a course in mathematical statistics and large sample theory springer texts in statistics springer 2016 p bickel and k doksum mathematical statistics basic ideas and selected topics num ber v 1 in holden day series in probability and statistics prentice hall 2001 cdc national health and nutrition examination survey 2015 16 data files 2018 url https wwwn cdc gov nchs nhanes t s ferguson a bayesian analysis of some nonparametric problems the annals of statistics pages 209 230 1973 s ghosal and a van der vaart fundamentals of nonparametric bayesian inference cambridge series in statistical and probabilistic mathematics cambridge university press 2017 j k ghosh and r v ramamoorthi bayesian nonparametrics springer 2003 levi s levi s size chart and guide 2019 url https www levi com us en us cms sizeguide a shendruk women s clothing retailers are still ignoring the reality of size in the us quartzy jun 2018 url https qz com quartzy 1309509 we analyzed 750 pairs of jeans and found definitive skinny bias by us retailers g van valkenhoef and t tervonen hitandrun hit and run and shake and bake for 48 https wwwn cdc gov nchs nhanes https www levi com us en us cms sizeguide https www levi com us en us cms sizeguide https qz com quartzy 1309509 we analyzed 750 pairs of jeans and found definitive skinny bias by us retailers https qz com quartzy 1309509 we analyzed 750 pairs of jeans and found definitive skinny bias by us retailers sampling uniformly from convex shapes 2016 url https cran r project org package hitandrun r package version 0 5 3 49 https cran r project org package hitandrun https cran r project org package hitandrun 1 introduction 2 the multinomial distribution 3 the proportion of the parameter space favoring the bayes estimator volume calculations 3 1 the surface area of bold 0 mu mumu ekekekekekek 3 2 the surface area of bold 0 mu mumu 2 r 2 r 2 r 2 r 2 r 2 r 3 3 the surface area of bold 0 mu mumu 3 r 3 r 3 r 3 r 3 r 3 r 3 4 the surface area of k r for k 4 3 5 applying the volume calculations to bayes estimators 3 6 simulation results for other priors 3 6 1 uniform prior 3 6 2 dir ck let token ck prior 3 6 3 dir c c let token c prior for c 1 4 average risk across the parameter space 5 on simulation of total variation distances between the true distribution and the distributions of 1 mle based frequentist estimators and 2 nonparametric bayes estimators 5 1 simulated l 1 distances 6 data example stocking jeans 7 final remarks