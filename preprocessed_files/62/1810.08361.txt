adaptive noisy data augmentation panda for simultaneous construction of multiple graph models yinan li 1 xiao liu 2 and fang liu 1 1 department of applied and computational mathematics and statistics 2 department of psychology university of notre dame notre dame in 46556 u s a abstract we extend the data augmentation technique panda by li et al 2018 that regularizes single graph estimation to jointly learning multiple graphical models with various node types in a unified framework we design two types of noise to augment the observed data the first type regularizes the estimation of each graph while the second type promotes either the structural similarity referred as the joint group lasso regularization or the numerical simi larity referred as the joint fused ridge regularization among the edges in the same position across graphs the computation in panda is straightforward and only involves obtaining maximum likelihood estimator in generalized linear models in an iterative manner the simu lation studies demonstrate panda is non inferior to existing joint estimation approaches for gaussian graphical models and significantly improves over the na ve differencing approach for non gaussian graphical models we apply panda to a real life lung cancer microarray data to simultaneously construct four protein networks keywords adjacency matrix joint group lasso jgl joint fused ridge jfr sparse group lasso sgl sparse fused ridge sfr sparsity and similarity corresponding author email fang liu 131 nd edu 1 ar x iv 1 81 0 08 36 1 v 2 st at m e 2 1 m ay 2 01 9 1 introduction undirected graphical models ugm are often used to capture the bilateral conditional dependency structure among a set of variables called nodes the structure can be mathematically expressed by a p p symmetric adjacency matrix a if entry a i j is 0 then nodes i and j are conditionally independent otherwise there is conditional dependency and an edge is drawn between the two nodes in the graph in addition to the extensive research and work on single graph construction there are also increasing practical needs and research interests in comparing multiple graphs the data of which are collected under different conditions for example medical researchers collect and compare gene expression data from normal tissues and tumor tissues to learn how the connection pattern and network structure among the genes nodes change real life graphs are often expected to take on sparse structures especially for large scale networks it is also expected that in the case of multiple graphs the change in condition would only affect a small subset of edges while leaving the majority of connection patterns intact across the graphs to meet the real life expectations sparsity promoting regularization is often applied when constructing both single and multiple graphs in the single graph setting neighborhood selection ns is a popular and simple method to recover the structure of a ugm that guarantees the structure consistency yang et al 2012 2015 a when the underlying conditional distribution of one node given all others can be modeled by an exponential family ns transforms the original graph construction problem into building p generalized linear models glm where sparsity regularization such as l 1 can be applied directly methodology and theory have been developed in linear regression for gaussian graphical models ggm yuan 2010 logistic regression for ising models ravikumar et al 2010 bernoulli nodes hofling and tibshirani 2009 and categorical nodes jalali et al 2011 kuang et al 2017 and poisson regression for count nodes allen and liu 2012 as well as mixed graph models mgm when there are different node types in a ugm yang et al 2012 fellinghauer et al 2013 yang et al 2014 li et al 2018 also provide the a data augmentation approach named panda for constructing a single ugm that offers a wide spectrum of regularization effects with properly designed augmented noise terms for multiple graph construction a na ve approach is estimating each graph separately and then comparing the structures in a post hoc manner for differences a better approach is to jointly estimate multiple graphs simultaneously acknowledging and leveraging the expectation that the structures would largely remain consistent across the multiple graphs while the discrepancy is in the minority for example danaher et al 2014 propose the fused graphical lasso and group graphical lasso to estimate the precision matrices of multiple ggms simultaneously yang et al 2015 b consider the sequential version of the fused graphical lasso for training time evolving ggms for constructing multiple ugms with non gaussian nodes the only work we could locate is zhang et al 2017 where the group lasso and fused lasso penalties are applied to construct graphs with a mixture of discrete and gaussian nodes via a pseudo likelihood approach in this paper we aim to develop an approach that provides an unified framework to effectively jointly train multiple ugms while satisfying the expectation on edge sparsity in each graph and dissimilarity sparsity across graphs toward that end we extend the data augmentation concept of the panda technique developed by li et al 2018 that regularizes single ugm estimation to jointly learning the structures of multiple graphs specifically we design two types of noise to be tagged onto the observed data the first type regularizes the estimation of each graph and the second type is designed to promote either the structural or the numerical similarities on the edges in the same position across multiple graphs the noisy data matrices are first tagged onto the observed data in each graph and then the data are combined across all graphs finally glms are run node by node on the augmented combined data to simultaneously constructing the graphs our contributions are summarized as follows to the best of our knowledge the extension of panda to the multiple graph setting referred 2 to as pandam hereafter m for multiple offers the first general framework to jointly construct multiple ugms ggms included when each node in the graphs given the other nodes can be modelled by an exponential family the augmented noisy data in pandam facilitate regularizing the structures of single graphs as well as imposing similarity constraints across graphs for the latter we design noises to yield either the joint group lasso jgl penalty to promote structural similarities or the joint fused ridge jfr penalty to promote numerical similarities on the edges in the same position across multiple graphs both the jgl and the jfr regularizers are convex in addition both regularizers can be used for variable selection and estimation in glms in which the jgl is equivalent to the sparse group lasso regularization simon et al 2013 and the jfr is equivalent to sparse fused ridge pandam recasts the constrained optimization problem as solving maximum likelihood es timation mle from glms iteratively from noise augmented data as such it does not employ complex optimization techniques but may leverage existing software on glms when q l 1 n l ne 1 ne 2 3 p n l is the sample size of the observed data in graph l 1 q ne 1 and ne 2 are the sizes of the two types of augmented noises and p is the number of nodes in each graph the variance terms of the augmented noises are adaptive to the most up to date parameter estimates in each iteration when the graphs are ggms besides ns we also develop a pandam approach based on the cholesky decomposition of the precision matrices extending huang et al 2006 for single ggm estimation and a pandam approach that realizes regularization through quadratic optimization extending the sparse column wise inverse operator scio for single ggm estimation liu and xi 2015 the theoretical properties established for for the noise augmented loss function by li et al 2018 in the single graph setting also apply to the multiple graph case including the gaussian tail bound and the almost sure a s convergence of the loss function to its expectation we offer a bayesian interpretation for pandam and connect pandam with the empirical bayes framework and maximum a posteriori probability map estimate in each iteration of pandam the rest of the paper is organized as follows section 2 presents the pandam technique for joint estimation of multiple ugms and ggms and offers a bayesian perspective on pandam section 3 applies pandam in simulated multiple graphs and compares its performance against the commonly used estimation approaches section 4 applies pandam to a real life lung cancer microarray data to construct four ggms simultaneously the paper concludes in section 5 with some final remarks 2 pandam for simultaneous construction of multiple graphs the key component in the extension of panda to the multiple graph setting is to design two type of noises that regularizes each graph and promotes structural or numerical similarities among the edges in the same position across graphs we briefly review panda for single graph estimation and its regularization effects and theoretical properties in sec 2 1 we propose pandam jgl and pandam jfr for constructing multiple ggms and ugms in the ns framework in sec 2 2 for multiple ggms we also provide the pandam cd and pandam scio approaches with the jgl and the jfr regularizations in sec 2 3 1 and 2 3 2 respectively the bayesian interpretation is provided in section 2 5 2 1 overview on panda for single graph estimation panda is a noise augmentation na technique for regularizing glm and single ugm estimation and it belongs to the family of noise injection ni regularization techniques ni may refer to 3 adding or multiplying noises directly onto data the data remain the original dimension n p or attaching a noise matrix on to the observed data either n or p increases ni has been proved to be effective in promoting generalization abilities of machine learning methods such as deep learning see li et al 2018 for a brief overview on ni the model training and optimization in ni is realized through iterative algorithms with adaptive noises injected in each iteration panda offers several noise types and noise generating distributions ngd each leading to a different regularization effect on the edge estimation table 1 lists some examples of the noise types in the regression based ns framework see li et al 2018 for a more complete list ejk denotes the augmented data to covariate node xk and jk denotes the regression coefficient for xk when a glm is run with outcome node xj if jk 0 then there is no edge between nodes k and j the value of augmented noise ejj to outcome xj is x j where x j is the sample mean of the data in node j when nodes are of different types in a graph say xj is gaussian while noise type ngd k 6 j regularization effect p bridge ejk n 0 jk ne p j 1 k 6 j jk 2 elastic net ejk n 0 jk 1 2 ne p j 1 k 6 j jk 2 ne p j 1 k 6 j 2 jk adaptive lasso ejk n 0 jk 1 jk ne p j 1 k 6 j jk jk in the bridge type 1 leads to the lasso regularizer and 2 leads to the ridge regularizer 0 2 0 2 0 are tuning parameters jk in adaptive lasso is a consistent estimate for jk panda also yields scad group lasso fused ridge types of regularization see li et al 2018 table 1 panda regularization noises and the corresponding regularization effects xk is poisson due to the asymmetry in the regression models on xj and xk jk and kj would have different interpretations from a regression perspective however the actual magnitude of jk would not be important if the goal is to decide there an edge jk 0 or not in the panda algorithm a threshold 0 is often prespecified for setting jk jk 0 when jk kj 0 then there is no edge otherwise there is an edge between nodes j and k once the noisy data are generated from a ngd they are tagged onto the observed data and the augmented data now have a sample size of n ne p where ne is the size of the noisy data with the enlarged sample size existing software on glms or regression models can be employed to get obtain the mle on jk the procedure iterates until convergence with the variance term in the ngd updated every iteration with the newly estimated jk from the last iteration the regularization effects listed in table 1 in obtained by taking expectation of the noise aug mented loss function over the distribution of the augmented noise in practice the expectation can be realized by either letting ne while keeping v ejk ne o 1 for a given jk in each iteration or by taking the average of noise augmented loss functions from m consecutive iterations letting ne or m would lead to the same targeted regularization effect for ggm for non gaussian ugm ne would achieve the targeted regularization effect arbitrarily well while m would lead to the second order approximation of the targeted regularization effect and the higher order deviation is negligible when jk is not so large or the tuning parameters from the ngds are small li et al 2018 for ggm in addition to the ns approach panda can estimate the ggm through the cd of the precision matrix through the scio estimator of the precision matrix through regularization of partial correlation coefficients to identify the hub nodes as in the space approach peng et al 2009 or through regularizing the precision matrix as a whole with the graphical ridge penalty 2 2 pandam ns for simultaneous construction of multiple ugms assume there are q graphs with the same set of nodes the data of which are collected under different conditions the observed sample size n l may differ by graph for l 1 q we propose pandam for the simultaneous multiple graph estimation in the ns framework by performing 4 node wise glm on the combined noise augmented data across multiple graphs assuming that the conditional distribution of node xj j 1 p given all other nodes x j can be modelled via an exponential family p x l j x l j exp x l j l j bj l j hj x l j 1 where x l j x l 1 x l j 1 x l j 1 x l p and l j is the natural parameter in the glm framework l j l j 0 k 6 j l jkx l k if the canonical link function is used e g the identity link for gaussian xj and the logit link for bernoulli xj if l jk 0 or l kj 0 then there is no edge between nodes j and k otherwise the two nodes are connected with an edge yang et al 2014 2015 a prove that asymptotically with probability 1 that the neighborhood structure of conditionally exponential family graphical models can be recovered exactly to estimate jk 1 jk q jk simultaneously we employ two types of noise the first type denoted by e 1 regularizes the estimation of each graph and can be generated from any ndg given in li et al 2018 that is e l ijk 1 ind n 0 v e l ijk 1 2 for i 1 ne 1 and k 6 j the actual form of v e l jik 1 depends on the targeted regularization effect in each graph e g if the bridge type noise is used then v l jk l 1 l jk as given in table 1 where l 1 is the graph specific tuning parameter the second type of noise denoted by e 2 achieves either the joint group lasso jgl or joint fused ridge jfr regularization on jk the jgl regularization realized through the noise generated from eqn 3 promotes structural similarity among the q graphs by placing the group lasso penalty on jk the jfr regularization realized through the noise generated from eqn 4 promotes the numerical similarity by placing the fused ridge penalty jk jgl eijk 2 e 1 ijk 2 e q ijk 2 ind n 0 2 q l 1 l 2 jk 1 2 i for k 6 j 3 jfr eijk 2 e 1 ijk 2 e q ijk 2 ind n 0 2 tt for k 6 j 4 where iq q is the identity matrix the entries in matrix t are ts s 1 ts 1 s 1 s q s 1 for s 1 q and 0 otherwise the the augmented noises ejj 1 and ejj 2 for outcome node xj are set at x j where x j is the sample mean of the observed data in node j combined from all graphs e l ijj h q l 1 n l 1 q l 1 n l i 1 x l ij for l 1 q h 1 2 and i 1 ne 1 ne 2 5 for ggm with centered outcome nodes e l ijj h 0 in addition to the jgl and jfr regularizers a joint fused lasso type of regularization can be imposed jk by using k k 1 k 6 k in the covariance matrix of eijk 2 however it does not outperform the jfr penalty in terms of promoting similarity on parameter estimation or similarity on edge patterns when jointly estimating multiple graphs we therefore we focus the discussion on the fused ridge in the rest of the paper once the noisy data e 1 of size ne 1 and e 2 of ne 2 for each graph are generated they are tagged onto the observed data as illustrated in figure 1 which uses q 3 as an example but the way the data are combined is similar for all q 2 we recommend centerizing the observed data on each covariate node in x l j and standardizing all nodes if the graphs are ggm or standardizing x l j and centering the outcome node x l j prior to the augmentation the j th glm on the 5 figure 1 a schematic of data augmentation for three graphs in pandam noise augmented to the outcome nodes xj is x j as defined in eqn 5 combined data from all q graphs has node xj as the outcome and q p 1 covariate nodes in the example in figure 1 the first set p 1 covariate values corresponding to the first n 1 observations for xj comes from the first graph while the second and third sets of p 1 covariate values are all 0 similarly for the data from the second and third graphs the noisy data are combined in a columnwise fashion rather than diagnonally as in as the observed data let denote the collection of all the regression parameters from the p regression models with q p 1 regression coefficients excluding the intercepts per model x x 1 x q contains the observed data across q graphs e 1 e 1 1 e q 1 and e 2 e 1 2 e q 2 denote the augmented noises of types 1 and 2 across the graphs the summed negative log likelihood function over p glm and q graphs on the combined noise augmented data is lp x e 1 e 2 l x l e 1 e 2 where 6 l e 1 e 2 q l 1 p j 1 2 h 1 ne h i 1 hj e l ijj h l j 0 k 6 j l jk e l ijk h eijj h bj l ij h and l ij h l j 0 k 6 j l jk e l ijk h when the canonical link is used for ggm with centered nodes we may set l j 0 0 for all j and l proposition 1 establishes the expected regularization effects of pandam ns for joint estimating multiple ugms the proof can be found in appendix a proposition 1 regularization effects of pandam ns jgl and pandam ns jfr the expectation of lp x e 1 e 2 in eqn 6 over the distribution of e 1 in eqn 2 and e 2 in eqns 3 or 4 is ee 1 e 2 lp x e 1 e 2 l x p 1 p 2 c o 2 h 1 p j 1 k 6 j ne h 4 jkv 2 ejk h 7 with p 1 ne 1 2 p j 1 b j q l 1 l j 0 q l 1 k 6 j l 2 jk v e l ijk 1 and p 2 2 ne 2 2 p j 1 b j q l 1 l j 0 k 6 j q l 1 l 2 jk 1 2 for jgl 2 ne 2 2 p j 1 b j q l 1 l j 0 k 6 j l v s l jk v jk 2 for jfr where s in p 2 denotes the combinatorics set q 2 among the q graphs and c is a known 6 constant that relates to ne 1 ne 2 and p for ggm c 0 for non gaussian ugm c q l 1 p j 1 2 h 1 ne h i 1 hj eijj h l j 0 eijj h bj q l 1 l j 0 operationally are estimated through running p glm on the noised augmented data combined across the q graphs in an iterative fashion the algorithmic steps are listed in algorithm 1 most remarks on the specification of the algorithmic parameters for the panda algorithms in single graph estimation in li et al 2018 apply directly to the multiple graph case which are summarized in remark 1 the only additional consideration is the relative sizes on ne 1 and ne 2 given in remark 2 algorithm 1 pandam ns jgl and pandam ns jfg for joint estimation of q ugms 1 input random initial estimates l 0 j for j 1 p and l 1 q a ngd to generate e 1 eqn 2 a ngd to generate e 2 either eqns 3 or 4 noisy data sizes ne 1 and ne 2 maximum iteration t threshold 0 width of moving average ma window m banked parameter estimates after convergence r 2 t 1 convergence 0 3 while t t and convergence 0 4 for j 1 p a generate e l ijk 1 for i 1 ne 1 and e l ijk 2 for i 1 ne 2 with l t 1 j plugged in the variance terms of the ngds b centerize x l j in each graph for non gaussian ugms or standardize x l for l 1 q if the graphs are ggm c obtain augmented data x by combining the original data x with e 1 and e 2 in a similar manner as in figure 1 d run glm with outcome node x j with the a proper canonical link function and linear predictor q l 1 l j 0 x 1 j 1 j x 2 j 2 j x q j q j to obtain the mle l t j for l 1 2 q e if t m calculate the ma l t j m 1 b t m 1 l b j otherwise l t j l t j for l 1 q end for 5 plug in l t j in eqn 6 to calculate the overall loss function and apply one of the convergence 6 criteria remark 1 to l t let convergence 1 if the convergence is reached 7 end while 8 continue to execute the command lines 4 and 6 for another r iterations and record l b j for b t 1 t r and l 1 q let l jk l t 1 jk l t r jk 9 set l jk l kj 0 and claim no edge between nodes j and k if max l jk min l jk 0 max l jk min l jk 0 or max l kj min l kj 0 max l kj min l kj 0 otherwise there is an edge between nodes j and k 10 output 1 q remark 1 algorithmic parameters specification maximum iteration t should be set at a relatively large value to ensure the algorithm meets the convergence criteria including visual examination of the trace plot of loss function l t to see if it stabilizes and plateaus with only mild random fluctuation calculation of the percentage change in the loss function l t 1 l t l t to see if it falls below a prespecified threshold or application of a formal statistical test on whether the algorithm converges provided in sec 4 4 of li et al 2018 it should be noted similar to 7 the single graph estimation that the estimate l jk cannot be exactly 0 so it is necessary to use a cutoff 0 often a close to 0 positive value which is justified in an asymptotic sense in that any fluctuation around jk diminishes as ne or m remark 2 choice of ne 1 ne 2 and m large ne 1 ne 2 or m are needed to obtain the expected regularization effects in practice we would first ensure q l 1 n l ne 1 ne 2 qp so that there are unique solution for the mle in each regression at each iteration ne 1 v ejk 1 relates to the amount of regularization on single graph estimation while 2 ne 2 relates to the amount of regularization on edge similarity across multiple graphs though ne 1 and ne 2 are specified independently they are somewhat related for example if the lasso type of noise is used and 1 ne 1 is large then each individual graph would be sparse and the graphs may already be similar enough even if 2 ne 2 is not large if the single graphs are still dense after properly tuning 1 ne 1 and if it is believed that the dense structure would remain roughly the same across graph except for a few disagreements then ne 2 2 can be set a large value to promote the edge similarity finally though pandam allows ne 1 and ne 2 vary by graph this additional complexity and flexibility is unnecessary as the same expected regularization effects can be achieved by using the same ne 1 and ne 2 across graphs as long as m is relatively large a special case of ugm is ggm where the nodes follow multivariate gaussian distributions constructing the ggm is equivalent to estimating the precision matrix of the multivariate gaussian distribution specifically if the entry jk for j 6 k is 0 then there is no edge between nodes j and k otherwise there is in the case of ggm we would first standardize the observed data in each node of each graph then the augmented noises eijj 1 and eijj 2 to the outcome nodes can be set at 0 algorithm 1 runs p linear regressions in each iteration to obtain the ols estimates on j and the loss function used in the stopping criterion is the sum of squared errors sse summed over the p linear regressions on the noise augmented data lp x e 1 e 2 q l 1 nl i 1 p j 1 x l ij k 6 j x l ik l jk 2 2 h 1 ne h i 1 p j 1 0 q l 1 k 6 j e l ijk h l jk 2 8 the variance of the error term 2 j in each regression is estimated by ssej q l 1 n l j where ssej is from the regression with outcome node j and j trace xj x jx j 1 x j is the degrees of freedom calculated as the trace of the hat matrix on the noise augmented data x x e it is known that for ggm the following relationship exists between jk and jk the j k th entry in the precision matrix of a gaussian distribution for k 6 j hastie et al 2009 that is for the l th ggm l jk l jj l jk and l jj l j 2 with the estimated 2 j algorithm 1 can output the edge weights for ggm in addition to the structures of the graphs by setting jk l jk l jj l jk 2 j the penalty terms p 1 and p 2 in proposition 1 for the ggm case have closed forms and are listed in corollary 1 corollary 1 regularization effects of pandam ns for multiple ggm estimation the two expected regularizers in eqn 7 realized by pandam ns for q ggms are p 1 ne 1 q l 1 p j 1 k 6 j v e l ijk 1 l 2 jk q l 1 l 1 ne 1 p j 1 k 6 j l jk 2 if the bridge type noise is used p 2 2 ne 2 p j 1 k 6 j q l 1 l 2 jk 1 2 for jgl 2 ne 2 p j 1 k 6 j l v s l jk v jk 2 for jfr where s in p 2 denotes the combinatorics set q 2 among the q graphs 8 2 3 additional pandam approaches for simultaneous construction of multi ple ggm with the connection between the graph structure and the precision matrix for ggm there exist additional approaches for constructing ggms in additional to the ns approaches given in section 2 2 such as the pandam cd and pandam scio approaches presented below 2 3 1 pandam cd for multiple ggm estimation the cholesky decomposition cd approach refers to estimating the precision matrix through the ldl decomposition a variant of the cd compared to the ns approach in section 2 the cd approach guarantees symmetry and positive definiteness of the estimated precision matrix wlog let x l n p np 0 l apply the cd decomposition to l for l 1 q as in l l l d l 1 l l such that l d l 1 pj 1 l j 2 d l diag l 21 l 2 p and l l is a lower uni triangular matrix with elements l jk for j k 0 for k j and 1 for j k similar to pandam ns pandam cd estimating the edges and q precision matrices via aug menting two types of noise the first type e 1 is defined in the same way as eqn 2 for example if the bridge noise is used then e l ijk 1 ind n 0 l 1 l 2 j l jk and e l ijj 1 0 on standardized x for i 1 ne 1 9 the second type e 2 is then drawn from either eqns 10 or 11 to achieve the jgl and the jfr regularizations respectively jgl e 1 ijk 2 e q ijk 2 ind n 0 2 l 2 j q l 1 l 2 jk 1 2 i for k j 10 jfr e 1 ijk 2 e q ijk 2 ind n 0 2 l 2 j tt for k j 11 for i 1 ne 2 where iq q is the identity matrix and the entries in matrix tq q are ts s 1 ts 1 s 1 s q s 1 for s 1 q and 0 otherwise e l ijj 2 can be set at 0 on standardized x l for j 1 p a once e 1 and e 2 are generated they are tagged onto the original x and then the augmented data across q graphs are combined as depicted in figure 2 for any j from 1 to p though the data augmentation scheme looks similar to figure 1 in the ns setting there is an important difference that is the dimension of the covariate nodes grows with j given how the precision matrix is estimated through the cd approach specifically let 1 p l l x l with a bit algebra we get x l 1 1 and x l j j 1 k 1 x l k l jk j for j 2 p where j n 0 l 2 j 12 with the augmented data from the graphs are combined in the way as depicted in figure 2 we update eqn 12 to x 1 1 and x j q l 1 j 1 k 1 x k l jk j for j 2 p where j n 0 2 j 13 eqn 13 suggests a sequence of regression models x 2 regressed on x 1 x 3 regressed on x 1 x 2 and so on there is no regression model for j 1 after solving for l jk from the series of regression models l 2 j can be calculated as l 2 j n l 1 n l i 1 x l ij j 1 k 1 x l ik l jk 2 14 9 figure 2 a schematic of data augmentation in pandam cd for three ggms any j 1 p when j 1 there are no covariate nodes in the regression model and x 1 the first column in the combined augmented data is used to estimate 21 when j 1 l 21 can be set at the sample variance of x l 1 proposition 2 show the pandam cd approach described above achieves the targeted regularization effects over the distribution of e 1 and e 2 in the multiple ggm estimation proposition 2 regularization effects of pandam cd for multiple ggm estimation the noise augmented loss function across the q graphs is lp x e 1 e 2 q l 1 nl i 1 p j 1 l j 2 x l ij j 1 k 1 x l ik l jk 2 2 h 1 ne h i 1 p j 1 l j 2 eijj h q l 1 j 1 k 1 e l ijk h l jk 2 the expectation of which over the distribution of e 1 and e 2 is e lp x e 1 e 2 jgl l x ne 1 q l 1 p j 1 j 1 k 1 v e l ijk 1 l 2 jk 2 ne 2 p j 1 j 1 k 1 q l 1 l 2 jk 1 2 15 jfr l x ne 1 q l 1 p j 1 j 1 k 1 v e l ijk 1 l 2 jk 2 ne 2 p j 1 k 6 j l v s l jk v jk 2 16 where s denotes the combinatorics set q 2 among the q graphs if the bridge type noise is used on e 1 then the penalty term q l 1 ne 1 p j 1 j 1 k 1 v e l ijk 1 l 2 jk in eqns 15 and 16 on each graph becomes q l 1 l 1 ne 1 p j 1 j 1 k 1 l jk 2 which is the regu larization huang et al 2006 originally proposed to estimate the precision matrix of a single ggm by solving the constrained optimization problem jk arg min jk 2 j n i 1 xij j 1 k 1 xik jk 2 j 1 k 1 jk for k 1 j 1 j 1 p it is not surprising that the penalty forms in proposition 2 look similar to those in corollary 1 given that both are based on the linear regression framework however the parameter l jk has different meaning and interpretation for the ns and cd settings though the notation is the same in the ns setting the precision matrix is solved from l jk through l jj l j 2 and l jk l jk l jj whereas in the cd setting l jk makes the lower uni triangle matrix l l and the precision matrix is calculated as l l diag l 1 2 l p 2 l l the algorithmic steps for implementing pandam cd are available in algorithm s 1 in the sup plementary materials in general the steps are similar to algorithm 1 except that the set of 10 covariates grows with j in each iteration and an additional inner loop is added within each re gression in each iteration to alternatively solve for l jk and l 2 j as they are inter dependent the solution l 2 j depends on the estimate l jk which in turn depends on l 2 j as the distribution of the augmented noise based on which l jk is calculated depends on l 2 j 2 3 2 pandam scio for multiple ggm estimation let j be the j th column of the precision matrix p p in a ggm liu and xi 2015 propose the scio estimator as the solution to a quadratic optimization problem that does not involve the maximization of the likelihood per se j arg min j 2 n 1 jx x j 1 j j k 6 j jk separately for j 1 p li et al 2018 provide a panda counterpart to scio in the single graph setting and simplify the problem to calculating j n x x 1 1 j where x is the noise augmented data and 1 j is the jth column of the identity matrix ip p we now extend panda scio for single ggm estimation to multiple ggms with either the jgl or the jfr regularization the first type of noise e 1 is drawn from the ngd in eqn 1 for example if the bridge type of noise is used then e l ijk 1 ind n 0 l 1 l jk and e l ijj 1 0 for i 1 ne 1 17 the similarity promoting noise e 2 is drawn from eqn 18 and 19 for the jgl and jfr regular izations respectively jgl e 1 ijk 2 e q ijk 2 ind n 0 2 l 2 j q l 1 l 2 jk 1 2 i for k 6 j and e l ijj 2 0 18 jfr e 1 ijk 2 e q ijk 2 ind n 0 2 tt for k 6 j and eijj 2 0 19 for i 1 ne 2 where iq q is the identity matrix and the entries in matrix tq q are ts s 1 ts 1 s 1 s q s 1 for s 1 q and 0 otherwise the noisy data generated from eqns 17 18 and 19 are first scaled to obtain 2 ne l j 1 and 2 ne l j 2 before being tagged onto the observed data x in a similar fashion as in figure 1 the algorithmic steps for carrying out pandam scio is given in algorithm s 2 in the supplementary materials proposition 3 establishes the regularization effects on with the proof given in appendix b proposition 3 regularization effects of pandam scio for multiple ggms let denote two matrices combined by row and denote two matrices combined by col umn denote the collections of parameters and noises for column j j 1 p by j 1 j q j ej 1 e 1 j 1 e q j 1 and ej 2 e 1 j 2 e q j 2 denote the observed data the collections of all model parameters and all augmented noises by x x 1 x q 1 p e 1 e 1 1 ep 1 and e 2 e 1 2 ep 2 let j 1 j 1 j the noise augmented loss function and its expectation over the distribution of e are respectively lp x e 1 e 2 2 n 1 p j 1 j x jx j j p j 1 j j 20 e lp x e 1 e 2 21 l x l 1 ne 1 p j 1 k 6 j q l 1 l jk 2 2 ne 2 p j 1 k 6 j q l 1 l 2 jk 1 2 for jgl 2 ne 2 p j 1 k 6 j l v s l jk v jk 2 for jfr where s denotes the combinatorics set q 2 among the q graphs 11 the minimizer of eqn 20 can be solved by taking its derivative and setting it to be 0 resulting in j n x jx j 1 j note the inverse of x jx j exists since the augmented data x j has a sample size larger than its feature dimensionality also noted is that ee x jx j for the jgl case is a block diagonal matrix given that the noises in different graphs are independent further simplifying the solution to l j 2 n x l j x l j 1 1 j l 1 q 2 4 panda jgl and panda jfr beyond graphical models the jgl and jfr regularizers realized through pandam can be applied beyond the framework of graphical models for variable selection and parameter estimation in regression models in general suppose there are p predictors belonging to q groups in a regression model e g several genes on the same pathway in a regulatory network denote the groups by l 1 q and the variables in group l by xj l for j 1 pl if the goal is to introduce sparsity across variables within each group and as well at the group level across groups then we could employ the following two types of noises through pandam lasso noise eij l 1 ind n 0 l 1 j l 1 for i 1 ne 1 j 1 pl and l 1 q 22 jgl noise ei 1 l 2 eipl l 2 ind n 0 2 p l j 1 2 j l 12 i for i 1 ne 2 l 1 q 23 where ipl pl is the identity matrix e 1 leads to sparsity across the variables within each group while e 2 leads to sparsity across the groups the combined regularization in eqns 22 and 23 as a matter of fact corresponds to the sparse group lasso penalty simon et al 2013 if the goal is to introduce sparsity across variables within each group while shrinking the parame ters at the group level i e each group as a whole then we could employ the following two types of noises through pandam lasso noise eij l 1 ind n 0 l 1 j l 1 for i 1 ne 1 j 1 pl and l 1 q 24 jfr noise ei 1 l 2 eiql l 2 ind n 0 2 tt for i 1 ne 2 l 1 q 25 where the entries in matrix tpl pl are tjj 1 tj 1 j 1 j pl j 1 for j 1 pl and 0 otherwise to the best of our knowledge there does not seem to exist a counterpart in the regression setting to the combined regularization introduced through the noises in eqns 24 and 25 we hereby coin the term sparse fused ridge regularization to refer to the regularization yielded by eqns 24 and 25 that can be employed for regression models 2 5 bayesian perspectives on pandam both the panda technique and bayesian modeling impose regularization through introducing endogenous information li et al 2018 draw connections between the two paradigms in two aspects first the various regularization effects realized by panda have counterpart priors on the model parameters through bayesian hierarchical modeling second the parameters estimates obtained by panda at every iterations can be seen as obtaining the maximum a posterior map estimate in an empirical bayes eb like framework in the pandam setting there also exist priors that lead to the jgl and jfr regularizations in the bayesian hierarchical setting as mentioned in sec 2 4 the jfr regularization per regression is equivalent to the sparse group lasso in expectation over the distribution of augmented e 1 and e 2 since the focus on the priors that lead to the second regularization term jgl or jfr we use the lasso type e 1 for demonstration wlog the priors corresponding to the regularization effects generated by other types of e 1 e g ridge elastic net can be found in the literature let 12 jk 1 jk q jk xu and ghosh 2015 develop the bayesian version for the sparse group lasso which reformulated in the context of the regression run by pandam is jk 2 2 2 j n 0 2 jv where 2 2 1 2 q v diag 2 2 1 26 2 2 1 q l 1 1 2 l 2 1 2 exp l 2 1 2 g l 1 l 2 22 2 2 27 2 j 2 j the priors that lead to the jfr regularization with the lasso regularization on each graph is jk 2 j exp 1 2 2 j jk diag 2 2 tt jk where 1 q 28 1 q q l 1 l 2 1 l exp l 21 l 2 2 29 2 j 2 j where the entries ts s 1 ts 1 s 1 k q s 1 for s 1 j 1 j 1 q and 0 otherwise the 2 jktt jk component in eqn 28 promotes the edge similarity numerically across graphs whereas the mixture of the gaussian component jkdiag 1 q 2 jk in eqn 28 with the rayleigh distribution on in eqn 29 leads to a laplace prior jk that promotes the sparsity within each graph we also provide an eb like interpretation for pandam ns in jointly constructing multiple graphs we use the bridge type noise for e 1 for illustration purposes which can be easily modified to accommodate other ngds the eb like priors on l jk l 1 q and j 1 k 6 j 1 p in iteration t 1 that eventually lead to the jgl and jfr penalties on the edges upon convergence are ggm jgl 1 jk q jk 2 j exp 2 2 j 1 q l 1 l 1 l 2 jk l t jk 2 l 2 jk t jk 2 30 ggm jfr 1 jk q jk 2 j exp 2 2 j 1 q l 1 l 1 l 2 jk l t jk 2 jktt jk 31 and 2 j 2 j in eqns 30 and 31 ugm jgl 1 jk q jk exp 12 q l 1 l 1 l 2 jk l t jk 2 l 2 jk t jk 2 32 ugm jfr 1 jk q jk exp 12 q l 1 l 1 l 2 jk l t jk 2 jktt jk 33 for ggm and ugm respectively each prior above contains two additive components the first component q l 1 l 1 l 2 jk l t jk regularizes the sparsity in each graph and the second component imposes either the jgl or the jfr regularizations both the first component as well as the jgl prior are formulated with an estimate l t jk from the last iteration t but not the jfr prior the priors listed eqns 30 and 33 in iteration t 1 are formulated using the most up to date parameter estimates from the augmented data in the last iteration t thus the connection with eb the parameter estimate t jk can be either the map estimate or a random posterior sample for jk 13 from the t th iteration as pointed out in li et al 2018 when the expected regularization effect is convex in jk which is the case for jgl the maps obtained upon the convergence of the eb like pandam algorithm are the same regardless of whether the prior in each iteration is constructed using the map or a random posterior sample if the expected regularization effect is non convex in jk which is the case for jfr plugging in the map estimate or a random posterior sample from the last iteration for t jk in the priors would lead to different maps upon the convergence using a random posterior sample would eventually lead to a converged multi modal posterior distribution on jk while using the map from the last iteration for t jk leads to the map that is an initial value dependent local optimum upon convergence this is because using map to construct the prior in each iteration is equivalent to iteratively solves convex optimization in the same way as panda but only in the bayesian framework while by using a random posterior draw allows the posterior sampling to explore the whole parameter space 3 simulation we apply pandam ns jgl and pandam ns jfr to jointly train three ggm graphs and three pgm graphs respectively in the ggm case we compare panda to the joint graphical lasso approach with the group lasso gl penalty and the fused lasso fl penalty danaher 2013 danaher et al 2014 and to the sparse group lasso with the lasso and gl penalty note the sparse group lasso originally is not developed for constructing multiple graphs but rather being used in the regression setting we adapt the sparse group lasso to construct multiple graphs and expect it yields similar results to pandam ns jgl since there does not exist any approach for jointly estimating multiple ugms when the all nodes are non gaussian to the best of our knowledge we thus compare pandam with the na ve differencing approach in the non gaussian ugm case that constructs each graph separately and then compares the edge connection patterns in the estimated graphs pairwise in a post hoc manner the simulation schemes are summarized in table 2 for simplicity we kept n the same across the 3 graphs for each graph type we first applied r function simgraph to simulate two p p matrices a 0 and b l l 1 2 3 where a 0 acts as the baseline structure and b l characterizes the deviation from the baseline in graph l the adjacency matrix a l was constructed as a l a 0 1 a 0 6 b l where 1 is the indicator function the adjacency matrices a l simulated for the ggm case are shown in figure 3 each graph is relatively sparse either in the banded and 3 hub graphs and the differences in edges across are sparse as well in other words the 3 graphs look similar expect for a small proportions of different edges the last 3 columns in table 2 given a l we then simulated the nodes values gaussian and poisson x respectively using r function xmrf sim graph n p total non zero edges different edge patterns in type edges each pair of graphs g 1 g 2 g 3 g 1 g 2 g 1 g 3 g 2 g 3 ggm scale free 100 50 1225 224 224 225 66 65 69 ggm banded 100 50 1225 214 208 217 38 35 39 ggm hub 100 50 1225 38 38 38 18 18 18 pgm scale free 50 50 1225 297 280 276 59 68 59 table 2 simulation schemes for multiple graph models we run 100 repetitions in each graph type case for pandam ns we used the lasso type noise e 1 to yield edge sparsity in each graph and attempted both jgl and jfr type noises e 2 to promote similarity across graphs for the ggm case we also employed the scio based approach to construct the graphs in addition the ns approach the specifications of the tuning and algorithm parameters in pandam are given in table 3 1 the tuning parameter associated with the 14 scale free banded a 3 hub figure 3 adjacency matrices of the 3 ggms to be compared lasso type noise e 1 is left to vary so to examine how the regularization effect and the estimation accuracy change with 1 and 2 which is 1 4 of 1 in pandam as ne 1 and ne 2 are fixed larger 1 and 2 imply more sparsity in each graph and in dissimilarity across the graphs graph method 2 2 t ne 1 jgl ne 2 jgl ne 1 jfr ne 2 jfr m 0 ggm ns 1 0 1 4 80 4 000 4 000 2 000 2 000 1 10 4 ggm scio 1 0 1 4 150 2 000 2 000 2 000 2 000 1 10 4 pgm ns 1 0 1 4 80 4 000 4 000 2 000 2 000 2 10 4 table 3 tuning and algorithm parameters in pandam ns in the simulation study figure 4 depicts the roc curves with the false positive counts fp and true positive counts tp for different approaches positive is defined as detecting a disagreement in edge connection between two nodes in between two graphs the fp and tp shown in the figures are the sum of fp and the sum of tp respectively over the 3 pairwise comparisons among the 3 graphs for pandam we examine a grid of 1 values to generate a range of fp and tp for the roc curves for the joint graphical lasso and the sgl approaches we also vary the tuning parameter 1 that controls the sparsity of each graph and set the tuning parameter for controlling the similarity across the graphs at 1 4 1 for the na ve differencing since each graph is estimated separately there is only one tuning parameter which controls the sparsity of each graph and is set to the same across the 3 graphs for each graph type first as expected the na ve differencing approach performs the worst across all cases with lower tp for a given fp compared to the joint training methods pandam joint graphical lasso and sparse group lasso second in the construction of the scale free ggms pandam jfr ns ns 15 false positive count tr ue p os iti ve c ou nt 0 250 500 750 1000 0 25 50 75 10 0 m joint graphical lasso fl pandam jgl ns pandam jgl scio pandam jfr scio false positive count tr ue p os iti ve c ou nt 0 300 600 900 1200 0 15 30 45 60 pandam jgl pandam jfr false positive count tr ue p os iti ve c ou nt 0 250 500 750 1000 0 10 20 30 40 pandam jgl ns pandam jfr ns pandam jgl scio pandam jfr scio false positive count tr ue p os iti ve c ou nt 0 125 250 375 500 0 10 20 30 40 50 m joint graphical lasso gl m joint graphical lasso fl pandam jgl ns pandam jfr ns figure 4 roc curves in the simulation study on multiple graph construction the approaches in the legend of each plot are arranged in a descending order in performance and boxed boxed together if they have similar performance with the sparse group lasso penalty and the joint graphical lasso with the fused lasso penalty seem to perform the best for small fp followed by pandam ns jgl pandam scio jgl and pandam scio jfr which delivered very similar performance the joint graphical lasso with the group lasso penalty seems to be slightly worse compared to the above 3 pandam approach in this simulation setting which tend to surpass the joint graphical lasso approach with the group lasso penalty as fp increases third in the construction of the ggms with the banded adjacency matrices the joint graphical lasso with the fused lasso penalty and the sparse group lasso seem to be the best at the small fp range followed by all the pandam techniques and the joint graphical lasso approach with the group lasso penalty fourth in the construction of the ggms with the 3 hub nodes the sparse group lasso and pandam jfr scio lead the performance the joint graphical lasso with the group lasso penalty and pandam jgl scio follow with a similar performance pandam jgl ns and pandam jfr ns preformed slightly better than the differencing approach all taken together the sparse group lasso adapted for estimating graph models is the best for simultaneously constructing ggms for the examined network structures the performance between pandam and the joint graphical lasso alternates depending on the network structure and the specific regularizer employed by pandam and the joint graphical lasso respectively among all the joint estimation approaches examined in this study only the joint graphical lasso leads to positive definite estimates for the precision matrix in the ggm case 16 4 case study the lung cancer microarray data to demonstrate the use of the pandam technique in the estimation and differentiation of mul tiple ugms we apply panda to a real life data set with protein expression levels of subjects diagnosed with acute myeloid leukemia aml the data set is available from the md anderson department of bioinformatics and computational biology the subjects in the data are classified by aml subtype according to the french american british fab classification system based on the criteria including cytogenetics and cellular morphology we focus on 4 aml subtypes out of the 11 subtypes recorded in the data m 0 17 subjects m 1 34 subjects m 2 68 subjects and m 4 59 subjects data on the other 7 aml subtypes are not used because of the small sample sizes of these subtypes 5 13 the similarity across the protein networks from difference aml subtypes are often of interest as the knowledge about common protein interactions across various aml subtypes can be helpful for developing treatments for aml pandam is used to jointly construct 4 ggms for the 4 aml subtypes m 0 m 1 m 2 m 4 among 18 proteins nodes that are known to be involved in the apoptosis and cell cycle regulation according to the kegg database kanehisa et al 2011 the protein expression levels of the 18 proteins are assumed to follow gaussian distributions because the main objective is to find common edges across the 4 ggms the jgl regularization was used to promote similar non zero patterns we set both n 1 e and n 2 e at 600 for the tuning parameters we specified 1 and 0 to obtain a lasso type penalty and selected 1 1 0 005 2 1 0 010 3 1 0 022 4 1 0 022 and 2 0 018 using the extended bic criterion chen and chen 2008 foygel and drton 2010 haslbeck and waldorp 2015 with a grid search we ran 50 iterations in r version 3 4 0 on the linux x 86 64 operating system the computation took approximately 46 seconds figure 5 displays the four ggms estimated using pandam ns jgl the red edges indicate protein interactions common to all 4 aml subtypes m 0 m 1 m 2 and m 4 and four are identified including the edges between pten and pten p pten and bad p 155 pten p and bad p 136 and akt p 308 and bcl 2 the shared structure we obtained using pandam was also identified in previous studies on protein networks of aml subtypes peterson et al 2015 in addition for the proteins involved in the shared structure proteins pten and pten p had been shown to have relatively high expression in the four aml subtypes we consider here compared with other subtypes such as m 6 and m 7 kornblau et al 2009 in subtypes m 0 m 1 and m 2 the expression levels of proteins bad p 136 and bcl 2 tend to be high and that of akt p 308 tends to be low the shared structure in the estimated graphs could shed light on similarities in the structures of the protein networks across aml different subtypes which in turn will guide treatment development for aml 5 discussion we extend the data augmentation regularization technique panda for constructing a single graph to jointly estimating multiple graphs referred to as pandam toward that end we design two types of noises to augment the observed data from multiple graphs the first type regularizes the structure of each graph such as with sparsity while the second type promotes either the structural similarity the jgl regularization or the numerical similarity the jfr regularization among the edges in the same positions across the graphs to the best our knowledge our extension of panda to the multiple graph setting offers the first approach for jointly constructing multiple ugms in general ggms and mgms included when the conditional distribution of each node in each graph given the other nodes can be modelled by an exponential family for general ugms we implement pandam in the ns framework to estimate the model parameters for ggms there are other alternatives to the ns framework such as through the cholesky decomposition of the precision matrix or through the scio estimator in all cases we establish the expected regularization effect in pandam computationally pandam 17 http bioinformatics mdanderson org supplements kornblau aml rppa aml rppa xls http bioinformatics mdanderson org supplements kornblau aml rppa aml rppa xls akt akt p 308 akt p 473 bad bad p 112 bad p 136 bad p 155 bax bcl 2 bclxl ccnd 1 gsk 3 gsk 3 p myc pten pten p tp 53 xiap subtype m 0 akt akt p 308 akt p 473 bad bad p 112 bad p 136 bad p 155 bax bcl 2 bclxl ccnd 1 gsk 3 gsk 3 p myc pten pten p tp 53 xiap subtype m 1 akt akt p 308 akt p 473 bad bad p 112 bad p 136 bad p 155 bax bcl 2 bclxl ccnd 1 gsk 3 gsk 3 p myc pten pten p tp 53 xiap subtype m 2 akt akt p 308 akt p 473 bad bad p 112 bad p 136 bad p 155 bax bcl 2 bclxl ccnd 1 gsk 3 gsk 3 p myc pten pten p tp 53 xiap subtype m 4 figure 5 joint estimation via pandam ns jgl of ggms in four subtypes of aml the edge width is proportional to its weight the red edges are common to all 4 ugms while the dashed black edges are not runs glms to obtain mle iteratively on the combined augmented data across graphs and the algorithms can be programmed with a few lines in any software that offers built in glm functions when the augmented data are large the computation could take some time but usually the panda algorithms does not require many iterations to converge there are two penalty terms in pandam users need to choose at least two tuning parameters in the implement of pandam first users need to first to decide which ngds to use for the first and the second types noise for the first type we refer users to li et al 2018 for the second type it does not seem there is much difference between the jfl and jgl regularization in the simulation studies despite their conceptual disparity once the ngds are determined the collection of all tuning parameters can be chosen by cross validation or information based criteria see vujac i et al 2015 via a grid search such as the extended bic as we employed in the simulation studies to choose the tuning parameters lead to the better identification of the graphical structures or to better predictive power on the other hand danaher et al 2014 states there is no theoretically optimal value for the parameters that controls the relative separate vs simultaneous regularization across graphs since the optimal value would need to be a function of the number of covariates and group sizes among other things pandam enjoys the theoretical properties established by li et al 2018 in the single graph setting including the gaussian tail bound of the augmented loss function and the almost sure convergence of the noise augmented loss function to its expectation which is a penalized loss 18 function with the designed regularization effect on the other hand we do investigate whether the estimated graphs are structurally consistent for true graph structures which this can be a future research topic though a likely challenging one to the best of our knowledge there is no theoretical work on that topic yet not even for ggms which have nice connection with the precision matrices of gaussian distributions for example danaher et al 2014 while proposing the joint graphical lasso for jointly estimating multiple ggms do not investigate establish the structural consistency of the jointly estimated ggms for true structures one possible direction is to borrow the work by yang et al 2014 and yang et al 2015 a in the single graph setting which connect the conditional distribution of each node given others with ugms via exponential families and glms and shows the graph constructed by minimizing the regularized loss function with l 1 penalty terms placed on the regression coefficients is structurally consistent for the underlying ugm references allen g i and liu z 2012 a log linear graphical model for inferring genetic networks from high throughput sequencing data ieee international conference on bioinformatics and biomedicine pages 1 6 chen j and chen z 2008 extended bayesian information criteria for model selection with large model spaces biometrika 95 3 759 771 danaher p 2013 jgl performs the joint graphical lasso for sparse inverse covariance estimation on multiple classes r package version 2 3 danaher p wang p and witten d m 2014 the joint graphical lasso for inverse covariance estimation across multiple classes j roy statist soc ser b 76 373 397 fellinghauer b b uhlmann p ryffel m von rhein m and reinhardt j d 2013 stable graphical model estimation with random forests for discrete continuous and mixed variables computational statistics and data analysis 64 132 142 foygel r and drton m 2010 extended bayesian information criteria for gaussian graphical models in advances in neural information processing systems pages 604 612 haslbeck j and waldorp l j 2015 structure estimation for mixed graphical models in high dimensional data arxiv preprint arxiv 1510 05677 hastie t tibshirani r and friedman j 2009 the elements of statistical learning springer new york 2 edition hofling h and tibshirani r j 2009 estimation of sparse binary pairwise markov networks using pseudo likelihoods the journal of machine learning research 10 883 906 huang j liu n pourahmadi m and liu l 2006 covariance matrix selection and estima tion via penalised normal likelihood biometrika 93 85 98 jalali a ravikumar p k vasuki v and sanghavi s 2011 on learning discrete graphical models using group sparse regularization international conference on artificial intelligence and statistics pages 378 387 kanehisa m goto s sato y furumichi m and tanabe m 2011 kegg for integration and interpretation of large scale molecular data sets nucleic acids research 40 d 1 d 109 d 114 kornblau s m tibes r qiu y h chen w kantarjian h m andreeff m coombes k r and mills g b 2009 functional proteomic profiling of aml predicts response and survival blood 113 1 154 164 19 kuang z geng s and page d 2017 a screening rule for l 1 regularized ising model estima tion neural information processing systems li y liu x and liu f 2018 adaptive noisy data augmentation for regularization of undirected graphical models arxiv page arxiv 1810 04851 liu w and xi l 2015 fast and adaptive sparse precision matrix estimation in high dimensions journal of multivariate analysis 135 153 162 peng j wang p zhou n and zhu j 2009 partial correlation estimation by joint sparse regression models journal of the american statistical association 104 735 746 peterson c stingo f c and vannucci m 2015 bayesian inference of multiple gaussian graphical models journal of the american statistical association 110 509 159 174 ravikumar p wainwright m j and lafferty j d 2010 high dimensional ising model selection using 1 regularized logistic regression ann statist 38 3 1287 1319 simon n friedman j hastie t and tibshirani r 2013 a sparse group lasso journal of computational and graphical statistics 22 2 231 245 vujac i i abbruzzo a and wit e 2015 a computationally fast alternative to cross validation in penalized gaussian graphical models journal of statistical computation and sim ulation 85 18 3628 3640 xu x and ghosh m 2015 bayesian variable selection and estimation for group lasso bayesian analysis 10 4 909 936 yang e allen g i liu z and ravikumar p k 2012 graphical models via generalized linear models advances in neural information processing systems 25 1367 1375 yang e allen g i liu z and ravikumar p k 2014 mixed graphical models via exponen tial families proceedings of the seventeenth international conference on artificial intelligence and statistics pages 1042 1050 yang e ravikumar p allen g i and liu z 2015 a graphical models via univariate exponential family distributions journal of machine learning research 16 3813 3847 yang s lu z shen x wonka p and ye j 2015 b fused multiple graphical lasso siam journal on optimization 25 2 916 943 yuan m 2010 high dimensional inverse covariance matrix estimation via linear programming journal of machine learning research 11 2261 2286 zhang y ouyang z and zhao h 2017 a statistical framework for data integration through graphical models with application to cancer genomics the annals of applied statistics 11 1 161 184 appendix a proof of proposition 1 the proof is similar to the proof for proposition 2 in li et al 2018 on the regularization effect of panda for a single ugm the only difference in pandam is the additional penalty p 2 brought by the second type of noise e 2 wlog we derive p 2 in the framework of multiple 20 ggms the proof can be easily combined with the proof for proposition 2 in li et al 2018 to obtain p 2 for ugms in general ee 1 e 2 lp x e 1 e 2 q l 1 nl i 1 p j 1 x l ij k 6 j x l ik l jk 2 ee 1 e 2 2 h 1 ne h i 1 p j 1 eijj h q l 1 k 6 j e l ijk h l jk 2 l x ne 1 p j 1 q l 1 k 6 j ee e l 2 ijk 1 l 2 jk ne 2 p j 1 k 6 j q l 1 ee e l 2 ijk 2 l 2 jk q l 1 l 1 v 1 2 e e l ijk 2 e v ijk 2 l jk v jk a 1 there are no cross product terms among e 1 because all e 1 noise terms are independent when e 2 is of the jgl type all e 2 terms are also independent so the cross product terms among e 2 are 0 and the boxed term above becomes ne 2 p j 1 k 6 j q l 1 ee e l 2 ijk 2 l 2 jk ne 2 p j 1 k 6 j q l 1 ve e l ijk 2 l 2 jk ne 2 2 p j 1 k 6 j q l 1 q l 1 l 2 jk 1 2 l 2 jk ne 2 2 p j 1 k 6 j q l 1 l 2 jk 1 2 when e 2 is of the jfr type the boxed terms in eqn a 1 becomes ne 2 p j 1 k 6 j q l 1 ee e l 2 ijk 2 l 2 jk q l 1 v l 1 2 e e l ijk 2 e v ijk 2 l jk v jk a 2 the covariance matrix of e l ijk 2 for l 1 q is 2 tt the diagonal element of which is 2 2 and the off diagonal elements lead the following covariance cov e 1 ijk 2 e 2 ijk 2 2 2 when q 2 cov e l ijk 2 e l 1 ijk 2 cov e q ijk 2 e 1 ijk 2 2 for l 2 when q 3 and 0 otherwise plugging these terms into eqn a 2 we have ne 2 p j 1 k 6 j 2 2 l 2 jk 4 2 1 jk 2 jk 2 ne 2 2 p j 1 k 6 j 1 jk 2 jk 2 for q 2 ne 2 2 p j 1 k 6 j 1 jk 2 jk 2 refine 2 2 2 as 2 itself is a tuning parameter ne 2 p j 1 k 6 j 2 2 q l 1 l 2 jk 2 2 q l 2 l jk l 1 jk 2 2 q jk 1 jk for q 3 ne 2 2 p j 1 k 6 j l v s l jk v jk 2 denote by s the combinatorics set q 2 among the q graphs all taken together when e 2 is of the jfr type the boxed term in eqn a 1 is 2 ne p j 1 k 6 j l v s l jk v jk 2 b proof of proposition 3 ee lp x e 1 e 2 2 n 1 p j 1 jee x jx j j p j 1 j j p j 1 j 1 2 n x jxj ee e j 1 ej 1 ee e j 2 ej 2 j p j 1 j j 1 2 n p j 1 j x jxj j p j 1 j j p j 1 k 6 j 2 h 1 ne h q l 1 v l 1 l l 1 cov e l ijk he v ijk h l jk v jk 21 l x 1 ne 1 p j 1 k 6 j q l 1 l jk 2 2 ne 2 p j 1 k 6 j q l 1 l 2 jk 1 2 for jgl 2 ne 2 p j 1 k 6 j l v s l jk v jk 2 for jfr where s denotes the combinatorics set q 2 among the q graphs 22 supplementary materials to adaptive noisy data augmentation panda for simultaneous construction multiple graph models yinan li 1 xiao liu 2 and fang liu 1 1 department of applied and computational mathematics and statistics 2 department of psychology university of notre dame notre dame in 46556 u s a 23 s 1 pandam cd algorithm for multiple ggm estimation algorithm s 1 pandam cd for joint estimation of q ggms 1 input random initial estimates l 0 j for j 1 p and l 1 q a ngd for generating e 1 eqn 2 a ngd for e 2 eqns 3 or 4 maximum iteration t noisy data sizes ne 1 and ne 2 thresholds 0 width of moving average ma window m banked parameter estimates after convergence r inner loop k in alternatively estimating j and 2 j 2 t 0 convergence 0 3 while t t and convergence 0 4 for j 2 to p 5 for k 1 k a generate e l ijk 1 for i 1 ne 1 and e l ijk 2 for i 1 ne 2 with l t 1 j plugged in the variance terms of the ngds combine x l e l 1 and e l 2 for l 1 q to obtain the augmented data as given in figure 2 c obtain the ols estimates t j 1 t j q t j from the regression model x j x 1 1 j 1 1 j x q 1 j 1 q j where x l 1 j 1 refers to the 1 st to the j 1 th columns of x l for l 1 q d if t m calculate the ma l t j m 1 t b t m 1 l b j otherwise l t j l t j for l 1 q calculate 2 l t j via eqn 14 end for 6 end for 7 calculate the overall loss function l t p j 1 q l 1 n l 2 l t j 8 apply a convergence criteria to l t let convergence 1 if the convergence is reached 9 end while 10 continue to execute the command lines 4 and 6 for another r iterations and record l b j for b t 1 t r and l 1 q calculate the degrees of freedom t j trace xj x jx j 1 x j and 2 b j sse t j n b j let l jk l t 1 jk l t r jk 11 set l jk 0 if max l jk min l jk 0 max l jk min l jk 0 for k j oth erwise set jk r 1 t r b t 1 b l jk set d r 1 t r b t 1 diag 2 b 1 2 b p calculate l l l d l l 12 output l 24 s 2 pandam scio algorithm for multiple ggm estimation algorithm s 2 pandam scio for joint estimation of q ggms 1 pre processing standardize x l for 1 1 q separately 2 input random initial estimates l 0 j for j 1 p and l 1 q a ngd to generate e 1 eqn 2 a ngd to generate e 2 eqns 18 or 19 maximum iteration t noisy data sizes ne 1 and ne 2 thresholds 0 width of moving average ma window m banked parameter estimates after convergence r 3 t 0 convergence 0 4 while t t and convergence 0 5 for j 1 p a generate e l ijk 1 for i 1 ne 1 and e l ijk 2 for i 1 ne 2 with l t 1 j plugged in the variance terms of the ngds b obtain augmented data x by combining the standardized x l and 2 ne l j 1 2 ne l j 2 from a according to figure 1 c calculate t j 2 n x jx j 1 j and obtain t j 1 t j q t j d if t m calculate the ma 1 t j m 1 t b t m 1 1 b j otherwise 1 t j 1 t j end for 6 plug in l t j for j 1 p in the loss function in eqn 20 to obtain l t 7 apply a convergence criteria to l t let convergence 1 if the convergence is reached 8 end while 9 continue to execute the command line 5 for another r iterations and record l b j for b t 1 t r and l 1 q let l jk l t 1 jk l t r jk 10 set l jk l kj 0 if max l jk min l jk 0 max l jk min l jk 0 or max l kj min l kj 0 max l kj min l kj 0 for k 6 j otherwise set jk min l jk l kj 11 output l l 1 l p 25 1 introduction 2 pandam for simultaneous construction of multiple graphs 2 1 overview on panda for single graph estimation 2 2 pandam ns for simultaneous construction of multiple ugms 2 3 additional pandam approaches for simultaneous construction of multiple ggm 2 3 1 pandam cd for multiple ggm estimation 2 3 2 pandam scio for multiple ggm estimation 2 4 panda jgl and panda jfr beyond graphical models 2 5 bayesian perspectives on pandam 3 simulation 4 case study the lung cancer microarray data 5 discussion a proof of proposition 1 b proof of proposition 3 s 1 pandam cd algorithm for multiple ggm estimation s 2 pandam scio algorithm for multiple ggm estimation