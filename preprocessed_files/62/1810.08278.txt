interpolating between optimal transport and mmd using sinkhorn divergences jean feydy thibault s journ fran ois xavier vialard dma cole normale sup rieure cmla ens paris saclay dma cole normale sup rieure ligm upem shun ichi amari alain trouv gabriel peyr brain science institute riken cmla ens paris saclay dma cole normale sup rieure abstract comparing probability distributions is a fun damental problem in data sciences simple norms and divergences such as the total vari ation and the relative entropy only compare densities in a point wise manner and fail to capture the geometric nature of the problem in sharp contrast maximum mean discrep ancies mmd and optimal transport dis tances ot are two classes of distances be tween measures that take into account the ge ometry of the underlying space and metrize the convergence in law this paper studies the sinkhorn divergences a family of geometric divergences that inter polates between mmd and ot relying on a new notion of geometric entropy we provide theoretical guarantees for these divergences positivity convexity and metrization of the convergence in law on the practical side we detail a numerical scheme that enables the large scale application of these divergences for machine learning on the gpu gradi ents of the sinkhorn loss can be computed for batches of a million samples 1 introduction countless methods in machine learning and imaging sciences rely on comparisons between probability dis tributions with applications ranging from shape matching vaillant and glaun s 2005 kaltenmark et al 2017 to classification frogner et al 2015 and generative model training goodfellow et al 2014 a common setting is that of measure fitting given a unit mass positive empirical distribution m 1 x on a feature space x a loss function l m 1 x m 1 x r and a model distribution m 1 x parameterized by a vector we strive to minimize 7 l through gradient descent numerous papers focus on the construction of suitable models 7 but which loss function l should we use if x is endowed with a ground distance d x x r taking it into account can make sense and help descent algorithm to overcome spurious local minima geometric divergences for machine learning unfortunately simple dissimilarities such as the to tal variation norm or the kullback leibler relative en tropy do not take into account the distance d on the feature space x as a result they do not metrize the convergence in law aka the weak topology of measures and are unstable with respect to deforma tions of the distributions supports we recall that if x is compact n converges weak towards denoted n if for all continuous test functions f c x n f f where f def x fd e f x for any random vector x with law the two main classes of losses l which avoid these shortcomings are optimal transport distances and maximum mean discrepancies they are continu ous with respect to the convergence in law and metrize its topology that is n l n 0 the main purpose of this paper is to study the theoreti cal properties of a new class of geometric divergences which interpolates between these two families and thus offers an extra degree of freedom through a parame ter that can be cross validated in typical learning scenarios 1 1 previous works ot distances and entropic regularization a first class of geometric distances between measures is that of optimal transportation ot costs which are computed as solutions of a linear program kan torovich 1942 see 1 below in the special case 0 enjoying many theoretical properties these costs allow us to lift a ground metric on the fea ar x iv 1 81 0 08 27 8 v 1 m at h s t 1 8 o ct 2 01 8 interpolating between optimal transport and mmd using sinkhorn divergences ture space x towards a metric on the space m 1 x of probability distributions santambrogio 2015 ot distances sometimes referred to as earth mover s dis tances rubner et al 2000 are progressively being adopted as an effective tool in a wide range of situa tions from computer graphics bonneel et al 2016 to supervised learning frogner et al 2015 unsuper vised density fitting bassetti et al 2006 and genera tive model learning montavon et al 2016 arjovsky et al 2017 salimans et al 2018 genevay et al 2018 sanjabi et al 2018 however in practice solving the linear problem required to compute these ot distances is a challenging issue many algorithms that leverage the properties of the underlying feature space x d have thus been designed to accelerate the computa tions see peyr and cuturi 2017 for an overview out of this collection of methods entropic regulariza tion has recently emerged as a computationally effi cient way of approximating ot costs for 0 we define ot def min 1 2 x 2 c d kl 1 where kl def x 2 log d d d d where c x y is some symmetric positive cost func tion we assume here that c x x 0 and where the minimization is performed over coupling measures m 1 x 2 as 1 2 denotes the two marginals of typically c x y x y p on x rd and setting 0 in 1 allows us to retrieve the earth mover p 1 or the quadratic wasserstein p 2 distances the idea of adding an entropic barrier kl to the original linear ot program can be traced back to schr dinger s problem l onard 2013 and has been used for instance in social sciences galichon and salani 2010 crucially as highlighted in cuturi 2013 the smooth problem 1 can be solved efficiently on the gpu as soon as 0 the celebrated sinkhorn algorithm detailed in section 3 allows us to compute efficiently a smooth geometric loss ot between sam pled measures mmd norms still to define geometry aware dis tances between measures a simpler approach is to in tegrate a positive definite kernel k x y on the feature space x on a euclidean feature space x rd we typically use rbf kernels such as the gaussian kernel k x y exp x y 2 2 2 or the energy distance conditionally positive kernel k x y x y the kernel loss is then defined for as lk def 1 2 2 k def 1 2 x 2 k x y d x d y 2 if k is universal micchelli et al 2006 i e if the linear space spanned by functions k x is dense in c x we know that k metrizes the convergence in law such euclidean norms introduced for shape matching in glaunes et al 2004 are often referred to as maximum mean discrepancies mmd gret ton et al 2007 they have been extensively used for generative model gans fitting in machine learn ing li et al 2015 dziugaite et al 2015 mmd norms are cheaper to compute than ot and have a smaller sample complexity i e approximation error when sampling a distribution 1 2 interpolating between ot and mmd using sinkhorn divergences unfortunately though the flat geometry that mmds induce on the space of probability measures m 1 x does not faithfully lift the ground distance on x for instance on x rd let us denote by the translation of by rd defined through f f for continuous functions f c rd wasserstein distance discrepancies defined for c x y x y p are such that ot 0 1 p in sharp contrast mmd norms rely on convolutions that weigh the frequencies of independently according to the fourier transform of the kernel func tion for instance up to a multiplicative constant of d the energy distance l is given by rd 1 cos 2 d 1 d sz kely and rizzo 2004 lemma 1 where is the fourier transform of the probability measure except for the trivial case of a dirac mass x 0 for some x 0 rd we thus always have l with a value that strongly depends on the smoothness of the refer ence measure in practice as evidenced in figure 5 this theoretical shortcoming of mmd losses is reflected by vanishing gradients or similar artifacts next to the extreme points of the measures supports sinkhorn divergences on the one hand ot losses have appealing geometric properties on the other hand cheap mmd norms scales up to large batches with a low sample complexity why not interpolate between them to get the best of both worlds following genevay et al 2018 see also ramdas et al 2017 salimans et al 2018 sanjabi et al 2018 we consider a new cost built from ot that we call a sinkhorn divergence s def ot 12 ot 1 2 ot 3 such a formula satisfies s 0 and interpolates between ot and mmd ramdas et al 2017 ot 0 0 s 1 2 2 c 4 feydy s journ vialard amari trouv peyr the entropic bias why bother with the auto correlation terms ot and ot for pos itive values of in general ot 6 0 so that minimizing ot with respect to results in a biased solution as evidenced by figure 1 the gra dient of ot drives towards a shrunk measure whose support is smaller than that of the target mea sure this is most evident as tends to infinity ot c x y d x d y a quantity that is minimized if is a dirac distribution located at the median resp the mean value of if c x y x y resp x y 2 in the literature the formula 3 has been introduced more or less empirically to fix the entropic bias present in the ot cost with a structure that mimicks that of a squared kernel norm 2 it was assumed or con jectured that s would define a positive definite loss function suitable for applications in ml this paper is all about proving that this is indeed what happens 1 3 contributions the purpose of this paper is to show that the sinkhorn divergences are convex smooth positive definite loss functions that metrize the convergence in law our main result is the theorem below that ensures that one can indeed use s as a reliable loss function for ml applications whichever value of we pick theorem 1 let x be a compact metric space with a lipschitz cost function c x y that induces for 0 a positive universal kernel k x y def exp c x y then s defines a symmetric pos itive definite smooth loss function that is convex in each of its input variables it also metrizes the con vergence in law for all probability radon measures and m 1 x 0 s 6 s 5 s 0 6 n s n 0 7 notably these results also hold for measures with bounded support on a euclidean space x rd en dowed with ground cost functions c x y x y or c x y x y 2 which induce laplacian and gaussian kernels respectively this theorem legitimizes the use of the unbiased sinkhorn divergences s instead of ot in model fitting applications indeed computing s is roughly as expensive as ot the computation of the correc tive factors being cheap as detailed in section 3 and the debiasing formula 3 allows us to guarantee that the unique minimizer of 7 s is the target distribution see figure 1 section 3 details how to implement these divergences efficiently our algo rithms scale up to millions of samples thanks to freely available gpu routines to conclude we showcase in section 4 the typical behavior of s compared with ot and standard mmd losses 2 proof of theorem 1 we now give the proof of theorem 1 our argument relies on a new bregman divergence derived from a weak continuous entropy that we call the sinkhorn entropy see section 2 2 we believe this convex entropy function to be of independent interest note that all this section is written under the assumptions of theorem 1 the proof of some intermediate results can be found in the appendix 2 1 properties of the ot loss first let us recall some standard results of regularized ot theory peyr and cuturi 2017 thanks to the fenchel rockafellar theorem we can rewrite cuturi s loss 1 as ot def max f g c x 2 f g 8 exp 1 f g c 1 where f g is the tensor sum x y x 2 7 f x g y the primal dual relationship linking an optimal transport plan solving 1 to an optimal dual pair f g that solves 8 is exp 1 f g c 9 crucially the first order optimality conditions for the dual variables are equivalent to the primal s marginal constraints 1 2 on 9 they read f t g a e and g t f a e 10 a l ot b l s figure 1 removing the entropic bias solution in red of the fitting problem min l for some shown in blue here c x y x y on the unit square x in r 2 and 1 the positions of the red dots were optimized by gradient descent starting from a normal gaussian sample interpolating between optimal transport and mmd using sinkhorn divergences where the sinkhorn mapping t m 1 x c x c x is defined through t f 7 y x 7 min x c x y f x 11 with a softmin operator of strength defined through min x x def log x exp 1 x d x 12 dual potentials the following proposition recalls some important properties of ot and the associated dual potentials its proof can be found in section b 1 proposition 1 properties of ot the optimal po tentials f g exist and are unique a e up to an additive constant i e k r f k g k is also optimal at optimality we get ot f g 13 we recall that a function f m 1 x r is said to be differentiable if there exists f c x such that for any displacement with m 1 x 2 we have f t f t f o t 14 the following proposition whose proof is detailed in section b 2 shows that the dual potentials are the gradients of ot proposition 2 ot is weak continuous and differ entiable its gradient reads ot f g 15 where f g satisfies f t g and g t f on the whole domain x and t is the sinkhorn map ping 11 let us stress that even though the solutions of the dual problem 8 are defined a e the gradient 15 is defined on the whole domain x fortunately an optimal dual pair f 0 g 0 defined a e satisfies the optimality condition 10 and can be extended in a canonical way to compute the gradient pair f g c x 2 associated to a pair of measures using f t g 0 and g t f 0 is enough 2 2 sinkhorn and haussdorf divergences having recalled some standard properties of ot let us now state a few original facts about the corrective symmetric term 1 2 ot used in 3 we still suppose that x d is a compact set endowed with a symmetric lipschitz cost function c x y for 0 the associated gibbs kernel is defined through k x y x x 7 exp c x y 16 crucially we now assume that k is a positive universal kernel on the space of signed radon measures definition 1 sinkhorn negentropy under the as sumptions above we define the sinkhorn negentropy of a probability radon measure m 1 x through f def 1 2 ot 17 the following proposition is the cornerstone of our ap proach to prove the positivity of s providing an al ternative expression of f its proof relies on a change of variables exp f in 8 that is detailed in the section b 3 of the appendix proposition 3 let x d be a compact set endowed with a symmetric lipschitz cost function c x y that induces a positive kernel k then for 0 and m 1 x one has 1 f 12 min m x log d d 1 2 2 k 18 the following proposition whose proof can be found in the section b 4 of the appendix leverages the alter native expression 18 to ensure the convexity of f proposition 4 under the same hypotheses as propo sition 3 f is a strictly convex functional onm 1 x we now define an auxiliary hausdorff divergence that can be interpreted as an ot loss with decoupled dual potentials definition 2 hausdorff divergence thanks to proposition 2 the sinkhorn negentropy f is differ entiable in the sense of 14 for any probability mea sures m 1 x and regularization strength 0 we can thus define h def 1 2 f f 0 it is the symmetric bregman divergence induced by the strictly convex functional f bregman 1967 and is therefore a positive definite quantity 2 3 proof of the theorem we are now ready to conclude first remark that the dual expression 8 of ot as a maximization of linear forms ensures that ot is convex with respect to and with respect to but not jointly convex if 0 s is thus convex with respect to both inputs and as a sum of the functions ot and f see proposition 4 convexity also implies that ot 2 ot 6 ot ot 1 ot 6 ot using 15 to get 2 ot f 1 ot f and summing the above in equalities we show that h 6 s which implies 5 feydy s journ vialard amari trouv peyr to prove 6 note that s 0 h 0 which implies that since f is a strictly convex functional finally we show that s metrizes the convergence in law 7 in the section b 5 of the appendix 3 computational scheme we have shown that sinkhorn divergences 3 are pos itive definite convex loss functions on the space of probability measures let us now detail their imple mentation on modern hardware encoding measures for the sake of simplicity we focus on discrete sampled measures on a euclidean feature space x rd our input measures and m 1 x are represented as sums of weighted dirac atoms n i 1 i xi m j 1 j yj 19 and encoded as two pairs x and y of float arrays here rn and rm are non negative vectors of shapes n and m that sum up to 1 whereas x rd n and y rd m are real valued tensors of shapes n d and m d if we follow python s convention 3 1 the sinkhorn algorithm s working with dual vectors proposition 1 is key to the modern theory of regularized optimal trans port it allows us to compute the ot cost and thus the sinkhorn divergence s thanks to 3 using dual variables that have the same memory footprint as the input measures solving 8 in our discrete setting we only need to store the sampled values of the dual po tentials f and g on the measures supports we can thus work with dual vectors f rn and g rm defined through fi f xi and gj g yj which encode an implicit transport plan from to 9 crucially the optimality condition 10 now reads i 1 n j 1 m fi lsemk 1 log k 1 gk 1 c xi yk 20 gj lsenk 1 log k 1 fk 1 c xk yj 21 where lsenk 1 vk log n k 1 exp vk 22 denotes a stabilized log sum exp reduction if f g is an optimal pair of dual vectors that satisfies equations 20 21 we deduce from 13 that ot i xi j yj n i 1 ifi m j 1 jgj 23 but how can we solve this coupled system of equations given x and y as input data the sinkhorn algorithm one simple answer by enforcing 20 and 21 alternatively updating the vec tors f and g until convergence cuturi 2013 start ing from null potentials fi 0 gj this numerical scheme is nothing but a block coordinate ascent on the dual problem 8 one step after another we are en forcing null derivatives on the dual cost with respect to the fi s and the gj s convergence the sinkhorn loop converges quickly towards its unique optimal value it enjoys a linear convergence rate peyr and cuturi 2017 that can be improved with some heuristics thibault et al 2017 when computed through the dual expression 23 ot and its gradients 26 27 are robust to small perturbations of the values of f and g monitoring convergence through the l 1 norm of the updates on f and breaking the loop as we reach a set tolerance level is thus a sensible stopping criterion in practice if is large enough say 05 on the unit square with an earth mover s cost c x y x y waiting for 10 or 20 iterations is more than enough symmetric ot problems all in all the baseline sinkhorn loop provides an efficient way of solving the discrete problem ot for generic input measures but in the specific case of the symmetric corrective terms ot and ot introduced in 3 we can do better the key here is to remark that if the dual problem 8 becomes a concave maximization problem that is symmetric with respect to its two variables f and g hence there exists a unique optimal dual pair f g f on the diagonal which is characterized in the discrete setting by the symmetric optimality condition i 1 n fi lsenk 1 log k 1 fk 1 c xi xk 24 fortunately given and x the optimal vector f that solves this equation can be computed by iterating a well conditioned fixed point update fi 12 fi lsenk 1 log k 1 fk 1 c xi xk 25 this symmetric variant of the sinkhorn algorithm can be shown to converge much faster than the standard interpolating between optimal transport and mmd using sinkhorn divergences loop applied to a pair on the diagonal and three iterations are usually enough to compute accurately the optimal dual vector 3 2 computing the sinkhorn divergence and its gradients given two pairs x and y of float arrays that encode the probability measures and 19 we can now implement the sinkhorn divergence s the cross correlation dual vectors f rn and g rm associated to the discrete problem ot can be computed using the sinkhorn iterations 20 21 the autocorrelation dual vectors p rn and q rm respectively associated to the symmetric problems ot and ot can be computed using the symmetric sinkhorn update 25 the sinkhorn loss can be computed using 3 and 23 s i xi j yj n i 1 i fi pi m j 1 j gj qj what about the gradients in this day and age we could be tempted to rely on the automatic differ entiation engines provided by modern libraries which let us differentiate the result of twenty or so sinkhorn iterations as a mere composition of elementary oper ations genevay et al 2018 but beware this loop has a lot more structure than a generic feed forward network taking advantage of it is key to a x 2 x 3 gain in performances as we now describe crucially we must remember that the sinkhorn loop is a fixed point iterative solver at convergence its so lution satisfies an equation given by the implicit func tion theorem thanks to 15 using the very definition of gradients in the space of probability measures 14 and the intermediate variables in the computation of s we get that is i xi j yj fi pi 26 and xis i xi j yj xi 27 where x r is equal to fi pi on the xi s and is defined through x log m j 1 exp log j 1 gj 1 c x yj log n i 1 exp log i 1 pi 1 c x xi graph surgery with pytorch assuming conver gence in the sinkhorn loops it is thus possible to com pute the gradients of s without having to backprop through the twenty or so iterations of the sinkhorn al gorithm we only have to differentiate the expression above with respect to x but does it mean that we should differentiate c or the log sum exp operation by hand fortunately no modern libraries such as pytorch paszke et al 2017 are flexible enough to let us hack the naive autograd algorithm and act as though the optimal dual vectors fi pi gj and qj did not depend on the input variables of s as documented in our reference code github com jeanfeydy global divergences an appropriate use of the detach method in py torch is enough to get the best of both worlds an au tomatic differentiation engine that computes our gra dients using the formula at convergence instead of the baseline backpropagation algorithm all in all as evi denced by the benchmarks provided figure 3 this trick allows us to divide by a factor 2 3 the time needed to compute a sinkhorn divergence and its gradient with respect to the xi s 3 3 scaling up to large datasets the sinkhorn iterations rely on a single non trivial operation the log sum exp reduction 22 in the ml literature this softmax operator is often understood as a row or column wise reduction that acts on n m matrices but as we strive to implement the update rules 20 21 and 25 on the gpu we can go further batch computation first if the number of sam ples n and m in both measures is small enough we can optimize the gpu usage by computing sinkhorn divergences by batches of size b in practice this can be achieved by encoding the cost function c as a 3 d tensor of size b n m made up of stacked matrices c xi yj i j while f and g become b n and b m tensors respectively thanks to the broadcasting syn tax supported by modern libraries we can then seam lessly compute in parallel loss values s k k for k in 1 b the keops library unfortunately though tensor centric methods such as the one presented above can not scale to measures sampled with large numbers n and m of dirac atoms as these numbers exceed 10 000 huge n m matrices stop fitting into gpu memories to alleviate this problem we leveraged the keops library charlier et al 2018 that pro vides online map reduce routines on the gpu with full pytorch integration performing online log sum exp reductions with a running maximum the keops primitives allow us to compute sinkhorn divergences with a linear memory footprint as evidenced by the feydy s journ vialard amari trouv peyr 102 103 104 105 106 10 4 10 3 10 2 10 1 100 101 102 out of mem out of mem number of points n t im e s ec computing an energy distance gradient between samples of size n m pytorch on cpu pytorch on gpu pytorch keops 102 103 104 105 106 10 4 10 3 10 2 10 1 100 101 102 out of mem out of mem number of points n t im e s ec computing log j exp xi yj with samples of size n m pytorch on cpu pytorch on gpu pytorch keops figure 2 the keops library allows us to break the memory bottleneck using cuda routines that sum kernel values without storing them in memory we can outperform baseline tensorized implementations of the energy distance experiments performed on x r 3 with a cheap laptop s gpu gtx 960 m 102 103 104 105 106 10 4 10 3 10 2 10 1 100 101 102 number of points n t im e s ec on a cheap laptop s gpu gtx 960 m sinkhorn divergence naive sinkhorn divergence energy distance 102 103 104 105 106 10 4 10 3 10 2 10 1 100 101 102 number of points n t im e s ec on a high end gpu p 100 sinkhorn divergence naive sinkhorn divergence energy distance figure 3 sinkhorn divergences scale up to finely sampled distributions as a rule of thumb sinkhorn divergences take 20 50 times as long to compute as a baseline mmd even though the explicit gradient formula 26 27 lets us win a factor 2 3 compared with a naive autograd implementation for the sake of this benchmark we ran the sinkhorn and symmetric sinkhorn loops with fixed numbers of iterations 20 and 3 respectively which is more than enough for measures on the unit hyper cube if 05 here we work in r 3 benchmarks of figures 2 3 computing the gradient of a sinkhorn loss with 100 000 samples per measure is then a matter of seconds 4 numerical illustration in the previous sections we have provided theoretical guarantees on top of a comprehensive implementation guide for the family of sinkhorn divergences s let us now describe the geometry induced by these new loss functions on the space of probability measures gradient flows to compare mmd losses lk with cuturi s original cost ot and the de biased sinkhorn divergence s a simple yet relevant experiment is to let a model distribution t flow with time t along the wasserstein 2 gradient flow of a loss functional 7 l that drives it towards a target distribu tion santambrogio 2015 this corresponds to the non parametric version of the data fitting problem evoked in section 1 where the parameter is nothing but the vector of positions x that encodes the sup port of a measure 1 n n i 1 xi understood as a model free idealization of fitting problems in ma chine learning this experiment allows us to grasp the typical behavior of the loss function as we discover the deformations of the support that it favors 2 k s 1 0 s 0 1 o t 1 0 o t 0 0 t 0 t 25 t 50 t 1 00 t 5 00 figure 4 gradient flows for 1 d measures sampled with n m 5000 points we display t in red and in blue through kernel density estimations on the segment 0 1 the legend on the left indicates the function that is minimized with respect to here k x y x y c x y x y and 10 on the second and fourth lines 01 on the third in 1 d the optimal transport problem can be solved using a sort algorithm for the sake of comparison we can thus display the true dynamics of the earth mover s distance in the fifth line interpolating between optimal transport and mmd using sinkhorn divergences 2 k s 1 0 s 0 1 o t 1 0 t 0 t 25 t 50 t 1 00 t 5 00 figure 5 gradient flows for 2 d measures the setting is the same as in figure 4 but the measures t in red and in blue are now directly displayed as point clouds of n m 500 points the evolution of the support of t is thus made apparent and we display as a green vector field the descent direction xil in figures 4 and 5 1 m m j 1 yj is a fixed target measure while 1 n n i 1 xi t is parameterized by a time varying point cloud x t xi t ni 1 r d n in dimension d 1 or 2 starting from a set initial condition at time t 0 we simply integrate the ode x t n x l 1 n n i 1 x xi t with a euler scheme and display the evolution of t up to time t 5 interpretation in both figures the fourth line highlights the entropic bias that is present in the ot loss t is driven towards a minimizer that is a shrunk version of as showed in theorem 1 the de biased loss s does not suffer from this issue just like mmd norms it can be used as a reliable positive definite divergence going further the dynamics induced by the sinkhorn divergence interpolates between that of an mmd and optimal transport 0 as shown in 4 here c x y x y and we can indeed remark that the second and third lines bridge the gap between the flow of the energy distance l in the first line and that of the earth mover s cost ot 0 which moves particles according to an optimal transport plan please note that in both experiments the gradient of the energy distance with respect to the xi s vanishes at the extreme points of s support crucially for small enough values of s recovers the translation aware geometry of ot and we observe a clean convergence of t to as no sample lags behind 5 conclusion recently introduced in the ml literature the sinkhorn divergences were designed to interpolate between mmd and ot we have now shown that they also come with a bunch of desirable properties positivity convexity metrization of the convergence in law and scalability to large datasets to the best of our knowledge it is the first time that a loss derived from the theory of entropic optimal transport is shown to stand on such a firm ground as the foundations of this theory are progressively be ing settled we now hope that researchers will be free to focus on one of the major open problems in the field the interaction of geometric loss functions with concrete machine learning models feydy s journ vialard amari trouv peyr bibliography arjovsky m chintala s and bottou l 2017 wasserstein gan arxiv preprint arxiv 1701 07875 bassetti f bodini a and regazzini e 2006 on minimum kantorovich distance estimators statis tics probability letters 76 12 1298 1302 bonneel n peyr g and cuturi m 2016 wasserstein barycentric coordinates histogram re gression using optimal transport acm transac tions on graphics 35 4 bregman l m 1967 the relaxation method of finding the common point of convex sets and its ap plication to the solution of problems in convex pro gramming ussr computational mathematics and mathematical physics 7 3 200 217 charlier b feydy j and glaun s j 2018 kernel operations on the gpu with autodiff without mem ory overflows http www kernel operations io accessed 2018 10 04 cuturi m 2013 sinkhorn distances lightspeed computation of optimal transport in adv in neural information processing systems pages 2292 2300 dziugaite g k roy d m and ghahramani z 2015 training generative neural networks via maximum mean discrepancy optimization in pro ceedings of the thirty first conference on uncer tainty in artificial intelligence pages 258 267 franklin j and lorenz j 1989 on the scaling of multidimensional matrices linear algebra and its applications 114 717 735 frogner c zhang c mobahi h araya m and poggio t a 2015 learning with a wasserstein loss in advances in neural information processing systems pages 2053 2061 galichon a and salani b 2010 matching with trade offs revealed preferences over competing characteristics preprint hal 00473173 genevay a peyr g and cuturi m 2018 learn ing generative models with sinkhorn divergences in international conference on artificial intelligence and statistics pages 1608 1617 glaunes j trouv a and younes l 2004 diffeo morphic matching of distributions a new approach for unlabelled point sets and sub manifolds match ing in computer vision and pattern recognition 2004 cvpr 2004 proceedings of the 2004 ieee computer society conference on volume 2 pages ii ii ieee goodfellow i pouget abadie j mirza m xu b warde farley d ozair s courville a and ben gio y 2014 generative adversarial nets in advances in neural information processing systems pages 2672 2680 gretton a borgwardt k m rasch m sch lkopf b and smola a j 2007 a kernel method for the two sample problem in advances in neural in formation processing systems pages 513 520 kaltenmark i charlier b and charon n 2017 a general framework for curve and surface comparison and registration with oriented varifolds in com puter vision and pattern recognition cvpr kantorovich l 1942 on the transfer of masses in russian doklady akademii nauk 37 2 227 229 l onard c 2013 a survey of the schr dinger prob lem and some of its connections with optimal trans port arxiv preprint arxiv 1308 0215 li y swersky k and zemel r 2015 generative moment matching networks in proceedings of the 32 nd international conference on machine learn ing icml 15 pages 1718 1727 micchelli c a xu y and zhang h 2006 univer sal kernels journal of machine learning research 7 dec 2651 2667 montavon g m ller k r and cuturi m 2016 wasserstein training of restricted boltzmann ma chines in advances in neural information process ing systems pages 3718 3726 paszke a gross s chintala s chanan g yang e devito z lin z desmaison a antiga l and lerer a 2017 automatic differentiation in pytorch peyr g and cuturi m 2017 computational op timal transport arxiv 1610 06519 ramdas a trillos n g and cuturi m 2017 on wasserstein two sample testing and related families of nonparametric tests entropy 19 2 rubner y tomasi c and guibas l j 2000 the earth mover s distance as a metric for image re trieval international journal of computer vision 40 2 99 121 salimans t zhang h radford a and metaxas d 2018 improving gans using optimal transport arxiv preprint arxiv 1803 05573 sanjabi m ba j razaviyayn m and lee j d 2018 on the convergence and robustness of train ing gans with regularized optimal transport arxiv preprint arxiv 1802 08249 santambrogio f 2015 optimal transport for ap plied mathematicians volume 87 of progress in non linear differential equations and their applications springer interpolating between optimal transport and mmd using sinkhorn divergences sz kely g j and rizzo m l 2004 hierarchical clustering via joint between within distances ex tending ward s minimum variance method j clas sification 22 151 183 thibault a chizat l dossal c and papadakis n 2017 overrelaxed sinkhorn knopp algorithm for regularized optimal transport arxiv preprint arxiv 1711 01851 vaillant m and glaun s j 2005 surface match ing via currents in biennial international confer ence on information processing in medical imaging pages 381 392 springer feydy s journ vialard amari trouv peyr a standard results before detailing our proofs we first recall some well known results regarding the kullback leibler diver gence and the softmin operator defined in 12 a 1 the kullback leibler divergence first properties for any pair of radon measures m x on the compact metric set x d the kullback leibler divergence is defined through kl def log d d 1 1 if otherwise it can be rewritten as an f divergence associated to x r 0 7 x log x x 1 r 0 with 0 log 0 0 as kl d d if otherwise 28 since is a strictly convex function with a unique global minimum at 1 0 we thus get that kl 0 with equality iff dual formulation the convex conjugate of is defined for u r by u def sup x 0 xu x eu 1 and we have x u xu 29 for all x u r 0 r with equality if x 0 and u log x this allows us to rewrite the kullback leibler divergence as the solution of a dual concave problem proposition 5 dual formulation of kl under the assumptions above kl sup h fb x r h eh 1 30 where fb x r is the space of bounded measurable functions from x to r proof lower bound on the sup if is not absolutely continuous with respect to there exists a borel set a such that a 0 and a 0 consequently for h 1 a h eh 1 a otherwise if we define h log d d and see that h eh 1 kl if hn log d d 11 n 6 d d 6 n fb x r the mono tone and dominated convergence theorems then allow us to show that hn ehn 1 n kl upper bound on the sup if h fb x r and combining 28 and 29 allow us to show that kl h eh 1 d d h hd d 0 the optimal value of h eh 1 is bounded above and below by kl we get 30 since h eh 1 is a convex function of taking the supremum over test functions h fb x r defines a convex divergence proposition 6 the kl divergence is a jointly con vex function onm x m x going further the density of continuous functions in the space of bounded measurable functions allows us to restrict the optimization domain proposition 7 under the same assumptions kl sup h c x r h eh 1 31 where c x r is the space of bounded continuous functions on the compact set x proof let h i i hi 1 ai be a simple borel function on x and let us choose some error margin 0 since and are radon measures for any i in the finite set of indices i there exists a compact set ki and an open set vi such that ki ai vi and i i max vi ki vi ki 6 moreover for any i i there exists a continuous func tion i such that 1 ki 6 i 6 1 vi the continuous function g i i hi i is then such that g h 6 h and eg eh 6 eh so that h eh 1 g eg 1 6 h eh as we let our simple function approach any measurable function in fb x r choosing arbitrarily small we then get 31 through 30 interpolating between optimal transport and mmd using sinkhorn divergences we can then show that the kullback leibler diver gence is weakly lower semi continuous proposition 8 if n and n are weakly converging sequences inm x we get lim inf n kl n n kl proof according to 31 the kl divergence is defined as a pointwise supremum of weakly continuous appli cations h 7 h eh 1 for h c x r it is thus lower semi continuous for the convergence in law a 2 softmin operator proposition 9 the softmin interpolates between a minimum and a sum under the assumptions of the definition 12 we get that min x x 0 min x supp x if and are two continuous functions in c x such that 6 min x x 6 min x x 32 finally if k r is constant with respect to x we have that min x k x k min x x 33 proposition 10 the softmin operator is continu ous let n be a sequence of probability measures converging weakly towards and n be a sequence of continuous functions that converges uniformly to wards then for 0 the softmin of the values of n on n converges towards the softmin of the values of on i e n n min x n n x min x x b proofs b 1 dual potentials we first state some important properties of solutions f g to the dual problem 8 please note that these results hold under the assumption that x d is a com pact metric space endowed with a ground cost func tion c x x r that is lipschitz with respect to both of its input variables the existence of an optimal pair f g of potentials that reaches the maximal value of the dual objec tive is proved using the contractance of the sinkhorn map t defined in 11 for the hilbert projective met ric franklin and lorenz 1989 while optimal potentials are only defined a e as highlighted in proposition 1 they are extended to the whole domain x by imposing similarly to the clas sical theory of ot santambrogio 2015 remark 1 13 that they satisfy f t g and g t f 34 with t defined in 11 we thus assume in the follow ing that this condition holds the following proposi tions studies the uniqueness and the smoothness with respect to the spacial position and with respect to the input measures of these functions f g defined on the whole space proposition 11 uniqueness of the dual potentials up to an additive constant let f 0 g 0 and f 1 g 1 be two optimal pairs of dual potentials for a problem ot that satisfy 34 then there exists a con stant k r such that f 0 f 1 k and g 0 g 1 k 35 proof for t 0 1 let us define ft f 0 t f 1 f 0 gt g 0 t g 1 g 0 and t ft gt exp 1 ft gt c 1 the value of the dual objective between the two optimal pairs as is a concave function bounded above by 0 1 ot it is constant with respect to t hence for all t in 0 1 0 t 1 e ft gt c f 1 f 0 g 1 g 0 2 this is only possible if a e in x y f 1 x f 0 x g 1 y g 0 y 2 0 i e there exists a constant k r such that f 1 x f 0 x k a e g 1 y g 0 y k a e as we extend the potentials through 34 the softmin operator commutes with the addition of k 33 and lets our result hold on the whole feature space proposition 12 lipschitz property the optimal potentials f g of the dual problem 8 are both lipschitz functions on the feature space x d feydy s journ vialard amari trouv peyr proof according to 34 f is a softmin combination of lipschitz functions of the variable x using the algebraic properties of the softmin operator detailed in 32 33 one can thus show that f is a lipschitz function on the feature space the same argument holds for g proposition 13 the dual potentials vary contin uously with the input measures let n and n be weakly converging sequences of measures in m 1 x given some arbitrary anchor point xo x let us denote by fn gn the unique sequence of opti mal potentials for ot n n such that fn xo 0 then fn and gn converge uniformly towards the unique pair of optimal potentials f g for ot such that f xo 0 up to the value at the anchor point xo we thus have that n n fn f gn g proof for all n in n the potentials fn and gn are lipschitz functions on the compact bounded set x as fn xo is set to zero we can bound fn on x by times the diameter of x combining this with 34 we can then produce a uniform bound on both fn and gn there exists a constant m r such that n n x x m 6 fn x gn x 6 m being equicontinuous and uniformly bounded on the compact set x the sequence fn gn n satisfies the hy potheses of the ascoli arzela theorem there exists a subsequence fnk gnk k that converges uniformly to wards a pair f g of continuous functions k tend to infinity we see that f xo 0 and using the conti nuity of the softmin operator proposition 10 on the optimality equations 10 we show that f g is an optimal pair for ot now according to proposition 11 such a limit pair of optimal potentials f g is unique fn gn n is thus a compact sequence with a single possible adherence value it has to converge uniformly towards f g b 2 proof of proposition 2 the proof is mainly inspired from santambrogio 2015 proposition 7 17 let us consider and times t in a neighborhood of 0 as in the statement above we define t t t t and the variation ratio t given by t def ot t t ot t using the very definition of ot and the continuity property of proposition 13 we now provide lower and upper bounds on t as t goes to 0 weak continuity as written in 13 ot can be computed through a straightforward continu ous expression that does not depend on the value of the optimal dual potentials f g at the anchor point xo ot f g combining this equation with proposition 13 that guarantees the uniform convergence of potentials for weakly converging sequences of probability measures allows us to conclude lower bound first let us remark that f g is a suboptimal pair of dual potentials for ot t t hence ot t t t f t g t t exp 1 f g c 1 and thus since ot f g exp 1 f g c 1 one has t f g exp 1 f g c o 1 f g o 1 since g and f satisfy the optimality equations 10 upper bound conversely let us denote by gt ft the optimal pair of potentials for ot t t satisfying gt xo 0 for some arbitrary anchor point xo x as ft gt are suboptimal potentials for ot we get that ot ft gt exp 1 ft gt c 1 and thus since ot t t t ft t gt t t exp 1 ft gt c 1 t 6 ft gt t t exp 1 ft gt c o 1 6 ft gt o 1 conclusion now let us remark that as t goes to 0 t and t thanks to proposition 13 we thus know that ft and gt converge uniformly towards f and g combining the lower and upper bound we get t t 0 f g f g since and both have an overall mass that sums up to zero interpolating between optimal transport and mmd using sinkhorn divergences b 3 proof of proposition 3 the definition of ot is that ot max f g c x 2 f g e f g c 1 reduction of the problem thanks to the sym metry of this concave problem with respect to the variables f and g we know that there exists a pair f g f of optimal potentials on the diagonal and ot max f c x 2 f e f f c 1 thanks to the density of continuous functions in the set of simple measurable functions just as in the proof of proposition 7 we show that this maximiza tion can be done in the full set of measurable functions fb x r ot max f fb x r 2 f e f f c 1 max f fb x r 2 f exp f k exp f where denotes the smoothing convolution operator defined through k x x k x y d y for k c x x and m x optimizing on measures through a change of variables exp f i e f log d d keeping in mind that is a probability measure we then get that ot max m x 2 log d d k 1 1 2 ot min m x log d d 1 2 k 12 where we optimize on positive measures m x such that and expansion of the problem as k x y exp c x y is positive for all x and y in x we can remove the constraint from the optimiza tion problem 1 2 ot min m x log d d 1 2 k 12 indeed restricting a positive measure to the support of lowers the right hand term k without having any influence on the density of with respect to finally let us remark that the constraint is already encoded in the log d d operator which blows up to infinity if has no density with respect to all in all we thus have f 12 ot min m x log d d 1 2 k 12 which is the desired result existence of the optimal measure in the ex pression above the existence of an optimal is given as a consequence of the well known fact from ot the ory that optimal dual potentials f and g exist so that the dual ot problem 8 is a max and not a mere supremum nevertheless since this property of f is key to the metrization of the convergence in law by sinkhorn divergences let us endow it with a direct alternate proof proposition 14 for any m 1 x assuming that x is compact there exists a unique m x such that f log d d 1 2 k 12 moreover proof notice that for m 1 x m x e def log d d 1 2 k kl 1 1 2 2 k 1 2 since c is bounded on the compact set x x and is a probability measure we can already say that 1 f 6 e 12 1 2 e c 1 2 upper bound on the mass of since x x is compact and k x y 0 there exists 0 such that k x y for all x and y in x we thus get 2 k 1 2 feydy s journ vialard amari trouv peyr and show that e 1 12 2 k 1 2 1 1 1 1 2 as we build a minimizing sequence n for f we can thus assume that n 1 is uniformly bounded by some constant m 0 weak continuity crucially the banach alaoglu theorem asserts that m x 1 6 m is weakly compact we can thus extract a weakly con verging subsequence nk from the minimizing sequence n using proposition 8 and the fact that k is continuous on x x we show that 7 e is a weakly lower semi continuous function realizes the minimum of e and we get our existence result uniqueness we assumed that our kernel k is pos itive universal the squared norm 7 2 k is thus a strictly convex functional and using proposition 6 we can show that 7 e is strictly convex this ensures that is uniquely defined b 4 proof of proposition 4 let us take a pair of measures 0 6 1 inm 1 x and t 0 1 according to proposition 14 there exists a pair of measures 0 1 inm x such that 1 t f 0 tf 1 1 t e 0 0 te 1 1 e 1 t 0 t 1 1 t 0 t 1 f 1 t 0 t 1 which is enough to conclude to show the strict in equality let us remark that 1 t e 0 0 te 1 1 e 1 t 0 t 1 1 t 0 t 1 would imply that 0 1 since 7 2 k is strictly convex as 7 kl is strictly convex on the set of measures that are absolutely continuous with respect to we would then have 0 1 and a con tradiction with our first hypothesis b 5 proof of the metrization of the convergence in law the regularized ot cost is weakly continuous and the uniform convergence for dual potentials ensures that h and s are both continuous too paired with 6 this property guarantees the convergence towards 0 of the hausdorff and sinkhorn divergences as soon as n conversely let us assume that s n 0 resp h n any weak limit n of a sub sequence nk k is equal to since our diver gence is weakly continuous we have s n 0 resp h n and positive definiteness holds through 6 in the meantime since x is compact the set of prob ability radon measures m 1 x is sequentially com pact for the weak topology n is thus a compact sequence with a unique adherence value it converges towards