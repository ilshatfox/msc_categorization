ar x iv 2 00 6 16 54 5 v 1 cs c r 3 0 ju n 20 20 transactions on information forensics and security 1 adversarial deep ensemble evasion attacks and defenses for malware detection deqiang li and qianmu li abstract malware remains a big threat to cyber security calling for machine learning based malware detection while promising such detectors are known to be vulnerable to evasion attacks ensemble learning typically facilitates countermeasures while attackers can leverage this technique to improve attack effectiveness as well this motivates us to investigate which kind of robustness the ensemble defense or effectiveness the ensemble attack can achieve particularly when they combat with each other we thus propose a new attack approach named mixture of attacks by rendering attackers capable of multiple generative methods and multiple manipulation sets to perturb a malware example without ruining its malicious functionality this naturally leads to a new instantiation of adversarial training which is further geared to enhancing the ensemble of deep neural networks we evaluate defenses using android malware detectors against 26 different attacks upon two practical datasets experimental results show that the new adversarial training significantly enhances the robustness of deep neural networks against a wide range of attacks ensemble methods promote the robustness when base classifiers are robust enough and yet ensemble attacks can evade the enhanced malware detectors effectively even notably downgrading the virustotal service index terms adversarial machine learning deep neural networks ensemble adversarial malware detection i introduction m alware gains due attention from communities while still being a big threat to cyber security for example symantec reported that 246 002 762 new malware variants emerged in 2018 1 kaspersky detected 5 321 142 malicious android packages in 2018 2 worse yet there is an increas ing number of malware variants that attempted to undermine anti virus tools and indeed evaded many malware detection systems 3 in order to relieve the severe situation communities have restored to machine learning techniques 4 5 while obtain ing impressive performance the learning based models can be evaded by adversarial examples see for example 6 7 8 9 interestingly a few manipulations are enough to perturb a malware example into an adversarial one by which the perturbed malware example is detected as benign rather than malicious 10 11 this facilitates the research in the context of adversarial malware detection amd catering robust malware detectors against attacks researchers have proposed enhancing the robustness of classifiers using ensemble methods such as adversarial training d li and q li are with the school of computer science and engineering nanjing university of science and technology nanjing 210094 china e mail lideqiang qianmu njust edu cn more information can be found at http ieeexplore ieee org of ensemble 12 or ensemble adversarial training 13 for the counterpart attackers can leverage ensemble methods to promotes attack effectiveness as well such as by evading several classifiers 14 15 or by waging multiple attacks simultaneously 16 17 this naturally raises the question how is the effectiveness of ensemble attack or the robustness of ensemble defense when they combat with each other our contributions in this paper we make the following contributions first we propose a new attack approach named mixture of attacks which enables attackers to leverage multiple generative methods and multiple manipulation sets to produce adversarial malware examples to realize it we adapt max attack 16 into the amd context and further propose iter ating max attack in a greedy manner so as to boost attack effectiveness in addition we adapt salt and pepper noises attack and pointwise attack 18 both of which are gradient free aiming to wage effective attacks when gradients of loss function suffer from certain issues 19 second we instantiate the adversarial training 7 using a mixture of attacks and propose applying a manipulation set with the cardinality as large as possible further we utilize this instantiation to harden the ensemble of deep neural networks along with a theoretical analysis third we validate the robustness of malware detectors against 26 evasion attacks which are categorized into five approaches gradient based gradient free obfuscation mix ture of attacks and transfer attack specifically there are 10 gradient based attacks projected gradient descent pgd 1 20 pgd 2 20 pgd 21 grosse 22 bit gradient ascent 7 bit coordinate ascent 7 pgd adam 23 gra dient descent with kernel density estimation gdkde 6 fast gradient sign method fgsm 24 and jacobian based saliency map attack 25 4 gradient free attacks 2 mimicry attacks salt and pepper noises and pointwise 5 obfuscation attacks java reflection string encryption variable renaming junk code injection and all four techniques above combined 3 mixtures of attacks max pgds iterative max pgds and iterative max pgds gdkde where pgds means the mixture contains pgd 1 pgd 2 pgd and pgd adam 4 transfer attacks we implement 6 android malware detectors including basic dnn with no efforts to harden the model 3 hardened dnns incorporating adversarial training with attack rfgsm dubbed at rfgsm 7 pgd adam dubbed at adam 23 and max pgds and 2 adversarial deep en sembles i e the hardened ensemble of deep neural networks incorporating adversarial training we conduct systematical 1556 6013 2020 ieee personal use is permitted but republication redistribution requires ieee permission http arxiv org abs 2006 16545 v 1 http ieeexplore ieee org transactions on information forensics and security 2 experiments on drebin 26 and androzoo 27 datasets centering at the aforementioned question our findings are that the hardened models incorporating max pgds can detect more malware examples than the basic dnn at rfgsm and at adam at the cost of lowering detection accuracy on benign examples and thus f 1 score in the ab sence of attacks adversarial deep ensemble cannot serve as a remedy and even reduces the detection accuracy on both malicious and benign examples the hardened models incorporating max pgds signif icantly outperform the basic dnn at rfgsm and at adam against evasion attacks however all models can not defeat two types of attacks attack mimicking benign examples e g gdkde and mimicry and mixture of attacks e g iterative max pgds the ensemble promotes robustness against attacks when base classifiers are robust enough the usefulness of ensemble however is un deterministic when base models are vulnerable to attacks interestingly the experimental results confirm our theoretical analysis virustotal service 28 notably suffers from the iterative max pgds gdkde attack this is important because it shows adversarial evasion attacks may be a practical threat to cyber security when attributing the important features for adversarial deep ensemble we observe that sub effective features e g com google ads adactivity are emphasized thus leading to its robustness against attacks while trading off accuracy in non adversarial settings this implies the necessity of robust feature extraction last but not the least we make our codes publicly available at https github com deqangss adv dnn ens malware paper outline the rest of the paper is organized as follows section ii reviews the related work section iii presents the ensemble of deep neural networks evasion attacks including two attacks adapted into amd first time and adversarial training section iv describes the mixture of attacks and adversarial deep ensemble section v evaluates attacks and defenses section vi discusses certain issues we concern section vii concludes the paper and shows future research ii related work we review ensemble methods from the attacker s and the defender s perspectives respectively a ensemble attacks there are two ensemble based approaches improving the effectiveness of adversarial examples i by attacking multiple classifiers and ii by using multiple attack methods for type i liu et al 14 suggest improving the transfer ability of adversarial examples by attacking an ensemble of deep learning models rather than a single one this is because adversarial examples that can fool multiple models tend to have strong transferability 14 15 dong et al 29 further investigate three manners of organizing the base models and demonstrate that the ensemble of averaging logits outperforms the others for boosting the attack effectiveness for type ii araujo et al 17 show the difference between 2 and norm based adversarial examples geometrically trame r et al 16 further propose attacking a classifier using multiple types of manipulations e g constrained by 1 or norm the empirical results show that the max attack lets the attacker evade the victim effectively in the context of amd one classical means is to perturb the discriminative features derived by random forest 30 recently al dujaili et al 7 demonstrate the difference between four attacks in a sense that deep neural network based malware detectors enhanced by training with one attack cannot resist the other attacks we apply the aforementioned two approaches together with accommodating i the discrete input domain and ii the constraint of retaining malicious functionality we exploit the max strategy by permitting attackers to have multiple generative methods and multiple manipulation sets which is different from the study 16 that focuses on the types of manipulations b ensemble defenses biggio et al 31 32 propose defending against evasion attacks using bagging and random subspace techniques which can produce evenly distributed weights one limitation is that the base classifier is restricted to the linear algorithm for deep learning models abbasi et al 33 propose specialists 1 ensemble and xu et al 34 combine multiple feature squeezing techniques so as to detect adversarial examples ef fectively both defenses however are defeated by a following study 35 which demonstrates that the ensemble of weak defenses cannot mitigate evasion attacks trame r et al 13 further introduce ensemble adversarial training which learns one robust model by augmenting training data with adversarial examples transferred from multiple models grefenstette et al 12 empirically demonstrate that adversarial training of ensemble achieves better robustness than the ensemble of ad versarially trained multiple classifiers pang et al 36 suggest enhancing the ensemble by diversifying base classifiers in the context of malware detection smutz and stavrou 37 propose leveraging ensemble classifier to detect adversarial examples that are treated as outliers stokes et al 38 show the resilience of ensemble to evasion attacks moreover stacking ensemble is utilized to hinder adversarial malware examples 39 all these defenses neglect the adversarial training that is an effective means to resist evasion attacks as a comparison we aim to circumvent a wide range of evasion attacks in the amd context we also enhance the ensemble model by adversarial training i e adversarial training of ensemble 12 while incorporating a mixture of attacks along with a theoretical analysis of ensemble iii preliminaries we present the ensemble of deep neural networks the evasion attacks and the framework of adversarial training https github com deqangss adv dnn ens malware transactions on information forensics and security 3 a ensemble of deep neural networks given a malware example z in the file space z i e z z mapped as a d dimensional feature vector i e feature representation x x by feature extraction method z x an ensemble classifier f x y takes x as input and returns the predicted label 0 or 1 i e y 0 1 where 0 means benign file and 1 means malicious we consider the deep ensemble that is a linear combination of l deep neural networks dnns fi l i 1 let w denote a weight vector w w 1 w 2 wl satisfying w w with w w 1 w 1 wi 0 for i 1 l for waging effective attacks we apply logit ensemble 29 which is f x softmax l i 1 wizi x s t w w 1 where zi denotes the logits of fi further a stable version would be implemented to transform zi to zi max zi so as to avoid numerical overflow where max returns the maximum element in a vector we obtain the predicted label by f x argmaxj f x where argmax returns the index of maximum element in a vector the parameters of ensemble collectively denoted by are optimized by minimizing a cost over the joint feature vector and label space namely min e x y x y l f x y 2 where l r y y is a loss function e g cross entropy 40 for simplifying notations we use f rather than f to denote a parameterized ensemble of deep neural networks b evasion attacks 1 definition we focus on evasion attacks in the context of amd which generally confines attackers by perturbing malware examples in the test phase rather than training phase and retaining malicious functionality 7 11 41 the terminology of adversarial malware example and adversarial example are used interchangeably referring to the perturbed example that can evade the victim successfully two versions of evasion attacks are used corresponding to the file space z and the feature space x respectively in the file space the attacker perturbs the malware example z into z in order to mislead the classifier f 42 formally given malware example label pair z y and manipulation set mz the attacker intents to achieve z z 3 s t mz f z 6 y f z y where refers to the operation of applying manipulations and manipulations in mz retain the functionality of z e g junk codes injection instead of perturbing executable malware examples directly an attacker could derive manipulations in the feature space x guiding the generation of adversarial examples in the file space 43 44 let mx denote representation manipulations for x z which is derived by mx z z z z for mz 4 formally given a tuple x y manipulations in the feature space mx and the inverse feature extraction 1 the attacker intents to achieve z 1 x 5 s t x x x x mx f x 6 y f x y notably feature space evasion attack necessitates following two assumptions assumption 1 below says the inverse feature extraction 1 is solvable and otherwise the corresponding adversarial example only exists in theory assumption 1 solvability assumption 43 given z z and feature extraction the attacker can obtain z 1 x when the representation x is perturbed from x for x z eq 5 shows 1 works upon the manipulation set mx which is derived by eq 4 it is a brute force solution by calculating all perturbed files in advance incurring efficiency issues researchers thus suggest alternatives 43 44 45 46 one example is predetermining mx empirically based on the file space manipulation set mz this would rely on the below assumption that the perturbed representation is bounded by a box constraint 44 let u and u respectively denote the lower and upper boundaries in the feature space i e elements in u are not greater than corresponding elements in u assumption 2 manipulation assumption 44 x perturbed from x satisfies x u u for x x with regard to assumption 2 i e mx u x u x we cannot modify the file z by following x x partic ularly one reason is features may be interdependent i e modifying a representation value triggers other correlated ones changed resulting in dependent manipulations 43 46 however assumption 2 misses to capture this dependence in this work we implement 1 x subject to retaining malicious functionality which may break the intuition of 1 x x x x our preliminary experiments show that the attack effectiveness barely suffers from this side effect for most of the used features are independent 1 see section v a 3 2 threat models we consider a threat model specified by attacker s capability of perturbing examples and attacker s knowledge of the target system 6 43 45 41 47 attacker capability as aforementioned earlier an attacker is capable of perturbing malware examples in the test phase with malicious functionality preservation in addition the attacker is permitted to generate an example that is miss classified with high cost e g maximizing classifier s loss 43 7 attacker knowledge there are three attack scenarios white box vs black box vs grey box in the white box scenario the attacker knows everything of the targeted system including defense mechanisms in the black box scenario the attacker does not know internals of the victim except for the predicted labels the grey box scenario resides in between and we 1 this observation is application special to accommodate various types of feature extraction we need to bridge the gap between the feature space attack and the file space attack avoiding the use of empirical mx we leave this problem in further work transactions on information forensics and security 4 permit the attacker to know the dataset the feature extraction method and the learning algorithm but not defense methods and learned parameters of the targeted malware detector 3 attack strategies in the literature we consider a broad range of methods to specify evasion attacks which are cate gorized into four approaches gradient based attacks gradient free attacks obfuscations and transfer attacks gradient based attacks this approach permits attackers to wage white box attacks deriving adversarial examples using gradients of classifier s loss function for example p p 1 2 norm based projected gradient descent pgd is an effective means to maximizing the classifier s loss appropriately 21 note that the perturbation x is continuous during optimization process when following the direction of gradients we map the perturbed representation by looking for the feasible nearest neighbor 20 in addition several related methods are adapted or proposed in the context of amd in cluding grosse 22 fast gradient sign method fgsm 24 7 jacobian based saliency map attack jsma 48 25 bit gradient ascent bga 7 bit coordinate ascent bca 7 and pgd adam 23 while noting that grosse jsma bga and bca permit the feature addition only moreover another method named gradient descent with kernel density estimation gdkde 6 lifts perturbed examples into the populated region of benign ones gradient free attacks this approach permits attackers to wage grey box attacks the mimicry is usually applied known as perturbing a malware example to mimic the benign ones 43 45 we further adapt two methods both are originally proposed in the context of image processing to modify malware examples salt and pepper noises and pointwise 18 algorithm 1 shows the former one in the feature space which perturbs representations using salt and pepper noises repetitively algorithm 2 describes the pointwise that given an adversarial example reduces its degree of manipulations while keeping its adversarial property obfuscations this approach enables attackers to wage file space evasion attacks with knowing nothing about the victim model i e zero query black box attack typically software sample can be manipulated using certain techniques e g variable renaming indeed researchers have reported the obfuscated malware examples can bypass detection 49 45 this inspires us to investigate this attack approach transfer attacks this approach suggests attackers waging transfer attack when knowing certain knowledge of f such as a portion of features 43 44 the procedure mainly has following steps perform reverse engineering to obtain a surrogate model which resembles the targeted model f perturb malware examples against the surrogate model target the classifier f using the perturbed examples c minmax adversarial training adversarial training lets classifiers know certain attacks proactively by augmenting the training data with adversarial examples 50 24 51 52 in particular it has been proposed to consider adversarial training with the optimal algorithm 1 salt and pepper noises attack in the feature space input the feature representation label pair x y the classifier f the manipulation set mx a scalar 0 max 1 the number of scalars ns the number of repetitions nrept output the perturbed point x 1 initialize x x 2 repeat 3 produce evenly spaced scalars 1 ns over the range of 0 max 4 for j 1 to ns do 5 generate salt and pepper noises x with the maximum degree of manipulation j d and x mx 6 set x x x 7 if f x 6 y then 8 set x x and max j 9 break 10 end 11 until nrept is reached 12 return x algorithm 2 pointwise attack in the feature space input the feature representation label pair x y the classifier f the adversarial representation x x 1 x d the perturbed representation set xm output the perturbed point x 1 repeat 2 set x x 3 shuffle the list of indices d 1 d to another list d 1 d randomly 4 for i 0 to d do 5 if x i x i then 6 continue 7 modify x locally by setting x i x i 8 if x xm f x y then 9 reset x i x i 10 end 11 until x x 12 return x attack which in a sense corresponds to the worst case scenario and therefore leads to classifiers that are robust against the non optimal ones namely the minmax adversarial training 7 min e x y x y l f x y max x xm l f x y 6 where denotes the parameters of a dnn f and xm u u denotes a predetermined set of perturbed representations it is worthy reminding that the inner maximization is intractable because of the non convex dnn resulting in local maxima transactions on information forensics and security 5 iv methodology we elaborate the mixture of attacks and adversarial deep ensemble of which the adversarial deep ensemble relies on a mixture of attacks a mixture of attacks we propose mixture of attacks an ensemble based approach permitting attackers to perturb a malware example via multiple attack methods and multiple manipulation sets though this setting gives attackers more freedom it is practical in the context of amd for instance researchers suggest perturbing android packages by adding instructions into the file of androidmanifest xml 11 while addition operation can be applied to the file of classes dex as well 25 and moreover certain objects e g string can be hidden 45 1 overall idea with regard to assumption 1 and 2 which are empirically handled in section v we consider feature space evasion attacks let h denote the space of generative method and x denote the space of manipulation set such that definition 1 generative method a generative method h h takes as input the representation x with a constraint mx x and returns a perturbed representation x h mx x we characterize the strength of an attack upon h and mx via some scoring measurements wherein the classifier loss l f h mx x y is leveraged for a given x y tuple the higher loss value indicates a stronger attack the mixture of attacks has the same objective as the aforementioned approaches e g gradient based aiming to maximize a score when manipulating malware examples in contrast to attack strategies that design a generative method upon a manipulation set a mixture of attacks attempts to construct n generative methods hi ni 1 and m manipulation sets mj x mj 1 and then combine them to wage an attack where n 1 and m 1 2 two attack strategies in order to realize the mixture of attacks we apply two straightforward strategies as follows max strategy given a representation label pair x y n generative methods hi ni 1 and m manipulation sets mj x mj 1 the attacker attempts to choose a generative method say h and a manipulation set say m x both of which joint to produce the optimal attack namely that h m x argmax h mx l f h mx x y 7 s t h hi ni 1 mx m j x mj 1 iterative max strategy the attacker performs the max strategy iteratively with each round adding perturbations on the resulting example came from the last iteration in the first iteration perturbations are applied on x algorithm 3 unifies and summarizes the two strategies we calculate perturbed examples using two loops corresponding to generative methods hi ni 1 and manipulation sets m j x mj 1 line 3 line 7 for line 8 and line 9 we select the optimal combination of attack method and manipulation set to perturb an example herein parts of the algorithm from line 3 to line 9 belongs to the max attack the iterative case continues to algorithm 3 iterative max attack in the feature space input the feature representation label pair x y n generation methods hi ni 1 m manipulation sets mj x mj 1 the score measurement l the number of iterations n a small constant 0 output the perturbed point x 1 initialize x 0 x 2 for k 1 to n do 3 for i 1 to n do 4 for j 1 to m do 5 calculate hi mj x x k 1 6 end 7 end 8 select h and m x via eq 7 9 set x h m x x 10 set x k x 11 if l x k y l x k 1 y then 12 return x 13 end 14 return x proceed with taking as input the resulting point calculated by max attack as shown in line 10 the procedure halts until the predetermined number of iteration is reached line 2 or the convergence criterion is met line 11 the change of score is less than a small scalar such as 10 9 the iterative case improves the attack effectiveness greedily resulting in more directions being explored in the future one may consider more strategies to improve the max attack b adversarial deep ensemble we enhance the robustness of deep ensemble by adversarial training technique incorporating the max attack in the end we instantiate the minmax training see eq 6 as min e x y x y l f x y max h mx l f h mx x y 8 s t h h mx x because of the information barrier between the defender and the attacker we shall use the notations h and x to replace attacker s empirical set hi ni 1 and m j x mj 1 respectively in contrast to the former study 7 there are three differences h contains multiple approximate maximizers x contains a huge number of manipulation sets f is a deep ensemble rather than a single dnn first we may apply all possibly approximate maximizers to enhance the malware detectors for the aim of defending against a wide range of attacks owing to the efficiency issue we choose and use the gradient based maximizers second let m x and m x denote two manipulation sets the theorem below suggests producing adversarial examples upon the union of all manipulation sets namely mx x transactions on information forensics and security 6 theorem 1 upon two manipulation sets m x and m x given a representation label pair x y the generative method h perturbs x by maximizing the classifier loss l which leads to l f h m x x y l f h m x x y when m x m x theorem 1 can be easily proved given m x m x we derive x m x x x m x and xm x x x m x and further get xm xm due to h m x x xm we reason h m x x xm and obtain l f h m x x y l f h m x x y recalling the assumption 2 i e box constraint we apply mx x rigorously the theorem works when the generative method is the exact solution to maximizing the loss l which is an intricate problem see discussion in section iii c nevertheless our preliminary experiments show that the projected gradient descent based maximizers follow this theorem well third we design a robust ensemble as below 1 base classifiers the generalization error of ensemble drops significantly when base classifiers are effective and in dependent 53 we consider diversifying base classifiers using the approach of data sample manipulation 54 specifically we have each base classifier perceive adversarial examples that are produced by an approximate maximizer this means we regularize the base classifiers using adversarial training but incorporating distinct attacks formally the objective denoted by jens is jens j l i 1 ji 9 where j denotes the eq 8 is a factor to balance the two items and ji is the regularization for ith base classifier ji min i e x y x yl fi i h i mx x y 10 here i i 1 l 1 denotes the parameters of base dnn fi mx x and h i is an approximate maximizer in order to accommodate the unperturbed examples the lth base classifier is learned from the pristine training set 2 combination we optimize weights w by the eq 8 when given the dnns f 1 f 2 fl with frozen parameters the optimal weights can be solved by lagrange multiplier 55 56 in order to make the optimization compatible to that of dnn we leverage projected gradient descent wi 1 projw wi wj f v 11 where 0 is the learning rate and projw projects v into the space w we use the algorithm of duchi et al 56 to conduct the projection 3 analysis we present a theoretical analysis of the deep ensemble against evasion attacks specifically we quantify the robustness by comparing to an ideal dnn using a relaxation of averaging mean square error over logits formally given an adversarial example set x m xm an ideal dnn f and a learned dnn f the error is defined as errorf x m ex x m z x i z x i 2 where z and z denote the logits of dnn f and f respectively let z x z x z x denote the offset between f and f without confusion we drop x for z x and x m for errorf x m leading to the compactness z and errorf instead of the logits error one may consider others e g error upon softmax without loss of generalization we herein aim to accommodate the logit ensemble but the result obtained below can be extended to other ensembles we additionally make a hypothesis of f s base classifiers being non negatively correlated i e e zi e zj 0 for i j 1 l and i 6 j in a sense that all dnns are learned from the same dataset and to solve similar tasks which is validated in section vi let e z 2 denote the smallest error achieved by one of base dnns we present the following theorem theorem 2 given the deep ensemble f with e zi e zj 0 for i j 1 l and i 6 j the error of the ensemble satisfies error f e z 2 l proof we derive error f e z 2 e l i 1 wi zi 2 l i 1 w 2 ie zi 2 l i 1 l j 6 i wiwje zi e zj 12 the optimal weights for the problem min w w l i 1 w 2 ie zi 2 is wi l i 1 1 e zi 2 1 1 e zi 2 i 1 l 13 substituting eq 13 into eq 12 we obtain error f l i 1 w 2 ie zi 2 1 l i 1 1 e zi 2 e z 2 l this leads the theorem follows from theorem 2 we draw insight 1 the error of ensemble error f could be smaller than the best base dnn thus showing more robust than any individual classifier that is enclosed into the ensemble the error of ensemble could be arbitrarily large when base classifiers cannot resist the evasion attacks v experiments and evaluation a experimental setup in this section we describe data pre processing classifiers i e defenses training and manipulations in file and feature spaces transactions on information forensics and security 7 1 data pre processing we validate the effectiveness of attacks and defenses using android malware detectors on drebin 26 and androzoo 27 datasets the drebin dataset 26 contains 5 615 malicious android packages and sha 256 values of 123 453 benign examples based on the given sha 256 values we downloaded 42 333 benign apks from the apk markets including 29 252 applications from googleplay store 7 552 applications from appchina and 5 529 from other resources such as anzhi androzoo 27 is an apk repository to obtain the recent files we downloaded 134 976 apks that have the attached date from july 1 st to december 31 st in 2017 we sent all apks to the virustotal service which is an ensemble of over 70 anti virus scanners e g kaspersky mcafee fireeye comodo etc based on the feedback we obtain 15 467 malware examples and 91 295 benign examples an apk is labeled as malicious if there are at least five anti virus scanners say it is malicious and is labeled as benign if no scanners detect it we split both datasets respectively into three disjoint sets for training 60 validation 20 and testing 20 feature extraction apk is a zip file which contains an droidmanifest xml classes dex and others e g res the androidmanifest xml describes an apk s information such as the package name and permission announcement the functionality is built into classes dex which is understandable by java virtual machine jvm following prior adversarial learning studies 26 45 22 25 we use the drebin features which consist of 8 subsets of features including 4 subsets of features extracted from androidmanifest xml denoted by s 1 s 2 s 3 s 4 respectively and 4 subsets of features extracted from the disassembled dex code denoted by s 5 s 6 s 7 s 8 respectively more specifi cally i s 1 contains the features that are related to the access of an apk to the hardware of a smartphone e g camera touchscreen or gps module ii s 2 contains the features that are related to the permissions requested by the apk in question iii s 3 contains the features that are related to the application components e g activities service receivers etc iv s 4 contains the features that are related to the apk s communications with the operating system v s 5 contains the features that are related to the critical system api calls which cannot run without appropriate permissions or the root privilege vi s 6 contains the features that correspond to the used permissions vii s 7 contains the features that are related to the api calls that can access sensitive data or resources on a smartphone viii s 8 contains the features that are related to ip addresses hostnames and urls found in the disassembled code in order to extract features of the applications we utilize the androgurad 3 3 5 which is a static apk analysis toolkit 57 note that 141 apks in drebin dataset cannot be analyzed moreover a feature selection is conducted to remove those low frequency features for the sake of computational efficiency 45 as a result we keep 10 000 features at top frequencies the apk is mapped into the feature space as a binary feature vector where 1 0 corresponds to a feature represent the presence absence in an apk 2 training classifiers we train six classifiers i the basic dnn with no effort to enhance the model dubbed basic dnn ii enhanced dnn incorporating adversarial training with the inner maximizer solved by iterative fgsm using randomized rounding dubbed at rfgsm 7 iii enhanced dnn incorporating adversarial training with the inner maximizer solved by adam optimizer dubbed at adam 23 iv enhanced dnn incorporating adversarial training with the inner maximizer solved by a mixture of attacks dubbed at ma v adversarial deep ensemble with the inner maximizer solved by the mixture of attacks see eq 8 dubbed ade ma vi the ade ma with diversity promoted see eq 9 dubbed dade ma hyper parameter settings we use dnns with two fully connected hidden layers each layer having neurons 160 and the relu activation function for outer minimization see eq 8 all classifiers are optimized by using adam with epochs 150 mini batch size 128 and learning rate 0 001 for the inner maximization the iterative fgsm is imple mented as norm based pgd attack with step size 0 01 and iterations 100 the adam based maximizer is set up with the step size 0 02 iterations 100 and random starting points 23 the mixture of attacks is realized as the max attack which has four maximizers 1 norm based pgd attack with step size 1 0 and iterations 50 2 norm based pgd attack with step size 1 0 and iterations 100 norm based pgd attack with aforementioned settings and adam based pgd attack with aforementioned settings notably dade ma has an extra hyper parameter we experimentally justify this hyper parameter and choose 1 on drebin dataset and 0 1 on androzoo dataset all attacks perturb the feature representations upon a manipulation set mx will be elaborated later model selection we learn the classifiers using drebin and androzoo training datasets respectively a selected model is the one that obtains the best accuracy on the validation set the accuracy is the percentage of examples being classified correctly for adversarial training an additional term is considered which is the accuracy of correctly classifying adversarial examples produced by the corresponding inner maximizers the selected model is used for evaluation 3 specifying manipulations we specify manipulations applied to android applications and estimate the perturbations for drebin 26 feature representations accordingly manipulation set mz in the file space given an apk we consider both incremental and decremental manipulations for incremental manipulations the attacker can insert some manifest features e g request extra permissions and hard ware state additional activities services intent filter etc however some objects are hard to insert such as content provider because the absence of uniform resource identifier uri will corrupt an apk with respect to the dex file junk codes e g null opcode debugging information dead functions or classes can be injected without destroying the apk example the similar means can be performed for the string e g ip address injection as well transactions on information forensics and security 8 when the attacker uses decremental manipulations the apk s information in the xml files can be changed e g package name however it is impossible to remove activity entirely because an activity may represent a class implemented in the classes dex code nevertheless we can rename an ac tivity and change its relevant information e g activity label while noting that the related components in the dex should be modified accordingly the other components e g service provider and receiver also can be modified in the similar fashion the method names and class names that are defined by developers could be replaced by random strings too note that the corresponding statement instantiation reference and announcements should be changed accordingly moreover user specified strings can be obfuscated using encryption and the cipher text will be decrypted at running time further the attacker can hide public and static system apis using java reflection and encryption together this is shown by the example in list 1 which retrieves the device id and sends the content outside the phone via sms all of the modifications mentioned above only obfuscate an apk without changing its functionalities telephonymanager telecom default string str telecom getdeviceid string encrypt str eaqmvmzdguv vxdavv 9 t plain text sendtextmessage string mtd name decryptstring converttostring encrypt str smsmanager smgr smsmanager getdefault method send sms null send sms smgr getclass getmethod mtd name string class string class string class pendingintent class pendingintent class send sms invoke smgr 50 1234567 null str null null listing 1 java code snippet to hide the api method sendtextmessage one challenge is that the attacker needs to perform fine grained manipulations on compiled files automatically at scale while preserving the functionalities of malware samples this is important because a small mistake in a malware example can render the file un executable the preservation of malicious functionalities may be estimated by using a dynamic malware analysis tool e g sandbox manipulation set mx in the feature space the aforemen tioned manipulations modify static android features such as api calls we observe that two kinds of perturbations can be applied to the feature representations as follows flipping 0 to 1 the attacker can increase the repre sentation values of appropriate objects such as compo nents e g activity system apis and ip address flipping 1 to 0 the attacker can flip 1 to 0 by removing or hiding objects e g activity name public or static apis table i summarizes the operations in the drebin feature space we observe that the operations are not applicable to s 6 because this subset of features rely upon s 2 and s 5 meaning that modifications on s 2 or s 5 may cause changes table i overview of manipulations in the feature space where x indicates that the operation of flipping 0 to 1 or flipping 1 to 0 can cannot be performed on features in the corresponding subset feature sets of features 0 1 1 0 manifest s 1 hardware 17 x s 2 requested permissions 247 x s 3 application components 8 619 x x s 4 intents 866 x dexcode s 5 restricted api calls 118 x x s 6 used permission 20 s 7 suspicious api calls 19 x x s 8 network addresses 94 x x of representation on s 6 in addition we highlight that the manipulation set is larger than two recent studies 25 11 that only consider flipping 0 to 1 in the feature space b evaluating the effectiveness of attacks and defenses we evaluate the attacks and defenses with centering at the earlier aforementioned question that is disintegrated as four sub questions in the following rq 1 how is the accuracy of adversarial deep ensemble for detecting malware examples in the absence of attacks rq 2 how is the robustness of adversarial deep ensemble against a broad range of attacks and how is the usefulness of ensemble against attacks rq 3 how is the accuracy of anti virus scanners under the mixture of attacks rq 4 why the enhanced classifiers can cannot defend against certain attacks metrics the effectiveness of classifiers is measured by five standard metrics false positive rate fnr false negative rate fnr accuracy acc balanced accuracy bacc 58 and f 1 score 59 the balanced accuracy and f 1 score are considered because of the imbalanced dataset rq 1 how is the accuracy of adversarial deep ensemble for detecting malware examples in the absence of attacks for answering rq 1 we evaluate the above mentioned six classifiers i e basic dnn at rfgsm at adam at ma ade ma and dade ma on drebin and androzoo datasets respectively table ii summarizes the results we observe that when compared with the basic dnn the adversarial training based defenses achieve lower fnrs at most a 2 13 decrease on drebin dataset and 1 42 on androzoo but higher fpr at most a 4 64 increase on drebin dataset and 3 84 on androzoo for these defenses at ma achieves the lowest fnr 1 59 on drebin dataset and 1 32 on androzoo and ade ma encounters the highest fpr 4 96 on drebin dataset and 4 32 on androzoo this can be explained as follows by injecting adversarial malware examples into the training set the learning process makes the model search for malware examples in a bigger space resulting in the drop in fnr and increase in fpr at ma ade ma and dade ma attain the similar fnr and fpr in the absence of attacks due to the fact that the same generative methods are leveraged transactions on information forensics and security 9 table ii effectiveness of the classifiers when there are no adversarial attacks dataset classifier effectivenss fnr fpr acc bacc f 1 drebin basic dnn 3 72 0 32 99 28 97 98 96 93 at rfgsm 7 2 74 2 45 97 51 97 40 90 23 at adam 23 3 27 1 45 98 34 97 64 93 22 at ma 1 59 3 66 96 58 97 37 87 18 ade ma 1 59 4 96 95 44 96 72 83 61 dade ma 1 95 3 69 96 52 97 18 86 94 andro zoo basic dnn 2 74 0 48 99 18 98 39 97 26 at rfgsm 7 2 05 0 66 99 13 98 65 97 11 at adam 23 1 61 0 96 98 94 98 72 96 51 at ma 1 32 2 13 97 99 98 27 93 60 ade ma 1 45 4 32 96 11 97 11 88 28 dade ma 1 57 3 65 96 66 97 39 89 77 moreover these three defenses increase the balanced accuracy to their achieved accuracy as more malware examples are detected interestingly both ensemble based defenses tend to have higher fnr and fpr than at ma the underlying reason might be that adversarial deep ensemble focuses on more perturbed examples that are outliers in summary we draw insight 2 in the absence of attacks the hardened models using mixture of attacks can detect more malware examples than the basic dnn and the other hardened models because of their smaller fnr at the price of small side effect in the fpr classification accuracy and balanced accuracy but no table side effect in f 1 score adversarial ensemble exacerbates this situation further lowering the effectiveness a little in terms of all measurements rq 2 how is the robustness of adversarial deep ensemble against a broad range of attacks and how is the usefulness of ensemble against attacks for answering rq 2 we randomly select 800 malware examples from the test set to wage evasion attacks attempting to fool the aforementioned six classifiers namely basic dnn at rfgsm at adam at ma ade ma and dade ma for gradient based methods in the settings of grosse 22 bga 7 bca 7 jsma 25 and 1 norm based pgd dubbed pgd 1 we perturb one feature per time with the maximum iterations 100 for gdkde 6 pgd adam 23 and norm based pgd attack dubbed pgd we iterate the algorithm with the maximum iterations 1 000 and step size 0 01 the 2 norm based pgd attack dubbed pgd 2 is set with the maximum iterations 1 000 and step size 1 0 for gradient free attacks we wage salt and pepper noises attack dubbed salt pepper with nrept 10 max 1 and ns 1 000 moreover let mimicry nben denote a mimicry attack in which we use nben benign examples to guide the perturbation of a malware example leading to nben perturbed examples then we select the one from these nben perturbed example that causes the miss classification with the smallest perturbations pointwise takes the resultant examples of mimicry nben as input dubbed pointwise nben the obfuscations are implemented via the avpass which is a tool to obfuscate android applications 49 we apply five attacks java reflection string encryption variable renaming junk code injection and the four techniques above combined in order to wage mixture of attacks we leverage four pgd attacks pgd adam pgd 1 pgd 2 and pgd thus denoted max attack as max pgds the iterative version is denoted as i max pgds with iteration n 5 and 10 9 furthermore by jointing the pgds and gdkde we wage i max pgds gdkde attack when performing transfer attacks for the targeted model we treat the other five classifiers as surrogate models individually this means that given a malware example we perturb it upon the other five models respectively all the perturbed examples will be sent to query the targeted model table iii reports the accuracy of classifiers against attacks on drebin and androzoo datasets from the upper half of table iii we make the following observations first three defenses at ma ade ma and dade ma significantly enhance the robustness of dnns achieving the accuracy of 90 13 under 19 attacks 88 25 under 18 attacks and 89 75 under 19 attacks respectively these achievements considerably outperform the basic dnn at rfgsm and at adam except for a lower accuracy smaller than 8 than at adam under the gdkde attack and a small lower 1 99 than at rfgsm under an obfuscation attack second ade ma and dade ma improve the robustness against gradient free attacks obfuscation attacks and transfer attacks when came to the gradient based attacks and mixture of attacks however both defense models are not useful and even hinder the robustness in some cases for instance compared to the at ma neither of the two ensembles mitigate the i max pgds effectively a 26 88 decrease for ade ma and 12 38 decrease for dade ma third all defenses suffer from the gdkde 78 13 pgd 84 5 mimicry 30 83 pointwise 81 75 and the three mixtures of attacks 79 63 for gdkde and mimicry the reason may be that these gener ative methods produce adversarial representations that have similar data distribution as benign examples leading that no learning based classifiers can detect these attacks effectively pointwise further promotes the attack effectiveness when using mimicry 30 as the initial attack for pgd the reason may be that the corresponding maximizer does not suffice to obtain adversarial examples in the training phase this further leads to the effectiveness of max pgds from the lower half of table iii we additionally make the following observations fourth the effectiveness of gradient based attacks is not comparative to the gradient free attacks e g mimicry which counters the results in drebin dataset this may be that the feature representations of malicious examples are closer to benign ones in the androzoo dataset than that in drebin fifth though at ma achieves higher accuracy of detecting most of gradient based attacks its robustness is lower than at adam under the pgd 2 attack and also lower than at rfgsm under the pgd attack this can be explained that at ma induces a loss over the four attacks which in turn leads to lower robustness than hardened models that focus on an attack solely transactions on information forensics and security 10 table iii effectiveness of the six classifiers including basic dnn adversarial training at rfgsm at adam at mixture of attacks ma adversarial deep ensemble ade ma and diversified ade ma dade ma against adversarial evasion attacks dataset attack type attack name accuracy basic dnn at rfgsm at adam at ma ade ma dade ma drebin no attack 96 63 97 25 96 63 98 38 98 38 98 25 gradient based attacks grosse 22 0 00 58 50 63 63 92 00 88 25 89 88 bga 7 0 00 97 25 96 63 98 38 98 25 98 25 bca 7 0 00 60 75 63 38 92 00 89 63 93 13 jsma 25 0 00 65 25 63 63 92 00 89 25 91 38 fgsm 7 0 00 97 25 96 63 98 38 98 38 98 25 gdkde 6 0 00 66 75 78 13 70 13 76 13 71 13 pgd adam 23 0 38 93 50 89 25 96 63 94 75 96 63 pgd 1 20 0 00 41 50 49 00 90 13 85 25 89 75 pgd 2 20 0 63 96 00 89 38 96 00 94 50 96 50 pgd 20 0 00 48 75 74 13 72 38 81 75 84 50 gradient free attacks salt pepper 78 25 96 25 94 00 95 00 97 63 95 75 mimicry 1 53 88 90 63 87 00 95 75 98 13 94 75 mimicry 30 10 63 62 13 60 75 80 38 83 00 77 75 pointwise 30 9 50 59 38 59 00 79 25 81 75 76 50 obfuscation java reflection 96 63 97 25 96 63 98 38 98 38 98 25 string encryption 96 42 96 93 96 55 98 21 98 34 98 08 variable renaming 96 84 97 47 96 58 98 35 98 35 98 23 junk code injection 95 70 99 26 98 81 99 11 99 70 99 56 all techniques combined 90 66 99 60 99 40 98 21 99 40 97 61 mixture of attacks ma max pgds 0 00 30 13 45 00 71 13 71 00 79 63 i max pgds 0 00 22 75 15 00 65 13 38 25 52 75 i max pgds gdkde 0 00 4 50 14 88 63 13 36 13 51 00 transfer attacks gdkde 36 75 88 83 87 60 92 25 95 30 92 58 pgd 1 17 03 96 08 92 68 97 35 98 20 97 05 pgd 34 35 96 65 95 25 96 75 97 33 97 18 i max pgds gdkde 23 10 95 53 94 38 94 23 97 33 96 93 androzoo no attack 97 75 98 13 98 63 98 75 98 25 98 13 gradient based attacks grosse 22 22 63 39 00 26 00 92 25 89 25 89 38 bga 7 23 00 98 13 98 63 98 75 98 13 98 13 bca 7 22 63 39 00 26 00 92 13 89 63 91 63 jsma 25 41 25 47 63 43 50 92 50 89 38 91 38 fgsm 7 37 75 98 13 98 63 98 75 98 25 98 13 gdkde 6 1 13 10 38 2 63 30 63 54 50 50 88 pgd adam 23 50 25 85 13 55 13 93 50 95 75 97 25 pgd 1 20 22 63 37 88 26 00 89 88 88 38 88 50 pgd 2 20 51 13 74 00 82 25 76 50 95 63 96 38 pgd 20 22 63 97 75 78 25 52 38 92 13 88 13 gradient free attacks salt pepper 10 00 70 38 87 88 69 75 81 13 97 00 mimicry 1 0 13 21 75 13 25 58 13 69 38 71 00 mimicry 30 0 00 2 75 0 75 22 88 48 88 46 25 pointwise 30 0 00 1 75 0 38 19 88 46 75 40 63 obfuscation java reflection 97 98 97 98 98 85 99 14 96 54 96 25 string encryption 95 15 95 59 95 59 96 92 94 71 94 71 variable renaming 96 82 97 27 97 50 97 73 96 82 96 59 junk code injection 31 43 69 52 68 57 86 67 98 10 99 05 all techniques combined 13 79 34 48 37 93 48 28 100 0 96 55 mixture of attacks max pgds 22 63 37 25 26 00 42 25 83 88 82 75 i max pgds 22 63 36 00 25 75 29 75 59 63 72 75 i max pgds gdkde 0 63 3 13 0 25 19 13 36 50 30 13 transfer attacks gdkde 6 95 48 63 42 13 77 35 74 70 76 73 pgd 1 20 63 81 20 70 03 93 53 96 95 96 88 pgd 48 20 96 15 91 70 97 53 98 83 98 80 i max pgds gdkde 3 45 89 08 75 55 85 55 93 93 97 15 transactions on information forensics and security 11 1 5 20 40 60 80100 80 85 90 95 100 ac cu ra cy n d re bi n grosse 1 5 20 40 60 80100 bga 1 5 20 40 60 80100 bca 1 5 20 40 60 80100 jsma 0 1 fgsm basic dnn at rfgsm at adam at ma ade ma dade ma 10 50 200400600800100 0 70 80 90 100 ac cu ra cy n d re bi n gdkde 10 50 200400600800100 0 pgd adam 1 5 20 40 60 80 100 pgd 1 10 50 200400600800100 0 pgd 2 10 50 200400600800100 0 pgd 1 5 20 40 60 80100 80 85 90 95 100 ac cu ra cy o n an dr oz oo grosse 1 5 20 40 60 80100 bga 1 5 20 40 60 80100 bca 1 5 20 40 60 80100 jsma 0 1 fgsm 10 50 200400600800100 0 70 80 90 100 ac cu ra cy o n an dr oz oo gdkde 10 50 200400600800100 0 pgd adam 1 5 20 40 60 80 100 iterations pgd 1 10 50 200400600800100 0 pgd 2 10 50 200400600800100 0 pgd figure 1 accuracy of classifiers against gradient based attacks with different iterations on drebin and androzoo datasets figure 1 depicts the accuracy of classifiers against gradient based attacks with different iterations with more details we further make the observations as follows sixth once the iterations exceed a certain extent gdkde can evade all hardened models effectively moreover the hardened models incorporating adversarial training with max pgds cannot thwart the pgd attack as effectively as the pgd 1 attack seventh ade ma tends to achieve better accuracy than the at ma when attacks are launched with small iterations while this situation exchanges when iterations are increased to a large extent it is worth noting that at ma attains the best accuracy on the unperturbed malware examples in the androzoo dataset resulting in the following phenomenon that along with the iteration increased at ma obtains higher accuracy than ade me at the start of curves while lower accuracy in the middle and back to higher accuracy in the end to some extent this confirms our theoretical analysis of ensemble that promotes the robustness when base classifiers are robust enough eighth under most of the attacks such as groose bca jsma and pgd 1 the defense of dade ma serves as a remedy for ade ma against attacks at large iterations about 60 this explains why dade ma circumvents more attacks than ade ma see table iii in summary we draw insights insight 3 the hardened models incorporating mixture of attacks can defend against a broad range of attacks effectively but remaining vulnerable to mimicry attacks and mixtures of attacks ensemble promotes the robustness against an attack when base classifiers are robust enough rq 3 how is the accuracy of anti virus scanners under the mixture of attacks for answering rq 3 we wage transfer attacks to target the virustotal the surrogate model is dade ma and the generative method is i max pgds gdkde we perturb the randomly selected 800 malware examples from drebin test set apktool is leveraged to perform reverse engineering 60 we finally obtain 800 perturbed malware examples along with their unperturbed versions which are together queried virustotal service figure 2 exhibits the experimental results of attacking virus total from the box plot of figure 2 a we observe that malware examples are predicted as malicious confidently except that about 1 of them 8 files in 800 examples being detected by below 24 scanners while adversarial attacks notably affect the prediction by noting that most of the perturbed examples are detected by lower than 25 scanners nonetheless no transactions on information forensics and security 12 no attack attack 0 20 40 60 nu m be r o f s ca nn er s no attack attack a ka sp er sk y m ca fe e av ira co m od o sy m an te c es et n o d 32 f se cu re m ic ro so ft 0 20 40 60 80 100 ac cu ra cy b figure 2 waging transfer attack against virustotal service a box plot of the number of anti virus scanners that predict a queried example as malicious when applied 800 pristine mal ware examples i e no attack and their 800 perturbed ones i e attack we perturb the malware examples using i max pgds gdkde attack against dade ma b accuracy of 8 scanners when applied the un perturbed examples attacks can evade virustotal thoroughly figure 2 b showcases 8 famous scanners we observe that the attack barely affects the kaspersky eset nod 32 and f secure mcafee co modo and symantec however present vulnerability to these adversarial examples in summary we draw insight 4 the transfer version of mixture of attacks can downgrade the virustotal service and evade certain anti virus scanners effectively rq 4 why the enhanced classifiers can cannot defend against certain attacks for answering rq 4 we statistically attribute important features for the defender and the attacker respectively here the important feature is the one that has a large influence on the classification accuracy to this purpose we present a case study on the defense dade ma and the attack i max pgds gdkde on the drebin dataset for the attacker we intuitively investigate the features perturbed by the adversary with high frequencies for the defender an intuitive solution is deficient due to the intricate structure of deep ensemble therefore we introduce an alternative using feature selection technique via a surrogate model the procedure proceeds as follows i train a surrogate model on the training set to mimic dade ma ii rank the features based on feature importance extracted from the surrogate model iii mask feature representations based on the ranking result by setting non important features value as 0 iv send the masked feature representations to dade ma and calculate the change of accuracy v repeat step iii iv and terminate it until a predetermined number of important features is reached or the accuracy is lower than a threshold vi refine the importance upon the retained features using permutation importance 61 we use random forest to learn the surrogate model aiming to obtain feature importance coarsely in step iii v we leverage binary search to speed up filtering out trivial features table iv demonstrates the top 20 important features for the classifier dade ma and the attack i max pgds gdkde respectively we make the following observations first 18 features 90 benefit the detection of benign examples which may be induced by the imbalanced dataset second 8 important features 40 of the classifier are manipulated by i max pgds gdkde frequently in particular the feature getnetworkinfo which promotes malware detection third features of the classifier that rank at top are neglected by this attack two reasons could account for this the attack is failed to search for some of these features both malicious and benign applications use these features frequently such as android permission internet for accessing internet service finally com google ads adactivity is at the 5 th place though counter intuitive it may be that adversarial deep ensemble enforces the model to focus on this neutral feature the above observations collaboratively explain why dade ma obtains an accuracy of 51 against i max pgds gdkde attack and sacrifices detection accuracy in the absence of attacks insight 5 the hardened model is prone to use sub effective features so as to gain robustness against attacks but a little trading off accuracy in absence of attacks vi discussion functionality estimation we emulate malware behaviors by cuckoodroid 62 that is an automated static and dynamical analysis toolkit for apks due to the efficiency issue we randomly select 10 apks from 800 perturbed examples that are sent to virustotal along with their original versions to conduct the estimation we observe that two malware exam ples and their perturbed versions cannot run on the emulator and thus exclude them from the testing for other perturbed applications 3 of them execute successfully on the emulator 2 apps can be deployed into android runtime but failed to run and the remained 3 apps cannot be installed this shows more research is needed to solve the problem of retaining malicious functionality small vs large degree of manipulations in this study we let attackers suffice iterations to maximize the classifier s loss when perturbing malware examples as long as the malicious functionality is preserved in term of 1 norm some attacks perturb malware examples slightly for example the jsma attack against the basic dnn with the average perturbations 4 48 some others perturb malware examples to a large extent for example the pgd attack against the basic dnn with the average perturbations 2 772 87 in addition the i max pgds gdkde attack perturbs malware examples with the average perturbations 2 600 02 53 23 and 45 16 when target ing the basic dnn at adam and dade ma respectively validating the hypothesis of theoretical analysis we em pirically justify the hypothesis see section iv b 3 that base classifiers of ensemble are non negatively correlated under adversarial example attacks in this end we directly let per turbed examples pass through a deep ensemble and mea sure the pearson correlation coefficient between any two base models upon the logit belonging to the label 1 the ideal classifier is treated as a constant and thus neglected by the applied measurement we perturb the 800 malware examples selected from drebin datasets using mimicry 30 and i max pgds gdkde attacks against ade ma there transactions on information forensics and security 13 table iv top 20 important features for the defense of dade ma vs i max pgds gdkde attack we further report whether the feature facilitates the classification accuracy for benign or malicious examples and whether the feature is frequently manipulated by flipping 1 to 0 or flipping 0 to 1 defender attacker android widget videoview start com airpush android pushservicestart 61159 android widget mediaplayer start httpserver httpserverservice android permission access coarse location android telephony telephonymanager getline 1 numb android permission internet android telephony telephonymanager listen com google ads adactivity android location locationmanager removeupdates android permission access fine location android location locationmanager requestlocationupdates android location locationmanager requestlocationupdates android location locationmanager getlastknownlocation android location locationmanager removeupdates android net connectivitymanager getnetworkinfo android net connectivitymanager getactivenetworkinfo android permission access fine location getsystemservice android permission read contacts android permission access network state android app activitymanager getrunningtasks android location locationmanager getbestprovider android permission access coarse location android permission read contacts android telephony telephonymanager getdeviceid android net connectivitymanager getnetworkinfo android permission mount unmount filesystems getpackageinfo android telephony telephonymanager getcelllocation android widget videoview setvideouri getpackageinfo android permission wake lock android net connectivitymanager getactivenetworkinfo android permission send sms android net wifi wifimanager iswifienabled android widget videoview setvideopath sendtextmessage printstacktrace android permission write settings are 5 base models in ade ma each attack resulting in 10 correlation coefficients wherein the mean value is 0 4 0 16 0 16 is the standard deviation under mimicry attack and 0 18 0 22 under i max pgds gdkde moreover we conduct the same estimation for dade ma the results are 0 56 0 15 under mimicry attack and 0 39 0 17 under i max pgds gdkde most of the observations confirm our statement except for several cases in ade ma vii conclusion and future work we have studied the usefulness of ensemble for both the defender and the attacker in the context of adversarial malware detection we propose the mixture of attacks and adversarial deep ensemble the adversarial deep ensemble can defend against a broad range of evasion attacks while cannot thwart mimicry attacks and mixtures of attacks ensemble methods promote the robustness against evasion attacks when base classifiers are robust enough for the attacker the ensemble methods notably improve the attack effectiveness we hope this paper will inspire more research into the context of adversarial malware detection future research prob lems are plentiful such as intriguing properties of different adversarial malware examples seeking effective attacks robust feature extraction malicious functionality estimation defense validation metrics and further designing robust defenses references 1 symantec internet security threat report 2019 istr symantec tech rep february 2019 2 v chebyshev 2019 march 5 mobile malware evolution online available https securelist com 3 cisco 2018 cisio online online available https www cisco com 4 y ye t li and et al a survey on malware detection using data mining techniques acm comput surv vol 50 no 3 pp 41 1 41 40 2017 5 y fan s hou and et al gotcha sly malware scorpion a metagraph 2 vec based malware detection system in proceedings of kdd 2018 2018 pp 253 262 6 i c b biggio and d m et al evasion attacks against machine learning at test time in machine learning and knowledge discovery in databases european conference springer 01 2013 pp 387 402 7 a al dujaili a huang e hemberg and u m oreilly adversarial deep learning for robust detection of binary encoded malware in 2018 ieee security and privacy workshops spw ieee 2018 pp 76 82 8 s hou y ye y song and m abdulhayoglu make evasion harder an intelligent android malware detection system in proceedings of the twenty seventh ijcai 2018 pp 5279 5283 9 l chen y ye and t bourlai adversarial machine learning in malware detection arms race between evasion attack and defense in eisic 2017 2017 pp 99 106 10 f kreuk a barak s aviv reuven m baruch b pinkas and j keshet deceiving end to end deep learning malware detectors using adversarial examples in corr 2018 11 k grosse n papernot p manoharan m backes and p mcdaniel adversarial perturbations against deep neural networks for malware classification arxiv preprint arxiv 1606 04435 2016 12 e grefenstette r stanforth b o donoghue j uesato g swirszcz and p kohli strength in numbers trading off robustness and computation via adversarially trained ensembles arxiv preprint arxiv 1811 09300 2018 13 f trame r a kurakin n papernot i goodfellow d boneh and p mcdaniel ensemble adversarial training attacks and defenses arxiv preprint arxiv 1705 07204 2017 14 y liu x chen c liu and d song delving into transfer able adversarial examples and black box attacks arxiv preprint arxiv 1611 02770 2016 15 h kwon y kim k w park h yoon and d choi advanced ensem ble adversarial example on unknown deep neural network classifiers ieice transactions on information and systems vol e 101 d pp 2485 2500 10 2018 16 f trame r and d boneh adversarial training and robustness for multiple perturbations arxiv preprint arxiv 1904 13000 2019 17 a araujo r pinot and et al robust neural networks using random ized adversarial training arxiv preprint arxiv 1903 10219 2019 18 l schott j rauber m bethge and w brendel towards the first adversarially robust neural network model on mnist arxiv preprint arxiv 1805 09190 2018 19 a athalye n carlini and d a wagner obfuscated gradients give a false sense of security circumventing defenses to adversarial examples corr vol abs 1802 00420 2018 20 d li q li y ye and s xu enhancing deep neural networks against adversarial malware examples arxiv preprint arxiv 2004 07919 2020 21 a madry a makelov l schmidt d tsipras and a vladu towards deep learning models resistant to adversarial attacks arxiv preprint arxiv 1706 06083 2017 https securelist com https www cisco com transactions on information forensics and security 14 22 k grosse n papernot and et al adversarial examples for malware detection in european symposium on research in computer security springer 2017 pp 62 79 23 d li q li y ye and s xu enhancing robustness of deep neural networks against adversarial malware samples principles framework and application to aics 2019 challenge in the aaai 19 workshop on artificial intelligence for cyber security aics 2019 2019 24 i j goodfellow j shlens and c szegedy explaining and harnessing adversarial examples 2014 arxiv preprint arxiv 1412 6572 25 x chen c li d wang s wen j zhang s nepal y xiang and k ren android hiv a study of repackaging malware for evading machine learning detection ieee transactions on information foren sics and security vol 15 pp 987 1001 2020 26 d arp m spreitzenbarth and et al drebin effective and explainable detection of android malware in your pocket in ndss vol 14 2014 pp 23 26 27 k allix t f bissyande j klein and y le traon androzoo collecting millions of android apps for the research community in proceedings of the 13 th international conference on mining software repositories ser msr 16 new york ny usa acm 2016 pp 468 471 online available http doi acm org 10 1145 2901739 2903508 28 2019 may virustotal online available https www virustotal com 29 y dong f liao and et al boosting adversarial attacks with momen tum in proceedings of the cvpr 2018 pp 9185 9193 30 c smutz and a stavrou malicious pdf detection using metadata and structural features in proceedings of the 28 th annual computer security applications conference acm 2012 pp 239 248 31 b biggio g fumera and f roli multiple classifier systems for robust classifier design in adversarial environments international journal of machine learning and cybernetics vol 1 no 1 4 pp 27 41 2010 32 multiple classifier systems under attack in international work shop on multiple classifier systems springer 2010 pp 74 83 33 m abbasi and c gagne robustness to adversarial examples through an ensemble of specialists in iclr 2017 workshop 2017 34 w xu d evans and y qi feature squeezing detecting adversarial examples in deep neural networks arxiv preprint 1704 01155 2017 35 w he j wei x chen n carlini and d song adversarial example defense ensembles of weak defenses are not strong in 11 th usenix workshop on offensive technologies woot 17 vancouver bc usenix association aug 2017 36 t pang k xu c du n chen and j zhu improving adversarial ro bustness via promoting ensemble diversity in international conference on machine learning 2019 pp 4970 4979 37 c smutz and a stavrou when a tree falls using diversity in ensemble classifiers to identify evasion in malware detectors in ndss 2016 38 j w stokes d wang m marinescu m marino and b bussone attack and defense of dynamic analysis based adversarial neural malware classification models 12 2017 39 j parikh 2018 august protecting the protector hardening machine learning defenses against adversarial attacks online available https www blackhat com us 18 speakers jugal parikh html 40 n papernot p mcdaniel i goodfellow s jha z b celik and a swami practical black box attacks against deep learning systems using adversarial examples arxiv preprint 2016 41 b biggio and f roli wild patterns ten years after the rise of adversarial machine learning pattern recognition vol 84 pp 317 331 2018 42 w xu y qi and d evans automatically evading classifiers a case study on pdf malware classifiers in ndss january 2016 43 p l nedim rndic practical evasion of a learning based classifier a case study in security and privacy sp 2014 ieee symposium on ieee 2014 pp 197 211 44 a demontis m melis and et al why do adversarial attacks transfer explaining transferability of evasion and poisoning attacks in 28 th usenix security symposium usenix security 2019 santa clara ca usa august 14 16 2019 n heninger and p traynor eds usenix association 2019 pp 321 338 45 a demontis m melis and et al yes machine learning can be more secure a case study on android malware detection ieee transactions on dependable and secure computing vol 16 no 4 pp 711 724 july 2019 46 f pierazzi f pendlebury j cortellazzi and l cavallaro intriguing properties of adversarial ml attacks in the problem space arxiv preprint arxiv 1911 02142 2019 47 b biggio g fumera and f roli security evaluation of pattern classifiers under attack ieee tkde vol 26 pp 984 996 04 2014 48 n papernot p mcdaniel and et al the limitations of deep learning in adversarial settings in 2016 euros p ieee 2016 pp 372 387 49 j jung c jeon and et al avpass leaking and bypassing antivirus detection model automatically in black hat usa briefings black hat usa las vegas nv jul 2017 50 l xu z zhan s xu and k ye an evasion and counter evasion study in malicious websites detection in cns 2014 ieee conference on ieee 2014 pp 265 273 51 a kurakin i goodfellow and s bengio adversarial machine learning at scale arxiv preprint arxiv 1611 01236 2016 52 l xu z zhan s xu and k ye cross layer detection of malicious websites in third acm conference on data and application security and privacy codaspy 13 2013 pp 141 152 53 w hoeffding probability inequalities for sums of bounded random variables journal of the american statistical association vol 58 no 301 pp 13 30 1963 54 z h zhou ensemble methods foundations and algorithms chapman and hall crc 2012 55 d p bertsekas constrained optimization and lagrange multiplier methods academic press 2014 56 j duchi s shalev shwartz and et al efficient projections onto the l 1 ball for learning in high dimensions in proceedings of the 25 th icml acm 2008 pp 272 279 57 a desnos 2019 androguard online online available https github com androguard androguard 58 k h brodersen c s ong k e stephan and j m buhmann the balanced accuracy and its posterior distribution in 2010 20 th international conference on pattern recognition 2010 pp 3121 3124 59 m pendleton r garcia lebron j h cho and s xu a survey on systems security metrics acm comput surv vol 49 no 4 pp 1 35 dec 2016 60 2019 may apktool online available https ibotpeaches github io apktool 61 l breiman random forests machine learning vol 45 no 1 pp 5 32 2001 62 i revivo and o caspi cuckoodroid in black hat usa las vegas nv jul 2017 63 b kolosnjaji a demontis and et al adversarial malware binaries evading deep learning for malware detection in executables in 2018 26 th european signal processing conference eusipco sep 2018 pp 533 537 http doi acm org 10 1145 2901739 2903508 https www virustotal com https www blackhat com us 18 speakers jugal parikh html https github com androguard androguard https ibotpeaches github io apktool i introduction ii related work ii a ensemble attacks ii b ensemble defenses iii preliminaries iii a ensemble of deep neural networks iii b evasion attacks iii b 1 definition iii b 2 threat models iii b 3 attack strategies in the literature iii c minmax adversarial training iv methodology iv a mixture of attacks iv a 1 overall idea iv a 2 two attack strategies iv b adversarial deep ensemble iv b 1 base classifiers iv b 2 combination iv b 3 analysis v experiments and evaluation v a experimental setup v a 1 data pre processing v a 2 training classifiers v a 3 specifying manipulations v b evaluating the effectiveness of attacks and defenses vi discussion vii conclusion and future work references