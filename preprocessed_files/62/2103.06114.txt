microsoft word vk mattersarising sub 2 docx 1 a critical reappraisal of predicting suicidal ideation using fmri timothy verstynen 1 konrad paul k rding 2 1 departments of psychology carnegie mellon neuroscience institute and biomedical engineering carnegie mellon university pittsburgh pa 2 departments of bioengineering and neuroscience university of pennsylvania philadelphia pa usa correspondence timothy verstynen email timothyv andrew cmu edu competing interests tdv works at the same institution as the senior author of just et al 2017 author contributions both authors contributed to writing and conceptualization tv wrote the analysis 2 abstract for many psychiatric disorders neuroimaging offers a potential for revolutionizing diagnosis and potentially treatment by providing access to preverbal mental processes in their study machine learning of neural representations of suicide and emotion concepts identifies suicidal youth 1 just and colleagues report that a naive bayes classifier trained on voxelwise fmri responses in human participants during the presentation of words and concepts related to mortality can predict whether an individual had reported having suicidal ideations with a classification accuracy of 91 here we report a reappraisal of the methods employed by the authors including re analysis of the same data set that calls into question the accuracy of the authors findings the analysis is a case study in the dangers of overfitting in machine learning 3 main text unlike many areas of medicine the fields of psychiatry and clinical psychology suffer from a critical lack of the ability to directly measure the internal processes that are the root of most psychiatric disorders 2 instead these fields rely on indirect assessments via verbal report or behavioral analyses that can often be unreliable indicators of internal thoughts and experiences over the past few years machine learning methods applied to functional neuroimaging data have presented a promising avenue for the field of computational psychiatry to potentially measure preverbal internal processes 3 offering hope for the development of neural biomarkers of psychiatric diseases in one such study just and colleagues 1 reported promising findings on a potential neural biomarker for suicidal ideation the authors reported a 91 classification accuracy for predicting a participant s group membership suicidal ideating individuals n 17 non ideating control n 17 using leave one out cross validation loocv with a classifier trained on functional mri fmri responses to a list of words such a robust ability to identify individuals who are likely suicidally ideating based off of pre verbal neural processes could revolutionize psychiatric approaches to suicide however the procedures described in the original paper suggest several problems first the use of loocv can inflate the estimated classification accuracy as well as overall type i error 4 second and most importantly the feature selection appears to have relied on the same data that is used in the final model evaluation in the section titled identifying the most discriminable concepts and locations of the supplementary information the authors state that they used a forward stepwise selection procedure to identify the best combination of concepts i e words and locations i e sphere of voxels from anatomically defined regions of interest that maximized their model accuracy in predicting whether a participant was in the suicidal ideation or control group according to the text feature selection happened along two different dimensions words and regions for words the authors only used data from 6 out of 30 words in their final model the authors present no a priori reason for why this subset of words would be better at discriminating between groups therefore we assume that this subset was determined solely by the described forward stepwise search process we can not be entirely sure because the authors did not share all of their code however as we shall point out below we have reasons to believe that we understand their approach reasonably well for regions the authors used multiple selection procedures for determining which clusters of voxels to include in their model resulting in 5 out of approximately 25 on average 25 based on group analysis regions being included in the final model first the authors evaluate voxels based on a stability score of responses across trials no information is provided for how this stability is quantified for each fold of the cross validation procedure the hold out test subject was not included in the voxel stability analysis 4 though it is not clear why because voxel stability is already an independent measure from the classifier performance second the authors selected the best subset of stable voxel clusters on each fold separately for each group on average there were 11 stable regions for the suicidal ideation group and 14 regions for the control group the final analysis only included 2 from the suicidal ideation group and 3 from the control group again presumably identified using the stepwise selection procedure this is problematic however because group assignment is already influencing features included in the final classifier analysis it is in the group subset that the forward stepwise search appears to have been applied given the sample size and structure of the classification problem we can see no way that a consistent set of features i e one set of words and regions to serve as a biomarker across all subjects can be identified without using data from all participants this reflects what we are calling feature hacking 5 a form of circular inference 6 that contaminates the validity of out of sample validation tests feature hacking is the process of inflating model performance in cross validation tests by selecting the best subset of the features that maximizes performance of the hold out test set that is used as a benchmark for how well the classifier generalizes to predicting unseen data using the code and data provided by the authors 7 we conducted a reanalysis of the feature selection process 8 we started by simply attempting to replicate the deterministic feature selection method as described in the original manuscript using a logistic regression classifier on group membership and a forward stepwise feature selection in three stages first we selected the subset of words that best distinguished the two groups using average response data from all stable regions with stability determined excluding the data from the out of sample participant second we selected the set of stable regions from the suicidal ideation group that best distinguished group membership using all 30 words finally we selected the stable regions from the control group using all 30 words feature selection on regions was run separately for the two groups because this follows the logic of the original analysis it is worth noting that this method still suffers from circular inference because all data are being used in the feature selection process as highlighted above the sample size is too small to enable a completely unbiased feature selection process our analysis was unable to replicate the original feature set we identified only one word vitality one region from the patient group left angular gyrus and one region from the control group left anterior cingulum only the latter region overlapped with the original set of features importantly using these words and regions the loocv classifier method from the original paper falls to 32 see table 1 thus using a standard forward stepwise selection procedure we were unable to either replicate the features or model accuracy reported in the original paper 5 we next set out to see how much the feature selection process employed by the authors impacted the classifier performance to start we re ran the original classifier reported by just et al 2017 but removed feature selection along the two dimensions words and regions separately these results are reported in table 1 removing feature selection on words but including the same set of selected regions as used in the original paper reduced classifier accuracy by 32 removing feature selection on the set of stable regions while keeping the original 6 words used in the original paper dropped classifier accuracy by 26 thus the classifier accuracy reported by just et al 2017 is highly sensitive to the unique set of words and regions used the only feature selection method employed in the original paper that does not suffer from circularity is the original selection of the stable voxel clusters i e regions here stability was determined by excluding the held out test subject for each run of the loocv classifier thus the only truly unbiased model that can be run on the data is one in which all words and all stable regions for both groups are used this model returns a classification accuracy of 41 well below chance and a full 50 below the accuracy reported by just et al 2017 table 1 change in leave one out cross validation loocv accuracy when different feature selection approaches are applied configuration loocv accuracy pre selected features words regions from just et al 2017 91 features from forward stepwise search using logistic regression 32 all words pre selected regions 59 pre selected words all regions 65 all words all regions 41 using information from data in a validation set to determine the structure of a model leads to inflated estimates of performance this can happen either by selecting the observations e g only including the subset of participants that maximize validation set performance or features 6 e g applying arbitrary transformations of variables based on validation set performance based on information from what should be a protected part of the sample our reanalysis shows that the classification results reported by just et al 2017 are likely inflated due to the use of feature hacking our analysis clearly shows that the most conservative approach using all words and all stable regions yields classification performance that does not outperform chance our analysis suggests that the paper by just and colleagues employs feature hacking without a more detailed description of the methods and independent evaluation of the feature selection process itself we are forced to conclude that the reported ability to discriminate the suicidally ideating from non ideating controls is not supported by the data reported in the original article references 1 just m a pan l cherkassky v l mcmakin d l cha c nock m k brent d machine learning of neural representations of suicide and emotion concepts identifies suicidal youth nat hum behav 1 911 919 2017 2 fuchs t subjectivity and intersubjectivity in psychiatric diagnosis psychopathology 43 268 274 2010 3 bzdok d meyer lindenberg a machine learning for precision psychiatry opportunities and challenges biol psychiatry cogn neurosci neuroimaging 3 223 230 2018 4 flint c cearns m opel n redlich r mehler d m a emden d winter n r leenings r eickhoff s b kircher t krug a nenadic i arolt v clark s baune b t jiang x dannlowski u hahn t systematic misestimation of machine learning performance in neuroimaging studies of depression neuropsychopharmacology 46 1510 1517 2021 5 cawley g c talbot n l c on over fitting in model selection and subsequent selection bias in performance evaluation j mach learn res 11 2079 2107 2010 6 kriegeskorte n simmons w k bellgowan p s f baker c i circular analysis in systems neuroscience the dangers of double dipping nat neurosci 12 535 540 2009 7 just m code and data for just et al 2017 at http www ccbi cmu edu suicidal ideation 7 nathumbeh 2017 just nathumbeh 2017 data and code html 8 code and data for verstynen kording a critical reappraisal of predicting suicidal ideation using fmri at https github com coaxlab reappraisal of neuromarkers of suicide