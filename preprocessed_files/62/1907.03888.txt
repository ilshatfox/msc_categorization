residual entropy barnaby rowe senior data scientist fidelity international 4 cannon street london ec 4 m 5 ab united kingdom barnaby t p rowe gmail com barney rowe fil com abstract we describe an approach to improving model fitting and model generalization that considers the entropy of distributions of modelling residuals we use simple simulations to demonstrate the observational signatures of overfitting on ordered sequences of modelling residuals via the autocorrelation and power spectral density these results motivate the conclusion that as commonly applied the least squares method assumes too much when it assumes that residuals are uncorrelated for all possible models or values of the model parameters we relax these too stringent assumptions in favour of imposing an entropy prior on the unknown model dependent but potentially marginalizable distribution function for residuals we recommend a simple extension to the mean squared error loss function that approximately incorporates this prior and can be used immediately for modelling applications where meaningfully ordered sequences of observations or training data can be defined 1 introduction the method of least squares is the dominant approach in regression problems the mean squared error mse is frequently therefore the default loss function in many practical applications and is commonly used for regression in fields as diverse as machine learning statistics physical sciences decision theory econometrics and finance it is criticised for over use and a lack of robustness e g 2 but has yet to be displaced in this paper we will level another criticism at the mse and other similarly constructed loss functions they leave out important information as a result of their too strict assumptions about the uncorrelatedness of model residuals statistical analysis of correlated residuals has a long history in econometrics and time series analysis that began with tests of the presence of autocorrelation at lag l 1 in residuals from least squares regression 17 6 8 tests were later developed for the presence of autocorrelation at any lag 3 13 the question became acute as econometrics developed as autocorrelations in residuals for autoregressive time series models where some of the regressors are lagged dependent variables leads in general to non consistency and bias sophisticated tests have therefore been developed to help validate such models 4 9 15 and make them robust to further confounding real world factors such as conditional heteroskedasticity e g 10 while prior work in this area has to been to devise tests of autocorrelation that can be applied after best fitting regression models have been determined e g by least squares we propose that the loss function itself could be extended to penalize models showing autocorrelated residuals during the process of fitting such correlations can either be created by the presence of correlated errors in observations of the independent variable or by the application of the wrong model in the first section preprint under review ar x iv 1 90 7 03 88 8 v 2 st at m e 1 a ug 2 01 9 we illustrate this for the important category of overfitting or over specified models by performing simple simulations of model fitting in one dimension in later sections we make a simple entropy argument for an extension to the mse loss function that is readily and efficiently calculated at least in one dimension or wherever residuals can be meaningfully ordered e g time series forecasting we illustrate from our simulations how this modified loss function will effectively penalize overfitting models potentially providing natural regularization better generalization and enhanced robustness to outliers 2 simulations of overfitting in one dimension consider a sample y y 0 y 1 yn 1 of n data values for our dependent variable of interest y situated at corresponding locations x 0 xn 1 in the indpendent variable we will construct a regression model for estimates of y which we will denote y x for a vector of model parameters the residuals r between the model and the data are defined as r r 0 rn 1 where rn yn y xn for each element n 0 n 1 of the sample to explore the effects of overfitting we will draw y as i i d standard normal variables the distribution of residuals you would expect from a good fit to the data and then attempt to fit these values the aim is to simulate what happens when a model is given unnecessary additional freedom 2 1 a fourier series model we simulate a simple case of n 100 observations for our randomly drawn sample y with corresponding evenly spaced locations xn 0 5 n n for n 0 n 1 we define the model of order m as a truncated fourier series y s m x a 0 a 1 am 1 b 1 bm 1 1 2 a 0 m 1 m 1 am cos 2 mx bm sin 2 mx 1 figure 1 shows three examples of least squares fitting y s m x to different randomly generated samples y and the resulting residuals we can see that as the overfitting order m increases to 41 there is observable structure in the sequence of residuals r the aim of this paper is to show that there is valuable information about modelling in the way that residual values are distributed with respect to one another in the domain of input variables the tools of correlation and spectral analysis are the primary aids for assessing these properties we define the circular residual autocorrelation function at lag l as rr l n 1 n 0 rn rn n l n 1 n 0 rnr n 2 where r n denotes the complex conjugate of rn we will assume real valued rn hereafter so that rnr n r 2 n and where rn denotes an element of the sequence r extended by periodic summation to an infinitely long repeating sequence of period n so that for p z we have rn p r pmodn the term in the denominator of 2 commonly referred to as the residual sum of squares rss or sum of squared errors of prediction sse normalizes the function so that rr l 0 1 at lags l 6 0 statistically independent residuals have e rr 0 for purely real residuals 2 is an even symmetric function of l and is hermitian in general this definition of the residual autocorrelation function is convenient because it can be related simply to the discrete fourier transform dft of the sequence of residuals r r 0 r k r n 1 where r k n 1 n 0 rn e i 2 n kn 3 and the inverse transform is defined as rn 1 n n 1 k 0 r k e i 2 n kn 4 2 0 4 0 2 0 0 0 2 0 4 x 3 2 1 0 1 2 3 y y n 0 1 best fitting curve for m 1 0 4 0 2 0 0 0 2 0 4 x 3 2 1 0 1 2 3 y residual from best fitting curve for m 1 0 4 0 2 0 0 0 2 0 4 x 3 2 1 0 1 2 3 y y n 0 1 best fitting curve for m 21 0 4 0 2 0 0 0 2 0 4 x 3 2 1 0 1 2 3 y residual from best fitting curve for m 21 0 4 0 2 0 0 0 2 0 4 x 3 2 1 0 1 2 3 y y n 0 1 best fitting curve for m 41 0 4 0 2 0 0 0 2 0 4 x 3 2 1 0 1 2 3 y residual from best fitting curve for m 41 figure 1 three examples of randomly drawn samples y and the least squares best fitting model left column and resulting residuals right column for model orders m 1 top row m 21 middle row and m 41 bottom row of the fourier series model of equation 1 the residuals reduce in amplitude but also appear less random as the model order increases we will often shorthand the dft operations on a full sequence using the symbol f e g r f r and r f 1 r using the circular form of the wiener khinchin theorem for dfts we may then write rr l f 1 r r n 1 n 0 r 2 n which can be calculated with great computational efficiency thanks to the fast fourier transform fft algorithm 5 figure 2 shows rr l for the model fits of figure 1 significant negative correlation between neighbouring residuals is seen at lag l 1 for the m 21 and m 41 models the m 41 model shows the strongest departures from rr 0 at lags l 6 0 with an oscillating character figure 3 a shows the average of residual autocorrelation values across 105 independent realizations of y from a suite of simulations at each model order m 50 there is structure as well as being useful for the efficient calculation of rr l the sequence r r is a powerful equivalent representation it defines the power spectral density of our sequence of residuals which we denote prr k r r k 5 3 0 5 0 0 0 5 1 0 m 1 0 5 0 0 0 5 1 0 re si du al a ut oc or re la tio n m 21 0 10 20 30 40 50 lag l 0 5 0 0 0 5 1 0 m 41 0 0 0 1 0 2 0 3 0 4 0 5 x figure 2 residual autocorrelation function values for the randomly drawn samples and least squares best fitting models of figure 1 for later convenience we will also define a corresponding correlation spectral power density rr k prr k n 1 n 0 r 2 n the dft of rr l prr k and rr k are both even symmetric functions of k for purely real residuals for i i d residuals with a defined variance these functions are expected to show a flat white noise spectrum with constant expectation values e prr n 1 n 0 r 2 n e rr 1 for all k 0 n 1 figure 3 b shows the averaged power spectral signature of residuals from the same suite of simulations as shown in figure 3 a where we have defined the power spectral signature as srr k prr k max k prr 6 for some sequence of residualsr with power spectral density prr k this choice makes the structure of the power spectrum easy to visualize with increasing m we see that an increasing share of the lower k spectral modes are suppressed starting with the lowest k the apparent structure in the modelling residuals from overfitting arises because these residuals no longer match the flat power spectrum distribution that would be expected if they could be assumed to be i i d in a very real sense they have been high pass filtered 2 2 a chebyshev polynomial series model the fourier series model 1 is a special case it forms an othonormal basis its coefficients am and bm have direct correspondence to the real and imaginary parts of the dft itself and it provides an optimally compact in terms of m series representation of arbitrary real valued sequences r the suppression of lower k power in prr k by overfitting models in the simulations performed above is therefore in fact effectively total not only on average but also in individual cases in general models will often not have complete freedom to precisely replicate arbitrary variation in observed data although see 18 nor will they so wholly suppress fourier modes in residuals for this reason it is interesting to generalize to a set of model basis functions that do not match the modes of the dft precisely and cannot represent arbitrary patterns of residuals in any finite series candidates for such a basis are the chebyshev polynomials of the first kind tm x see 1 we 4 0 10 20 30 40 50 lag l 0 10 20 30 40 49 o ve rfi tt in g or de r m averaged residual autocorrelation 1 00 0 75 0 50 0 25 0 00 0 25 0 50 0 75 1 00 0 10 20 30 40 50 k 0 10 20 30 40 49 o ve rfi tt in g or de r m averaged residual power spectral signature 1 00 0 75 0 50 0 25 0 00 0 25 0 50 0 75 1 00 figure 3 a averaged residual autocorrelation function values as a function of fourier series model order m calculated over 105 realizations of y per order b averaged residual power spectral signature for the same realizations and model define the chebyshev polynomial series model of order m as fc m x a 0 a 1 am 1 m 1 m 0 amtm x 7 the absolute values of these functions are bounded by 1 on the interval 1 1 and they are a convenient basis for polynomial interpolation in bounded spaces as for the fourier series we simulate a simple case of n 100 observations in our sample y we place these at corresponding evenly spaced locations xn 1 2 n n for n 0 n 1 once more we performed 105 independent realizations of y for each chebyshev model order m 100 as the chebyshev polynomial series model does not afford a complete representation of an arbitrary sequence r at any level of truncation unlike the fourier series the choice of maxmimum m is somewhat arbritrary figure 4 shows the averaged residual autocorrelation rr l and power spectral signature srr k as a function of order m across this suite of simulations we see similar behaviour to the fourier series case with increasing modelling order m leading to increasing supression of lower k power due to the less compact representation of arbitrary sequences provided by chebyshev polynomials the suppression of lower k power by overfitting is more gradual 3 implications for the mean squared error loss function as discussed in section 1 the mse loss function is very commonly used in optimization and regression applications and is defined using the notation above as simply mse r 1 n n 1 n 0 r 2 n underpinning the widespread use of the mse as a loss function is the gauss markov theorem that for a linear model fit to observations with uncorrelated equal variance and zero mean errors the best linear unbiased estimator of any linear combination of the observations is that which minimizes the mse where observational errors can be assumed to also follow a gaussian distribution this estimator is also the maximum likelihood estimator e g 14 but we have seen that residuals are not necessarily uncorrelated the model residuals passed to the mse loss function will only be uncorrelated in general if the true model has been applied and the model parameters are optimal anything else will in general either underfit or overfit introducing correlations as shown above and as discussed in a specific application by 16 in empirical settings and machine learning applications the true model may be unknown and potentially unknowable the application of the mse loss function in regression tasks often by default assumes too much in assuming that model residuals are uncorrelated irrespective of model selection or under variation in model parameter values however plausible this assumption may be about the underlying observational errors this causes us to unwittingly discard valuable information the information discarded is not captured by the mse which is invariant under permutation of any ordered sequence of the residual because it is embedded within knowledge of the relative 5 0 10 20 30 40 50 lag l 0 10 20 30 40 50 60 70 80 90 100 o ve rfi tt in g or de r m averaged residual autocorrelation 1 00 0 75 0 50 0 25 0 00 0 25 0 50 0 75 1 00 0 10 20 30 40 50 k 0 10 20 30 40 50 60 70 80 90 100 o ve rfi tt in g or de r m averaged residual power spectral signature 1 00 0 75 0 50 0 25 0 00 0 25 0 50 0 75 1 00 figure 4 a averaged residual autocorrelation function values as a function of chebyshev polynomial model order m calculated over 105 realizations of y per order b averaged residual power spectral signature for the same realizations and model locations of residuals with respect to one another the aim of this paper is to make initial step towards using this information in the optimization process so as to build better regularized and better generalizing models if this can be done at all computational efficiency and ease of adoption will be valuable characteristics of the solution with these aims in mind we will explicitly target as simple a modification of the widely used mse loss function as possible 4 incorporating knowledge of correlated residuals into the loss function 4 1 a multivariate gaussian distribution the least squares case we begin with the simplest possible applicable probabily distribution for correlated residuals a multivariate gaussian we write the probability density function as p r 0 rn 1 1 2 n det exp 1 2 r t 1 r 8 where for convenience we have adopted vector notation to describe the sequence of residuals r r 0 rn 1 t with corresponding mean and covariance matrix we will assume 0 for ordinary least squares it would be additionally assumed that 2 in where in is the n n identity matrix and 2 is the variance of errors then identifying 8 with the statistical likelihood of the model l y y 2 leads to the log likelihood function ln l y y 2 n 2 2 mse y y n 2 ln 2 2 9 minimizing the mse therefore maximizes the likelihood irrespective of any value taken by the likely unknown variance 2 4 2 a new loss function we will seek a new loss function of the form l y y mse y y x y y where the function x is to be determined to identify a candidate we will relax assumptions about the covariance matrix of residuals in 8 relative to the least squares case and instead assume the slightly more general 2 p 10 where p is an n n symmetric positive semidefinite correlation matrix with unit values on the diagonals we will have little a priori knowledge of the contents of p which as we know from 6 section 2 can vary with model and model parameters as well as with any correlations that may existing in the underlying observational errors we propose to weaken the common assumption that p in by instead introducing the following form for the prior probability on p p p exp h p 11 where h p n 2 1 ln 2 1 2 ln det p is the differential entropy for a multivariate gaussian with covariance matrix p and is a scaling constant that we introduce to allow the user to balance the contribution to the loss function from this prior the prior can then be simplified to p p det p 2 12 we motivate the functional form of 11 by direct analogy to the boltzmann entropy formula e g 11 in the intuitive belief that the most likely distribution of residuals is that which has the greatest number of possible configurations per unit of variance as det p is maximized when p in applying this prior will penalize distributions of residuals showing any correlations whether from overfitting or underfitting 16 crucially we will also assume that p can be approximated as a circulant matrix although not true in general particularly for small samples the approximation becomes asymptotically more accurate as n increases and or as residual correlations themselves become negligible since in is a circulant matrix any circulant matrix p is fully determined by its first column which we label as p is a correlation matrix the column consists of the true values around which the circular residual autocorrelation function rr l of equation 2 defined for a single sample of residual values will be distributed it can be seen that the average of rr l over realizations of residuals from multiple independent samples of observations y given the same model will form an asymtotically consistent estimator for but for any sufficiently free circulant matrix model of p even a single sample of residuals allows us to calculate a rr l that can itself be identified as the maximum likelihood estimate of we will denote this estimate rr l if p is assumed circulant this also defines a corresponding maximum likelihood estimate for the full matrix p y y if p is a circulant matrix its eigenvalues are the dft of the same holds for the eigenvalues of p p k f k rr k 13 where k 0 n 1 since the determinant of a matrix is given by the product of its eigenvalues det p can be written as det p y y n 1 k 0 p k n 1 k 0 rr k 14 as discussed above the mse loss function can be related to the log likelihood function of the multivariate gaussian with uncorrelated residuals in a full formally correct treatment this likelihood should be expanded to include a freely varying p which can then be marginalized over as a nuisance parameter combining both the prior p p and the likelihood of observed correlated residuals given p in the integrand but this is reserved for future work instead we propose an approximate treatment that assumes that the likelihood of any observed residual correlations given some p can be approximated as very sharply peaked indeed as being a dirac delta function p p around p this makes the marginalization trivial to compute and adding a penalty term ln p p to the multivariate gaussian log likelihood of 9 to create new loss function l y y ln l y y ln p p y y 15 where we will now omit terms that have no dependence on the parameter vector dividing both sides by n 2 we arrive at the equivalent l y y mse y y 2 n n 1 k 0 ln rr k 16 defining the mean log power of residual correlations as mlp 1 n n 1 k 0 ln rr k and making a final approximation that the mse is a consistent sufficient estimator for 2 valid in the limit 7 0 10 20 30 40 50 overfitting order m 0 5 10 15 20 25 30 35 1 1 n n 1 k 0 ln rr k fourier series model 0 20 40 60 80 100 overfitting order m 0 5 10 15 20 25 30 35 1 1 n n 1 k 0 ln rr k chebyshev series model figure 5 a median solid blue line and 5 th 95 th percentile boundaries dashed blue lines of values of 1 mlp r for the simulations of section 2 1 as a function of fourier series model order m the black lines show the distribution of this quantity estimated from y in the absence of any model fit b the same but for the chebyshev simulations of section 2 2 red lines following investigation we have concluded that the structure seen at high m values is due to low level aliasing artifacts introduced by the significant variation in the higher order chebyshev polynomials at spatial frequencies higher than the nyquist frequency these artifacts are also present but not visible in the low k modes of figure 4 p in we arrive at the following proposal for a residual entropy sensitive extension of the mse loss function l y y mse y y 1 mlp y y 17 figure 5 shows statistics of realized values of 1 mlp r i e the proposed multiplier on the mse in the 1 case for every residual sample r in the simulations of section 2 we see an increasing function of overfitting order m although unavoidable aliasing in the experimental setup introduces artifacts at very high m in the chebyshev case motivating the use of our proposed loss function 17 in the suppression of residual correlations and thereby providing automatic regularization and a safeguard against overfitting 5 conclusions least squares has been the dominant approach towards empirical regression problems for over 200 years 12 14 and the mse loss function is at its core yet we do believe that the mse discards valuable information about the locations of residuals we also believe that penalizing models that show discernible structure in residuals residuals that have hitherto been blindly assumed to be uncorrelated is both intuitively appealing and can be theoretically justified on entropic principles we have made a first attempt towards incorporating these beliefs directly into the loss function we presented results that suggest the mse may be improved by a simple although approximate multiplicative factor given in equation 17 a factor that can be calculated with relative computational efficiency thanks to the fft algorithm but many issues and outstanding questions remain we have made mathematical approximations that are asymptotically true e g p being approximately circulant valid only for large n or as p in or that the mse forms a consistent and sufficient estimator for 2 in practice valid as p in assumptions were also made to avoid marginalization over p and to retain the mse as part of the proposed loss function approximating the likelihood function of residual correlations as being sharply distributed around the observed maximum likeli hood estimate the impact of these simplifying assumptions on practical optimization needs to be tested and a fuller treatment may in fact lead to a better approximate form by which to encourage greater residual entropy to calculate a meaningful power spectral density of residuals the residuals must first be ordered into a sequence that has some meaning in the input space this is trivial in one dimension less 8 so in higher dimensions for higher dimensional regression problems we would initially propose using residuals ordered along each of the most significant principal components of the input space in turn as determined via pca structure seen in residuals along any of the principal directions in the space of input observations should be penalized but whether this will be a powerful enough constraint is an open question for very high dimensional problems it might be prudent to work in configuration lag space and estmate correlations in annular bins of cosine distance where data are not regularly spaced in the independent variables it may also be valuable to average binned residuals before calculating rr k clearly an important next step will be applying the loss function of equation 17 in practical settings to explore its behaviour outside of artificial simulations and the dynamic interplay of the mse and the multiplicative factor 1 mlp of utmost importance will be to establish whether the new addition to the loss function is safe i e that modelling outcomes are no worse than when using mse alone finally an important application not explored here is that of outlier rejection to the extent that contaminating outliers drag a best fitting model away from the bulk of the other data points they will necessarily introduce positive correlations in modelling residuals the loss function described above offers hope of systematically enhancing robustness to outliers by penalizing such fits automatically references 1 g b arfken h j weber and f e harris mathematical methods for physicists a comprehensive guide elsevier science 2013 2 j o berger statistical decision theory and bayesian analysis 2 nd ed springer series in statistics springer new york 1985 3 g e p box and d a pierce distribution of residual autocorrelations in autoregressive integrated moving average time series models journal of the american statistical association 65 332 1509 1526 1970 4 t s breusch testing for autocorrelation in dynamic linear models australian economic papers 17 31 334 355 1978 5 j cooley and j tukey an algorithm for the machine calculation of complex fourier series mathematics of computation 19 90 297 301 1965 6 j durbin and g s watson testing for serial correlation in least squares regression i biometrika 37 3 4 409 428 12 1950 7 j durbin and g s watson testing for serial correlation in least squares regression ii biometrika 38 1 2 159 178 06 1951 8 j durbin and g s watson testing for serial correlation in least squares regression iii biometrika 58 1 1 19 1971 9 l g godfrey testing against general autoregressive and moving average error models when the regressors include lagged dependent variables econometrica 46 6 1293 1301 1978 10 l g godfrey and a r tremayne the wild bootstrap and heteroskedasticity robust tests for serial correlation in dynamic regression models computational statistics data analysis 49 2 377 395 4 2005 11 t l hill an introduction to statistical thermodynamics dover books on physics dover publications 2012 12 a m legendre nouvelles m thodes pour la d termination des orbites des com tes nineteenth century collections online ncco science technology and medicine 1780 1925 f didot 1805 13 g m ljung and g e p box on a measure of lack of fit in time series models biometrika 65 2 297 303 08 1978 14 w h press s a teukolsky w t vetterling and b p flannery numerical recipes in fortran the art of scientific computing 1992 15 f pro a testing for residual correlation of any order in the autoregressive process communications in statistics theory and methods 47 3 628 654 2018 9 16 b rowe improving psf modelling for weak gravitational lensing using new methods in model selection mon not roy astron soc 404 350 366 may 2010 17 j von neumann distribution of the ratio of the mean square successive difference to the variance ann math statist 12 4 367 395 12 1941 18 c zhang s bengio m hardt b recht and o vinyals understanding deep learning requires rethinking generalization corr abs 1611 03530 2017 10 1 introduction 2 simulations of overfitting in one dimension 2 1 a fourier series model 2 2 a chebyshev polynomial series model 3 implications for the mean squared error loss function 4 incorporating knowledge of correlated residuals into the loss function 4 1 a multivariate gaussian distribution the least squares case 4 2 a new loss function 5 conclusions