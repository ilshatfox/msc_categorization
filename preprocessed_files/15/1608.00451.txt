numerical tolerance for spectral decompositions of random matrices avanti athreya michael kane bryan lewis zachary lubberts vince lyzinski youngser park carey e priebe and minh tang abstract we precisely quantify the impact of statistical error in the quality of a numerical approximation to a random matrix eigendecomposition and under mild conditions we use this to introduce an optimal numerical tolerance for residual error in spectral decompositions of random matrices we demon strate that terminating an eigendecomposition algorithm when the numerical error and statistical error are of the same order results in computational savings with no loss of accuracy we also repair a flaw in a ubiquitous termination condition one in wide employ in several computational linear algebra implementations we illustrate the practical consequences of our stopping criterion with an analysis of simulated and real networks our theoretical results and real data examples establish that the tradeoff between statistical and numerical error is of significant import for data science 1 introduction the rapid and accurate computation of the eigenvalues and eigenvectors of a matrix is of universal importance in mathematics statistics and engineering in practice of course numerical methods to compute such spectral decompositions necessarily involve the imposition of a stopping criterion at which a given linear algebraic algorithm terminates typically when the residual numerical error is less than some user specified tolerance when matrices have random entries however there is a second important source of error the inherent statistical error between say matrix entries and their mean in addition to the numerical error from algorithmic approximations both of these sources of error contribute to the discrepancy between computed and theoretical eigendecompositions and their interplay can be important for determining the optimal termination of an algorithm stopping an algorithm when the numerical error is too large can yield unacceptably inaccurate output but stopping when the numerical error is very small and is effectively dwarfed by the statistical error can be computationally expensive without resulting in a meaningful improvement in accuracy we focus on determining an optimal error tolerance for the numerical computation of an eigendecomposition of a random symmetric matrix a such matrices are ubiquitous in data science from errorful observations of data to adjacency matrices of random networks in particular spectral decompositions of the adjacency and laplacian matrices of random networks have broad applications from the distributional convergence of random walks on graphs chung et al 1996 to the solution of a relaxation of the min cut problem fiedler 1973 further for random dot product graphs rdpgs a particular model which we de fine and describe in more detail in section 6 the spectral decomposition of a can serve as a statistical estimate for underlying graph parameters see sussman et al 2012 as described above some nonzero sampling error is inherent in such procedures that is there is random discrepancy between a statistical estimate for a parameter and the true value of the parame ter itself if the order of magnitude of this sampling error is known for instance if one can obtain a lower bound for the sampling error incurred when using a spectral decomposition of a to estimate the spectral decomposition of its mean then the accuracy in the numerical algorithm for the spectral decomposition of a should be weighed against this inherent vari 1 ar x iv 1 60 8 00 45 1 v 3 st at c o 3 1 ja n 20 20 ability there may be little gain in a very careful determination of the eigenvectors of a if these eigenvectors are with high probability close to some fixed nonzero distance from the true model parameters to be precise suppose a is a symmetric square matrix let sa denote the diagonal matrix of the d largest magnitude eigenvalues of a and ua the matrix of corresponding eigenvectors numerical methods for both the singular value decomposition and eigendecomposition and in turn for sa and ua abound iterative methods such as the power method lanczos factorization and rayleigh quotient iteration to name but a few see higham 2002 stewart 2001 for a comprehensive overview begin with an initialization s 0 for the eigenvalues or singular values of a and u 0 for the eigenvector s and compute successive updates s k and u k for the eigenvalues and eigenvectors respectively until a specified error often a function of the difference between au k and u ks k is sufficiently small for example many algorithms are set to terminate when the relative error is suitably small that is when 1 1 au k u ks k 2 a 2 where 2 denotes the spectral norm under suitable rank and eigengap assumptions on a convergence of the iterates is guaranteed for generic initializations when the spectral norm of a is known the relative error on the left hand side of eq 1 1 is directly computable alternatively when a 2 must be approximated numerically it can be replaced by the absolute value of an approximation of the largest magnitude eigenvalue of a for further details on error analysis and stopping criteria for numerical methods see arioli et al 1989 1992 hestenes and stiefel 1952 kahan 1967 rigal and gaches 1967 stewart 2001 unfortunately the stopping criterion in equation 1 1 has a known flaw when the col umns of the initialization u 0 belong to an invariant subspace of a that is not the span of the top d eigenvectors or because of numerical error they appear to belong to such a subspace then even when this stopping criterion is satisfied the wrong eigenspace may be approximated typically it is assumed that uta u 0 is nonsingular but such nonsingularity is frequently unverifiable and it is unclear how numerical imprecision affects it we offer the alternative stopping criterion 1 2 au k u ks k 2 a 2 s 1 k 2 and show that under our model assumptions and choice of when this criterion is satisfied s 1 k 2 is of strictly lower order than the relative error so the stopping criteria in eq 1 1 and eq 1 2 are very similar however when the wrong eigenspace is being approximated our added term will be too large so it will prevent the algorithm from terminating consequently when this new stopping criterion holds the algorithmic output approximates the correct eigenspace our main result is the following let a be a symmetric random matrix with independent uniformly bounded entries we suppose that the mean matrix p defined by pij e aij is rank d and positive semidefinite let s 2 ij n ij 1 denote the matrix of variances for a so that 1 3 2 ij v aij e aij pij 2 for all 1 i j n 2 let be the error tolerance in the numerical algorithm used to compute the rank d eigende composition of a then under mild assumptions on the eigenvalues of p and the order of the rows of s we find that need not be much smaller than a 1 22 before further reduction will not improve the accuracy of the numerical eigendecomposition see section 5 we also show in section 6 via numerical simulations on real and simulated data that this choice of can lead to considerable computational savings compared to relying on a default algorithmic tolerance in a problem of estimating the latent positions for a random graph model and that it does not negatively affect subsequent inference the organization of the paper is as follows in the following section 2 we set our notation terminology and random matrix model in section 3 we state our results on the order of the statistical error in the approximation of the eigendecomposition of p when using the exact eigendecomposition of the observed matrix a in section 4 we state our results on the order of the numerical error in the approximation of the eigenvectors of a when stopping an algorithm according to condition 1 2 for a given error tolerance in section 5 we synthesize these results to obtain an optimal choice of in the stopping criterion our simulation results are contained in section 6 and all proofs are located in the appendix a 2 setting and notation let represent our sample space and p our probability mea sure the expectation of a random variable will be denoted by e if v is a vector v denotes its euclidean length for any n n real valued matrix m m denotes the transpose tr m the trace m f the matrix frobenius norm and m 2 the spectral norm for any symmet ric matrix m with some number of non zero eigenvalues let 1 m d m denote in decreasing order the d eigenvalues of m with largest magnitude so 1 m 2 m etc we now describe the random matrix model we will consider throughout this work model 2 1 lpsm we say a random symmetric matrix a of order n with independent entries ai j ij ij follows a low rank positive semidefinite mean lpsm model or that a is an lpsm if the mean matrix p e a is positive semidefinite and of rank d where d is fixed recalling that 2 ij v aij as in eq 1 3 we define the following quantities 2 1 2 n max i n j 1 2 ij and 2 n min i n j 1 2 ij note that we clearly have 2 n 2 n from these definitions for our main results we will make the following assumptions on the signal in the matrix p and the order of the variance 2 n and 2 n assumption 2 2 sufficient signal sufficient noise suppose that c 1 n 1 p d p c 2 n and c 1 n 2 n 2 n c 2 n for some c 1 c 2 0 and c 1 c 2 0 3 remark 2 3 in the case that the matrix a is obtained from real data which is truncated at a given floating point precision we will have at least 2 n 2 n cn for some constant c 0 depending on this precision when the variance is too large the signal overwhelms the noise corrupting the computed eigendecomposition on the other hand when the variance is very small early termination may not be optimal indeed in the very low variance case since a is likely to be extremely close to p the major source of error is numerical rather than statistical so incremental reduction of this error can still improve the final quality of the approximation we use the following notation for the spectral decomposition of p 2 2 p up u p sp 0 up u p where sp is the diagonal matrix of the d non zero eigenvalues of p and up is the matrix of associated eigenvectors since p is positive semidefinite it can be expressed as xx where x ups 1 2 p denote the spectral decomposition of a analogously 2 3 a ua u a sa s a ua u a where sa is the diagonal matrix of the d largest eigenvalues of a in absolute value and ua is the matrix of the associated eigenvectors we define x uas 1 2 a in section 3 we state upper and lower bounds on ua upw f and x xw f where w is an orthogonal matrix our approach to determining an appropriate error tolerance for random matrix eigende compositions is interwoven with the matrix size in other words the error tolerance is allowed to depend on n and we consider the probabilistic implications of this for large n as such we employ a strong version of convergence in probability and we rely on notions of asymptotic order both of which we describe below definition 2 4 if dn is a sequence of events indexed by n we say that dn occurs with overwhelming probability if for any c 0 there exists n 0 c such that for all n n 0 c p dn 1 1 nc definition 2 5 if w n n are two quantities depending on n we will say that w is of order n and use the notation w n n to denote that there exist positive constants c c such that for n sufficiently large c n w n c n we write w n o n if there exists a constant c such that for n sufficiently large w n c n we write w n o n if w n n 0 as n and w n o 1 if w n 0 as n when n o w n we write w n n 3 statistical error bounds for eigendecompositions of lpsm matrices we now state theorem 3 1 our principal result on the order of the statistical error in approximating the matrix of top d eigenvectors up of p with the matrix of top d eigenvectors ua of a note the asymmetry in the statements of the lower and upper bounds in theorem 3 1 for the upper bound we show that there is some orthogonal matrix w such that ua upw f can be made small where w accounts for the nonuniqueness of the eigenvectors for example sign changes for a univariate eigenspace or linear combinations of eigenvectors in a higher dimensional eigenspace for the lower bound we show that for any w ua upw f is at 4 least as large as some quantity depending on the variance in the approximation of p by a this discrepancy between the eigenvectors of p and a is a consequence of inherent random error and cannot be rectified by linear transformations the proofs for all of the results in this section may be found in appendix a 1 theorem 3 1 let a be an lpsm matrix satisfying assumption 2 2 with overwhelming probability there exists an orthogonal matrix w such that ua upw f o 1 n and for any orthogonal matrix w ua upw f 1 n in particular there exists an orthogonal matrix w such that ua upw f 1 n corollary 3 2 in the setting of the previous theorem the same bounds hold for the spectral norm ua upw 2 recall that the scaled eigenvector matrices are defined by x ups 1 2 p and x uas 1 2 a we also show bounds on the estimation error between the random x and the true x which we will see in section 6 is useful in network inference theorem 3 3 suppose a is an lpsm matrix satisfying assumption 2 2 then there is a sequence n o log n n such that with overwhelming probability there exists an orthogonal matrix w satisfying x xw f c p n where 3 1 c 2 p tr s 1 2 p u p e a p 2 ups 1 2 p and e a p 2 diag j 2 ij i with overwhelming probability for all orthogonal matri ces w we also have the associated lower bound x xw f c p n moreover c p is of constant order and is bounded away from zero corollary 3 4 in the setting of the previous theorem 1 d c p n x xw 2 c p n 4 numerical error bounds for eigendecompositions of lpsm matrices given a nu merical eigendecomposition algorithm let s k be the kth iterate of the diagonal matrix of the largest d approximate eigenvalues of a and let u k be the kth iterate of the n d matrix of the 5 corresponding orthonormal approximate eigenvectors as described in eq 1 1 we consider algorithms that terminate when the following stopping criterion is achieved 4 1 au k u ks k 2 a 2 where is the user specified error tolerance remark 4 1 as mentioned in section 1 the stopping criterion above is computationally attractive and appealing to intuition but has a significant drawback if u k is the matrix whose columns are the eigenvectors corresponding to d 1 2 d then 4 1 will hold but the desired approximation of the top d eigenvectors fails thus in our analysis we consider the alternative stopping criterion 4 2 au k u ks k 2 a 2 s 1 k 2 this criterion addresses complications that can occur because of problematic initializations specifically an initialization u 0 that belongs to an invariant subspace of a other than the one spanned by the top d eigenvalues or because of numerical error appears to belong to such a subspace as an example we generate a matrix a with seven eigenvalues in the interval 10 11 and 93 eigenvalues in the interval 0 1 if we call irlba an implementation of implicitly restarted lanczos bidiagonalization baglama et al 2003 with an initial vector v whose first seven entries are zero the eigenspace associated to the top seven eigenvalues is entirely missed this still occurs when the entries in the first components of the initial vector are very small but nonzero 10 100 on the other hand if we run irlba on qaq where q is a generic unitary matrix with starting vector qv the correct eigenspace is approximated such numerical complications are likely to depend on details of the algorithmic implementation and while we suspect that in practice problematic initializations should rarely occur for commonly used algorithms it is difficult to make general statements about their likelihood importantly we show in thm 4 2 that for our recommended choice of when the criterion in 4 2 is satisfied the term s 1 k 2 is of strictly lower order than the relative error thus off of the set of problematic initializations the termination conditions in 4 1 and 4 2 are functionally equivalent on the other hand problematic initializations will go undetected by stopping criterion 4 1 but on such initializations criterion 4 2 will prevent the algorithm from terminating generating a flag that the algorithm terminated for reasons other than the achievement of the stopping criterion this makes it clear to the user that something has gone awry equation 4 2 also has a significant theoretical advantage we show that under this condition and our lpsm assumptions with high probability depending only on the random nature of a the nearest eigenvalues of a to those of s k are precisely 1 a d a and the eigenvectors u k approximate the top d eigenvectors in ua as desired since we make no assumptions about the algorithm used to compute the approximate eigenvectors and eigenvalues besides the stopping criterion in order to guarantee this attachment between approximation and truth we need a stronger condition than equation 4 1 6 to choose optimally that is small enough for accuracy but not so small as to squan der computational resources we need to understand how the stopping criterion impacts the separation between the algorithmically computed kth iterate matrix u k and ua the true ma trix of the d largest eigenvectors of a similarly we must understand how the algorithm impacts the separation between the true and approximate eigenvalues we describe both in the following proposition the proof of which may be found in appendix a 2 theorem 4 2 suppose that a is a symmetric matrix whose top d eigenvalues 1 a d a satisfy 1 a d a n and whose remaining eigenvalues d 1 a n a satisfy d 1 a n a o n let u k s k be approximate matri ces of eigenvectors and eigenvalues for a with the diagonal entries of sa s k nonincreasingly ordered suppose o 1 n then for n sufficiently large au k u ks k 2 a 2 s 1 k 2 guarantees that s k sa 2 a 2 and there is an orthogonal matrix w and constant c 0 such that u k uaw f c moreover s 1 k 2 o 1 n 5 optimal numerical tolerance we are now ready to give our results combining the bounds on the statistical and numerical error and suggesting an optimal choice of in the stopping criterion so that the numerical error is of just smaller order than the statistical error indeed as we will soon see with high probability choosing any smaller does not change the order of the error in the approximation u k upw so the further effort and computation required to achieve this reduction in the numerical error is essentially wasted the proof of all of the results in this section may be found in appendix a 3 theorem 5 1 let a be an lpsm matrix and let u k s k be the approximated eigenvectors and eigenvalues of a satisfying equation 4 2 where denotes the error tolerance let c p be defined as in equation 3 1 then there is a constant c 0 and a sequence n o log n n such that with overwhelming probability for n sufficiently large there exists an orthogonal matrix w such that 5 1 c p a 2 n c u k upw f c p d a n c where the lower bound holds for all w if o a 1 22 then the lower bound is of or der a 1 22 if the rate at which 0 is increased then the order of the lower bound in equation 5 1 is not improved 7 corollary 5 2 in the setting of the previous theorem the following inequalities also hold c p d a 2 n c u k upw 2 c p d a n c theorem 5 3 let a be a lpsm matrix and let u ks 1 2 k be the numerical approximation of x uas 1 2 a where u k s k satisfy equation 4 2 for a given error tolerance let c p be defined as in equation 3 1 then there exists a constant c 0 and sequence n o log n n such that with overwhelming probability for n sufficiently large there exists an orthogonal matrix w such that 5 2 c p 2 a 2 n c u ks 1 2 k ups 1 2 p w f p 2 2 c p a 2 n c where the lower bound holds for all w if o a 1 22 then the lower bound is of or der a 1 22 if the rate at which 0 is increased then the order of the lower bound in equation 5 1 is not improved corollary 5 4 in the setting of the previous theorem the following inequalities also hold c p 2 d a 2 n c u ks 1 2 k ups 1 2 p w 2 p 2 2 c p a 2 n c in particular theorems 5 1 and 5 3 ensure that when a 1 22 the statistical error in the approximation of upw respectively of ups 1 2 p w by u k respectively u ks 1 2 k dominates the numerical error so with high probability the additional computational resources used to achieve this small numerical error have been squandered remark 5 5 in practice for the finite sample case the constants in theorem 5 1 cannot be determined prior to computation upper bounds for certain constants can be given but they are typically far from sharp see tang et al 2017 nevertheless we can suggest a large sample rule of thumb namely under our model assumptions choosing to be of just slightly smaller order than 1 a 2 allows us to account for the unknown constants thus a potential heuristic is to let be approximately 1 log log n a 2 that is we simply want to be of just smaller order than 1 a 2 and the log log n factor is but one of many that would allow us to achieve this from the point of view of implementation the reader may wonder how to impose a tolerance of order 1 a 2 without actually calculating the spectral norm of a itself to address this note that the maximum absolute row sum of a a is both inexpensive to compute and serves as an upper bound for the spectral norm so 1 a can be employed as a conservative tolerance for the computation of the largest magnitude eigenvalue of a once this top eigenvalue is computed subsequent computations can proceed with 1 a 2 as the tolerance 6 simulations in our simulations we examine the impact of optimal stopping for in ference in statistical networks particularly spectral decompositions of adjacency matrices for 8 random dot product graphs rdpgs rdpgs are independent edge random graphs in which each vertex has an associated latent position which is a vector in some fixed finite dimensional euclidean space the probability of a connection between two vertices is given by the inner product of their latent position vectors random dot product graphs are an example of the more general latent position random graphs hoff et al 2002 and as we delineate below the popular stochastic block model holland et al 1983 can be interpreted as an rdpg formally definition 6 1 random dot product graph rdpg let be the subset of rd such that for any two elements x 1 x 2 x 1 x 2 0 1 let x x 1 xn be a n d matrix whose rows are elements of suppose a is a random adjacency matrix given by p a x i j x i xj aij 1 x i xj 1 aij we then write a rdpg x and say that a is the adjacency matrix of a random dot product graph with latent position x of rank at most d when a rdpg x it is necessarily an lpsm matrix as described previously given x the probability pij of adjacency between vertex i and j is simply x i xj the dot product of the associated latent positions xi and xj we define the matrix p pij of such probabilities by p xx we will also write a bernoulli p to represent that the existence of an edge between any two vertices i j where i j is a bernoulli random variable with probability pij edges are independent we emphasize that the graphs we consider are undirected and have no self edges a vital inference task for rdpgs is the estimation of the matrix of latent positions x from a single observation of a suitably large adjacency matrix a there is a wealth of recent literature on the implications of latent position estimation for vertex classification multisample network hypothesis testing and multiscale network inference see for example athreya et al 2016 lyzinski et al 2014 2017 sussman et al 2012 tang et al 2017 for clarity in the case of simulations we focus on the stochastic block model a graph in which vertices are partitioned into k separate blocks and the probability of a connection between two vertices is simply a function of their block memberships thus a stochastic block depends on a vector of vertex to block assignments and a k k block probability matrix b whose entries brs give the probability of connection between any vertex in block r and any vertex in block s a stochastic block model whose block probability matrix is positive semidefinite can be regarded as a random dot product graph whose latent position matrix has k distinct rows and the spectral decomposition methods we apply to random dot product graphs can be used to infer b and for a stochastic block model as well sussman et al 2012 in the case of real data we address community detection for a subset of the youtube net work drawn from the stanford network analysis project snap 1 our code for simulations can be found at www cis jhu edu parky tolerance tolerance html first we investigate stopping criteria for eigendecompositions of stochastic block model 1 for the particular youtube data on which this is based see the stanford network analysis project at http snap stanford edu data index html 9 graphs in figure 6 1 panel a we generate several instantiations of stochastic block model graphs each with n 900 vertices and equal size blocks and 3 3 block probability matrix whose entries are 0 05 on the diagonal and 0 02 on the off diagonal we recall that the procrustes error between two n d matrices x and y is given by min o od d x y o f where od d denotes the collection of d d orthogonal matrices with real entries that is the procrustes distance between the two matrices is zero if there exists an orthogonal transfor mation of the row vectors that renders one matrix equal to the other the non identifiability inherent to a rdpg namely the fact that xw xw xx makes such a procrustes measure the appropriate choice when inferring latent positions associated to random dot prod uct graphs since they can only be inferred up to an orthogonal transformation using the irlba algorithm to compute the first three eigenvectors of the associated ad jacency matrices we plot the average procrustes error with standard error bars between the top three estimated eigenvectors and the true eigenvectors of the associated p matrix i e p zbz where z is the n 3 matrix of block assignments we observe that when the numerical tolerance is of order 2 6 this error stabilizes note that when n 900 1 log log n n 2 6 the default tolerance used in this implementation of irlba how ever is 10 6 which is roughly 2 20 and stopping at our specified tolerance reduces the number of iterations by a factor of 2 without negatively affecting the results for much larger stochastic block models the corresponding computational reduction can be substantial in figure 6 1 panel b we repeat this procedure for a stochastic block model with n 9000 vertices and several different block probability matrices b which are scalings of one another observe that our stopping criterion of 1 log log n n is appropriate across the whole range of these different block probability matrices saving computation without degrading the results finally we consider the impact of our stopping criterion on the inference task of community detection in a youtube network youtube networks generally exhibit hierarchical community structure see lyzinski et al 2017 and can be modeled by an sbm rdpg with a potentially large number of blocks for the task of community detection in a youtube network we wish to cluster the rows of the matrix of eigenvectors associated to the adjacency matrix of a youtube network we use k means clustering with silhouette width rousseeuw 1987 the latter of which is a measure of the rectitude of any particular clustering to determine the optimal number of clusters the youtube network we examine has 1 134 890 nodes and 2 987 624 edges and the spectral decomposition of the adjacency matrix proceeds via irlba with zhu and ghodsi 2006 used to choose the estimated embedding dimension of d 26 for each k 1 2 and corresponding tolerance 2 k irlba generates approximations s k for the the top d 26 eigenvalues of a and u k for the matrix of associated eigenvectors next we use k means clustering with silhouette width to determine the optimal number of clusters to cluster the rows of the matrix u k we conduct this clustering procedure again now computing the spectral decomposition with the default tolerance in irlba of 10 6 ten iterations are run for each of the above clustering methods and the top plot of figure 6 2 shows the mean adjusted rand index ari plotted with standard error for these two clusterings we see 10 0 9 1 0 1 1 1 2 1 3 1 2 3 4 5 e rr o r ite ra tio n 5 10 15 20 tolerance 2 k a n 9000 1 2 1 4 1 6 1 8 2 0 0 10 20 30 40 e rr o r ite ra tio n 5 10 15 20 tolerance 2 k b b b 150 b 160 b 170 b 180 b 190 b 200 b 210 b 220 b 230 b 240 b 250 b figure 6 1 panel a top plot represents average procustes distance between top three approx imate eigenvectors of the adjacency matrix and the associated eigenvectors of the probability matrix for a three block stochastic block model plotted against the numerical error tolerance for the eigendecomposition bottom plot shows number of iterations panel b top plot is average procrustes distance with error bars between approximate eigenvectors of a and the associated eigenvectors of p for sbms with different b which are scalings of one another by a factor b plotted against the numerical error tolerance for the eigendecomposition our heuristic of 1 log log n n for optimal tolerance performs well across a range of parameters bottom plot shows number of iterations that at tolerance 2 10 0 00097 the ari is nearly 1 morever the ari is very close to 0 95 by tolerance 2 7 our heuristic of 1 log log n n for n 1134890 yields a tolerance of 0 000356 all of these tolerances 2 7 2 10 3 5 10 4 are of significantly larger magnitude than the default tolerance in irlba which for this implementation is 10 6 and in some other implementations can be 10 5 this suggests that stopping earlier can save computational time without negatively impacting subsequent inference in the second plot from the top in figure 6 2 we see how the behavior of the ari for successive pairs 2 k 2 k 1 of tolerances the bottom two plots in figure 6 2 indicate the number of iterations and the elapsed time in seconds required at each tolerance 7 acknowledgments the authors thank john conroy donniell fishkind and joshua cape for helpful comments as well as the anonymous referees whose suggestions improved the manuscript this work is partially supported by the d 3 m xdata graphs and simplex programs of the defense advanced research projects agency darpa references m arioli j w demmel and i s duff solving sparse linear systems with sparse backward error siam journal on matrix analysis and applications 10 165 190 1989 11 0 7 0 8 0 9 1 0 0 00 0 25 0 50 0 75 1 00 5 10 15 50 100 150 200 250 a ri 1 a ri 2 ite ra tio n e tim e se c 5 10 15 20 tolerance 2 k figure 6 2 ari for comparison of clusterings of the youtube network with default tolerance in irlba and with tolerance 2 k top plot ari comparing tolerance of 2 k and 2 k 1 second plot and the number of iterations and time in seconds third and fourth plots our heuristic for the optimal tolerance is 1 log log n n 0 00035 2 11 5 in this case the ari is already very close to 0 95 by tolerance 2 7 which is significantly larger than the default tolerance 10 6 in irlba m arioli i s duff and d ruiz stopping criteria for iterative solvers siam journal on matrix analysis and applications 13 138 144 1992 a athreya v lyzinski d j marchette c e priebe d l sussman and m tang a limit theorem for scaled eigenvectors of random dot product graphs sankhya a 78 1 18 2016 j baglama d calvetti and l reichel irbl an implicitly restarted block lanczos method for large scale hermitean eigenproblems siam journal on scientific computing 24 1650 1677 2003 s boucheron g lugosi and p massart concentration inequalities using the entropy method annals of probability 31 1583 1614 2003 fan chung a grigoryan and s t yau upper bounds for eigenvalues of the discrete and continuous laplace operators advances in mathematics 117 165 178 1996 m fiedler algebraic connectivity of graphs czechoslovak mathematical journal 23 2 298 305 1973 m r hestenes and e l stiefel methods of conjugate gradients for solving linear systems journal of research of the national bureau of standards 49 409 436 1952 n j higham accuracy and stability of numerical algorithms society for industrial and applied mathematics second edition 2002 p d hoff a e raftery and m s handcock latent space approaches to social network 12 analysis journal of the american statistical association 97 460 1090 1098 2002 p w holland k laskey and s leinhardt stochastic blockmodels first steps social networks 5 2 109 137 1983 w kahan inclusion theorems for clusters of eigenvalues of hermitian matrices technical report computer science department university of toronto 1967 v lyzinski d l sussman m tang a athreya and c e priebe perfect clustering for stochastic blockmodel graphs via adjacency spectral embedding electronic journal of statistics 8 2905 2922 2014 v lyzinski m tang a athreya y park and c e priebe community detection and classification in hierarchical stochastic blockmodels ieee transactions on network science and engineering 4 13 26 2017 r i oliveira concentration of the adjacency matrix and of the laplacian in random graphs with independent edges http arxiv org abs 0911 0600 2009 j l rigal and j gaches on the compatibility of a given solution with the data of a linear system journal of the acm 14 543 548 1967 k rohe s chatterjee and b yu spectral clustering and the high dimensional stochastic blockmodel annals of statistics 39 1878 1915 2011 p rousseeuw silhouettes a graphical aid to the interpretation and validation of cluster analysis journal of computational and applied mathematics 20 53 65 1987 g w stewart matrix algorithms volume ii eigensystems society for industrial and applied mathematics 2001 d l sussman m tang d e fishkind and c e priebe a consistent adjacency spectral embedding for stochastic blockmodel graphs journal of the american statistical associa tion 107 499 1119 1128 2012 m tang a athreya d l sussman v lyzinski and c e priebe a semiparametric two sample hypothesis testing for random dot product graphs journal of computational and graphical statistics 26 344 354 2017 j a tropp user friendly tail bounds for sums of random matrices foundations of compu tational mathematics 12 389 434 2012 m zhu and a ghodsi automatic dimensionality selection from the scree plot via the use of profile likelihood computational statistics and data analysis 51 918 930 2006 appendix a proofs of results a 1 proofs of theorems 3 1 and 3 3 in this section we prove our results about the statistical error incurred when using the exact eigendecomposition of the observed matrix a to approximate the eigendecomposition of its mean matrix p lemma a 1 suppose a is an lpsm matrix and suppose assumption 2 2 let c 0 and let 0 satisfy n c 1 2 there is a constant c such that for large enough n p a p c n log n 2 proof we argue as in oliveira 2009 define for all 1 i j n the order n symmetric matrices eij eie t i if i j eie j eje i otherwise 13 http arxiv org abs 0911 0600 where ei are the standard unit vectors then naturally a i j aijeij and likewise for p so a p i j a p ijeij i j ij the matrices ij are independent with mean zero and clearly satisfy the bound ij 2 eij 2 the latter equality following since the eigenvalues of eij all belong to 1 0 1 and eij 6 0 since the ij are symmetric we see that e ij ij e 2 ij 2 ije 2 ij 2 iieii if i j 2 ij eii ejj otherwise this gives i j e 2 ij 2 max i j 2 ij 2 n the last equality following from the definition in equation 2 1 applying the matrix bernstein inequality see theorem 1 6 tropp 2012 shows that for all t 0 p a p 2 t 2 n exp t 2 2 2 n t 3 choosing t 1 3 2 n log n c n log n in the above completes the proof lemma a 2 let a be an lpsm matrix and suppose assumption 2 2 holds for c 0 n c 1 2 there is a positive constant c such that for large enough n p uau a upu p 2 c log n n 2 let w 1 w 2 be the singular value decomposition of u p ua and let w w 1 w 2 then there is a constant c such that for large enough n p u p ua w f c log n n 2 there is a constant c such that for large enough n p u p a p up f c log n there are constants c 1 c 2 c 3 0 such that for large enough n p w sa spw f c 1 log n 3 p w s 1 2 a s 1 2 p w f c 2 log n n 1 2 3 p s 1 2 p w w s 1 2 a f c 3 log n n 3 2 3 the proofs of these lemmas follow immediately from the proofs of prop 16 and lem 17 in lyzinski et al 2017 so we omit them here 14 lemma a 3 let a be an lpsm matrix and suppose assumption 2 2 let c 0 n c 1 2 then there is a sequence n o log n n such that for large enough n with probability at least 1 3 there exists a rotation matrix w such that x xw f a p ups 1 2 p f n this lemma follows as in theorem 18 of lyzinski et al 2017 lemma a 4 let a be an lpsm matrix and let v ups 1 2 p if assumption 2 2 holds and for c 0 n c 1 2 then there is a constant c 0 such that for large enough n p a p v f c p c log n n 4 where c p 2 e a p v 2 f 1 proof let a be an arbitrary symmetric matrix with all entries in the intervals ij ij and for each 1 r s n define zrs a r s p v 2 f where a r s agrees with a in every entry except the r s and s r th ones where it equals a rs expanding the formula for z a p v 2 f and using symmetry of a p we see that z a p v 2 f i j k a p ik a p i vkjv j i k a p ik a p i v v t k since all terms which do not feature ars remain the same when a r s is introduced we see that for r 6 s z zrs 2 a p rs a p r v v t s a p rs a r s p r v v t s 2 a p sr a p s v v t r a p sr a r s p s v v t r 2 a a rs a p v v t rs a p v v t sr a p rs v v t ss v v t rr now since a a rs and a p rs we have for r 6 s z zrs 2 16 2 a p v v t 2 rs a p v v t 2 sr 2 v v t 2 ss v v t 2 rr and for r s we obtain z zrr 2 8 2 a p v v t 2 rr 2 v v t 2 rr summing over r and s we see that r s z zrs 2 16 2 r s a p v v t 2 rs 8 4 r v v t 2 rr 16 4 r s v v t 2 ss v v t 2 rr 16 2 a p v v t 2 f 8 4 r s v v t 2 rr v v t 2 ss 16 2 z v 22 16 4 n diag v v t 2 f 16 2 d p z 16 4 nd d p 2 15 since v v t ups 1 p u t p applying theorems 5 and 6 in boucheron et al 2003 we get the following bound for t 0 a 16 2 d p and b 16 4 nd d p 2 a 1 p z e z t 2 exp t 2 4 ae z 4 b 2 at note that there exist constants ca cb 0 such that a ca n and b cb n we expand e a p v 2 f e tr v t a p 2 v tr v te a p 2 v and observe that e a p 2 ij e k a p ik a p kj k 2 ik if i j 0 otherwise since i 6 j implies that a p ik a p kj are independent and both have mean zero in the case i j this is simply the definition of the entrywise variances then from assumption 2 2 we see that in the positive semidefinite ordering c 1 ni e a p 2 c 2 ni which means that c 2 n v 2 f c 2 p c 1 n v 2 f but since v ups 1 2 p and v 2 f s 1 2 p 2 f d i 1 i p 1 n 1 by virtue of assumption 2 2 which gives c 2 p 1 finally returning to equation a 1 and choosing t c log n n we obtain p z c 2 p t 2 establishing the required bound is now a matter of straightforward algebra we now prove theorem 3 3 proof from lemma a 3 with probability at least 1 3 x xw f a p v f o log n n and by lemma a 4 with probability at least 1 4 a p v f c p o log n n therefore with probability at least 1 7 x xw f c p o log n n we now prove theorem 3 1 16 proof we begin by computing ua uas 1 2 a s 1 2 a x s 1 2 a ups 1 2 p w s 1 2 a x xw s 1 2 a upw ups 1 2 p w s 1 2 a s 1 2 p w x xw s 1 2 a then ua upw f x xw s 1 2 a f s 1 2 p w s 1 2 a s 1 2 p w f p 2 s 1 2 p w w s 1 2 a f expanding x xw s 1 2 a 2 f we see that x xw s 1 2 a 2 f i j x xw 2 i j j a so a 2 x xw 2 f 1 a x xw s 1 2 a 2 f x xw 2 f d a taking square roots and applying the bounds 1 a p 2 a p 2 2 p 2 d a d p a p 2 d p yields the stated inequalities under assumption 2 2 we see that p 2 d p n from lemma a 1 a p 2 o n log n from lemma a 2 s 1 2 p w w s 1 2 a f o log n n 3 2 and from theorem 3 3 x xw f 1 then the lower and upper bounds are respectively 1 n o log n n and 1 n o log n n both of which are 1 n which completes the proof a 2 proof of thm 4 2 we make use of the following known results on matrix decom positions theorem a 5 stewart 2001 theorem 4 2 15 let the hermitian matrix a have the spec tral representation a jlj ymy where the matrix j y is unitary let the orthonor mal matrix z be of the same dimensions as j and let n be a hermitian matrix and let m and n denote the spectra of m and n let denote the minimum distance between any element in m and any element in n and suppose 0 then a 3 sin r j r z f az zn f where r j and r z denote the eigenspaces of the matrices j and z respectively and r j r z denotes the diagonal matrix of canonical angles between them theorem a 6 kahan 1967 let a cn n and b cd d each be hermitian let h cn d have orthonormal columns then to the eigenvalues 1 2 l of b there correspond l eigenvalues 1 2 l of a such that i i az zn 2 17 we now prove thm 4 2 proof suppose c 1 n 1 a d a c 2 n for some c 1 c 2 0 and c 3 n d 1 a n a let u k s k be approximate matrices of eigenvectors and eigenvalues for a with the diagonal entries of sa and s k nonincreasingly ordered suppose 1 c 4 n where c 4 c 3 c 1 c 3 we first show that if equation 4 2 holds with such then s k correctly estimates the top d eigenvalues of a to see why note that theorem a 6 guarantees that there exist eigenvalues i 1 a id a of a such that a 4 j s k ij a au k u ks k 2 a 2 where the latter inequality follows by eq 4 2 but if ij d for some j then j s k ij a a 2 c 3 n c 1 c 3 n c 4 n by the bound on c 4 so j s k 1 1 c 4 n and s 1 k 2 which contradicts eq 4 2 now by eq a 4 since 1 ij d we observe that s k sa 2 max j j s k j a max j j s k ij a au k u ks k 2 a 2 which proves the first statement we now show that there is a constant c 0 and orthogonal matrix w such that u k uaw f c since we have shown that for j d j s k j a a 2 c 2 n c 1 c 3 n we see that for any r d j s k r a c 2 n c 1 c 3 n c 3 n c 2 n c 4 n which means that minj d r j s k r a c 2 n c 4 n by theorem a 5 sin r ua r s k f au k u ks k f d au k u ks k 2 a 2 a 2 d c 1 n c 2 n c 4 n which means this bound also holds for uau a u ku k f by rohe et al 2011 proposition 2 1 there is an orthonormal matrix w such that a 5 uaw u k f 2 d c 1 n c 2 n c 4 n and since c 2 n c 4 n c 2 2 n once n is large enough this upper bound is at most 2 2 d c 1 c 2 c a 3 proofs of theorems 5 1 and 5 3 we decompose the total error as a sum of statis tical and numerical error our choice of guarantees that the statistical error is the dominant term proof throughout this proof whenever we bound terms involving a we work on the set of high probability where the various supporting bounds hold from the proof of theorem 3 1 ua upw f x xw s 1 2 a f n and using equation a 2 x xw 2 f 1 a n ua upw 2 f x xw 2 f d a n 18 so applying theorem 3 3 gives us c p a 2 n ua upw f c p d a n where n o log n n and the lower bound still holds if we replace w for any other w then for some choice of w and w 1 we see that u k upww 1 u k uaw 1 uaw 1 upww 1 so if we let w 1 be the closest orthogonal matrix to u a u k and w be the closest orthogonal matrix to u p ua then min w u k upw f u k uaw 1 f uaw 1 upw w 1 f c c p d a n arguing similarly we have for any w u k upww 1 f uaw 1 upww 1 f u k uaw 1 f c p a 2 n c note that the set ww 1 w is orthogonal w w is orthogonal so this lower bound holds for all w as soon as o 1 n since n o log n n is o 1 n the term c p a 2 becomes the dominant term so further reduction in makes no difference to the order of the lower bound now we address the scaled case let w 1 be the closest orthogonal matrix to u a u k as before and let w be the closest orthogonal matrix to u p ua as usual we see that u ks 1 2 k upspw w 1 u ks 1 2 k uas 1 2 a w 1 uas 1 2 a w 1 ups 1 2 p w w 1 where the last two terms are just x xw w 1 for some orthogonal matrix w from thm 4 2 we know that u a u k w 1 f u k uaw 1 f c then since saw 1 w 1 s k sau a u k sa w 1 u a u k w 1 s k u aau k sa w 1 u a u k w 1 s k u a u k w 1 s k u a au k u ks k sa w 1 u a u k we see that saw 1 w 1 s k f c s k 2 sa 2 a 2 which is further bounded by a 2 2 c c 1 arguing as in lemma a 2 we have s 1 2 a w 1 w 1 s 1 2 k f a 2 2 c c 1 d a 2 d a a 2 c a 2 we compute u ks 1 2 k uas 1 2 a w 1 u ks ks 1 2 k uas 1 2 a w 1 au ks 1 2 k u ks k au k s 1 2 k uas 1 2 a w 1 auaw 1 s 1 2 k a u k uaw 1 s 1 2 k u ks k au k s 1 2 k uas 1 2 a w 1 uas 1 2 a s 1 2 a w 1 w 1 s 1 2 k s 1 2 k a u k uaw 1 s 1 2 k u ks k au k s 1 2 k 19 which leads to u ks 1 2 k uas 1 2 a w 1 f a 1 2 2 s 1 2 k 2 s 1 2 a w 1 w 1 s 1 2 k f a 2 s 1 2 k 2 u k uaw 1 f s 1 2 k 2 au k u ks k f from thm 4 2 s 1 2 k 2 o 1 a 2 so from the bounds proved above and the stopping criterion we have for some constant c u ks 1 2 k uas 1 2 a w 1 f c a 2 returning to the bounds on the total error we have u ks 1 2 k upspw w 1 f p 2 x xw f p 2 c a 2 p 2 since from theorem 3 3 x xw f c p n where n o log n n we see that u ks 1 2 k upspw w 1 f p 2 c p p 2 n c now since a 2 p 2 a p 2 once n is large enough we have a 2 p 2 a p 2 2 p 2 2 p 2 and a 2 p 2 a p 2 p 2 1 2 p 2 which yields the final inequalities arguing as we did for u k upw f and from theorem 3 3 the lower bound holds for any w 20 1 introduction 2 setting and notation 3 statistical error bounds for eigendecompositions of lpsm matrices 4 numerical error bounds for eigendecompositions of lpsm matrices 5 optimal numerical tolerance 6 simulations 7 acknowledgments appendix a proofs of results a 1 proofs of theorems 3 1 and 3 3 a 2 proof of thm 4 2 a 3 proofs of theorems 5 1 and 5 3