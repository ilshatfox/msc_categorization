sharper lower and upper bounds for the gaussian rank of a graph emanuel ben david department of statistics columbia university abstract an open problem in graphical gaussian models is to determine the smallest number of observations needed to guarantee the existence of the maximum likelihood estimator of the covariance matrix with probability one in this paper we formalize a closely related problem in which the existence of the maximum likelihood estimator is guaranteed for all generic observations we call the number determined by this problem the gaussian rank of the graph representing the model we prove that the gaussian rank is strictly between the subgraph connectivity number and the graph degeneracy number these bounds are in general much sharper than the best bounds known in the literature and furthermore computable in polynomial time 1 introduction an open problem in graphical gaussian models is to determine the smallest number of ob servations needed to guarantee the existence of the maximum likelihood estimator mle of the covariance matrix this problem first arose in the dempster s paper 4 and has been frequently brought to attention by steffen laurtizen as evident in 2 and lauritzen s lec tures in durham symposium on mathematical aspects of graphical models 2008 hence we refer to this problem as the d l problem using the initials of dempster and lauritzen to well pose this problem it is necessary to exclude some observations from a set of at least zero probability for this purpose we can formalize the d l problem in its most general form as follows d l problem i for a given graphical gaussian model with respect to a graph g deter mine the smallest number of observations needed to guarantee the existence of the mle of the covariance matrix with probability one a slightly different well posed formalization of the d l problem is to require the exis tence of the mle of the covariance matrix henceforth for all generic observations in the ehb 2126 columbia edu 1 ar x iv 1 40 6 47 77 v 2 m at h s t 4 a ug 2 01 4 following sense the observations 1 x 1 xn are said to be generic if each n n principal submatrix of the sample covariance matrix s 1 n n i 1 xix i is non singular note that with probability one every n observations are generic thus the set of non generic observations has zero probability the problem is now formalized as follows d l problem ii for a given graphical gaussian model with respect to a graph g deter mine the smallest number of observations needed to guarantee the existence of the mle for all generic observations the number determined by the d l problem ii is said to be the gaussian rank of g denoted by r g the primary goal of this paper is to obtain sharp lower and upper bounds on r g in parallel to the d l problem ii the number determined by the d l problem i is said to be the weak gaussian rank of g denoted by rw g note again that being in general position is a generic property of any n observations that is with probability one for every n observations each n n principal submatrix of the sample covariance matrix is in non singular this obviously implies that r g is an upper bound for rw g as we mentioned earlier the d l problem first arose in 4 a celebrated work of dempster in which under the principle of parsimony in fitting parametric models dempster introduced graphical gaussian models and gave a partial analytic expression for the mle of the covariance matrix assuming that the mle exists in practice the mle has to be computed by an iterative numerical procedure such as the iterative proportional scaling procedure ips 15 however if the number of observations is not sufficiently large there is no guarantee that the output matrix will indeed correspond to the mle as it might not be positive definite both formalizations i and ii of the d l problem are important because in some sense their answers give us a better understanding of sparse graphs even when the main concern is not the mle moreover knowing exactly or closely enough how many observations are needed to estimate the covariance matrix is useful in applications of graphical gaussian models in situations where the sample size calculation is a very important aspect of the study for example in genetics and medical imaging in a graphical gaussian model the sparsity is given by a pattern of zeros in the inverse covariance matrix the pattern of zeros is determined by the missing edges of a graph and each zero entry of a generic inverse covariance matrix indicates that the corresponding pair of variables are conditionally independent given the rest of variables an attractive feature of graphical gaussian models or graphical models in general is that if the graph representing the model is sparse then the mle of the covariance matrix can exist even if the number of observations is much smaller than the dimension of the multivariate gaussian distribution intuitively we expect that the guassian rank of the graph the number determined by the d l problem to decrease as the graph becomes sparser however it is not clear what best measures the sparsity of a graph or how the gaussian rank varies accordingly the main theorem of this paper suggests two such measures of sparsity despite some efforts since 90 s not much progress has been made to resolve the d l problem the existing results are limited to a handful of publications as follows 1 throughout the paper we always assume that the observations are independent and identically distributed 2 r 1 7 for a decomposable graph g a graph that has no induced cycle of length larger than or equal to four the gaussian rank is equal to g the size of the largest complete subgraph of g r 2 7 for every graph g g rw g r g tw g 1 where tw g denotes the treewidth of g please see appendix a for the definition r 3 2 for cp a cycle of length p 3 2 cp rw cp r cp 3 tw cp 1 r 4 16 for g 3 3 the 3 3 grid 2 g 3 3 rw g 3 3 r g 3 3 3 tw g 3 3 1 4 in the literature the bounds given by r 2 are currently the best known bounds for the guassian rank restricted to the class of decomposable graphs these bounds are tight since tw g 1 g for a decomposable graph g but we may note that r 4 in 16 shows that for non decomposable graphs the bounds in r 2 are not necessarily tight intuitively it is apparent that g overestimates and tw g underestimates the sparsity of g and therefore sharper bounds may exist in fact in this paper we give much sharper bounds on the gaussian rank the lower and upper bounds we give are the subgraph connectivity number denoted by g and the graph degeneracy number denoted by g both these bounds are well known in graph theory formally we prove the following theorem theorem 1 1 let g v e be a graph then g 1 r g g 1 1 1 all the results stated in r 1 through r 4 now immediately follow from theorem 1 1 since by some simple calculations we can show that a for a decomposable graph g g g 1 g 1 tw g 1 b for any arbitrary graph g g g 1 r g g 1 tw g 1 c for a cycle of any length g g g 2 d for a k m grid with k and m 2 g g 2 note that by part d the guassian rank of every grid is 3 which is substantially less than the upper bound given by r 2 for grids of large dimensions the reason is that the treewidth of a k m grid is min k m which tends to as k and m 10 the organization of the paper is as follows in section 2 we review some basic notation and terminology in graphical models and matrix algebra and explain how the maximum likelihood problem for graphical guassian models leads to the d l problem in this section we also discuss the concept of in general positions for vectors and matrices which is a fundamental concept in subsequent sections in section 3 we give an alternative description of the gaussian rank which we then use to derive some of the properties of the gaussian rank in section 4 we prove theorem 1 1 the proof of the upper bound is based on two key observations that if a vertex v is removed from a graph g then 1 the gaussian rank of the resulting graph is 3 at most one less than r g 2 the gaussian rank of the resulting graph remains equal to r g if it is larger than the number of the vertices adjacent to v the proof of the lower bound is based on the concept of orthogonal representations of graphs and a theorem in 12 in section 5 we apply theorem 1 1 to some special graphs to exactly determine their gaussian ranks these graphs include symmetric graphs and random graphs we also obtain a tight numerical upper bound for the gaussian ranks of planar graphs 2 preliminaries in this section we establish some necessary notation terminology and definitions in graph ical models matrix algebra and geometric graph theory we also carefully explain how the maximum likelihood problem for graphical guassian models leads to the d l problem 2 1 graph theoretical notion definitions our notation presented here closely follows the notation established in 14 and 11 let g v e be an undirected graph where v v g is the vertex set and e e g is the edge set of g each edge e e is an unordered pair ij i j where i j v for ease of notation as in 16 we assume that e contains all the self loops that is i i for every i v otherwise stated we always assume that v g 1 p p 1 for a vertex v v the set and the number of vertices adjacent to v are denoted by ne v and degg v 2 a graph h is a subgraph of g denoted by h g if v h v g and e h e g 3 for a set v v the graph defined by g v v e v v is said to be the subgraph of g induced by v 4 for a vertex v v the induced subgraph g v v is denoted by g v note that g v is simply obtained by removing v and its incident edges from g 5 for an edge e e g e denotes the subgraph obtained by removing e from g that is g e v e e 2 2 graph parameters let g denote the set of all graphs a graph parameter is a function g r g 7 g in words a graph parameter is a function that assigns a number to each graph in graph theory two common graph parameters are a g min degg v v v said to be the minimum degree of g b g min s s v such that g v s is disconnected said to be the vertex connectivity number of g in words g is the smallest number k of vertices whose deletion separates the graph or makes it trivial now for every graph parameter we can define a new graph parameter as g max h h g 4 two such defined graph parameters are g and g known as the graph degeneracy number and the subgraph connectivity number of g respectively 14 note that g is the smallest number of vertex deletions sufficient to disconnect each subgraph of g since g g we have g g see part a in remark 2 1 2 1 a b c figure 1 denoted graphs are a a 4 5 grid g 4 5 b a cycle of length 8 c 8 c a complete 4 3 bipartite graph k 4 3 example 2 1 consider the graphs given by figure 1 a 1 b and 1 c a the graph denoted by g 4 5 in figure 1 a is a 4 5 grid it is easy to see that g 4 5 g 4 5 2 in fact for every k m grid with k and m 2 gk m gk m 2 b the graph denoted by c 8 in figure 1 b is a cycle of length 8 with c 8 c 8 2 this is obviously true for any arbitrary cycle cp with length p 3 c the graph denoted by k 4 3 in figure 1 c is a complete 4 3 bipartite graph in general a complete k m bipartite graph is a graph so that its vertex set can be partitioned into two sets with k and m elements such that two vertices are adjacent if and only if they are in different partitions of v one can check that kk m kk m min k m 14 remark 2 1 these simple facts can be found in 14 a if two graph parameters are such that 1 h 2 h for every subgraph h g then 1 g 2 g b if a graph parameter is non decreasing that is h g h g then g g for example for this reason g g and tw g tw g c for every graph g we have g g 1 g 1 tw g 1 in light of a and b this implies that g g 1 g 1 tw g 1 d there are algorithms specifically given in 14 that compute g and g re spectively in no more than o v 3 2 e 2 and o e steps in contrast clique number and treewidth cannot be computed in polynomial time 5 2 3 matrix algebra notation definitions the notation presented here closely follows the notation established in 17 and 14 for a vector u ui i v rv and v let u denote the subvector ui i r for a matrix a ai j rv v and and v let a denote the submatrix ai j i j r a principal submatrix a is simply denoted by a some other notations used throughout the paper are as follows 1 the set of p p symmetric matrices is denotes by s p 2 the set of symmetric positive semi definite matrices is denoted by s p 3 the set of symmetric positive definite matrices is denoted by s p 4 the set of matrices a s p of a fixed rank d is denoted by s p d 5 a 0 or a 0 denotes a is a positive semidefinite or positive definite matrix without specifying its dimension 6 the moore penrose generalized inverse of a is denoted by a note that a aa a and aa a a it is clear that a a 1 when a is non singular 7 let a be partitioned as a a a a a where v then the schur complement of a denoted by a is defined by a a a a a convention for convenience in subsequent sections otherwise stated we always assume that given matrices are non zero 2 4 vectors and matrices in general positions let v 1 vp be p vectors in rd these vectors are said to be in general position if any d vectors among them are linearly independent a similar concept is defined for square matrices a square matrix a of rank d is said to be in general position if every d d principal submatrix of a is non singular lemma 2 1 if p d vectors are randomly selected in rd with a probability distribution dominated by lebesgue measure then with probability one they are in general position in other words lemma 2 1 says that being in general position is a generic property of any p d vectors in rd proof first we note that randomly selecting p vectors in rd is equivalent to randomly se lecting a matrix w rd p let x xi j denote a d p matrix of variables for each i 1 id p we define the polynomial p x det x 1 i 1 x 1 id xd i 1 xd id and v w rd p p w 0 6 each v rd p is an algebraic set and its lebesgue measure is therefore zero thus pr w rd p v 1 this completes the proof since each d columns of a matrix w rd p v are linearly independent the next lemma shows how two concepts of in general positions for vectors and square matrices are related lemma 2 2 let a s p d then a is in general position if and only if there are in general position vectors v 1 vp rd such that a v 1 v p v 1 vp v i vj 1 i j p 2 2 proof suppose a s p d let each pair of 1 u 1 d ud denote a positive eigenvalue and its corresponding eigenvector of a let us set wi iui for i 1 d then we have a d i 1 wiw i w w where w w 1 w d v 1 vp rd p 2 3 note that v 1 vp rd are the columns of w suppose for i 1 id p the vectors vi 1 vid are linearly dependent thus there is a non zero vector x xi 1 xid rd such that i xivi w x 0 0 w w x 0 a x 0 0 thus a x 0 and consequently a is singular suppose v 1 vp rd are in general position let a be defined as in equation 2 2 then for each i 1 id p the submatrix a v i 1 v id vi 1 vid is obviously non singular corollary 2 1 suppose x 1 xn are n observations from a p variate distribution then with probability one the sample covariance matrix s and the sample correlation matrix r are in general positions proof let x x 1 x n y 1 yp rn p 7 be the data matrix then by lemma 2 1 with probability one y 1 yp the columns of x are in general position in rn the result now follows from lemma 2 2 since whenever the columns of x are in general position s 1 n x x is also in general position a useful property of an in general position matrix a s p d is that in some sense it can be extended to another in general position matrix in s p 1 r or s p 1 r 1 the next lemma formalizes this fact lemma 2 3 let a s p d be in general position a then there is a vector w rp such that a ww s p d is in general position this in return implies that the matrix 1 wt w a s p 1 d is in general position b if d p 1 then there is a vector u rp such that a uu s p d 1 is in general position this in return implies that the matrix 1 u u a uu s p 1 d 1 is in general position proof a let d and choose a non zero vector x rd such that x a x 1 we set x x 0 rp and w ax a x a x where v note that a 0 and x ax x a x 1 thus we have 1 w a w 1 x ax 0 and therefore b a ww s p by theorem 1 20 in 17 now we claim that b has rank d and is in general position it suffices to show that b a w w 0 for every set i 1 id v let v then w ax a a x x a x a x thus if we set y x a 1 a x then a y w our claim is established if we can show that 1 w w a 0 or equivalently 1 w a 1 w 1 y a y 0 8 we verify that the right hand side inequality holds by writing y a y x a 1 a x a x a 1 a x x a x 2 x a x x a a 1 a x x a x 2 x a x x a x x a x x x a a a a x x x a x x ax x a x x ax since a 0 and therefore a 0 1 therefore b 0 and rank b d this shows that b a ww s p d is in general position now by using theorem 1 20 and the guttman rank additivity formula in 17 we conclude that 1 w w a s p 1 d is in general position b first note that for a vector u rp rank a uu rank a if and only if u range a otherwise rank a uu rank a 1 9 now it suffices to find a vector u rp range a such that p u det a u u 6 0 for each i 1 id 1 v for this we note that range a and the zeros of the polynomials p x are all algebraic sets rp and therefore have zero measures thus we can choose a vector u in the nonempty set rp range a x p x 0 for this vector the matrix a uu s p d 1 is in general position the rest of the proof is similar to that of part a remark 2 2 as the proof shows part a of lemma 2 3 holds in particular for w ax where x x 0 and x rd satisfies x a x 1 2 5 graphical gaussian models let g v e be a graph with p vertices a gaussian distribution np 0 is said to be markov with respect to g if 1 i j 0 whenever ij 6 e the graphical gaussian model over g denoted by n g is the family of np 0 markov with respect to g let zg a s p ai j 0 for each ij e and pg zg s p then zg is an e dimensional linear space and pg is an open convex cone in zg note that pg is in fact the set of inverse covariance matrices for the graphical gaussian model n g 9 2 6 the maximum likelihood problem suppose x 1 xn are n observations taken from np 0 n g then the mle of is the solution to the following optimization problem 8 argmax n 2 log det n 2 tr 1 s subject to 1 i j 0 ij 6 e 0 recall that s 1 n n i 1 xix i is the sample covariance matrix in terms of the inverse co variance matrix 1 this optimization problem can be recast as the convex optimization problem argmin n 2 log det n 2 tr s 2 4 subject to pg a standard optimization procedure such as in 8 or 3 shows that the dual problem of 2 4 solves the following concave optimization problem argmax log det subject to i j si j ij e 0 2 5 which has a unique solution if and only if the feasible set determined by equation 2 5 is non empty note that the existence of a matrix that satisfies the condition given by equation 2 5 is essentially a positive definite completion problem for the future reference we record the conclusion as follows proposition 2 1 suppose x 1 xn are n observations taken from np 0 n g then the mle of exists if and only if there is a matrix p s p such that pi j si j for every ij e 3 the gaussian rank of a graph in this section we give another description for the gaussian rank of a graph using its unique property with respect to the positive definite completion problem given by equation 2 5 an advantage of this alternative description is that more easily can be verified and thus used to derive the properties of the gaussian rank as a graph parameter 3 1 an alternative description of the gaussian rank let g v e be a graph two matrices a b rp p are said to match on g denoted by a g b if ai j bi j for each ij e in other words two matrices match on g if 10 their euclidean projection onto zg are identical the matching relation g is transitive and invariant under scaling 2 more precisely a g b and u g v a u g b v 3 1 a g b dad g dbd for every diagonal matrix d 3 2 let us recall that by definition r g is the smallest number r with the property that for every observations x 1 xr if the sample covariance matrix s is of rank r and in general position then the mle exists in light of proposition 2 1 an alternative description of r g can be given as follows proposition 3 1 let g be a graph then r g is the smallest number r with the following property a s p r if a is in general position p s p such that p g a remark 3 1 similarly rw g is the smallest number r with the following property almost surely a s p r if a is in general position p s p such that p g a 3 2 some basic properties of the gaussian rank in this subsection we list some basic properties of the gaussian rank as a graph parameter let us keep in mind that r g r for some positive integer r if the condition in proposition 3 1 is satisfied also because of the scale invariant property of matching relation given by equation 3 2 whenever convenient we can assume that a s p r is a correlation matrix that is the diagonals of a are all 1 proposition 3 2 let r r g then for every in general position matrix a s p r there is a matrix p s p such that p g a proof let a r i 1 wiw i be a decomposition of a as in equation 2 3 let us set b r g i 1 wiw i and c r i r g 1 wiw i since a is in general position the matrix b s p r g is also in general position thus there is a matrix q s p such that q g b equation 3 1 now shows a b c g q c s p as desired in relation to the d l problem ii proposition 3 2 is an important property that intuitively seems obvious the reason is that if the mle exists for every generic r g observations then we expect this to be true for every generic r r g observations as well proposition 3 3 let v v be a vertex of g then the following holds a r g v r g r g v 1 b r g r g v 1 if v is adjacent to all other vertices 11 proof first without loss of generality we assume that p 2 and v 1 a let us set r r g the claim is that r g v r since r g v p 1 it suffices to consider the case r p 1 let a s p 1 r be in general position by part a in lemma 2 3 there is a vector w rp 1 such that b 1 w w a s p r is in general position let p s p such that p g b if we set v g v then it is clear that a g v p s p 1 now we show that r g r g v 1 let us set r 0 r g v and let a s p r 0 1 be in general position we assume without loss of generality that a is a correlation matrix if we partition a as a 1 u u b then c b uu 0 by using the guttman rank additivity formula in 17 and the fact that c b u u for each v we conclude that c s p 1 r 0 is in general position therefore there is a matrix q s p 1 such that q g v c let us set p 1 u u q uu s p the transitive property of the matching relation given by equation 3 1 now shows that q uu g v b and therefore p g a b suppose v 1 is adjacent to 2 p as before let us set r r g by part a above r g v r 1 thus it suffices to show that r g v r 1 for this let a s p 1 r 1 clearly it suffices to consider the case r p 1 by part b of lemma 2 3 there is a vector u rp 1 such that the matrix 1 u u a uu s p r is in general position therefore there is a matrix p s p such that p 1 u u q g 1 u u a uu for some matrix q s p 1 thus a uu g v q and consequently a g v q uu s p 1 corollary 3 1 let h be a subgraph of g then r h r g proof every subgraph of g is obtained by removing successively a finite number of vertices and edges from g therefore it suffices to show that r g v r g for each vertex v v and r g e r g for each edge e e the latter is obvious using the condition in proposition 3 1 12 a graph g is said to be the clique sum of two subgraphs g 1 v 1 e 1 and g 2 v 2 e 2 if v v 1 v 2 e e 1 e 2 and g v 1 v 2 is a complete graph including the empty graph we write this as g g 1 v 1 v 2 g 2 the following proposition is now immediate using a standard completion process given by 7 or 11 proposition 3 4 suppose g g 1 v 1 v 2 g 2 then r g max r g 1 r g 2 in particular if g 1 gk are all the connected components of g then r g max r gi i 1 k remark 3 2 in relation to the d l problem i we can show by slightly modifying our proofs and using the alternative description of rw g given by remark 3 1 that the properties of the gaussian rank discussed in this subsection are also valid properties of the weak gaussian rank this fact however will not be used in the paper 4 the proof of theorem 1 1 in this section we prove theorem 1 1 that is the bounds given by equation 1 1 the lower and the upper bounds for r g are proved in separate subsections 4 1 the upper bound r g g 1 first we state and prove the following key lemma lemma 4 1 let v v be a vertex of g if r g v degg v 1 then r g r g v proof if degg 0 then by proposition 3 4 r g max 1 r g v r g v thus we assume that degg v 1 for convenience let us assume that v 1 and ne v 2 degg v 1 note that v g v 2 p now we set r 0 r g v 2 r 0 and v g v note that ne v since r 0 degg v 1 let a s p r 0 we need to show that there is a matrix p s p such that p g a for this we assume that a is a correlation matrix and therefore can be partitioned as a 1 u u b 1 u u u a a u a a where b a a a a note also that u rp 1 and b s p 1 r 0 is in general position let us set x a 1 u now we have 1 u u a 0 and therefore 1 u a 1 u 0 13 this implies that x a x x b x 1 by part a of lemma 2 3 and remark 2 2 if we set w bx then b ww s p 1 r 0 is in general position therefore there is a matrix q s p 1 such that q g v b ww let us set p 1 w w q ww s p by equation 3 1 it is clear that q ww g v b also since w a x u and ne v we have p 1 j a 1 j whenever j ne v therefore p g a we now prove that the upper bound r g g 1 holds proof by mathematical induction let us assume that for any graph h with fewer than p vertices r h h 1 now let g be a graph with p vertices we assume without loss of generality that p 2 let v v such that degg v g on the contrary let us assume that r g g 2 then by part a of proposition 3 3 we obtain r g v r g 1 g 1 degg v 1 lemma 4 1 now implies that r g r g v the induction hypothesis then implies that r g r g v g v 1 the fact that g max g g v 14 implies that r g g 1 remark 4 1 note that lemma 4 1 also holds for the weak gaussian rank that is for any graph g if rw g v degg 1 then rw g rw g v consequently this implies rw g g 1 but the latter more easily follows from the fact that rw g r g g 1 4 2 the lower bound g 1 r g to prove that this lower bound holds we mainly rely on a theorem in 12 this theorem associates the connectivity of a graph with its certain geometric representations more details are as follows let g v e be graph an orthonormal representation of g in rd is a function v rd assigning a unit vector ui to each vertex i v such that u i uj 0 whenever ij 6 e in words assigns to each vertex a unit vector in rd such that the vectors assigned to nonadjacent vertices are orthogonal now a general position orthonormal representation of g in rd is an orthonormal representation in rd such that the p assigned vectors 1 u 1 p up are in general position in rd the next theorem is crucial for proving the lower bound in theorem 1 1 theorem 4 1 lovasz et al 12 13 if g v e is a graph with p vertices then the following are equivalent i g is p d connected ii g has a general position orthonormal representation in rd we now proceed to prove that the lower bound g 1 r g holds 14 proof since r h r g for every subgraph h of g in light of part a and part b in remark 2 1 it suffices to show that for every graph g we have r g g 1 first we set k g and d p k by theorem 4 1 there are in general position unit vectors u 1 up rd such that u i uj 0 for each ij 6 e let u i uj 1 i j p by lemma 2 2 we have s p d is in general position let w 1 wk rp be a basis of null and set a w 1 wk w 1 w k s p k corollary 5 2 in appendix b now implies that the matrix a s p k is in general position now let p s p such that p g a then tr p ij e pi j i j ij e ai j i j tr a 0 note that a 0 this shows that p s p since s p is a self dual cone that is s p q s p tr qb 0 for every b s p 0 and s p 0 thus there is no matrix p s p such that p g a this shows that r g g 1 5 some applications of theorem 1 1 a useful application of the bounds given by theorem 1 1 is that when the lower and upper bounds are equal the guassian rank is exactly determined in this section we briefly discuss for which graphs g g we also use theorem 1 1 to obtain a sharp numerical upper bound for the guassian ranks of the so called planar graphs 5 1 gaussian ranks of symmetric graphs and random graphs let g v e be a graph a permutation v v is said to be an automorphism of g if ij e i j e the graph g is said to be symmetric if for any two edges ij and i j e there is an automorphism of g such that i i and j j please see chapter 27 in 6 for a detailed discussion of symmetric graphs for example one can check that the graphs given by figure 2 a and figure 2 b are symmetric the symmetric property in particular implies that g is regular that is there is a positive integer k such that degg v k for every vertex v v an interesting feature of a symmetric graph g is that g g this consequently implies that g g theorem 1 1 therefore implies that for a symmetric graph the guassian rank is exactly determined as r g g g 5 1 note that equation 5 1 can hold for many non symmetric graphs as well such as the reg ular graph given by figure 2 c or even non regular graphs such as grids to the best our 15 knowledge the class of all graphs g satisfying g g or even g g is not fully characterized in the context of random graphs these graph parameters are identical almost surely for all random graphs to be precise a random graph g is a graph in which the edges are selected by a sequence of independent bernoulli trials with probability 0 1 6 theorem 5 1 bollob s et al almost surely for every random graph g we have g g the next result now follows from this theorem and theorem 1 1 corollary 5 1 almost surely for every random graph g we have r g g g a b c figure 2 denoted graphs in a and b are symmetric the graph given by c is not symmetric in each graph the equality in equation 5 1 holds and the gaussian ranks are easily computed to be 4 4 and 5 respectively 5 2 on the gaussian ranks of planar graphs a planar graph is a graph that can be drawn in a plane without the edges crossing each other in graph theory it is well known that planar graphs are at most 5 degenerate that is g 5 if g is planar theorem 1 1 therefore implies that when g is a planar graph r g 6 note that the upper bound 6 is tight for this consider the planar graph g in figure 3 since g g 5 by equation 5 1 we have r g 6 figure 3 a planar graph with degeneracy number 5 references 1 bollob s b thomason a random graphs of small orders random graphs 83 based on lectures presented at the 1 st pozn n seminar on random graphs north holland 1985 16 2 buhl s l on the existence of maximum likelihood estimators for graphical gaussian models scandinavian journal of statistics 20 1993 263 270 3 dahl joachim and vandenberghe lieven and roychowdhury vwani covari ance selection for nonchordal graphs via chordal embedding optimization methods and software 23 2008 501 520 4 dempster a p covariance selection biometrics 28 1972 157 175 5 diestel r graph theory graduate texts in mathematics springer verlag heidelberg 2010 6 graham r l gr tschel m and lov sz l handbook of combinatorics ii mit press cambridge ma usa 1995 7 grone r johnson c r sa e m and wolkowicz h positive definite completions of partial hermitian matrices linear algebra and its applications 58 1984 109 124 8 hastie trevor tibshirani robert friedman jerome the elements of statisti cal learning data mining inference and prediction springer series in statistics springer verlag new york 2001 9 householder alston the theory of matrices in numerical analysis blaisdell new york 196 springer verlag 2010 10 kloks ton treewidth computations and approximations volume 842 of lecture notes in computer science springer verlag berlin 1994 11 lauritzen steffen graphical models oxford university press 1996 12 lov sz l saks m schrijver a orthogonal representations and connectivity of graphs linear algebra applications 114 115 1989 439 454 13 lov sz l saks m schrijver a a correction orthogonal representations and connectivity of graphs linear algebra applications 313 2000 101 105 14 matula david w subgraph connectivity numbers of a graph in theory and ap plications of graphs lecture notes in mathematics springer verlag berlin heidelberg new york 642 1978 371 383 15 speed t p kiiveri h gaussian markov distribution over fnite graphs annals of statistics 14 1986 138 150 16 uhler caroline geometry of maximum likelihood estimation in gaussian graphical models the annals of statistics 40 2012 238 261 17 zhang fuzhen the schur complement and its applications numerical methods and algorithms springer dordrecht 2005 17 appendix a let g v e be a graph a a tree decomposition of g is a pair d s t with s xi i i a collection of subsets of v and t i f a tree with one vertex for each subset of s such that the following three conditions are satisfied 1 i i xi v 2 for all edges v w e there is a subset xi s such that both v and w are contained in xi 3 for each vertex u the set of vertices i u xi forms a sub tree of t b the width of the tree decomposition s t is tw s t max xi 1 i i c the treewidth of g is tw g min tw s t s t is a tree decomposition of g appendix b lemma 5 1 suppose a b s p then ab 0 if and only if tr ab 0 proof this easily follows from the fact that tr ab tr a 1 2 b 1 2 a 1 2 b 1 2 a 1 2 b 1 2 f proposition 5 1 let a s p d suppose a is non singular for some i 1 id p let us set k p d and v then if b s p k is such that ab 0 b is non singular proof first note that a is non singular thus by the guttman rank additivity formula we have rank a rank a rank a therefore a 0 and a can be partitioned as a id 0 a a 1 ik a 0 0 0 id a 1 a 0 ik where id and ik are identity matrices in rd d and rk k note now that tr ab tr id 0 a a 1 ik a 0 0 0 id a 1 a 0 ik b tr a 0 0 0 id a 1 a 0 ib b id 0 a a 1 ik tr a 0 0 0 c c c c 0 where c c c c c id a 1 a 0 ib b i 0 a a 1 ik s p 18 by lemma 5 1 this implies that a 0 0 0 c c c c a c a c 0 0 0 0 0 0 therefore c 0 c 0 and c c 0 thus k rank b rank c rank c rank b note that c b this shows that b is non singular corollary 5 2 suppose a s p d is in general position if b s p p d such that ab 0 then b is also in general position proof set k p d and let i 1 ik p if we set p then a is non singular therefore by proposition 5 1 b is non singular 19 1 introduction 2 preliminaries 2 1 graph theoretical notion definitions 2 2 graph parameters 2 3 matrix algebra notation definitions 2 4 vectors and matrices in general positions 2 5 graphical gaussian models 2 6 the maximum likelihood problem 3 the gaussian rank of a graph 3 1 an alternative description of the gaussian rank 3 2 some basic properties of the gaussian rank 4 the proof of theorem 1 1 4 1 the upper bound r g g 1 4 2 the lower bound g 1 r g 5 some applications of theorem 1 1 5 1 gaussian ranks of symmetric graphs and random graphs 5 2 on the gaussian ranks of planar graphs