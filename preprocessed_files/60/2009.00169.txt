a mathematical introduction to generative adversarial nets gan yang wang abstract generative adversarial nets gan have received considerable atten tion since the 2014 groundbreaking work by goodfellow et al 4 such attention has led to an explosion in new ideas techniques and applications of gans to bet ter understand gans we need to understand the mathematical foundation behind them this paper attempts to provide an overview of gans from a mathemati cal point of view many students in mathematics may find the papers on gans more difficulty to fully understand because most of them are written from computer science and engineer point of view the aim of this paper is to give more math ematically oriented students an introduction to gans in a language that is more familiar to them 1 introduction 1 1 background generative adversarial nets gan have received considerable attention since the 2014 groundbreaking work by goodfellow et al 4 such attention has led to an explosion in new ideas techniques and applications of gans yann lecun has called this gan and the variations that are now being proposed is the most interesting idea in the last 10 years in ml in my opinion in this note i will attempt to provide a beginner s introduction to gan from a more mathematical point of view intended for students in mathematics of course there is much more to gans than just the mathematical principle to fully understand gans one must also look into their algorithms and applications nevertheless i believe that understanding the mathematical principle is a crucial first step towards understanding gans and with it the other aspects of gans will be considerably easier to master 2010 mathematics subject classification primary 42 c 15 key words and phrases deep learning gan neural network the author is supported in part by the hong kong research grant council grants 16308518 and 16317416 as well as hk innovation technology fund its 044 18 fx 1 ar x iv 2 00 9 00 16 9 v 1 cs l g 1 s ep 2 02 0 2 yang wang the original gan which we shall refer to as the vanilla gan in this paper was introduced in 4 as a new generative framework from training data sets its goal was to address the following question suppose we are given a data set of objects with certain degree of consistency for example a collection of images of cats or handwrit ten chinese characters or van gogh painting etc can we artificially generate similar objects this question is quite vague so we need to make it more mathematically specific we need to clarify what do we mean by objects with certain degree of consistency or similar objects before we can move on first we shall assume that our objects are points in rn for example a grayscale digital image of 1 megapixel can be viewed as a point in rn with n 106 our data set training data set is simply a collection of points in rn which we denote by x rn when we say that the objects in the data set x have certain degree of consistency we mean that they are samples generated from a common probability distribution on rn which is often assumed to have a density function p x of course by assuming to have a density function mathematically we are assuming that is absolutely continuous some mathematicians may question the wisdom of this assumption by pointing out that it is possible in fact even likely that the objects of interest lie on a lower dimensional manifold making a singular probability distribution for example consider the mnist data set of handwritten digits while they are 28 28 images so n 784 the actual dimension of these data points may lie on a manifold with much smaller dimension say the actual dimension may only be 20 or so this is a valid criticism indeed when the actual dimension of the distribution is far smaller than the ambient dimension various problems can arise such as failure to converge or the so called mode collapsing leading to poor results in some cases still in most applications this assumption does seem to work well furthermore we shall show that the requirement of absolute continuity is not critical to the gan framework and can in fact be relaxed quantifying similar objects is a bit trickier and holds the key to gans there are many ways in mathematics to measure similarity for example we may define a distance function and call two pints x y similar if the distance between them is a beginner s guide to gan 3 small but this idea is not useful here our objective is not to generate objects that have small distances to some existing objects in x rather we want to generate new objects that may not be so close in whatever distance measure we use to any existing objects in the training data set x but we feel they belong to the same class a good analogy is we have a data set of van gogh paintings we do not care to generate a painting that is a perturbation of van gogh s starry night instead we would like to generate a painting that a van gogh expert will see as a new van gogh painting she has never seen before a better angle at least from the perspective of gans is to define similarity in the sense of probability distribution two data sets are considered similar if they are samples from the same or approximately same probability distribution thus more specifically we have our training data set x rn consisting of samples from a probability distribution with density p x and we would like to find a probability distribution with density q x such that is a good approximation of by taking samples from the distribution we obtain generated objects that are similar to the objects in x one may wonder why don t we just simply set and take samples from wouldn t that give us a perfect solution indeed if we know what is unfortu nately that is exactly our main problem we don t know all we know is a finite set of samples x drawn from the distribution hence our real challenge is to learn the distribution from only a finite set of samples drawn over it we should view finding as the process of approximating gans do seem to provide a novel and highly effective way for achieving this goal in general the success of a gan will depend on the complexity of the distribution and the size of the training data set x in some cases the cardinality x n can be quite large e g for imagenet data set n is well over 107 but in some other cases such as van gogh paintings the size n is rather small in the order of 100 only 1 2 the basic approach of gan to approximate the vanilla gan and subsequently other gans start with an initial probability distribution defined on rd where d may or may not be the same as n for the time being we shall set to be the standard normal distribution n 0 id although we certainly can choose to 4 yang wang be other distributions the technique gans employ is to find a mapping function g rd rn such that if a random variable z rd has distribution then g z has distribution note that the distribution of g z is g 1 where g 1 maps subsets of rn to subsets of rd thus we are looking for a g z such that g 1 or at least is a good approximation of sounds simple right actually several key issues remain to be addressed one issue is that we only have samples from and if we know g we can have samples g z where z is drawn from the distribution how do we know from these samples that our distribution g 1 is the same or a good approximation of assuming we have ways to do so we still have the issue of finding g z the approach taken by the vanilla gan is to form an adversarial system from which g continues to receive updates to improve its performance more precisely it introduces a discriminator function d x which tries to dismiss the samples generated by g as fakes the discriminator d x is simply a classifier that tries to distinguish samples in the training set x real samples from the generated samples g z fake samples it assigns to each sample x a probability d x 0 1 for its likelihood to be from the same distribution as the training samples when samples g zj are generated by g the discriminator d tries to reject them as fakes in the beginning this shouldn t be hard because the generator g is not very good but each time g fails to generate samples to fool d it will learn and adjust with an improve ment update the improved g will perform better and now it is the discriminator d s turn to update itself for improvement through this adversarial iterative process an equilibrium is eventually reached so that even with the best discriminator d it can do no better than random guess at such point the generated samples should be very similar in distribution to the training samples x so one may ask where do neural networks and deep learning have to do with all this the answer is that we basically have the fundamental faith that deep neural networks can be used to approximate just about any function through proper tuning of the network parameters using the training data sets in particular neural networks excel in classification problems not surprisingly for gan we shall model both the discriminator function d and the generator function g as neural networks with a beginner s guide to gan 5 parameters and respectively thus we shall more precisely write d x as d x and g z as g z and denote g 1 our objective is to find the desired g z by properly tuning 2 mathematical formulation of the vanilla gan the adversarial game described in the previous section can be formulated mathe matically by minimax of a target function between the discriminator function d x rn 0 1 and the generator function g rd rn the generator g turns random samples z rd from distribution into generated samples g z the discriminator d tries to tell them apart from the training samples coming from the distribution while g tries to make the generated samples as similar in distribution to the training samples in 4 a target loss function is proposed to be 2 1 v d g ex logd x ez log 1 d g z where e denotes the expectation with respect to a distribution specified in the sub script when there is no confusion we may drop the subscript the vanilla gan solves the minimax problem 2 2 min g max d v d g min g max d ex logd x ez log 1 d g z intuitively for a given generator g maxd v d g optimizes the discriminator d to reject generated samples g z by attempting to assign high values to samples from the distribution and low values to generated samples g z conversely for a given discriminator d ming v d g optimizes g so that the generated samples g z will attempt to fool the discriminator d into assigning high values now set y g z rn which has distribution g 1 as z rd has distribution we can now rewrite v d g in terms of d and as v d v d g ex logd x ez log 1 d g z ex logd x ey log 1 d y rn logd x d x rn log 1 d y d y 2 3 6 yang wang the minimax problem 2 2 becomes 2 4 min g max d v d g min g max d rn logd x d x rn log 1 d y d y assume that has density p x and has density function q x which of course can only happen if d n then 2 5 v d rn logd x p x log 1 d x q x dx the minimax problem 2 2 can now be written as 2 6 min g max d v d g min g max d rn logd x p x log 1 d x q x dx observe that the above is equivalent to min maxd v d under the constraint that g 1 for some g but to better understand the minimax problem it helps to examine min maxd v d without this constraint for the case where have densities 4 has established the following results proposition 2 1 4 given probability distributions and on rn with densities p x and q x respectively max d v d max d rn logd x p x log 1 d x q x dx is attained by dp q x p x p x q x for x supp supp the above proposition leads to theorem 2 2 4 let p x be a probability density function on rn for probability distribution with density function q x and d rn 0 1 consider the minimax problem 2 7 min max d v d min max d rn logd x p x log 1 d x q x dx then the solution is attained with q x p x and d x 1 2 for all x supp p theorem 2 2 says the solution to the minimax problem 2 7 is exactly what we are looking for under the assumption that the distributions have densities we discussed earlier that this assumption ignores that the distribution of interest may lie on a lower a beginner s guide to gan 7 dimensional manifold and thus without a density function fortunately the theorem actually holds in the general setting for any distributions we have theorem 2 3 let be a given probability distribution on rn for probability dis tribution and function d rn 0 1 consider the minimax problem 2 8 min max d v d min max d rn logd x d x log 1 d x d x then the solution is attained with and d x 1 2 almost everywhere proof the proof follows directly from theorem 3 7 and the discussion in subsection 3 5 example 2 like many minimax problems one may use the alternating optimization algorithm to solve 2 7 which alternates the updating of d and q hence g an updating cycle consists of first updating d for a given q and then updating q with the new d this cycle is repeated until we reach an equilibrium the following is given in 4 proposition 2 4 4 if in each cycle the discriminator d is allowed to reach its optimum given q x followed by an update of q x so as to improve the minimization criterion min q rn logd x p x log 1 d x q x dx then q converges to p here i have changed the wording a little bit from the original statement but have kept its essence intact from a pure mathematical angle this proposition is not rigorous however it provides a practical framework for solving the vanilla gan minimax problem namely in each cycle we may first optimize the discriminator d x all the way for the current q x and then update q x given the new d x just a little bit repeating this cycle will lead us to the desire solution in practice however we rarely optimize d all the way for a given g instead we usually update d a little bit before switching to updating g note that the unconstrained minimax problem 2 7 and 2 8 are not the same as the original minimax problem 2 2 or the equivalent formulation 2 3 where is constrained to be of the form g 1 nevertheless it is reasonable in practice 8 yang wang to assume that 2 2 and 2 3 will exhibit similar properties as those being shown in theorem 2 3 and proposition 2 4 in fact we shall assume the same even after we further restrict the discriminator and generator functions to be neural networks d d and g g as intended set g 1 under this model our minimax problem has become min max v d g where v d g ex logd x ez log 1 d g z 2 9 rn logd x d x log 1 d x d x 2 10 equation 2 9 is the key to carrying out the actual optimization since we do not have the explicit expression for the target distribution we shall approximate the ex pectations through sample averages thus 2 9 allows us to approximate v d g using samples more specifically let a be a subset of samples from the training data set x a minibatch and b be a minibatch of samples in rd drawn from the distribution then we do the approximation ex logd x 1 a x a logd x 2 11 ez log 1 d g z 1 b z b log 1 d g z 2 12 the following algorithm for the vanilla gan was presented in 4 vanilla gan algorithm minibatch stochastic gradient descent training of gener ative adversarial nets the number of steps to apply to the discriminator k is a hyperparameter k 1 the least expensive option was used in the experiments in 4 for number of training iterations do for k steps do sample minibatch of m samples z 1 zm in rd from the distribution sample minibatch of m samples x 1 xm x from the training set x a beginner s guide to gan 9 update the discriminator d by ascending its stochastic gradient with respect to 1 m m i 1 logd xi log 1 d g zi end for sample minibatch of m samples z 1 zm in rd from the distribution update the generator g by descending its stochastic gradient with respect to 1 m m i 1 log 1 d g zi end for the gradient based updates can use any standard gradient based learning rule the paper used momentum in their experiments proposition 2 4 serves as a heuristic justification for the convergence of the algo rithm one problem often encountered with the vanilla gan algorithm is that the updating of g from minimization of ez log 1 d g z may saturate early so instead the authors substituted it with minimizing e logd g z this is the well known logd trick and it seems to offer superior performance we shall examine this more closely later on 3 f divergence and f gan recall that the motivating problem for gan is that we have a probability distri bution known only in the form of a finite set of samples training samples we would like to learn this target distribution through iterative improvement starting with a probability distribution we iteratively update so it gets closer and closer to the target distribution of course to do so we will first need a way to measure the discrepancy between two probability distributions the vanilla gan has employed a discriminator for this purpose but there are other ways 10 yang wang 3 1 f divergence one way to measure the discrepancy between two probability distributions and is through the kullback leibler divergence or kl divergence let p x and q x be two probability density functions defined on rn the kl divergence of p and q is defined as dkl p q rn log p x q x p x dx note that dkl p q is finite only if q x 6 0 on supp p almost everywhere while kl divergence is widely used there are other divergences such as the jensen shannon divergence djs p q 1 2 dkl p m 1 2 dkl q m where m p x q x 2 one advantage of the jensen shannon divergence is that it is well defined for any probability density functions p x and q x and is symmetric djs p q djs q p in fact following from proposition 2 1 the minimization part of the minimax problem in the vanilla gan is precisely the minimization over q of djs p q for a given density function p as it turns out both dkl and djs are special cases of the more general f divergence introduced by ali and silvey 1 let f x be a strictly convex function with domain i r such that f 1 0 throughout this paper we shall adopt the convention that f x for all x 6 i definition 3 1 let p x and q x be two probability density functions on rn then the f divergence of p and q is defined as 3 1 df p q ex q f p x q x rn f p x q x q x dx where we adopt the convention that f p x q x q x 0 if q x 0 remark because the f divergence is not symmetric in the sense that df p q 6 df q p in general there might be some confusion as to which divides which in the fraction if we follow the original ali and silvey paper 1 then the definition of df p q would be our df q p here we adopt the same definition as in the paper 9 which first introduced the concept of f gan a beginner s guide to gan 11 proposition 3 1 let f x be a strictly convex function on domain i r such that f 1 0 assume either supp p supp q equivalent to p q or f t 0 for t 0 1 then df p q 0 and df p q 0 if and only if p x q x proof by the convexity of f and jensen s inequality df p q ex q f p x q x f ex q p x q x f supp q p x dx f r where the equality holds if and only if q x p x is a constant or f is linear on the range of p x q x since f is strictly convex it can only be the former thus for the equality to hold we must have p x rq x on supp q now clearly r 1 if supp p supp q then r 1 and we have df p q 0 the equality holds if and only if p q if f t 0 for all t 0 1 then we also have df p q f r 0 for r 1 we will have df p q f r 0 thus if df p q 0 we must have r 1 and p q it should be noted that f divergence can be defined for two arbitrary probability measures and on a probability space let be another probability measure such that namely both are absolutely continuous with respect to for example we may take 1 2 let p d d and q d d be their radon nikodym derivatives we define the f divergence of and as 3 2 df f p x q x q x d ex f p x q x again we adopt the convention that f p x q x q x 0 if q x 0 it is not hard to show that this definition is independent of the choice for the probability measure and proposition 3 1 holds for the more general df as well with f divergence measuring the discrepancy between two measures we can now consider applying it to gans the biggest challenge here is that we don t have an explicit expression for the target distribution as with the vanilla gan to compute df p q we must express it in terms of sample averages fortunately earlier work by nguyen wainwright and jordan 8 has already tackled this problem using the convex conjugate of a convex function 12 yang wang 3 2 convex conjugate of a convex function the convex conjugate of a convex function f x is also known as the fenchel transform or fenchel legendre transform of f which is a generalization of the well known legendre transform let f x be a convex function defined on an interval i r then its convex conjugate f r r is defined to be 3 3 f y sup t i ty f t as mentioned earlier we extend f to the whole real line by adopting the convention that f x for x 6 i below is a more explicit expression for f y lemma 3 2 assume that f x is strictly convex and continuously differentiable on its domain i r where io a b with a b then 3 4 f y yf 1 y f f 1 y y f io limt b ty f t y limt b f t limt a ty f t y limt a f t proof let g t ty f t the g t y f t on i which is strictly decreasing by the convexity of f t hence g t is strictly concave on i if y f t for some t io then t is a critical point of g so it must be its global maximum thus g t attains its maximum at t t f 1 y now assume y is not in the range of f then g t 0 or g t 0 on io consider the case g t 0 for all t io clearly the supreme of g t is achieved as t b since g t is monotonously increasing the case for g t 0 for all t io is similarly derived remark note that is a possible value for f the domain dom f for f is defined as the set on which f is finite a consequence of lemma 3 2 is that under the assumption that f is continuously differentiable supt i ty f t is attained for some t i if and only if y is in the range of f t this is clear if y f io but it can also be argued rather easily for finite boundary points of i more generally without the assumption of differentiability supt i ty f t is attained if and only if y f t for some t i where f t is the set of sub derivatives the following proposition summarizes some important properties of convex conjugate a beginner s guide to gan 13 proposition 3 3 let f x be a convex function on r with range in r then f is convex and is lower semi continuous furthermore if f is lower semi continuous then it satisfies the fenchel duality f f proof this is a well known result we omit the proof here the table below lists the convex dual of some common convex functions f x f y f x ln x x 0 f y 1 ln y y 0 f x ex f y y ln y y y 0 f x x 2 f y 1 4 y 2 f x 1 x 2 f y 1 y 2 y 1 1 f x 0 x 0 1 f y relu y f x g ax b a 6 0 f y b a y g y a 3 3 estimating f divergence using convex dual to estimate f divergence from samples nguyen wainwright and jordan 8 has proposed the use of the convex dual of f let be probability measures such that for some probability measure with p d d and q d d in the nice case of by f x f x we have df f p x q x q x d sup t t p x q x f t q x d x 3 5 sup t tp x f t q x d x 3 6 t x p x f t x q x d x ex t x ex f t x where t x is any borel function thus taking t over all borel functions we have 3 7 df sup t ex t x ex f t x 14 yang wang on the other hand note that for each x supt t p x q x f t is attained for some t t x as long as p x q x is in the range of sub derivatives of f thus if this holds for all x we have df ex t x ex f t x in fact equality holds in general under mild conditions theorem 3 4 let f t be strictly convex and continuously differentiable on i r let be borel probability measures on rn such that then 3 8 df sup t ex t x ex f t x where supt is taken over all borel functions t rn dom f furthermore assume that p x i for all x then t x f p x is an optimizer of 3 8 proof we have already establish the upper bound part 3 7 now we establish the lower bound part let p x d d x we examine 3 5 with q x 1 and supt tp x f t for each x denote gx t tp x f t let s dom f and assume so a b where a b r we now construct a sequence tk x as follows if p x is in the range of f say p x f tx we set tk x tx s if p x f t 0 for all t then gx t is strictly increasing the supreme of gx t is attained at the boundary point b and we will set tk x bk s where bk b if p x f t 0 for all t then gx t is strictly decreasing the supreme of gx t is attained at the boundary point a and we will set tk x ak s where ak a by lemma 3 2 and its proof we know that lim k tk x p x f tk x sup t tp x f t thus lim k ex tk x ex f tk x df to establish the last part of the theorem assume that p x i by lemma 3 2 set s t f 1 t for t in the range of f so we have f t ts t f s t s t ts t f s t s t s t thus g x t p x f t p x f 1 t it follows that gx t attains its maximum at t f p x this proves that t x f p x is an optimizer for 3 8 a beginner s guide to gan 15 the above theorem requires that what if this does not hold we have theorem 3 5 let f t be convex such that the domain of f contains a for some a r let be borel probability measures on rn such that 6 then 3 9 sup t ex t x ex f t x where supt is taken over all borel functions t rn dom f proof take 1 2 then let p d d and q d d be their radon nikodym derivatives since 6 there exists a set s 0 with s 0 0 on which q x 0 fix a t 0 in the domain of f let tk x k for x s 0 and tk x t 0 otherwise then ex tk x ex f tk x k s 0 f t 0 1 s 0 this proves the theorem as one can see we clearly have a problem in the above case if the domain of f is not bounded from above 3 8 does not hold unless in many practical applications the target distribution might be singular as the training data we are given may lie on a lower dimensional manifold fortunately there is still hope as given by the next theorem theorem 3 6 let f t be a lower semi continuous convex function such that the domain i of f has sup i b let be borel probability measures on rn such that s ab where s and ab then 3 10 sup t ex t x ex f t x df b s rn where supt is taken over all borel functions t rn dom f proof again take 1 2 then the decomposition ab s where ab and s is unique and guaranteed by the lebesgue decomposition theorem let pab d ab d ps d s d and q d d be their radon nikodym derivatives since s we may divide rn into rn c where supp q 16 yang wang clearly we have q x pab x 0 for x c thus sup t ex t x ex f tk x sup t t x pab x f t x q x d sup t c t x pab x d sup t t x pab x q x f t x q x d b s c f pab x q x q x d b s rn f p x q x q x d b s rn df b s rn this proves the theorem 3 4 f gan variational divergence minimization vdm we can formulate a generalization of the vanilla gan using f divergence for a given probability distribution the f gan objective is to minimize the f divergence df with respect to the probability distribution carried out in the sample space f gan solves the following minimax problem 3 11 min sup t ex t x ex f t x the f gan framework is first introduced in 9 and the optimization problem 3 11 is referred to as the variational divergence minimization vdm note vdm looks similar to the minimax problem in vanilla gan the borel function t here is called a critic function or just a critic under the assumption of by theorem 3 4 this is equivalent to min df one potential problem of f gan is that by theorem 3 5 if 6 then 3 11 is in general not equivalent to min df fortunately for specially chosen f this is not a problem theorem 3 7 let f t be a lower semi continuous strictly convex function such that the domain i of f has sup i b 0 assume further that f is continuously differentiable on its domain and f t 0 for t 0 1 let be borel probability measures on rn then is the unique optimizer of min sup t ex t x ex f t x a beginner s guide to gan 17 where supt is taken over all borel functions t rn dom f and inf is taken over all borel probability measures proof by theorem 3 6 for any borel probability measure we have sup t ex t x ex f t x df b s rn df now by proposition 3 1 df 0 and equality holds if and only thus is the unique optimizer 3 5 examples we shall now look at some examples of f gan for different choices of the convex function f example 1 f t ln t this is the kl divergence we have f u 1 ln u with domain i 0 f satisfies all conditions of theorem 3 7 the corresponding f gan objec tive is 3 12 min sup t ex t x ex ln t x 1 where t x 0 if we ignore the constant 1 term and set d x t x then we obtain the equivalent minimax problem min sup d 0 ex d x ex ln d x example 2 f t ln t 1 ln t t 1 ln 2 this is the jensen shannon divergence we have f u ln 2 eu with domain i ln 2 again f satisfies all conditions of theorem 3 7 the corresponding f gan objective is 3 13 min sup t ex t x ex ln 2 et x where t x ln 2 set d x 1 1 2 et x and so t x ln 1 d x ln 2 substituting in 3 13 yields min max d 0 ex ln d x ex ln 1 d x ln 4 18 yang wang ignoring the constant ln 4 the vanilla gan is a special case of f gan with f being the jensen shannon divergence example 3 f t 1 t 1 where 0 here we have f u u with domain i while f is not strictly con vex and continuously differentiable it does satisfy the two important conditions of theorem 3 7 of sup i 0 and f t 0 for t 0 1 the corresponding f gan objective is 3 14 min sup t ex t x ex t x for 1 if we require t to be continuous then the supremum part of 3 14 is precisely the total variation also known as the radon metric between and which is closely related to the wasserstein distance between and example 4 f t t 1 ln t t 1 t 0 the log d trick here f t 3 t 1 t 2 t 1 2 0 so f is strictly convex it satisfies all conditions of theorem 3 7 the explicit expression for the convex dual f is complicated to write down however we do know the domain i for f is the range of f t which is 0 the f gan objective is min sup t 0 ex t x ex f t x by theorem 3 6 and the fact b 0 3 15 sup t 0 ex t x ex f t x df take 1 2 so let p d d and q d d observe that df ex f p x q x ex p x q x 1 ln p x p x q x ex ln p x p x q x ex ln p x p x q x denote d x p x p x q x then the outer minimization of the minimax problem is 3 16 min ex ln d x ex ln d x a beginner s guide to gan 19 this is precisely the logd trick used in the original gan paper 4 for the vanilla gan to address the saturation problem now we can see it is equivalent to the f gan with the above f it is interesting to note that directly optimizing 3 15 is hard because it is hard to find the explicit formula for f in this case thus the vanilla gan with the logd trick is an indirect way to realize this f gan to implement an f gan vdm we resort to the same approach as the vanilla gan using neural networks to approximate both t x and to solve the minimax problem 3 11 min sup t ex t x ex f t x we assume the critic function t x comes from a neural network 9 proposes t x t x gf s x where s is a neural network with parameters taking input from rn and gf r i is an output activation function to force the output from v x onto the domain i of f for we again consider its approximation by probability distributions of the form g 1 where is an initially chosen probability distribution on rd usually gaussian where d may or may not be n and g is a neural network with parameters with input from rd and output in rn under this model the f gan vmd minimax problem 3 11 becomes 3 17 min sup ez gf s g z ex f gf s x like the vanilla gan since we do not have the explicit expression for the target distribution we shall approximate the expectations through sample averages more specifically leta be a minibatch of samples from the training data set x a minibatch and b be a minibatch of samples in rd drawn from the distribution then we employ the approximations ez gf s g z 1 b z b gf s g z 3 18 ex f gf s x 1 a x a f gf s x 3 19 the following algorithm for f gan vdm is almost a verbatim repeat of the vanilla gan algorithm 4 stated earlier 20 yang wang vdm algorithm minibatch stochastic gradient descent training of generative ad versarial nets here k 1 and m are hyperparameters for number of training iterations do for k steps do sample minibatch of m samples z 1 zm in rd from the distribution sample minibatch of m samples x 1 xm x from the training set x update s by ascending its stochastic gradient with respect to 1 m m i 1 gf s g zi f gf s xi end for sample minibatch of m samples z 1 zm in rd from the distribution update the discriminator g by descending its stochastic gradient with respect to 1 m m i 1 gf s g zi end for the gradient based updates can use any standard gradient based learning rule 4 examples of well known gans since inception gans have become one of the hottest research topics in machine learning many specially trained gans tailor made for particular applications have been developed modifications and improvements have been proposed to address some of the shortcomings of vanilla gan here we review some of the best known efforts in these directions a beginner s guide to gan 21 4 1 wasserstein gan wgan training a gan can be difficult which fre quently encounters several failure modes this has been a subject of many discussions some of the best known failure modes are vanishing gradients this occurs quite often especially when the discrim inator is too good which can stymie the improvement of the generator with an optimal discriminator generator training can fail due to vanishing gradi ents thus not providing enough information for the generator to improve mode collapse this refers to the phenomenon where the generator starts to produce the same output or a small set of outputs over and over again if the discriminator gets stuck in a local minimum then it s too easy for the next generator iteration to find the most plausible output for the current discriminator being stuck the discriminator never manages to learn its way out of the trap as a result the generators rotate through a small set of output types failure to converge gans frequently fail to converge due to a number of factors known and unknown the wgan 2 makes a simple modification where it replaces the jensen shannon divergence loss function in vanilla gan with the wasserstein distance also known as the earth mover em distance don t overlook the significance of this modification it is one of the most important developments in the topic since the inception of gan as the use of em distance effectively addresses some glaring shortcomings of divergence based gan allowing one to mitigate those common failure modes in the training of gans let be two probability distributions on rn or more generally any metric space denote by the set of all probability distributions x y on rn rn such that the marginals of are x and y respectively then the em distance wasserstein 1 distance between and is w 1 min rn rn x y d x y min e x y x y intuitively w 1 is called the earth mover distance because it denotes the least amount of work one needs to do to move mass to mass 22 yang wang in wgan the objective is to minimize the loss function w 1 as opposed to the loss function df in f gan the advantage of w 1 is illustrated by 2 through the following example let z be the uniform distribution on 0 1 in r define 0 z and z on r 2 then are singular distributions with disjoint support if 6 0 it is easy to check that djs ln 2 dkl and w 1 however visually for small 0 even though and have disjoint support they look very close in fact if a gan can approximate by for a very small we would be very happy with the result but no matter how close 0 is to 0 we will have djs ln 2 if we train the vanilla gan with the initial source distribution we would be stuck with a flat gradient so it will not converge more generally let be probability measures such that then we always have djs ln 2 by theorem 3 6 and 3 10 sup t ex t x ex f t x ln 2 thus gradient descend will fail to update in more practical setting if our target distribution is a gaussian mixture with well separated means then starting with the standard gaussian as initial source distribution will likely miss those gaussian distributions in the mixture whose means are far away from 0 resulting in mode collapse and possibly failure to converge note that by the same theorem 3 6 and 3 10 things wouldn t improve by changing the convex function f x in the f gan as we seen from the examples this pitfall can be avoided in wgan the next question is how to evaluate w 1 using only samples from the distri butions since we do not have explicit expression for the target distribution this is where kantorovich rubenstein duality 11 comes in which states that 4 1 w 1 sup t lip 1 rn ex t x ex t x where lip 1 rn denotes the set of all lipschitz functions on rn with lipschitz constant 1 here the critic t x serves the role of a discriminator with the duality wgan solves the minimax problem 4 2 min w 1 min sup t lip 1 rn ex t x ex t x a beginner s guide to gan 23 the rest follows the same script as vanilla gan and f gan we write g 1 where is a prior source distribution usually the standard normal in rd and g maps rd to rn it follows that ex t x ez t g z finally we approximate t x and g z by neural networks with parameters and respectively t x t x and g z g z stochastic gradient descend is used to train wgan just like all other gans indeed replacing the js divergence with the em distance w 1 the algorithm for vanilla gan can be copied verbatim to become the algorithm for wgan actually almost verbatim there is one last issue to be worked out namely how does one enforce the condition t x lip 1 this is difficult the authors have proposed a technique called weight clipping where the parameter weights is arti ficially restricted to the region 0 01 in other words all parameters in are clipped so they fall into the box 0 01 0 01 obviously this is not the same as restricting the lipschitz constant to 1 however since is compact so will be t this means the lipschitz constant will be bounded by some k 0 the hope is that sup ex t x ex t x sup t lipk rn ex t x ex t x where the latter is just k w 1 weight clipping may not be the best way to approximate the lipschitz condition alternatives such as gradient restriction 5 can be more effective there might be other better ways 4 2 deep convolutional gan dcgan dcgan refers to a set of architec tural guidelines for gan developed in 10 empirically the guidelines help gans to attain more stable training and good performance according to the paper these guidelines are replace any pooling layers with strided convolutions discriminator and fractional strided convolutions generator use batch normalization in both the generator and the discriminator 24 yang wang remove fully connected hidden layers for deeper architectures use relu activation in generator for all layers except for the output which uses tanh use leakyrelu activation in the discriminator for all layers here strided convolution refers to shifting the convolution window by more than 1 unit which amounts to downsampling fractional strided convolution refers to shifting the convolution window by a fractional unit say 1 2 of a unit which is often used for upsampling this obviously cannot be done in the literal sense to realize this we pad the input by zeros and then take the appropriate strided convolution for example suppose our input data is x x 1 x 2 xn and the convolution window is w w 1 w 2 w 3 for 1 2 strided convolution we would first pad x to become x x 1 0 x 2 0 0 xn and then execute x w nowadays strided convolution is just one of the several ways for up and down sampling 4 3 progressive growing of gans pggan generating high resolution im ages from gans is a very challenging problem progressive growing of gans devel oped in 6 is a technique that addresses this challenge pggan actually refers to a training methodology for gans the key idea is to grow both the generator and discriminator progressively starting from a low resolution image one adds new layers that model increasingly fine details as training progresses since low resolution images are much more stable and easier to train the training is very stable in the beginning once training at a lower resolution is done it gradually transit to training at a higher resolution this process continues until the desired resolution is reached in the paper 6 very high quality facial images have been generated by starting off the training at 4 4 resolution and gradually increasing the resolution to 8 8 16 16 etc until it reaches 1024 1024 it would be interesting to provide a mathematical foundation for pggan from earlier analysis there are pitfalls with gans when the target distribution is singular especially if it and the initial source distribution have disjoint supports pggan may have provided an effective way to mitigate this problem a beginner s guide to gan 25 4 4 cycle consistent adversarial networks cycle gan cycle gan is an image to image translation technique developed in 12 before this work the goal of image to image translation is to learn the mapping between an input image and an output image using a training set of aligned image pairs however often we may still want to do a similar task without the benefit of having a training set consisting of aligned image pairs for example while we have many paintings by claude monet we don t have photographs of the scenes and may wonder what those scenes would look like in a photo we may wonder how a monet painting of mt everest would look like even though claude monet had never been to mt everest one way to achieve these tasks is through neural style transfer 3 but this technique only transfers the style of a single painting to a target image cycle gan offers a different approach that allows for style transfer tanslation more broadly in a cycle gan we start with two training sets x and y for example x could be a corpus of monet scenery paintings and y could be a set of landscape photographs the training objective of a cycle gan is to transfer styles from x to y and vice versa in a cycle consistent way as described below precisely speaking a cycle gan consists of three components each given by a tailored loss function the first component is a gan vanilla gan but can also be any other gan that tries to generate the distribution of x with one notable deviation instead of sampling initially from a random source distribution such as a standard normal distribution cycle gan samples from the training data set y assume that x are samples drawn from the distribution and y are samples drawn from the distribution 0 the loss function for this component is 4 3 lgan 1 g 1 d ex log d x ey 0 log 1 d g 1 y where d is the discriminator network and g 1 is the generator network clearly this is the same loss used by the vanilla gan except the source distribution is replaced by the distribution 0 of y the second component is the mirror of the first component namely it is a gan to learn the distribution 0 of y with the initial source distribution set as the distribution of x the corresponding loss function is thus 4 4 lgan 2 g 2 d 0 ey 0 log d 0 y ex log 1 d 0 g 2 y 26 yang wang where d 0 is the discriminator network and g 2 is the generator network the third component of the cycle gan is the cycle consistent loss function given by 4 5 lcycle g 1 g 2 ey 0 g 2 g 1 y y 1 ex g 1 g 2 x x 1 the overall loss function is 4 6 l g 1 g 2 d d 0 lgan 1 g 1 d lgan 2 g 2 d 0 lcycle g 1 g 2 where 0 is a parameter intuitively g 1 translates a sample y from the distribution 0 into a sample from while g 2 translates a sample x from the distribution into a sample from 0 the loss function lcycle encourages consistency in the sense that g 2 g 1 y is not too far off y and g 1 g 2 x is not too far off x finally cycle gan is trained by solving the minimax problem 4 7 min g 1 g 2 max d d 0 l g 1 g 2 d d 0 5 alternative to gan variational autoencoder vae variational autoencoder vae is an alternative generative model to gan to understand vae one needs to first understand what is an autoencoder in a nutshell an autoencoder consists of an encoder neural network f that maps a dataset x rn to rd where d is typically much smaller than n together with a decoder neural network h that decodes elements of rd back to rn in other words it encodes the n dimensional features of the dataset to the d dimensional latents along with a way to convert the latents back to the features autoencoders can be viewed as data compressors that compress a higher dimensional dataset to a much lower dimensional data set without losing too much information for example the mnist dataset consists of images of size 28 28 which is in r 784 an autoencoder can easily compress it to a data set in r 10 using only 10 latents without losing much information a typical autoencoder has a bottleneck architecture which is shown in figure 1 the loss function for training an autoencoder is typically the mean square error mse lae 1 n x x h f x x 2 a beginner s guide to gan 27 figure 1 an autoencoder courtsey of jason anderson and three comp inc where n is the size of x for binary data we may use the bernoulli cross entropy bce loss function furthermore we may also add a regularization term to force desirable properties e g a lasso style l 1 loss to gain sparsity first developed in 7 vaes are also neural networks having similar architec tures as autoencoders with stochasticity added into the networks autoencoders are deterministic networks in the sense that output is completely determined by the input to make generative models out of autoencoders we will need to add ran domness to latents in an autoencoder input data x are encoded to the latents z f x rd which are then decoded to x h f z a vae deviates from an autoencoder in the following sense the input x is encoded into a diagonal gauss ian random variable x in rd with mean x and variance 2 x here 2 x 21 x 2 d x t rd and the variance is actually diag 2 another way to look at this setup is that instead of having just one encoder f in an autoencoder a vae neural network has two encoders and 2 for both the mean and the vari ance of the latent variable as in an autoencoder it also has a decoder h with randomness in place we now have a generative model 28 yang wang of course we will need some constraints on x and 2 x here vaes employ the following heuristics for training the decoder h decodes the latent random variables x to x that are close to x the random variable x x with x sampled uniformly from x is close to the standard normal distribution n 0 1 the heuristics will be realized through a loss function consisting of two components the first component is simply the mean square error between x and x given by l 1 g 1 n x x ez n x x id h z x 2 5 1 1 n x x ez n 0 id h x x z x 2 5 2 where denotes entry wise product here going from 5 1 to 5 2 is a very useful technique called re parametrization the second component of the loss function is the kl divergence or other f divergences between x and n 0 id for two gaussian random variables their kl divergence has an explicit expression given by 5 3 l 2 dkl x n 0 id 1 2 n x x d i 1 2 i x i x 2 1 ln 2 i the loss function for a vae is thus 5 4 lvae l 1 g l 2 where 0 is a parameter to generate new data from a vae one inputs random samples n 0 id into the decoder network h a typical vae architecture is shown in figure 2 this short introduction of vae has just touched on the mathematical foundation of vae there have been a wealth of research focused on improving the performance of vae and applications we encourage interested readers to further study the subject by reading recent papers a beginner s guide to gan 29 figure 2 a variational autoencoder courtesy of jason anderson and compthree inc references 1 s m ali and s d silvey a general class of coefficients of divergence of one distribution from another journal of the royal statistical society series b methodological 28 1 131 142 1966 2 m arjovsky s chintala and l bottou wasserstein generative adversarial networks in icml 2017 3 l a gatys a s ecker and m bethge image style transfer using convolutional neural networks in proceedings of the ieee conference on computer vision and pattern recognition pages 2414 2423 2016 4 i goodfellow j pouget abadie m mirza b xu d warde farley s ozair a courville and y bengio generative adversarial nets in nips 2014 5 i gulrajani f ahmed m arjovsky v dumoulin and a c courville improved training of wasserstein gans in advances in neural information processing systems pages 5767 5777 2017 6 t karras t aila s laine and j lehtinen progressive growing of gans for improved quality stability and variation in international conference on learning representations 2018 7 d p kingma and m welling auto encoding variational bayes arxiv preprint arxiv 1312 6114 2013 8 x nguyen m j wainwright and m i jordan estimating divergence functionals and the likelihood ratio by convex risk minimization ieee transactions on information theory 56 11 5847 5861 2010 9 s nowozin b cseke and r tomioka f gan training generative neural samplers using vari ational divergence minimization in advances in neural information processing systems pages 271 279 2016 30 yang wang 10 a radford l metz and s chintala unsupervised representation learning with deep convo lutional generative adversarial networks arxiv preprint arxiv 1511 06434 2015 11 c villani optimal transport old and new volume 338 springer science business media 2008 12 j y zhu t park p isola and a a efros unpaired image to image translation using cycle consistent adversarial networks in proceedings of the ieee international conference on com puter vision pages 2223 2232 2017 department of mathematics hong kong university of science and technology clear water bay kowloon hong kong e mail address yangwang ust hk 1 introduction 1 1 background 1 2 the basic approach of gan 2 mathematical formulation of the vanilla gan 3 f divergence and f gan 3 1 f divergence 3 2 convex conjugate of a convex function 3 3 estimating f divergence using convex dual 3 4 f gan variational divergence minimization vdm 3 5 examples 4 examples of well known gans 4 1 wasserstein gan wgan 4 2 deep convolutional gan dcgan 4 3 progressive growing of gans pggan 4 4 cycle consistent adversarial networks cycle gan 5 alternative to gan variational autoencoder vae references