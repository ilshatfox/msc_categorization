research article comparing methods addressing multi collinearity when developing prediction models artuur m leeuwenberg 1 maarten van smeden 1 johannes a langendijk 2 arjen van der schaaf 2 murielle e mauer 3 karel g m moons 1 johannes b reitsma 1 ewoud schuit 1 1 julius center for health sciences and primary care university medical center utrecht utrecht university utrecht the netherlands 2 department of radiation oncology university medical center groningen groningen university groningen the netherlands 3 european organisation for research and treatment of cancer headquarters brussels belgium correspondence artuur leeuwenberg julius center for health sciences and primary care university medical center utrecht utrecht university 3508 ga utrecht the netherlands email aleeuw 15 umcutrecht nl abstract clinical prediction models are developed widely across medical disciplines when predictors in such models are highly collinear unexpected or spurious predictor outcome associations may occur thereby potentially reducing face validity and explainability of the prediction model collinearity can be dealt with by exclusion of collinear predictors but when there is no a priori motivation besides collinearity to include or exclude specific predictors such an approach is arbitrary and possi bly inappropriate we compare different methods to address collinearity including shrinkage dimensionality reduction and constrained optimization the effective ness of these methods is illustrated via simulations in the conducted simulations no effect of collinearity was observed on predictive outcomes however a negative effect of collinearity on the stability of predictor selection was found affecting all compared methods but in particular methods that perform strong predictor selection e g lasso keywords multi collinearity prediction models normal tissue complication probability models 1 introduction multi collinearity between predictors is a common phenomenon in clinical prediction modeling for example in prediction of alzheimer s disease from mri images 1 prediction of metabolic acidosis in laboring women that had a high risk singleton pregnancy in cephalic presentation beyond 36 weeks of gestation 2 prediction of lung function in children 3 and prediction of complications of radiotherapy in cancer patients 4 5 multi collinearity is caused by dependence between predictors 6 when collinearity among predictors is high the data in itself provides limited information on how the explained variance in the outcome should be distributed over the collinear predictor coefficients in other words there is not just one model but there are multiple ways to assign coefficients that can predict the outcome in the data used to develop the model almost equally well 7 consequently model coefficients of collinear variables generally show large variance large standard errors even in large data sets although this is generally not considered problematic with regard to predictive performance 8 it can result in unexpected coefficients for individual predictors reducing the face validity and explainability of predictors included in the resulting model and thus the model in general 9 10 abbreviations ntcp normal tissue complication probability oar organ at risk ar x iv 2 10 1 01 60 3 v 1 st at m e 5 j an 2 02 1 2 leeuwenberg et al two commonmethods to address collinearity are predictor selection and predictor averaging both make strong assumptions about the predictive value of the collinear predictors predictor selection assumes that the excluded predictors have no added predictive value over the predictors that are retained in the model with respect to the outcome essentially imposing coefficients of zero predictor averaging assumes that the averaged predictors have the same predictive relation to the outcome imposing exact equivalence of the coefficients in some cases it is possible to convincingly motivate such assumptions using prior clinical knowledge or by resorting to data driven approaches e g backward selection however finding evidence in the data for such strong assumptions can be difficult especially when collinearity is high and the outcome is only weakly associated with the difference between collinear predictors therefore further research into more sophisticated methods to address collinearity is needed this article is organized as follows in section 2 we describe different methods for handling multi collinearity in section 3 we compare the described methods via simulations in a case study on the development of models for the prediction of complications of radiotherapy in cancer patients in terms of predictive performance and in terms of coefficient estimation including the choice of predictors in the final model in section 4 we discuss and summarize our conclusions 2 methods for collinearity 2 1 penalization of large coefficients we assume the interest is in a binary outcome y and candidate predictorsx the aim is to estimate the risk of y conditioned on the predictor values p y 1 x as a base model we assume standard logistic regression lr shown in equation 1 where 1 2 d is the vector of coefficients 0 the intercept and coefficient are estimated by maximizing the likelihood of the outcome in the data used for model development p y x 0 1 e x 0 1 in addition to the maximum likelihood of the outcome in the development data approaches like lasso and ridge include the size of the model s coefficients excluding the intercept as an extra penalty for coefficient estimation adding this penalty results in models with smaller coefficients that make less extreme predictions closer to the outcome proportion the penalty can also reduce the variance in the estimated coefficients induced by collinearity although lasso and ridge have similar structure penalizing high regression coefficients ridge was originally designed to address collinearity and lasso to perform predictor selection in high dimensional data lasso penalizes large coefficients linearly by extending the cost function with the l 1 norm of the coefficients which generally results in predictor selection of the most predictive features 11 ridge penalizes coefficient size quadratically resulting in a grouping effect of collinear predictors instead of selection 12 in practice the desire to perform predictor selection may be independent of the degree of collinearity present in the data and rather to enhance usability of a more parsimonious prediction model to facilitate a balance between predictor selection and grouping the elastic net method was developed 13 which combines the penalties of lasso and ridge penalization of coefficient size is a popular method in clinical prediction aimed to improve predictive performance over maximum likelihood recent simulation studies suggest these penalization approaches often improve the predictive performance on average but can show poor performance in small and low dimensional datasets 14 2 2 dropout regularization dropout regularization is a method aimed directly at reducing co adaptation of coefficients during model estimation and is widely used for regularization of neural networks 15 co adaptation refers to the degree to which the value of one regression coefficient depends on that of other coefficients dropout works in iterative gradient based training procedures like the one used in the current work described in appendix d 4 when using dropout at each gradient based learning step each predictor has a non zero probability to be dropped from the model with a certain probability effectively selecting a random sub model at each iteration this selected sub model is used to make predictions as part of that learning step and the involved coefficients are updated accordingly the coefficients selected at each step are updated independently of the dropped out predictors preventing co adaptation in the final model an alternative view to dropout is to consider it as an efficient approximation to taking the mean over the predictions of an exponentially large set of sub models without having to estimate all those models individually leeuwenberg et al 3 alternatively dropout can also be expressed as a penalty which for logistic regression models is most similar to ridge regu larization and includes a quadratic penalty on the size of coefficients in contrast to ridge dropout does not assign the penalty uniformly across the predictors dropout rewards infrequent predictors that enable the model to make confident predictions predicted risks close to 0 or 1 whenever the predictor of interest is active 16 2 3 dimensionality reduction the multi collinearity of predictors may be due to shared dependence on a smaller set of unobserved underlying variables that could themselves be related to the outcome principal component analysis pca can reduce the dimensionality of the original predictor space to obtain a smaller set of variables that explain most of the variance in the original predictors but is in itself uncorrelated these uncorrelated variables the principal components can be used as input to a logistic regression model to relate them to the outcome this combination of pcawith logistic regression is called incomplete principal component logistic regression pclr 17 18 19 with regard to the original model the effect of using pclr is that predictors that correlate strongly and are thus likely related the same principal components obtain similar coefficients in this study we focus on linear pca as this gives the possibility to rewrite the pclr model to an equivalent logistic model from the original predictors to the outcome details on this are given in appendix d this enables direct comparison of the coefficients with the other methods and reduces the importance of interpretability of the principal components as we can always observe the coefficients of each of the predictors in the final model linear autoencoders lae are similar to pca but do not find the exact same projection as pca however their components span the same directions 20 in contrast to pca or lae which determine the components based on the explained variance in the original predictors irrespective of the outcome we extend the training criterion of lae to find components that not only explain the variance of the original predictors but are also predictive of the outcome from now on referred to as linear autoencoder logistic regression laelr the relative importance of 1 explaining the variance in the predictors and 2 maximizing the likelihood of the outcome is determined by an additional parameter that like the number of used components needs to be tuned how to tune such parameters is discussed later in section 3 2 4 to summarize laelr can be seen as a compromise between pclr and logistic regression a more detailed formulation can be found in appendix d 2 4 constrained optimization besides penalizing the absolute size of coefficients as in lasso or ridge other penalties or criteria can be incorporated possibly using knowledge from the clinical domain or setting for example in some cases it may be valid to assume a priori that it is unlikely that certain predictors have a negative relation to the outcome e g in the later described case study one could assume that increasing radiation dosage to healthy tissue does not reduce the probability of complications in a logistic regression model not having a negative relation to the outcome means that these predictors should not get negative coefficient values in the final model encouraging the non negativity nn of certain coefficients can be modeled by adding a penalty for negative coefficient values to the maximum likelihood criterion 21 alternatively if the non negativity constraints are to be respected at all times they can be incorporated as hard constraints during the maximum likelihood estimation of the model through for example gradient projection 22 if the additional assumptions based on domain knowledge are correct and complementary to the information already present in the training data incorporating them can reduce the coefficients search space this may prevent selection of implausible models that satisfy maximum likelihood but are in fact inconsistent with clinical knowledge and consequently reduce the coefficient variance due to multi collinearity 3 motivating example 3 1 clinical background cancer patients receiving radiation therapy often experience complications after the therapy due to radiation damage to healthy tissue surrounding the tumor for example common complications for head and neck cancer patients are xerostomia decreased salivary flow resulting in dry mouth syndrome or dysphagia swallowing problems prediction models called normal tissue complication probability ntcp models are used to predict the risk for individual patients of developing complications after 4 leeuwenberg et al radiation based therapy based on patient disease and treatment characteristics including the dose distributions given to the healthy tissue surrounding the tumor the so called organs at risk oar besides informing patients about the expected risks of radiation induced complications ntcp models are clinically used to guide treatment decisions by looking at the difference in predicted risk of complications ntcp between treatment plans sometimes by pair wise treatment plan comparison 23 24 but also for complete treatment plan optimization 25 26 where the planned dosage is adjusted to minimize the risk of complications by minimizing the model predicted ntcp while maintaining tumor control for this setting proper handling of collinearity is crucial as in the process of treatment optimization unexpected coefficients may result in steering dosage to oar that due to collinearity may not seem important e g if coefficients are zero or negative but in fact are associated with increased complication risks 3 2 simulation study we report and planned the simulation study using the ademp strategy following morris and colleagues 27 3 2 1 aims the aims of this simulation study are to 1 study the effect of collinearity on development of clinical prediction models in terms of discrimination calibration and coefficient estimation in low dimensional settings the number of predictors is smaller than the number of events 2 compare the effectiveness of eight methods in handling the potentially negative effects of collinearity logistic regression lasso ridge elasticnet pclr laelr dropout and non negativity based constrained optimization 3 2 2 data generating mechanisms the simulations are based on four prediction modeling settings mimicking two outcomes in our motivating example xerostomia and dysphagia and two predictor sets per outcome a smaller predictor set with less collinearity where the given radiation is only indicated by the mean dose per oar and a larger predictor set with higher collinearity where more detailed dose volume predictors are added as well 1 these four initial settings are colored gray in table 1 a and c for the settings with small predictor sets and b and d for the larger predictor sets for these four settings predictor data were simulated from amultivariate normal distribution using the means and covariance matrix of the observed predictors of 740 head and neck patients with primary tumor locations pharynx larynx or the oral cavity that underwent radiotherapy at the university medical center groningen umcg andwere selected for having nomissing data in the predictors or outcome the simulated ground truth relation between predictors and outcome is constructed by fitting a logistic regression with ridge penalization on the corresponding real data 2 to study the effect of collinearity independently of the number of predictors and the number of events per variable epv we generated another four simulation settings for each setting with a large predictor set that inherently exhibits high collinearity b and d we generate 3 low collinearity variants b and d respectively and for each setting with a small predictor set that inherently exhibits a lower degree of collinearity a and c we generate high collinearity variants a and c respectively finally we end up with a total of eight simulation settings for which four pair wise comparisons can be made to assess the effect of collinearity to assess to what degree the simulation is accurate for the actual clinical prediction modeling problem we compare the results of the simulation to a comparable real data setting these real data experiments are indicated by a star in table 1 and have the same modeling characteristics as the corresponding simulations the same predictor covariance outcome prevalence and sample size 1 more detailed descriptions of the used predictor sets are given in appendix table 5 2 in a 5 fold cross validation on the real data ridge yielded good results in terms of calibration and discrimination but also included the largest proportion oar in the model relevant for this case study 3 we change the degree of collinearity by scaling the covariance matrix of the multi variate gaussian without changing the diagonal in doing so the degree of class separation and the outcome prevalence of the ground truth model may change to maintain the same degree of class separation in the data we scale the slope of the ground truth model additionally to maintain the same outcome prevalence we adjusted the ground truth intercept accordingly this way we change the degree of collinearity but maintain ground truth area under the receiver operator curve and outcome prevalence leeuwenberg et al 5 table 1 eight simulation settings that are evaluated for each method the sub scripted triangle is used to indicate high collinearity settings the star refers to the real data version of a simulated setting setting y n no predictors epv median vif a a xerostomia 592 7 23 5 a xerostomia 592 7 23 43 b xerostomia 592 19 8 5 b b xerostomia 592 19 8 43 c c dysphagia 592 13 6 7 c dysphagia 592 13 6 43 d dysphagia 592 43 2 7 d d dysphagia 592 43 2 43 3 2 3 estimators target of analysis we quantify collinearity by the median variance inflation factor vif the vif of a predictor reflects the relative increase in coefficient variance for that predictor due to the presence of other predictors a vif of 1 indicates absence of collinearity whereas a vif larger than 10 is often considered to reflect a high degree of collinearity 28 3 2 4 application of the methods besides standard logistic regression lr we compare all methods discussed in section 2 lasso ridge elasticnet pclr laelr dropout and lrnn the use of non negativity constraints for dosage coefficients through gradient projection these are listed in table 2 for a fair comparison we perform equal hyperparameter 4 tuning across methods for all models we tune hyperparameters using bayesian optimization 29 in a nested 3 fold cross validation setting with a log likelihood tuning criterion as general data preprocessing we standardize all predictors to have zero mean and unit variance more details about the exact training criteria for each method hyperparameter tuning and optimization can be found in appendix section d table 2 list of compared methods method abbreviation hyperparameters logistic regression lr lasso penalization lasso cl 1 inverse penalty importance ridge penalization ridge cl 2 inverse penalty importance elastic net penalization elasticnet cl 1 cl 2 inverse importance per penalty dropout regularization dropout dropout ratio principal component logistic regression pclr dpca number of components linear auto encoder logistic regression laelr dlae number of components clae inverse importance of reconstruction loss non negative logistic regression lrnn 3 2 5 performance measures we analyze our aims with regard to the measures stated in table 3 a less explored measure we use is the expected proportion of included coefficients that has the same direction of effect positive negative or zero across two simulated model construction repetitions the mean jaccard index of the coefficient signs mjics ranging from 0 to 1 this measure is formally defined in 4 parameters that are not part of the model itself but steer how the coefficients are determined e g the relative importance of the shrinkage penalty in lasso and ridge or the number of components for pclr among others 6 leeuwenberg et al equation 2 for arbitrary samples i and j to assess the robustness of the coefficient interpretation when developing a prediction model we consider methods that include the same predictors in themodel and assign the same directions of effect when repeating the model construction process to be more robust than methods that include different coefficients or assign different direction of effect across iterations mjics e sgn i sgn j sgn i sgn j 2 sgn x 1 iff x 0 01 1 iff x 0 01 0 otherwise 3 all measures are estimated by repeatedly sampling nrep 100 a new dataset from the constructed gaussian distributions refitting the models and evaluating them in a validation set generated from the same distributions as the development set of size n 10 000 the reported 95 confidence intervals are based on these repetitions and reflect variability of the entire model construction procedure sampling training data developing the model including hyperparameter tuning and sampling a new validation set for the real data settings a repeated 5 fold cross validation n 592 per fold on the real data is used to estimate each measure and their respective confidence intervals nrep 100 30 table 3 overview of the measures used to compare methods on predictive performance and coefficient estimation measure abbreviation ideal value predictive performance area under the receiver operator characteristic curve auroc 1 calibration intercept intercept 0 calibration slope slope 1 nagelkerke r squared r 2 1 coefficient estimation mean squared error between the estimated and the true coefficients mse 0 mean proportion of coefficients with the same direction of effect after repetition mjics 1 3 2 6 coding and execution all experiments were implemented in python 3 6 primarily using scikit learn 31 and pytorch 32 predictive performance measures are calculated in r using the val prob ci 2 function 33 the code used to conduct the experiments is available at https github com tuur collinearity 3 3 analysis of the results this section presents the simulation results with regard to predictive performance and coefficient estimation based on a compar ison between our simulations and the real data experiments in terms of predictive performance we concluded that the simulations are in accordance with the real data settings results of the real data experiments can be found in appendix section b https github com tuur collinearity leeuwenberg et al 7 a epv 23 mvif 5 a epv 23 mvif 43 b epv 8 mvif 5 b epv 8 mvif 43 lr 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 79 r 0 28 0 25 0 30 intercept 0 00 0 20 0 24 slope 0 94 0 78 1 14 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 80 0 78 0 81 r 0 31 0 26 0 33 intercept 0 01 0 23 0 28 slope 0 97 0 78 1 17 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 77 0 76 0 78 r 0 26 0 23 0 28 intercept 0 01 0 22 0 27 slope 0 81 0 66 1 01 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 80 r 0 28 0 26 0 31 intercept 0 00 0 26 0 25 slope 0 85 0 69 1 03 lasso 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 80 r 0 28 0 25 0 30 intercept 0 00 0 20 0 21 slope 1 06 0 85 1 31 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 80 0 78 0 81 r 0 31 0 27 0 33 intercept 0 01 0 20 0 25 slope 1 10 0 84 1 37 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 79 r 0 27 0 24 0 29 intercept 0 01 0 20 0 26 slope 1 04 0 78 1 42 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 79 0 78 0 81 r 0 30 0 28 0 32 intercept 0 00 0 24 0 20 slope 1 07 0 81 1 44 ridge 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 79 0 77 0 80 r 0 28 0 26 0 31 intercept 0 00 0 20 0 21 slope 1 11 0 88 1 45 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 80 0 78 0 81 r 0 31 0 27 0 34 intercept 0 01 0 21 0 26 slope 1 11 0 86 1 36 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 79 r 0 28 0 26 0 30 intercept 0 01 0 22 0 27 slope 1 09 0 86 1 48 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 80 0 78 0 81 r 0 31 0 28 0 33 intercept 0 00 0 24 0 20 slope 1 09 0 83 1 40 elasticnet 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 80 r 0 28 0 26 0 31 intercept 0 00 0 20 0 21 slope 1 10 0 88 1 38 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 80 0 78 0 81 r 0 31 0 27 0 34 intercept 0 01 0 20 0 26 slope 1 11 0 86 1 38 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 79 r 0 28 0 25 0 30 intercept 0 01 0 21 0 26 slope 1 07 0 83 1 43 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 80 0 78 0 81 r 0 31 0 28 0 33 intercept 0 00 0 24 0 21 slope 1 10 0 87 1 41 pclr 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 80 r 0 28 0 25 0 31 intercept 0 00 0 21 0 23 slope 0 96 0 79 1 16 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 80 0 78 0 81 r 0 31 0 27 0 34 intercept 0 01 0 22 0 28 slope 0 98 0 80 1 18 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 79 r 0 28 0 25 0 30 intercept 0 01 0 22 0 27 slope 0 92 0 74 1 14 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 79 0 78 0 81 r 0 30 0 27 0 33 intercept 0 00 0 25 0 21 slope 0 95 0 78 1 16 laelr 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 80 r 0 28 0 25 0 31 intercept 0 00 0 21 0 23 slope 0 96 0 79 1 18 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 80 0 78 0 81 r 0 31 0 27 0 34 intercept 0 02 0 23 0 28 slope 0 97 0 79 1 19 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 79 r 0 27 0 25 0 30 intercept 0 01 0 24 0 26 slope 0 92 0 73 1 18 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 79 0 78 0 81 r 0 30 0 27 0 33 intercept 0 00 0 25 0 20 slope 0 95 0 74 1 16 dropout 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 79 0 77 0 80 r 0 28 0 26 0 30 intercept 0 01 0 18 0 24 slope 1 11 0 84 1 50 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 80 0 78 0 81 r 0 31 0 27 0 33 intercept 0 00 0 18 0 28 slope 1 10 0 85 1 37 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 79 r 0 28 0 25 0 30 intercept 0 03 0 22 0 29 slope 1 04 0 79 1 35 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 80 0 78 0 81 r 0 31 0 28 0 33 intercept 0 01 0 23 0 26 slope 1 03 0 82 1 34 lrnn 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 80 r 0 28 0 25 0 30 intercept 0 00 0 20 0 24 slope 0 95 0 79 1 14 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 80 0 78 0 81 r 0 31 0 26 0 33 intercept 0 01 0 23 0 28 slope 0 97 0 79 1 17 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 79 r 0 27 0 25 0 29 intercept 0 01 0 22 0 27 slope 0 87 0 70 1 13 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 79 0 78 0 81 r 0 30 0 27 0 33 intercept 0 00 0 25 0 22 slope 0 92 0 76 1 14 figure 1 predictive performance results for the xerostomia simulations lowess smoothed calibration curves per simulation are plotted in grey the calibration curve over all repetitions is shown in blue perfect calibration the diagonal is dashed in red 8 leeuwenberg et al 3 3 1 predictive performance simulation results regarding calibration and discrimination for the xerostomia settings are reported in figure 1 results for dysphagia can be found in appendix figure 5 we observed no effects of collinearity on the predictive performance of any of the compared methods in terms of auroc r 2 intercept slope nor the calibration plots comparing a with a and b with b based on the calibration plots in figure 1 we do observe a slight overall overestimation of risk for lr compared to the other methods when extending the predictor set comparing b to a and b to a probably due to the lower epv we obtained similar results for the simulated dysphagia settings finding no effect of collinearity on predictive performance and little to no difference between the compared methods in any of the performance measures auroc r 2 intercept slope again lr yielded worse calibration compared to the other methods irrespective of the degree of collinearity as expected the difference between lr and the other compared methods was largest in terms of both calibration and discrimination in the setting with the lowest epv setting d with an epv of 2 indicating that lr suffers most from overfitting 3 3 2 coefficient estimation observing the estimation of the regression coefficients we find that in terms of mse between the estimated coefficients and the true coefficients particularly lr has a somewhat lower mse which is negatively affected by the increase in collinearity shown in figure 2 for xerostomia and in appendix figure 6 for dysphagia the remaining methods obtained lower mse and did not show a consistent effect of collinearity lr lasso ridge elasticnet pclr laelr dropout lrnn 0 0 0 2 0 4 0 6 0 8 1 0 1 2 m s e a a lr lasso ridge elasticnet pclr laelr dropout lrnn 0 0 0 2 0 4 0 6 0 8 1 0 1 2 m s e b b figure 2 across models the mean squared error between the estimated and the true coefficients for each method for the xerostomia settings red indicates high collinearity and blue low collinearity when observing the effect of collinearity on the stability of the predictor selection to what degree the same predictors were selected with the same directions of effect when repeating the model development process across simulations we find a negative effect of collinearity on selection stability for setting a epv of 23 across methods in figure 3 and in appendix figure 7 for settings b and c epvs of 8 and 6 respectively we only find a negative effect of collinearity for lr lasso and lrnn and no clear effect for the remaining methods ridge elasticnet pclr laelr and dropout for setting d epv of 2 we observe a clear positive effect of collinearity on the stability of predictor selection for ridge pclr and laelr a positive but weaker effect for elasticnet and dropout and a slight negative effect for lr lasso and lrnn our results suggests that the effect of collinearity can be explained by two aspects first collinearity negatively affects the stability of maximum likelihood based coefficient selection reducing mjics due to the increased variance in coefficient esti mation this explains why the negative effect remains present for lr and lrnn across all settings coefficient estimation for these methods is purely likelihood based the second aspect is that of regularization which can have a stabilizing effect of coefficient selection the degree of regu larization is determined by the hyperparameter tuning process which is indirectly impacted by the epv low epv settings are more likely to result in overfitting and consequently obtain a larger degree of regularization high epv settings are less prone leeuwenberg et al 9 lr lasso ridge elasticnet pclr laelr dropout lrnn 0 3 0 4 0 5 0 6 0 7 0 8 0 9 1 0 m ji c s a a lr lasso ridge elasticnet pclr laelr dropout lrnn 0 3 0 4 0 5 0 6 0 7 0 8 0 9 1 0 m ji c s b b figure 3 across models the mean proportion of coefficients with the same direction of effect after repetition for the xerostomia settings red indicates high collinearity and blue low collinearity to overfitting and are consequently obtain less regularization this can be observed from figure 4 for the xerostomia settings and in appendix figure 8 for dysphagia 5 h yp er pa ra m et er va lu e a b 0 20 40 60 80 100 c 2 c 1 cenet a b 0 25 0 30 0 35 0 40 0 45 a b 4 5 5 0 5 5 6 0 6 5 7 0 dlae dpca 1 2 3 figure 4 hyperparameter values for xerostomia per predictor set setting a being the small predictor set with high epv epv 23 and setting b the large predictor set with lower epv epv 8 the high collinearity settings in red and the low collinearity setting in blue the methods are distributed across three plots due to their different scales hyperparameter notation follows table 2 except for cenet which is the total shrinkage factor for elasticnet cl 1 cl 2 ridge dropout and elasticnet all quadratically penalize coefficient size resulting in a grouping effect of collinear predictors when regularization is strong and collinearity is high this constitutes a strong grouping effect which in turn stimulates stable predictor selection for pclr and laelr a larger degree of regularization implies a heavier dependence on the principal components that explain the variance among predictors as collinearity increases a smaller number of components is required to explain the same amount of variance among predictors this can be directly observed in figure 4 3 where for the large predictor set b hyperparameter tuning resulted in a smaller number of components for pclr and laelr when collinearity was higher this reliance on less components can in turn result in more stable coefficient estimation 5 notice that cl 1 cl 2 and cenet are inverse shrinkage factors meaning that lower values indicate a larger degree of shrinkage a larger degree of dropout ratio indicates a larger degree of regularization with regard to pclr and laelr it is important to notice that the number of components is only indicative of the degree of regularization within the same predictor set in figure 4 3 the larger number of components in setting b in comparison to setting a does not imply less regularization as the original dimension of b is in itself much larger than that of a 19 compared to 7 10 leeuwenberg et al for lasso and partially elasticnet more regularization implies a stronger predictor selection effect resulting in smaller models stronger selection in itself decreases the likelihood of by chance selecting the same coefficients when developing the model on a different sample we conjecture that this is the reason why lasso and lrnn have low overall mjics independently of collinearity compared to the other methods additionally as lasso s selection is likelihood based the negative impact of collinearity on predictor selection as observed for lr and lrnn also affects lasso this can be observed by the reduction of mjics in the high collinearity settings in figure 3 but also in appendix figure 7 4 discussion the current study investigated the effect of collinearity on predictive performance and the stability of coefficient estimation comparing eight different methods in a simulation study on the construction of prediction models that estimate the risk on complications after radiotherapy in head and neck cancer patients in this paper we found little to no impact of collinearity on predictive performance discrimination and calibration of the fitted models across methods and simulation settings as expected we found that collinearity has a negative impact on the stability of coefficient selection in high epv settings for all methods for settings with a lower epv that consequently required a larger amount of regularization the negative impact of collinearity on the stability of selected coefficients was lower for methods that distribute the explained outcome variance more evenly across collinear predictors via grouping ridge elasticnet and dropout or based on principal components pclr and laelr for standard logistic regression or methods that have a strong predictor selection effect lasso and non negative logistic regression the stability of predictor selection was lower compared to the other methods and was negatively influenced by collinearity across all simulations harrell 2001 8 mentioned that when there is no difference in the degree of collinearity between development and validation data collinearity is generally not considered a problem for predictive performance but can be problematic for reliable vari able selection when performing stepwise selection this was also confirmed by cohen cohen west and aiken 2003 34 and later also by dormann et al 2013 35 who compared 23 methods including various dimensionality reduction techniques and shrinkage basedmethods to address collinearity in five simulated ecological predictor response relationships the current study findings are in line with these earlier works and provides additional evidence to support this an important note to make is that in low dimensional settings where the number of predictors is smaller than the number of samples with correlating predictors earlier work by tibshirani 1996 11 13 and pavlou et al 2015 36 empirically found in their experiments that selection based approaches like lasso yielded lower predictive performance compared to for example ridge the current study did not find such a difference in predictive performance between lasso and ridge in any of the eight settings nevertheless for addressing collinearity in clinical prediction models we would recommend refraining from data driven pre dictor selection approaches like lasso because of the increased instability of predictor selection in the presence of collinearity even in relatively high epv settings if a model is model is interpreted at some point after its development e g to perform face validity checks or to explain predictions in clinical practice predictor selection could incorrectly give the impression that there is strong evidence in the data for the individual contribution of certain collinear predictors there are several limitations that should be considered when interpreting this study firstly the current work has our research has focused only on low dimensional settings and binary logistic regression models future studies may evaluate the effect of collinearity for instance in settings with multiple outcomes e g multinomial regression finally we focused on evaluation of predictive performance in the same population under no change of collinearity structure between the development and validation data collinearity has been shown to have a negative impact on performance under changes between development and validation data and is considered a difficult challenge to overcome for which a good understanding of the underlying mechanism causing the collinearity is crucial 35 we believe that beside being able to anticipate how harmful a change in collinearity between train and test data may be for predictive performance an interesting direction of future research is to study how background knowledge about the underlying collinearity mechanism and about potential changes of collinearity across development and validation data can be used to adapt prediction models accordingly acknowledgements this work is supported by the european union s horizon 2020 research and innovation programme under grant agreement no 825162 htx project leeuwenberg et al 11 references 1 teipel sj kurth j krause b grothe mj initiative adn others the relative importance of imaging markers for the pre diction of alzheimer s disease dementia in mild cognitive impairment beyond classical regression neuroimage clinical 2015 8 583 593 2 westerhuis me schuit e kwee a et al prediction of neonatal metabolic acidosis in women with a singleton term pregnancy in cephalic presentation american journal of perinatology 2012 29 03 167 174 3 narchi h alblooshi a prediction equations of forced oscillation technique the insidious role of collinearity respiratory research 2018 19 1 48 4 bosch v dl schuit e laan v dhp et al key challenges in normal tissue complication probability model development and validation towards a comprehensive strategy radiotherapy and oncology 2020 5 van der schaaf a bosch v dl both s schuit e langendijk j ep 1914 a method to deal with highly correlated explanatory variables in the development of ntcp models radiotherapy and oncology 2019 133 s 1040 6 schisterman ef perkins nj mumford sl ahrens ka mitchell em collinearity and causal diagrams a lesson on the importance of model specification epidemiology cambridge mass 2017 28 1 47 7 farrar de glauber rr multicollinearity in regression analysis the problem revisited the review of economic and statistics 1967 92 107 8 harrell jr fe regression modeling strategies with applications to linear models logistic and ordinal regression and survival analysis springer 2015 9 schuit e groenwold rh harrell fe et al unexpected predictor outcome associations in clinical prediction research causes and solutions cmaj 2013 185 10 e 499 e 505 10 moons kg altman dg vergouwe y royston p prognosis and prognostic research application and impact of prognostic models in clinical practice bmj 2009 338 b 606 11 tibshirani r regression shrinkage and selection via the lasso journal of the royal statistical society series b methodological 1996 58 1 267 288 12 hoerl ae kennard rw ridge regression biased estimation for nonorthogonal problems technometrics 1970 12 1 55 67 13 zou h hastie t regularization and variable selection via the elastic net journal of the royal statistical society series b statistical methodology 2005 67 2 301 320 14 riley rd snell ki martin gp et al penalisation and shrinkage methods produced unreliable clinical prediction models especially when sample size was small journal of clinical epidemiology 2020 15 hinton ge srivastava n krizhevsky a sutskever i salakhutdinov rr improving neural networks by preventing co adaptation of feature detectors arxiv preprint arxiv 1207 0580 2012 16 wager s wang s liang ps dropout training as adaptive regularization advances in neural information processing systems 2013 26 351 359 17 kendall mg others course in multivariate analysis charles griffin co 1965 18 aguilera am escabias m valderrama mj using principal components for estimating logistic regression with high dimensional multicollinear data computational statistics data analysis 2006 50 8 1905 1924 19 suarthana e vergouwe y moons kg et al a diagnostic model for the detection of sensitization to wheat allergens was developed and validated in bakery workers journal of clinical epidemiology 2010 63 9 1011 1019 12 leeuwenberg et al 20 kunin d bloom j goeva a seed c loss landscapes of regularized linear autoencoders international conference on machine learning 2019 3560 3569 21 hull d grefenstette g schulze b gaussier e sch tze h xerox trec 5 site report routing filtering nlp and spanish tracks nist special publication 1997 500238 167 180 22 calamai ph mor jj projected gradient methods for linearly constrained problems mathematical programming 1987 39 1 93 116 23 dritschilo a chaffey j bloomer w marck a the complication probability factor a method for selection of radiation treatment plans the british journal of radiology 1978 51 605 370 374 24 langendijk ja lambin p de ruysscher d widder j bos m verheij m selection of patients for radiotherapy with protons aiming at reduction of side effects the model based approach radiotherapy and oncology 2013 107 3 267 273 25 wolbarst ab chin lm svensson gk optimization of radiation therapy integral response of a model biological system international journal of radiation oncology biology physics 1982 8 10 1761 1769 26 kierkels rg wopken k visser r et al multivariable normal tissue complication probability model based treatment plan optimization for grade 2 4 dysphagia and tube feeding dependence in head and neck radiotherapy radiotherapy and oncology 2016 121 3 374 380 27 morris tp white ir crowther mj using simulation studies to evaluate statistical methods statistics in medicine 2019 38 11 2074 2102 28 neter j wasserman w kutner mh applied linear regression models irwin homewood il 1989 29 snoek j larochelle h adams rp practical bayesian optimization of machine learning algorithms advances in neural information processing systems 2012 2951 2959 30 kim jh estimating classification error rate repeated cross validation repeated hold out and bootstrap computational statistics data analysis 2009 53 11 3735 3745 31 pedregosa f varoquaux g gramfort a et al scikit learn machine learning in python journal of machine learning research 2011 12 2825 2830 32 paszke a gross s massa f et al pytorch an imperative style high performance deep learning library advances in neural information processing systems 2019 8026 8037 33 van calster b nieboer d vergouwe y de cock b pencina mj steyerberg ew a calibration hierarchy for risk models was defined from utopia to empirical data journal of clinical epidemiology 2016 74 167 176 34 cohen j cohen p west sg aiken ls applied multiple regression correlation analysis for the behavioral sciences routledge 2013 35 dormann cf elith j bacher s et al collinearity a review of methods to deal with it and a simulation study evaluating their performance ecography 2013 36 1 27 46 36 pavlou m ambler g seaman s de iorio m omar rz review and evaluation of penalised regression methods for risk prediction in low dimensional data with few events statistics in medicine 2016 35 7 1159 1177 37 brouwer cl steenbakkers rj bourhis j et al ct based delineation of organs at risk in the head and neck region dahanca eortc gortec hknpcsg ncic ctg ncri nrg oncology and trog consensus guidelines radiotherapy and oncology 2015 117 1 83 90 38 kingma dp ba j adam a method for stochastic optimization arxiv preprint arxiv 1412 6980 2014 39 udell m generalized low rank models stanford university 2015 leeuwenberg et al 13 40 riley rd ensor j snell ki et al calculating the sample size required for developing a clinical prediction model bmj 2020 368 41 beetz i schilstra c schaaf v da et al ntcp models for patient rated xerostomia and sticky saliva after treatment with intensity modulated radiotherapy for head and neck cancer the role of dosimetric and clinical factors radiotherapy and oncology 2012 105 1 101 106 42 lee tf liou mh ting hm et al patient and therapy related factors associated with the incidence of xerostomia in nasopharyngeal carcinoma patients receiving parotid sparing helical tomotherapy scientific reports 2015 5 1 1 13 43 christianen me schaaf v da laan v dhp et al swallowing sparing intensity modulated radiotherapy sw imrt in head and neck cancer clinical validation according to the model based approach radiotherapy and oncology 2016 118 2 298 303 14 leeuwenberg et al appendix a results for the simulated dysphagia settings method c epv 6 mvif 7 c epv 6 mvif 43 d epv 2 mvif 7 d epv 2 mvif 43 lr 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 86 0 84 0 87 r 0 38 0 33 0 41 intercept 0 02 0 31 0 33 slope 0 86 0 70 1 05 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 85 0 84 0 88 r 0 37 0 32 0 44 intercept 0 01 0 29 0 34 slope 0 89 0 68 1 10 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 81 0 78 0 83 r 0 27 0 22 0 31 intercept 0 04 0 41 0 29 slope 0 59 0 43 0 72 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 81 0 79 0 84 r 0 28 0 23 0 33 intercept 0 02 0 30 0 32 slope 0 64 0 47 0 76 lasso 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 86 0 85 0 87 r 0 38 0 35 0 41 intercept 0 02 0 28 0 29 slope 1 04 0 80 1 31 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 86 0 84 0 89 r 0 38 0 34 0 45 intercept 0 01 0 26 0 26 slope 1 08 0 81 1 39 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 83 0 81 0 85 r 0 32 0 28 0 36 intercept 0 00 0 27 0 28 slope 1 10 0 81 1 49 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 84 0 83 0 85 r 0 33 0 30 0 36 intercept 0 00 0 26 0 24 slope 1 14 0 83 1 65 ridge 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 87 0 85 0 88 r 0 39 0 36 0 41 intercept 0 02 0 28 0 30 slope 1 09 0 82 1 35 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 86 0 84 0 89 r 0 38 0 34 0 45 intercept 0 01 0 27 0 26 slope 1 12 0 83 1 44 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 84 0 82 0 85 r 0 33 0 29 0 36 intercept 0 00 0 24 0 29 slope 1 09 0 80 1 41 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 84 0 83 0 85 r 0 34 0 32 0 37 intercept 0 00 0 23 0 26 slope 1 12 0 88 1 47 elasticnet 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 86 0 85 0 88 r 0 39 0 36 0 41 intercept 0 02 0 27 0 30 slope 1 07 0 84 1 39 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 86 0 84 0 89 r 0 38 0 34 0 45 intercept 0 01 0 27 0 28 slope 1 12 0 83 1 44 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 84 0 82 0 85 r 0 33 0 29 0 36 intercept 0 00 0 25 0 28 slope 1 09 0 76 1 35 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 84 0 83 0 85 r 0 34 0 32 0 37 intercept 0 00 0 24 0 26 slope 1 12 0 89 1 47 pclr 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 86 0 85 0 87 r 0 38 0 34 0 41 intercept 0 02 0 29 0 33 slope 0 92 0 74 1 10 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 86 0 84 0 89 r 0 38 0 33 0 45 intercept 0 01 0 29 0 32 slope 0 96 0 73 1 20 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 83 0 81 0 85 r 0 32 0 27 0 36 intercept 0 01 0 28 0 31 slope 0 88 0 63 1 10 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 84 0 82 0 85 r 0 33 0 30 0 36 intercept 0 00 0 26 0 26 slope 0 93 0 68 1 20 laelr 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 86 0 85 0 87 r 0 38 0 34 0 41 intercept 0 02 0 30 0 32 slope 0 92 0 73 1 13 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 86 0 84 0 89 r 0 38 0 33 0 45 intercept 0 01 0 29 0 31 slope 0 96 0 73 1 19 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 83 0 81 0 85 r 0 31 0 27 0 36 intercept 0 01 0 30 0 35 slope 0 88 0 66 1 14 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 84 0 82 0 85 r 0 33 0 30 0 36 intercept 0 00 0 27 0 34 slope 0 94 0 70 1 26 dropout 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 86 0 85 0 87 r 0 39 0 36 0 42 intercept 0 06 0 27 0 34 slope 1 10 0 84 1 35 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 86 0 84 0 89 r 0 38 0 34 0 45 intercept 0 03 0 26 0 32 slope 1 12 0 84 1 45 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 84 0 82 0 85 r 0 33 0 29 0 36 intercept 0 07 0 23 0 40 slope 1 02 0 78 1 32 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 84 0 83 0 85 r 0 34 0 31 0 36 intercept 0 05 0 23 0 32 slope 1 02 0 77 1 28 lrnn 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 86 0 85 0 87 r 0 39 0 35 0 41 intercept 0 02 0 31 0 34 slope 0 90 0 74 1 09 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 86 0 84 0 89 r 0 38 0 34 0 45 intercept 0 01 0 29 0 32 slope 0 93 0 73 1 14 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 84 0 82 0 85 r 0 33 0 29 0 36 intercept 0 02 0 30 0 29 slope 0 82 0 66 0 99 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 84 0 83 0 85 r 0 34 0 30 0 36 intercept 0 00 0 25 0 28 slope 0 87 0 66 1 08 figure 5 predictive performance results for the dysphagia simulations lowess smoothed calibration curves per simulation are plotted in grey the calibration curve over all repetitions is shown in blue perfect calibration the diagonal is dashed in red leeuwenberg et al 15 lr lasso ridge elasticnet pclr laelr dropout lrnn 0 0 0 5 1 0 1 5 2 0 2 5 m s e c c lr lasso ridge elasticnet pclr laelr dropout lrnn 0 0 0 5 1 0 1 5 2 0 2 5 m s e d d figure 6 per method the mean squared error between the estimated and the true coefficients for each method for the dysphagia settings red indicates high collinearity and blue low collinearity lr lasso ridge elasticnet pclr laelr dropout lrnn 0 3 0 4 0 5 0 6 0 7 0 8 0 9 1 0 m ji c s c c lr lasso ridge elasticnet pclr laelr dropout lrnn 0 3 0 4 0 5 0 6 0 7 0 8 0 9 1 0 m ji c s d d figure 7 per method the mean proportion of coefficients with the same direction of effect after repetition for the dysphagia settings red indicates high collinearity and blue low collinearity h yp er pa ra m et er va lu e c d 0 20 40 60 80 100 c 2 c 1 cenet c d 0 300 0 325 0 350 0 375 0 400 0 425 0 450 0 475 c d 5 6 7 8 9 10 dlae dpca 1 2 3 figure 8 hyperparameter values for dysphagia per predictor set c being the small predictor set with relatively high epv epv 6 and d the large predictor set with lower epv epv 2 the high collinearity settings in red and the low collinearity setting in blue the methods are distributed across three plots due to their different scales hyperparameter notation follows table 2 except for cenet which is the total shrinkage factor for elasticnet cl 1 cl 2 16 leeuwenberg et al b results for the real data settings method a b c d lr 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 78 r 0 27 0 22 0 29 intercept 0 01 0 01 0 02 slope 0 86 0 72 0 95 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 76 0 75 0 78 r 0 25 0 21 0 28 intercept 0 01 0 02 0 03 slope 0 72 0 59 0 83 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 83 0 82 0 84 r 0 31 0 29 0 33 intercept 0 00 0 03 0 02 slope 0 83 0 76 0 88 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 80 0 79 0 82 r 0 26 0 24 0 29 intercept 0 03 0 09 0 01 slope 0 61 0 55 0 66 lasso 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 79 r 0 28 0 27 0 29 intercept 0 00 0 01 0 01 slope 1 14 1 05 1 20 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 77 0 76 0 78 r 0 27 0 26 0 29 intercept 0 00 0 02 0 02 slope 1 11 1 01 1 20 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 84 0 83 0 85 r 0 33 0 31 0 34 intercept 0 00 0 02 0 02 slope 1 04 0 96 1 09 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 84 0 83 0 84 r 0 31 0 29 0 33 intercept 0 00 0 02 0 03 slope 1 06 0 97 1 14 ridge 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 78 r 0 28 0 27 0 29 intercept 0 00 0 01 0 01 slope 1 15 1 09 1 20 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 78 r 0 28 0 27 0 29 intercept 0 00 0 01 0 01 slope 1 15 1 09 1 21 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 84 0 83 0 84 r 0 32 0 30 0 33 intercept 0 00 0 02 0 01 slope 1 04 0 97 1 09 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 84 0 83 0 84 r 0 32 0 30 0 33 intercept 0 00 0 01 0 02 slope 1 07 1 01 1 13 elasticnet 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 78 r 0 28 0 27 0 29 intercept 0 00 0 01 0 01 slope 1 15 1 06 1 21 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 78 r 0 28 0 27 0 29 intercept 0 00 0 01 0 01 slope 1 14 1 05 1 20 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 84 0 83 0 85 r 0 32 0 30 0 34 intercept 0 00 0 02 0 02 slope 1 03 0 96 1 10 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 84 0 83 0 84 r 0 31 0 30 0 33 intercept 0 00 0 02 0 02 slope 1 06 0 98 1 13 pclr 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 78 r 0 27 0 23 0 29 intercept 0 00 0 01 0 02 slope 0 89 0 77 0 97 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 78 r 0 27 0 19 0 29 intercept 0 00 0 02 0 02 slope 0 89 0 63 1 00 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 83 0 82 0 84 r 0 31 0 29 0 33 intercept 0 00 0 02 0 02 slope 0 89 0 82 0 95 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 83 0 81 0 84 r 0 29 0 27 0 32 intercept 0 00 0 06 0 04 slope 0 82 0 73 0 89 laelr 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 78 r 0 27 0 23 0 29 intercept 0 00 0 01 0 02 slope 0 89 0 76 0 97 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 78 r 0 27 0 22 0 29 intercept 0 00 0 03 0 04 slope 0 91 0 73 1 00 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 83 0 82 0 84 r 0 31 0 29 0 33 intercept 0 00 0 02 0 04 slope 0 89 0 80 0 95 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 82 0 81 0 84 r 0 28 0 24 0 31 intercept 0 00 0 08 0 09 slope 0 83 0 70 0 94 dropout 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 78 r 0 25 0 21 0 29 intercept 0 03 0 00 0 06 slope 1 13 0 93 1 40 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 78 r 0 26 0 23 0 29 intercept 0 02 0 02 0 05 slope 1 00 0 85 1 14 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 84 0 83 0 84 r 0 32 0 29 0 34 intercept 0 04 0 00 0 08 slope 1 06 0 97 1 17 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 83 0 83 0 84 r 0 31 0 29 0 33 intercept 0 05 0 01 0 10 slope 1 04 0 94 1 15 lrnn 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 78 0 77 0 78 r 0 27 0 20 0 29 intercept 0 01 0 00 0 02 slope 0 87 0 69 0 95 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 77 0 76 0 78 r 0 26 0 21 0 29 intercept 0 01 0 01 0 02 slope 0 79 0 62 0 88 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 84 0 83 0 85 r 0 32 0 30 0 34 intercept 0 00 0 02 0 02 slope 0 87 0 81 0 91 0 0 0 2 0 4 0 6 0 8 1 0 0 0 0 2 0 4 0 6 0 8 1 0 auc 0 84 0 83 0 84 r 0 31 0 29 0 33 intercept 0 00 0 03 0 02 slope 0 82 0 77 0 87 figure 9 predictive performance results for the real data xerostomia and dysphagia settings lowess smoothed calibration curves per simulation are plotted in grey the calibration curve over all repetitions is shown in blue perfect calibration the diagonal is dashed in red leeuwenberg et al 17 c baseline tables table 4 general study population characteristics for age we report the median and standard deviation for the other variables we report the number and the percentage of the whole population umcg n 740 patient characteristics at baseline age 63 10 gender men 552 75 women 188 25 primary tumor site pharynx 356 48 larynx 344 46 oral cavity 40 5 treatment modality conventional radiotherapy 159 21 accelerated radiotherapy 287 39 chemoradiation 252 34 bioradiation 42 6 t classification tis t 1 136 18 t 2 246 33 t 3 182 25 t 4 176 24 n classification n 0 356 48 n 1 64 9 n 2 303 41 n 3 17 2 predicted outcomes at m 6 xerostomia grade 2 200 27 dysphagia grade 2 102 14 18 leeuwenberg et al table 5 predictors used in each predictor set refers to continuous predictors and 0 1 refers to binary predictors pcm refers to the pharyngeal constrictor muscle organs at risk were delineated following consensus guidelines 37 dose is expressed in gray gy a b c d predictor dim 7 dim 19 dim 13 dim 43 age grade 1 complications at baseline 0 1 0 1 0 1 0 1 grade 2 complications at baseline 0 1 0 1 0 1 0 1 submandibular left mean dose submandibular left v 10 v 30 v 50 submandibular right mean dose submandibular right v 10 v 30 v 50 parotid left mean dose parotid left v 10 v 30 v 50 parotid right mean dose parotid right v 10 v 30 v 50 pcm superior dm pcm superior v 10 30 50 pcm med dm pcm med v 10 30 50 pcm inferior dm pcm inferior v 10 30 50 supraglottic dm supraglottic v 10 30 50 oralcavity ext dm oralcavity ext v 10 30 50 glotticarea dm glotticarea v 10 30 50 leeuwenberg et al 19 d definition of methods in this section we provide a concise but more precise description of each method used in the experiments of this paper to facilitate easier reproducibility d 1 the data in this appendix we use matrix x x 1 x 2 xn of dimension n d to describe the n feature vectors representing the n patients each feature vector xi is of dimension d the outcome vector y is used to denote the n outcome labels one or zero where yi is the value at dimension i of y corresponding to the observed ground truth outcome of xi d 2 the link function with the term link function we refer to the parameterized function that maps the predictor values to predicted outcome probability for all methods the link function used to calculate predicted outcome y i from input xi is of logistic regression form f xi 0 1 e xi 0 4 where 1 2 d is the vector of coefficients and 0 the intercept d 3 the objective function the objective function defines what are considered optimal coefficients for each method in some literature referred to as the training criterion a method s objective function is generally defined to return coefficients and intercept 0 that minimize the training error which is given by a loss function in some literature referred to as the cost function since all methods in this study have logistic regression form the only difference between the compared methods is how coefficients are obtained from the data i e they differ only in their objective function the component of the objective function that is shared by all methods is the cross entropy loss i e the negative log likelihood lml 0 n i 1 y log f xi 0 1 y log 1 f xi 0 5 for our baseline model regular logistic regression the corresponding objective consists purely of finding coefficient values and intercept value 0 that minimize the cross entropy equivalent to maximum likelihood 0 argmin 0 lml 0 6 in the sections below we will specify for each method how the corresponding objective differs d 4 loss minimization and hyperparameter tuning unless specified otherwise each method s loss function is minimized using adam 38 for at most 1000 epochs using early stopping with a patience of 500 epochs and a maximum learning rate of 0 1 for tuning of important hyperparameters that may come with certain methods we use nested cross validation in a bayesian optimization setting 29 using 10 iterations of a gaussian process with a certain prior and range the range and prior of each hyperparameter are given in the sections below d 5 lasso for lasso the only difference with regard to the baseline is an extension of the lml 0 with a penalty on the size of the coefficient values the l 1 norm of the coefficients shown in equation 7 where cl 1 is a hyperparameter determining the inverse importance of the l 1 penalty ll 1 lml 0 1 cl 1 7 0 argmin 0 ll 1 8 20 leeuwenberg et al the tuning range for cl 1 is 10 3 102 and we used a log linear prior the final objective is shown in equation 8 d 6 ridge for ridge the modification with regard to the baseline is very similar as for lasso except that the penalty on coefficient size is the l 2 norm of the this results in loss function and objective functions 9 and 10 respectively ll 2 lml 0 1 cl 2 2 9 0 argmin 0 ll 2 10 for ridge the hyperparameter cl 2 is tuned with the same range and prior as cl 1 for lasso d 7 elastic net elastic net is the application of both a lasso and a ridge penalty to the model consequently the final loss function and objective function of elastic net can be given by equations 11 and 12 respectively lelasticnet lml 0 1 cl 1 1 cl 2 2 11 0 argmin 0 lelasticnet 12 the tuning procedure of hyperparameters cl 1 and cl 2 is the same as for lasso and ridge d 8 principal component logistic regression plcr in pclr the input matrix x is first projected into its principal components using principal component analysis pca before applying logistic regression pca aims to find a linear projectionw that maps x to a smaller set of non correlating latent variables h w x the principal components that best capture the variance in x the loss function and objective to find projection w are given by equations 13 and 14 respectively 39 the number of latent variables or principal components is tuned using a linear prior over integer values in 4 d lpca w x w wx 2 13 w argmin w lpca w 14 subject tow w i after projecting each input vector xi x to its latent vector i h a logistic regression g i 0 is fitted to relate the latent variablesh to the outcome y using coefficients and intercept 0 following standard maximum likelihood optimization equation 15 0 argmin 0 lml 0 15 in this study we rewrite pca projection w and logistic regression g to an equivalent link function f xi 0 as in equation 4 this is possible as w is a linear projection by setting w 1 i and 0 0 this way f xi 0 is equivalent to first projecting xi to latent vector i and afterwards obtaining predicted y i using g i 0 by doing this pclr becomes directly comparable in terms of coefficients to the other methods mentioned in this article leeuwenberg et al 21 d 9 linear auto encoder logistic regression laelr linear auto encoders lae are similar to pca the aim of lae is to find a linear projection w from the input data x to a set of latent variables h that explain the variance in x in the case of lae through a linear reconstruction projection v in contrast to pca for lae there is no orthogonality constraint on h and the dimensions of h are not ordered by explained variance nevertheless lae find projections to the same axis as pca 20 llae w v x v wx 2 16 llaelr w v 0 lml 0 1 clae llae w v 17 similar to pclr h is related to y using a logistic regression function g i 0 and obtain the final coefficients of f xi 0 by setting w 1 i and 0 0 and the number of latent variables is tuned using a linear prior over inte ger values in 4 d however the difference between pclr and laelr in this study is that instead of optimizingw only on the reconstruction loss equation 16 we optimize bothw and g jointly on the combined loss and corresponding objective shown in equations 17 and 18 respectively this way projection w is not purely optimized to explain the variance in x but also for a part to facilitate explanation of variance in y if clae is very small the objective becomes similar to pclr whereas if clae is very large the overall objective is similar to standard logistic regression to empirically balance two loss functions we tune hyperparameter clae with a log linear prior in the same range as the penalty of lasso and ridge 10 3 102 w 0 argmin w 0 llaelr w v 0 18 d 10 dropout regularization dropout training was proposed as a regularization method to prevent co adaptation of weights in neural networks 15 dropout works in iterative gradient based training procedures like the one used in the current work described in appendix d 4 when using dropout a sub model is randomly selected at each training iteration effectively dropping out a random percentage of the model s coefficients this selected sub model is used to make predictions as part of that training iteration and the involved coefficients are updated accordingly because at each iteration not all coefficients are involved in the model update co adaptation of the coefficients is disrupted when training is completed the coefficients are scaled down by a factor 1 to maintain the same expected output of the model during testing as during training correcting for the fact that during training the full model was never used as a whole the current work uses the dropout implementation in pytorch which is inverted dropout used in most software implementations in inverted dropout the coefficients are temporarily scaled during training by a factor 1 1 instead of after training is completed this way no scaling is required when applying the model for logistic regression 6 the loss function of dropout can be given by equation 19 in which we abbreviate f xi 0 as fi for clarity 16 the corresponding objective is given by equation 20 ldropout 0 lml 0 1 2 1 n i 1 d j 1 fi 1 fi x 2 ij 2 j 19 w argmin w ldropout w 20 the loss function of dropout for logistic regression models can be summarized in two parts first like ridge it includes a quadratic penalty on the size of the coefficients shown on the far right of the equation 2 j second it includes an addi tional penalty discouraging moderate predictions during training close to 0 5 shown by fi 1 fi the degree of dropout regularization is determined by hyperparameter which we tune using a linear prior over the interval 0 1 0 5 6 for other model architectures than logistic regression the loss function of dropout is different the more general formulation is provided in the original article 15 22 leeuwenberg et al d 11 non negative logistic regression lrnn sometimes the coefficient search space can be constrained based on prior knowledge preventing the model s coefficient esti mation procedure from exploring coefficients that are assumed invalid by the modeler this can help reduce co adaptation of coefficients and their inflation and may improve the model s predictive performance if the assumption is valid in this study we explore the use of non negativity constraints on dosage coefficients oar as we believe increasing dose to oar should not result in a decrease in predicted risk of complications the only difference in this method compared to standard logistic regression is that the feasible coefficient values for all dosage parameters are constrained to the non negative region during loss minimization shown in the objective in eq 21 0 argmin 0 lml 0 with i oar i 0 21 in terms of implementation we enforce the constraint during our gradient based minimization through gradient projection setting all negative dosage coefficients to 0 after each coefficient update e performed sample size calculations in table 6 we report recommended sample size rss calculations we used the method by riley et al 2020 40 following their recommendations on the chosen parameters for doing this calculation reported below in table 6 as expected r 2 nagelkerke we take the mean of r 2 nagelkerke values reported in the literature for ntcp models with the same outcomes as the current study xerostomia 41 42 and dysphagia 43 six months after radiotherapy setting p r 2 nagelkerke s rss a 0 27 7 0 05 0 42 0 9 0 05 315 b 0 27 19 0 05 0 42 0 9 0 05 794 c 0 14 13 0 05 0 26 0 9 0 05 744 d 0 14 43 0 05 0 26 0 9 0 05 2460 table 6 parameters used in the sample size calculations for each setting the anticipated outcome proportion the number of predictors p the absolute margin of error nagelkerke s explained variance r 2 nagelkerke the expected uniform shrinkage factor s and the expected optimism leeuwenberg et al 23 f correlation plots in all correlation plots negative correlations are indicated in blue and positive correlations in red subm l dm 0 28 subm r dm 0 29 0 89 parotid l dm 0 31 0 86 0 77 parotid r dm 0 30 0 72 0 83 0 78 xer bsl 1 0 04 0 15 0 10 0 08 0 00 xer bsl 2 0 02 0 07 0 08 0 06 0 07 0 03 a g e subm l d m subm r d m parotid l d m parotid r d m x e r b sl 1 subm l dm 0 25 subm r dm 0 27 0 88 parotid l dm 0 26 0 85 0 72 parotid r dm 0 28 0 69 0 83 0 74 pcm sup dm 0 28 0 85 0 86 0 82 0 81 pcm med dm 0 25 0 87 0 89 0 74 0 72 0 79 pcm inf dm 0 11 0 11 0 13 0 11 0 12 0 35 0 11 supraglottic dm 0 08 0 36 0 38 0 27 0 27 0 12 0 60 0 69 oralcavity ext dm 0 28 0 83 0 85 0 80 0 80 0 95 0 73 0 40 0 07 glotticarea dm 0 15 0 33 0 34 0 31 0 31 0 56 0 16 0 89 0 54 0 60 dysfagie bsl 1 0 04 0 24 0 25 0 25 0 21 0 24 0 23 0 06 0 03 0 28 0 12 dysfagie bsl 2 0 09 0 18 0 17 0 23 0 23 0 17 0 20 0 06 0 13 0 21 0 01 0 10 a g e subm l d m subm r d m parotid l d m parotid r d m pc m sup d m pc m m ed d m pc m inf d m supraglottic d m o ralc avity e xt d m g lottica rea d m d y sfa g ie b sl 1 figure 10 correlation matrix for the a setting left and the c setting right subm l dm 0 28 subm l v 10 0 25 0 94 subm l v 30 0 24 0 95 0 97 subm l v 50 0 28 0 94 0 82 0 85 subm r dm 0 29 0 89 0 90 0 89 0 83 subm r v 10 0 24 0 90 0 96 0 94 0 78 0 94 subm r v 30 0 24 0 90 0 96 0 94 0 79 0 95 0 99 subm r v 50 0 28 0 82 0 77 0 76 0 83 0 93 0 80 0 82 parotid l dm 0 31 0 86 0 76 0 77 0 83 0 77 0 74 0 74 0 74 parotid l v 10 0 29 0 89 0 84 0 84 0 84 0 82 0 81 0 81 0 77 0 93 parotid l v 30 0 30 0 81 0 71 0 73 0 79 0 71 0 68 0 69 0 70 0 98 0 89 parotid l v 50 0 29 0 67 0 52 0 54 0 69 0 55 0 50 0 50 0 58 0 91 0 72 0 90 parotid r dm 0 30 0 72 0 70 0 69 0 69 0 83 0 73 0 74 0 80 0 78 0 80 0 77 0 62 parotid r v 10 0 28 0 79 0 79 0 77 0 73 0 87 0 81 0 82 0 82 0 82 0 89 0 78 0 60 0 92 parotid r v 30 0 28 0 67 0 65 0 64 0 64 0 78 0 67 0 69 0 75 0 75 0 75 0 77 0 61 0 98 0 87 parotid r v 50 0 28 0 48 0 43 0 43 0 50 0 62 0 46 0 47 0 63 0 58 0 54 0 60 0 52 0 90 0 67 0 89 xer bsl 1 0 04 0 15 0 13 0 14 0 17 0 10 0 13 0 12 0 09 0 08 0 10 0 05 0 05 0 00 0 03 0 02 0 04 xer bsl 2 0 02 0 07 0 08 0 07 0 05 0 08 0 09 0 08 0 05 0 06 0 07 0 06 0 04 0 07 0 08 0 06 0 05 0 03 a g e subm l d m subm l v 10 subm l v 30 subm l v 50 subm r d m subm r v 10 subm r v 30 subm r v 50 parotid l d m parotid l v 10 parotid l v 30 parotid l v 50 parotid r d m parotid r v 10 parotid r v 30 parotid r v 50 x e r b sl 1 figure 11 correlation matrix for the b setting 24 l e e u w e n b e r g e t a l subm l dm 0 25 subm l v 10 0 22 0 93 subm l v 30 0 23 0 95 0 96 subm l v 50 0 23 0 94 0 80 0 85 subm r dm 0 27 0 88 0 91 0 88 0 81 subm r v 10 0 22 0 90 0 97 0 93 0 77 0 93 subm r v 30 0 22 0 89 0 96 0 93 0 78 0 95 0 98 subm r v 50 0 23 0 81 0 79 0 76 0 82 0 93 0 80 0 83 parotid l dm 0 26 0 85 0 73 0 75 0 81 0 72 0 72 0 71 0 70 parotid l v 10 0 26 0 88 0 83 0 82 0 82 0 81 0 81 0 80 0 75 0 92 parotid l v 30 0 26 0 81 0 68 0 72 0 78 0 67 0 66 0 66 0 66 0 98 0 87 parotid l v 50 0 21 0 65 0 48 0 51 0 66 0 49 0 47 0 46 0 51 0 91 0 69 0 90 parotid r dm 0 28 0 69 0 69 0 66 0 65 0 83 0 71 0 73 0 80 0 74 0 78 0 72 0 56 parotid r v 10 0 27 0 78 0 79 0 75 0 72 0 87 0 81 0 81 0 83 0 79 0 88 0 75 0 56 0 92 parotid r v 30 0 25 0 63 0 63 0 60 0 59 0 77 0 65 0 67 0 75 0 71 0 73 0 71 0 55 0 98 0 86 parotid r v 50 0 24 0 45 0 43 0 41 0 45 0 62 0 45 0 47 0 63 0 54 0 53 0 53 0 47 0 91 0 69 0 90 pcm sup dm 0 28 0 85 0 82 0 79 0 81 0 86 0 82 0 82 0 83 0 82 0 89 0 75 0 61 0 81 0 89 0 74 0 59 pcm sup v 10 0 23 0 89 0 94 0 91 0 79 0 91 0 95 0 94 0 81 0 77 0 87 0 70 0 50 0 75 0 86 0 67 0 48 0 90 pcm sup v 30 0 25 0 87 0 86 0 84 0 81 0 88 0 86 0 86 0 83 0 80 0 89 0 74 0 56 0 79 0 89 0 72 0 54 0 97 0 94 pcm sup v 50 0 28 0 67 0 58 0 56 0 70 0 68 0 58 0 58 0 72 0 72 0 76 0 67 0 58 0 74 0 77 0 69 0 60 0 91 0 66 0 83 pcm med dm 0 25 0 87 0 86 0 85 0 80 0 89 0 87 0 86 0 82 0 74 0 79 0 70 0 53 0 72 0 78 0 68 0 51 0 79 0 83 0 81 0 63 pcm med v 10 0 23 0 86 0 93 0 89 0 74 0 87 0 94 0 91 0 74 0 67 0 74 0 63 0 44 0 65 0 73 0 60 0 42 0 75 0 87 0 79 0 53 0 88 pcm med v 30 0 24 0 88 0 92 0 90 0 78 0 89 0 93 0 92 0 78 0 71 0 78 0 68 0 48 0 71 0 78 0 66 0 47 0 78 0 88 0 83 0 57 0 94 0 94 pcm med v 50 0 23 0 72 0 65 0 66 0 71 0 74 0 66 0 67 0 74 0 65 0 67 0 64 0 51 0 64 0 67 0 62 0 49 0 69 0 64 0 69 0 63 0 91 0 65 0 76 pcm inf dm 0 11 0 11 0 12 0 06 0 14 0 13 0 13 0 11 0 13 0 11 0 18 0 03 0 05 0 12 0 19 0 04 0 07 0 35 0 25 0 28 0 35 0 11 0 06 0 02 0 20 pcm inf v 10 0 00 0 22 0 19 0 24 0 23 0 16 0 14 0 17 0 17 0 13 0 06 0 19 0 13 0 08 0 02 0 14 0 10 0 00 0 02 0 01 0 02 0 24 0 25 0 26 0 19 0 38 pcm inf v 30 0 03 0 04 0 03 0 09 0 02 0 02 0 02 0 03 0 02 0 05 0 00 0 12 0 05 0 06 0 01 0 13 0 08 0 14 0 09 0 10 0 15 0 21 0 08 0 18 0 26 0 78 0 50 pcm inf v 50 0 13 0 21 0 19 0 15 0 25 0 21 0 20 0 19 0 23 0 19 0 24 0 13 0 12 0 20 0 25 0 13 0 15 0 41 0 29 0 34 0 42 0 01 0 14 0 09 0 11 0 93 0 20 0 63 supraglottic dm 0 08 0 36 0 36 0 38 0 29 0 38 0 36 0 36 0 33 0 27 0 25 0 30 0 21 0 27 0 26 0 30 0 20 0 12 0 23 0 19 0 03 0 60 0 42 0 49 0 61 0 69 0 38 0 62 0 61 supraglottic v 10 0 15 0 63 0 66 0 65 0 55 0 61 0 65 0 64 0 54 0 45 0 46 0 46 0 32 0 43 0 41 0 43 0 31 0 43 0 51 0 46 0 29 0 67 0 75 0 71 0 50 0 20 0 73 0 34 0 04 0 57 supraglottic v 30 0 10 0 48 0 50 0 52 0 41 0 50 0 51 0 50 0 44 0 40 0 41 0 43 0 28 0 42 0 42 0 44 0 30 0 30 0 39 0 35 0 19 0 65 0 56 0 64 0 58 0 48 0 47 0 65 0 34 0 85 0 70 supraglottic v 50 0 08 0 17 0 16 0 18 0 12 0 20 0 16 0 17 0 17 0 15 0 11 0 18 0 14 0 16 0 13 0 20 0 14 0 02 0 07 0 04 0 06 0 44 0 20 0 28 0 54 0 72 0 23 0 57 0 71 0 92 0 31 0 66 oralcavity ext dm 0 28 0 83 0 78 0 76 0 82 0 85 0 79 0 79 0 83 0 80 0 86 0 74 0 61 0 80 0 86 0 74 0 61 0 95 0 86 0 92 0 87 0 73 0 73 0 74 0 61 0 40 0 02 0 17 0 47 0 07 0 43 0 27 0 09 oralcavity ext v 10 0 23 0 89 0 93 0 90 0 80 0 91 0 93 0 93 0 82 0 78 0 88 0 71 0 52 0 76 0 87 0 68 0 50 0 91 0 99 0 94 0 69 0 82 0 86 0 86 0 64 0 27 0 01 0 11 0 31 0 21 0 50 0 37 0 05 0 89 oralcavity ext v 30 0 26 0 82 0 77 0 75 0 82 0 84 0 78 0 78 0 83 0 80 0 86 0 74 0 60 0 78 0 84 0 72 0 58 0 95 0 85 0 93 0 87 0 73 0 72 0 73 0 62 0 37 0 04 0 16 0 44 0 07 0 44 0 26 0 09 0 97 0 88 oralcavity ext v 50 0 28 0 61 0 49 0 47 0 66 0 63 0 50 0 50 0 68 0 66 0 66 0 62 0 58 0 69 0 68 0 65 0 59 0 80 0 58 0 70 0 84 0 50 0 46 0 48 0 45 0 45 0 01 0 19 0 54 0 08 0 27 0 14 0 19 0 90 0 61 0 83 glotticarea dm 0 15 0 33 0 31 0 24 0 36 0 34 0 33 0 31 0 34 0 31 0 39 0 22 0 21 0 31 0 39 0 23 0 22 0 56 0 44 0 49 0 55 0 16 0 28 0 22 0 08 0 89 0 34 0 65 0 87 0 54 0 03 0 31 0 61 0 60 0 47 0 57 0 61 glotticarea v 10 0 01 0 18 0 12 0 21 0 20 0 11 0 08 0 11 0 12 0 10 0 02 0 16 0 11 0 05 0 05 0 11 0 10 0 03 0 02 0 02 0 04 0 19 0 17 0 20 0 16 0 36 0 95 0 48 0 19 0 35 0 63 0 43 0 22 0 01 0 03 0 02 0 00 0 37 glotticarea v 30 0 03 0 03 0 06 0 03 0 02 0 04 0 07 0 04 0 01 0 03 0 10 0 06 0 00 0 01 0 08 0 05 0 04 0 19 0 16 0 16 0 16 0 09 0 02 0 06 0 14 0 63 0 55 0 76 0 51 0 50 0 29 0 54 0 46 0 20 0 18 0 19 0 19 0 69 0 59 glotticarea v 50 0 16 0 38 0 35 0 30 0 42 0 37 0 36 0 34 0 39 0 32 0 40 0 25 0 21 0 34 0 39 0 26 0 25 0 57 0 44 0 49 0 56 0 19 0 32 0 27 0 10 0 82 0 15 0 52 0 89 0 48 0 11 0 18 0 62 0 62 0 47 0 59 0 64 0 93 0 17 0 49 dysfagie bsl 1 0 04 0 24 0 20 0 20 0 25 0 25 0 20 0 21 0 27 0 25 0 23 0 24 0 22 0 21 0 23 0 20 0 15 0 24 0 22 0 23 0 23 0 23 0 19 0 20 0 20 0 06 0 06 0 02 0 10 0 03 0 14 0 05 0 01 0 28 0 23 0 26 0 28 0 12 0 05 0 01 0 13 dysfagie bsl 2 0 09 0 18 0 13 0 14 0 20 0 17 0 13 0 14 0 19 0 23 0 20 0 23 0 25 0 23 0 17 0 21 0 26 0 17 0 14 0 17 0 21 0 20 0 12 0 14 0 21 0 06 0 04 0 06 0 04 0 13 0 09 0 10 0 14 0 21 0 15 0 21 0 21 0 01 0 03 0 02 0 02 0 10 a g e subm l d m subm l v 10 subm l v 30 subm l v 50 subm r d m subm r v 10 subm r v 30 subm r v 50 parotid l d m parotid l v 10 parotid l v 30 parotid l v 50 parotid r d m parotid r v 10 parotid r v 30 parotid r v 50 pc m sup d m pc m sup v 10 pc m sup v 30 pc m sup v 50 pc m m ed d m pc m m ed v 10 pc m m ed v 30 pc m m ed v 50 pc m inf d m pc m inf v 10 pc m inf v 30 pc m inf v 50 supraglottic d m supraglottic v 10 supraglottic v 30 supraglottic v 50 o ralc avity e xt d m o ralc avity e xt v 10 o ralc avity e xt v 30 o ralc avity e xt v 50 g lottica rea d m g lottica rea v 10 g lottica rea v 30 g lottica rea v 50 d y sfa g ie b sl 1 figure 12 correlation matrix for the d setting leeuwenberg et al 25 g coefficients in this section we report the mean coefficients of the estimated models in all simulated and real data settings together with some general statistics about the coefficients the sum and proportion of all negative dose coefficients 0 01 and p 0 01 respectively and the sum and proportion of all positive dose coefficients 0 01 and p 0 01 respectively table 7 mean model coefficients for a lr lasso ridge elasticnet pclr laelr dropout lrnn 0 01 0 13 0 71 0 00 0 01 0 00 0 00 0 01 0 09 0 00 0 00 0 00 0 00 0 04 0 66 0 00 0 03 0 42 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 01 1 48 1 10 2 04 1 24 0 94 1 52 1 20 0 93 1 56 1 20 0 94 1 50 1 38 1 06 2 04 1 37 1 06 1 94 1 20 0 83 1 55 1 36 1 09 1 63 p 0 01 0 16 0 00 0 50 0 01 0 00 0 00 0 01 0 00 0 25 0 00 0 00 0 00 0 04 0 00 0 50 0 03 0 00 0 25 0 00 0 00 0 00 0 00 0 00 0 00 p 0 01 0 83 0 50 1 00 0 86 0 50 1 00 0 99 0 75 1 00 0 96 0 75 1 00 0 96 0 50 1 00 0 96 0 75 1 00 1 00 1 00 1 00 0 84 0 50 1 00 intercept 1 32 1 58 1 12 1 27 1 53 1 07 1 26 1 52 1 08 1 26 1 50 1 09 1 32 1 58 1 12 1 32 1 57 1 12 1 27 1 52 1 06 1 32 1 58 1 12 age 0 05 0 25 0 15 0 04 0 23 0 10 0 05 0 22 0 11 0 04 0 21 0 10 0 05 0 26 0 14 0 05 0 26 0 15 0 06 0 23 0 12 0 05 0 25 0 14 subm l dm 0 19 0 51 0 82 0 21 0 00 0 72 0 24 0 00 0 39 0 22 0 00 0 55 0 28 0 36 0 63 0 28 0 36 0 44 0 26 0 09 0 40 0 23 0 00 0 81 subm r dm 0 38 0 35 1 00 0 34 0 00 0 86 0 31 0 05 0 68 0 32 0 01 0 81 0 38 0 00 0 85 0 37 0 08 0 85 0 30 0 10 0 58 0 36 0 00 0 94 parotid l dm 0 39 0 09 0 95 0 33 0 00 0 77 0 31 0 08 0 67 0 32 0 02 0 70 0 34 0 01 0 82 0 33 0 00 0 80 0 31 0 08 0 56 0 37 0 00 0 78 parotid r dm 0 40 0 18 0 91 0 35 0 00 0 78 0 33 0 08 0 58 0 34 0 01 0 63 0 35 0 08 0 74 0 36 0 10 0 74 0 33 0 06 0 67 0 40 0 00 0 81 xer bsl 2 0 25 0 02 0 47 0 18 0 01 0 43 0 19 0 01 0 39 0 19 0 01 0 39 0 24 0 02 0 46 0 24 0 02 0 45 0 18 0 02 0 35 0 25 0 02 0 47 xer bsl 3 0 13 0 04 0 30 0 08 0 01 0 26 0 09 0 03 0 26 0 08 0 03 0 24 0 12 0 04 0 29 0 12 0 05 0 30 0 09 0 05 0 27 0 13 0 04 0 30 table 8 mean model coefficients for a lr lasso ridge elasticnet pclr laelr dropout lrnn 0 01 0 65 3 03 0 00 0 00 0 00 0 00 0 01 0 15 0 00 0 00 0 00 0 00 0 30 3 76 0 00 0 34 3 66 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 01 2 08 1 17 3 74 1 27 0 97 1 65 1 27 1 00 1 64 1 26 1 00 1 63 1 74 1 15 5 87 1 78 1 15 5 81 1 28 0 99 1 68 1 43 1 13 1 84 p 0 01 0 28 0 00 0 50 0 00 0 00 0 00 0 01 0 00 0 25 0 00 0 00 0 00 0 06 0 00 0 50 0 08 0 00 0 50 0 00 0 00 0 00 0 00 0 00 0 00 p 0 01 0 71 0 50 1 00 0 85 0 50 1 00 0 99 0 75 1 00 0 97 0 75 1 00 0 93 0 50 1 00 0 92 0 50 1 00 1 00 1 00 1 00 0 70 0 50 1 00 intercept 1 35 1 56 1 04 1 28 1 49 1 02 1 28 1 48 1 01 1 28 1 48 1 01 1 34 1 56 1 04 1 34 1 56 1 04 1 29 1 53 1 01 1 34 1 56 1 04 age 0 02 0 77 0 85 0 01 0 16 0 09 0 01 0 20 0 14 0 01 0 18 0 14 0 04 0 67 1 22 0 04 0 73 1 19 0 00 0 20 0 17 0 02 0 24 0 35 subm l dm 0 26 2 04 2 82 0 15 0 00 0 73 0 24 0 13 0 37 0 20 0 00 0 38 0 32 2 22 3 98 0 31 2 09 3 95 0 26 0 12 0 39 0 25 0 00 0 97 subm r dm 0 26 1 57 1 86 0 35 0 00 1 01 0 31 0 15 0 59 0 33 0 00 0 87 0 28 3 00 1 89 0 29 2 49 1 77 0 32 0 12 0 52 0 29 0 00 1 11 parotid l dm 0 41 0 65 1 40 0 38 0 00 1 00 0 33 0 09 0 59 0 35 0 00 0 88 0 37 0 83 1 52 0 39 0 75 1 49 0 33 0 11 0 59 0 40 0 00 1 12 parotid r dm 0 51 0 18 1 28 0 39 0 01 0 83 0 38 0 22 0 67 0 38 0 15 0 69 0 47 0 10 1 50 0 46 0 02 1 39 0 37 0 21 0 78 0 49 0 07 0 97 xer bsl 2 0 14 0 08 0 44 0 09 0 03 0 38 0 12 0 06 0 35 0 10 0 02 0 38 0 13 0 08 0 42 0 13 0 09 0 42 0 11 0 05 0 35 0 14 0 08 0 43 xer bsl 3 0 06 0 12 0 26 0 03 0 08 0 18 0 03 0 10 0 19 0 03 0 09 0 19 0 05 0 12 0 26 0 05 0 14 0 26 0 04 0 12 0 19 0 06 0 12 0 27 26 leeuwenberg et al table 9 mean model coefficients for b lr lasso ridge elasticnet pclr laelr dropout lrnn 0 01 1 18 2 25 0 34 0 01 0 20 0 00 0 04 0 22 0 00 0 02 0 17 0 00 0 06 0 56 0 00 0 07 0 46 0 00 0 05 0 22 0 00 0 00 0 00 0 00 0 01 2 81 1 98 3 92 1 39 1 00 1 82 1 44 1 01 1 81 1 42 1 04 1 78 1 65 1 26 2 51 1 65 1 21 2 25 1 51 1 16 1 98 1 65 1 26 2 02 p 0 01 0 34 0 19 0 50 0 01 0 00 0 06 0 05 0 00 0 19 0 03 0 00 0 19 0 04 0 00 0 25 0 05 0 00 0 25 0 06 0 00 0 19 0 00 0 00 0 00 p 0 01 0 63 0 44 0 81 0 65 0 44 0 88 0 92 0 75 1 00 0 83 0 44 1 00 0 94 0 69 1 00 0 93 0 62 1 00 0 89 0 69 1 00 0 56 0 44 0 75 intercept 1 37 1 71 1 10 1 27 1 55 1 02 1 25 1 53 1 01 1 26 1 53 1 03 1 33 1 64 1 08 1 33 1 64 1 08 1 29 1 61 1 04 1 35 1 67 1 09 age 0 03 0 31 0 28 0 02 0 20 0 17 0 03 0 18 0 13 0 03 0 17 0 12 0 04 0 23 0 20 0 03 0 27 0 21 0 03 0 19 0 12 0 02 0 25 0 27 subm l dm 0 05 1 12 1 05 0 08 0 00 0 47 0 08 0 02 0 20 0 09 0 02 0 24 0 10 0 01 0 18 0 10 0 02 0 24 0 09 0 02 0 24 0 06 0 00 0 53 subm l v 10 0 12 0 29 0 49 0 07 0 01 0 30 0 07 0 04 0 21 0 07 0 02 0 24 0 07 0 11 0 24 0 07 0 10 0 25 0 08 0 04 0 26 0 10 0 00 0 40 subm l v 30 0 04 0 46 0 47 0 05 0 01 0 31 0 06 0 04 0 20 0 06 0 03 0 21 0 07 0 09 0 21 0 07 0 07 0 20 0 06 0 08 0 18 0 06 0 00 0 35 subm l v 50 0 10 0 23 0 47 0 07 0 01 0 27 0 08 0 02 0 17 0 07 0 03 0 21 0 09 0 09 0 26 0 09 0 10 0 23 0 08 0 04 0 19 0 09 0 00 0 33 subm r dm 0 16 0 47 0 81 0 10 0 00 0 44 0 10 0 01 0 25 0 10 0 00 0 27 0 12 0 02 0 27 0 12 0 01 0 27 0 12 0 00 0 32 0 12 0 00 0 53 subm r v 10 0 05 0 45 0 51 0 07 0 01 0 35 0 07 0 04 0 18 0 07 0 04 0 21 0 09 0 01 0 26 0 09 0 00 0 21 0 08 0 05 0 20 0 08 0 00 0 41 subm r v 30 0 07 0 51 0 56 0 08 0 01 0 41 0 08 0 10 0 18 0 08 0 03 0 26 0 09 0 02 0 17 0 09 0 09 0 19 0 08 0 05 0 21 0 08 0 00 0 38 subm r v 50 0 12 0 29 0 44 0 10 0 00 0 33 0 10 0 01 0 22 0 10 0 00 0 24 0 11 0 02 0 24 0 12 0 03 0 24 0 10 0 05 0 25 0 12 0 00 0 37 parotid l dm 0 09 0 54 0 74 0 07 0 00 0 26 0 08 0 06 0 16 0 08 0 01 0 17 0 08 0 02 0 18 0 10 0 03 0 20 0 09 0 04 0 18 0 09 0 00 0 34 parotid l v 10 0 14 0 28 0 57 0 11 0 01 0 37 0 10 0 04 0 23 0 10 0 05 0 27 0 10 0 16 0 36 0 10 0 09 0 34 0 11 0 02 0 25 0 14 0 00 0 43 parotid l v 30 0 17 0 43 0 73 0 11 0 00 0 37 0 10 0 02 0 23 0 10 0 00 0 25 0 10 0 02 0 22 0 10 0 00 0 22 0 10 0 03 0 27 0 12 0 00 0 40 parotid l v 50 0 06 0 55 0 60 0 05 0 03 0 29 0 06 0 11 0 18 0 05 0 08 0 23 0 07 0 08 0 21 0 06 0 21 0 23 0 06 0 09 0 19 0 07 0 00 0 35 parotid r dm 0 15 0 68 0 95 0 08 0 00 0 37 0 10 0 00 0 21 0 09 0 00 0 27 0 12 0 01 0 22 0 13 0 02 0 26 0 10 0 00 0 23 0 10 0 00 0 40 parotid r v 10 0 19 0 27 0 68 0 17 0 00 0 52 0 13 0 04 0 27 0 15 0 01 0 38 0 13 0 05 0 37 0 12 0 02 0 34 0 14 0 00 0 32 0 19 0 00 0 54 parotid r v 30 0 09 0 55 0 81 0 13 0 00 0 47 0 11 0 00 0 23 0 11 0 00 0 35 0 13 0 05 0 26 0 12 0 01 0 26 0 11 0 01 0 24 0 12 0 00 0 51 parotid r v 50 0 06 0 75 0 71 0 08 0 01 0 36 0 08 0 04 0 20 0 08 0 02 0 30 0 11 0 04 0 22 0 10 0 06 0 22 0 08 0 05 0 21 0 11 0 00 0 43 xer bsl 2 0 43 0 18 0 70 0 31 0 08 0 56 0 24 0 09 0 51 0 26 0 11 0 52 0 38 0 00 0 67 0 36 0 00 0 68 0 23 0 07 0 46 0 42 0 18 0 69 xer bsl 3 0 16 0 03 0 41 0 09 0 01 0 31 0 09 0 01 0 24 0 09 0 01 0 25 0 15 0 06 0 38 0 14 0 09 0 37 0 08 0 01 0 22 0 16 0 03 0 39 table 10 mean model coefficients for b lr lasso ridge elasticnet pclr laelr dropout lrnn 0 01 3 99 7 41 1 42 0 10 1 37 0 00 0 09 0 93 0 00 0 03 0 50 0 00 0 14 1 33 0 00 0 30 4 09 0 00 0 06 0 58 0 00 0 00 0 00 0 00 0 01 5 52 2 92 8 67 1 43 0 97 2 89 1 46 1 04 2 51 1 39 1 01 1 96 1 68 1 26 3 17 1 83 1 23 5 58 1 50 1 09 2 09 1 57 1 23 2 00 p 0 01 0 45 0 31 0 62 0 02 0 00 0 25 0 04 0 00 0 38 0 01 0 00 0 19 0 06 0 00 0 44 0 06 0 00 0 50 0 06 0 00 0 31 0 00 0 00 0 00 p 0 01 0 54 0 38 0 69 0 60 0 31 0 88 0 95 0 56 1 00 0 87 0 31 1 00 0 93 0 50 1 00 0 93 0 50 1 00 0 90 0 69 1 00 0 39 0 25 0 56 intercept 1 39 1 66 1 09 1 29 1 52 1 02 1 29 1 53 1 01 1 29 1 55 1 01 1 35 1 61 1 06 1 35 1 61 1 06 1 32 1 57 1 04 1 37 1 63 1 07 age 0 04 0 29 0 15 0 03 0 21 0 06 0 03 0 21 0 07 0 03 0 20 0 08 0 02 0 25 0 16 0 02 0 27 0 10 0 03 0 20 0 11 0 04 0 26 0 17 subm l dm 0 06 1 60 1 40 0 08 0 00 0 62 0 08 0 03 0 19 0 08 0 00 0 25 0 10 0 08 0 26 0 09 0 14 0 26 0 08 0 08 0 20 0 07 0 00 0 51 subm l v 10 0 00 1 51 1 65 0 06 0 01 0 46 0 07 0 04 0 14 0 07 0 01 0 34 0 09 0 03 0 17 0 09 0 13 0 18 0 07 0 05 0 18 0 07 0 00 0 56 subm l v 30 0 00 1 09 1 02 0 04 0 01 0 38 0 05 0 24 0 14 0 06 0 01 0 16 0 07 0 12 0 15 0 06 0 76 0 23 0 07 0 09 0 18 0 05 0 00 0 37 subm l v 50 0 07 0 88 1 01 0 06 0 07 0 37 0 07 0 08 0 16 0 06 0 07 0 18 0 07 0 35 0 28 0 09 0 25 0 38 0 08 0 07 0 23 0 08 0 00 0 41 subm r dm 0 24 1 33 2 12 0 14 0 00 0 89 0 12 0 03 0 43 0 11 0 01 0 47 0 12 0 04 0 18 0 15 0 03 0 90 0 11 0 02 0 23 0 10 0 00 0 72 subm r v 10 0 00 1 17 1 40 0 04 0 00 0 51 0 08 0 01 0 20 0 07 0 00 0 19 0 10 0 04 0 22 0 10 0 17 0 44 0 09 0 04 0 20 0 08 0 00 0 58 subm r v 30 0 22 1 19 1 60 0 11 0 00 0 64 0 09 0 02 0 27 0 09 0 00 0 35 0 09 0 13 0 20 0 09 0 47 0 33 0 09 0 04 0 25 0 15 0 00 0 73 subm r v 50 0 07 1 10 0 99 0 11 0 00 0 53 0 09 0 15 0 19 0 10 0 00 0 29 0 09 0 19 0 27 0 07 0 43 0 17 0 10 0 06 0 24 0 13 0 00 0 57 parotid l dm 0 35 2 12 2 97 0 09 0 00 0 50 0 10 0 04 0 34 0 09 0 00 0 21 0 11 0 01 0 29 0 13 0 03 0 56 0 10 0 01 0 20 0 05 0 00 0 38 parotid l v 10 0 11 1 00 0 91 0 15 0 00 0 79 0 12 0 02 0 64 0 13 0 00 0 73 0 13 0 05 0 57 0 14 0 05 0 80 0 12 0 02 0 34 0 16 0 00 0 73 parotid l v 30 0 08 1 20 1 82 0 11 0 00 0 57 0 09 0 01 0 32 0 10 0 00 0 53 0 09 0 15 0 30 0 09 0 17 0 47 0 10 0 02 0 34 0 12 0 00 0 57 parotid l v 50 0 06 1 31 0 93 0 03 0 01 0 31 0 05 0 16 0 14 0 05 0 14 0 16 0 07 0 25 0 25 0 06 0 37 0 21 0 05 0 10 0 18 0 09 0 00 0 36 parotid r dm 0 08 3 11 3 09 0 11 0 01 0 42 0 09 0 00 0 14 0 09 0 00 0 18 0 11 0 01 0 18 0 09 0 27 0 21 0 10 0 00 0 19 0 04 0 00 0 33 parotid r v 10 0 16 0 99 1 40 0 14 0 01 0 49 0 11 0 02 0 29 0 12 0 00 0 36 0 13 0 01 0 52 0 11 0 08 0 54 0 13 0 01 0 38 0 17 0 00 0 57 parotid r v 30 0 07 1 48 1 93 0 07 0 01 0 46 0 09 0 03 0 17 0 09 0 00 0 29 0 10 0 32 0 25 0 09 0 36 0 43 0 09 0 03 0 20 0 13 0 00 0 51 parotid r v 50 0 08 1 25 1 62 0 04 0 01 0 32 0 07 0 08 0 20 0 06 0 04 0 21 0 08 0 22 0 30 0 09 0 19 0 40 0 06 0 07 0 17 0 10 0 00 0 39 xer bsl 2 0 20 0 01 0 40 0 12 0 00 0 35 0 11 0 01 0 34 0 12 0 00 0 29 0 14 0 04 0 37 0 12 0 08 0 37 0 11 0 03 0 30 0 20 0 03 0 37 xer bsl 3 0 09 0 15 0 33 0 04 0 05 0 22 0 05 0 05 0 21 0 05 0 05 0 24 0 06 0 13 0 29 0 05 0 14 0 27 0 05 0 07 0 21 0 09 0 12 0 32 leeuwenberg et al 27 table 11 mean model coefficients for c lr lasso ridge elasticnet pclr laelr dropout lrnn 0 01 1 02 2 38 0 21 0 05 0 50 0 00 0 08 0 55 0 00 0 06 0 54 0 00 0 18 1 69 0 00 0 15 1 65 0 00 0 06 0 25 0 00 0 00 0 00 0 00 0 01 3 48 2 44 5 28 2 07 1 50 3 11 2 06 1 58 3 00 2 06 1 49 2 92 2 49 1 83 4 17 2 46 1 81 4 16 2 01 1 50 2 73 2 52 1 93 3 62 p 0 01 0 32 0 20 0 50 0 02 0 00 0 20 0 08 0 00 0 30 0 05 0 00 0 20 0 08 0 00 0 30 0 07 0 00 0 30 0 07 0 00 0 20 0 00 0 00 0 00 p 0 01 0 67 0 50 0 80 0 69 0 50 0 90 0 90 0 70 1 00 0 85 0 50 1 00 0 91 0 60 1 00 0 91 0 70 1 00 0 92 0 80 1 00 0 64 0 40 0 90 intercept 2 96 3 58 2 49 2 69 3 16 2 28 2 66 3 12 2 27 2 67 3 14 2 28 2 89 3 48 2 44 2 88 3 52 2 44 2 69 3 19 2 33 2 93 3 56 2 47 age 0 01 0 28 0 25 0 00 0 17 0 18 0 01 0 19 0 17 0 01 0 18 0 18 0 03 0 30 0 27 0 04 0 31 0 27 0 01 0 22 0 17 0 02 0 25 0 26 subm l dm 0 22 0 75 1 07 0 19 0 00 0 66 0 22 0 12 0 53 0 22 0 07 0 54 0 28 0 17 0 88 0 28 0 16 0 80 0 23 0 05 0 44 0 21 0 00 0 64 subm r dm 0 26 0 82 1 37 0 22 0 00 0 89 0 24 0 03 0 55 0 24 0 00 0 74 0 28 0 10 0 69 0 27 0 50 0 71 0 25 0 05 0 57 0 25 0 00 1 01 parotid l dm 0 12 0 54 0 86 0 14 0 03 0 60 0 18 0 07 0 43 0 17 0 05 0 46 0 21 0 33 0 58 0 21 0 30 0 41 0 19 0 10 0 40 0 15 0 00 0 63 parotid r dm 0 21 0 52 1 05 0 23 0 00 0 73 0 23 0 00 0 56 0 22 0 00 0 64 0 27 0 16 0 72 0 28 0 13 0 71 0 22 0 05 0 42 0 22 0 00 0 78 pcm sup dm 0 27 0 95 1 56 0 20 0 00 0 85 0 24 0 01 0 42 0 23 0 00 0 59 0 25 0 02 0 80 0 24 0 15 0 77 0 24 0 05 0 39 0 33 0 00 1 10 pcm med dm 0 22 0 83 1 51 0 29 0 00 1 00 0 23 0 00 0 47 0 25 0 00 0 72 0 26 0 71 0 55 0 29 0 38 0 55 0 23 0 00 0 56 0 23 0 00 1 04 pcm inf dm 0 33 0 43 1 19 0 13 0 01 0 50 0 15 0 06 0 40 0 15 0 03 0 42 0 18 0 03 0 58 0 15 0 01 0 52 0 13 0 04 0 45 0 21 0 00 0 63 supraglottic dm 0 19 0 78 1 02 0 18 0 08 0 65 0 17 0 17 0 47 0 17 0 21 0 55 0 21 0 60 0 65 0 21 0 58 0 61 0 18 0 20 0 44 0 20 0 00 0 74 oralcavity ext dm 0 62 0 75 1 86 0 40 0 00 1 27 0 30 0 13 0 89 0 32 0 00 1 00 0 33 0 19 1 05 0 33 0 19 1 16 0 27 0 11 0 72 0 57 0 00 1 55 glotticarea dm 0 02 0 79 1 03 0 03 0 37 0 50 0 02 0 19 0 32 0 03 0 20 0 39 0 03 0 37 0 57 0 04 0 19 0 56 0 02 0 24 0 30 0 15 0 00 0 83 dysfagie bsl 2 0 23 0 06 0 50 0 15 0 01 0 42 0 17 0 04 0 38 0 17 0 01 0 37 0 19 0 14 0 47 0 16 0 16 0 48 0 17 0 01 0 39 0 23 0 08 0 49 dysfagie bsl 3 0 42 0 12 0 81 0 31 0 01 0 61 0 30 0 09 0 56 0 30 0 08 0 55 0 38 0 10 0 77 0 36 0 10 0 80 0 29 0 08 0 61 0 42 0 13 0 80 table 12 mean model coefficients for c lr lasso ridge elasticnet pclr laelr dropout lrnn 0 01 3 14 7 37 0 74 0 08 1 10 0 00 0 11 1 31 0 00 0 06 0 61 0 00 0 31 3 40 0 00 0 39 4 87 0 00 0 08 0 50 0 00 0 00 0 00 0 00 0 01 5 40 2 87 10 86 1 88 1 24 3 37 1 88 1 30 3 21 1 82 1 33 2 82 2 36 1 63 6 30 2 44 1 59 6 79 1 84 1 23 2 72 2 33 1 62 3 33 p 0 01 0 40 0 20 0 60 0 03 0 00 0 20 0 08 0 00 0 40 0 04 0 00 0 20 0 10 0 00 0 50 0 10 0 00 0 50 0 09 0 00 0 30 0 00 0 00 0 00 p 0 01 0 60 0 40 0 80 0 60 0 30 0 90 0 90 0 60 1 00 0 82 0 40 1 00 0 88 0 50 1 00 0 88 0 50 1 00 0 89 0 70 1 00 0 48 0 30 0 70 intercept 2 89 3 43 2 41 2 65 3 22 2 22 2 62 3 10 2 24 2 62 3 06 2 21 2 82 3 40 2 37 2 82 3 39 2 37 2 66 3 13 2 26 2 86 3 41 2 40 age 0 07 0 70 0 79 0 02 0 18 0 34 0 02 0 22 0 29 0 02 0 18 0 28 0 07 0 25 0 36 0 09 0 26 0 59 0 02 0 20 0 25 0 06 0 26 0 42 subm l dm 0 22 2 42 3 20 0 10 0 00 0 54 0 15 0 15 0 38 0 14 0 01 0 39 0 16 0 39 0 44 0 24 0 34 1 86 0 14 0 17 0 33 0 17 0 00 0 97 subm r dm 0 16 2 67 2 36 0 12 0 00 0 88 0 19 0 17 0 42 0 19 0 00 0 43 0 19 0 49 0 36 0 21 0 62 0 49 0 20 0 01 0 43 0 14 0 00 0 98 parotid l dm 0 00 1 63 1 55 0 18 0 01 0 79 0 18 0 03 0 36 0 17 0 00 0 53 0 19 0 43 0 62 0 17 0 90 0 40 0 18 0 04 0 42 0 15 0 00 0 79 parotid r dm 0 23 1 26 1 62 0 30 0 04 0 97 0 24 0 10 0 52 0 26 0 00 0 73 0 26 0 32 0 78 0 28 0 27 0 95 0 25 0 05 0 61 0 29 0 00 1 01 pcm sup dm 0 47 2 25 3 16 0 21 0 00 0 96 0 25 0 13 0 59 0 24 0 01 0 69 0 33 0 11 1 48 0 29 0 02 1 23 0 24 0 11 0 43 0 24 0 00 1 20 pcm med dm 0 12 2 38 2 67 0 39 0 00 1 26 0 24 0 10 0 51 0 26 0 00 0 76 0 31 0 08 0 57 0 25 0 68 0 53 0 24 0 08 0 47 0 26 0 00 1 27 pcm inf dm 0 33 1 20 2 03 0 08 0 01 0 80 0 09 0 06 0 28 0 07 0 13 0 36 0 11 0 48 0 44 0 09 0 52 0 29 0 09 0 09 0 31 0 18 0 00 0 75 supraglottic dm 0 23 1 02 1 96 0 20 0 00 0 81 0 22 0 04 0 74 0 22 0 00 0 80 0 29 0 06 1 11 0 32 0 11 1 58 0 23 0 03 0 67 0 20 0 00 0 76 oralcavity ext dm 0 55 2 06 3 07 0 25 0 00 1 30 0 23 0 10 0 75 0 22 0 00 0 60 0 26 0 10 0 99 0 25 0 34 1 04 0 22 0 03 0 44 0 57 0 00 1 86 glotticarea dm 0 06 1 19 1 08 0 02 0 58 0 33 0 02 0 30 0 16 0 02 0 33 0 15 0 04 0 71 0 25 0 04 0 77 0 16 0 03 0 30 0 14 0 11 0 00 0 49 dysfagie bsl 2 0 15 0 16 0 49 0 11 0 01 0 41 0 14 0 02 0 33 0 13 0 01 0 39 0 13 0 11 0 46 0 13 0 08 0 44 0 15 0 09 0 40 0 15 0 14 0 50 dysfagie bsl 3 0 27 0 12 0 62 0 20 0 02 0 46 0 21 0 02 0 41 0 21 0 01 0 45 0 26 0 05 0 54 0 26 0 04 0 55 0 21 0 02 0 43 0 26 0 12 0 55 28 leeuwenberg et al table 13 mean model coefficients for d lr lasso ridge elasticnet pclr laelr dropout lrnn 0 01 7 10 10 95 4 18 0 05 0 29 0 00 0 32 1 22 0 01 0 19 1 10 0 00 0 41 1 25 0 00 0 40 1 57 0 00 0 48 0 98 0 17 0 00 0 00 0 00 0 01 9 90 6 87 13 90 1 91 1 23 2 92 2 35 1 64 3 61 2 17 1 40 3 25 2 77 1 86 4 09 2 75 1 75 4 25 2 63 1 98 3 58 2 85 2 05 3 63 p 0 01 0 44 0 35 0 55 0 03 0 00 0 10 0 15 0 03 0 30 0 08 0 00 0 25 0 16 0 00 0 35 0 16 0 00 0 40 0 21 0 10 0 33 0 00 0 00 0 00 p 0 01 0 54 0 42 0 62 0 46 0 33 0 62 0 76 0 62 0 90 0 64 0 33 0 88 0 76 0 55 0 97 0 77 0 55 0 97 0 72 0 57 0 85 0 34 0 20 0 45 intercept 3 08 3 81 2 62 2 42 2 82 2 09 2 43 2 92 2 09 2 43 2 84 2 10 2 67 3 20 2 31 2 65 3 20 2 26 2 58 3 02 2 25 2 79 3 33 2 42 age 0 02 0 48 0 43 0 00 0 13 0 12 0 00 0 11 0 13 0 00 0 13 0 13 0 01 0 26 0 32 0 01 0 21 0 34 0 01 0 16 0 18 0 06 0 27 0 35 subm l dm 0 14 1 82 1 98 0 06 0 00 0 38 0 07 0 03 0 17 0 07 0 00 0 26 0 07 0 05 0 19 0 06 0 07 0 16 0 08 0 04 0 21 0 04 0 00 0 31 subm l v 10 0 03 1 37 1 09 0 02 0 01 0 23 0 01 0 11 0 17 0 02 0 08 0 18 0 00 0 11 0 08 0 00 0 11 0 08 0 02 0 17 0 16 0 05 0 00 0 43 subm l v 30 0 02 1 01 1 41 0 02 0 01 0 15 0 02 0 08 0 15 0 03 0 08 0 15 0 01 0 15 0 14 0 01 0 16 0 13 0 03 0 12 0 17 0 02 0 00 0 24 subm l v 50 0 03 0 68 0 75 0 06 0 01 0 35 0 07 0 06 0 24 0 07 0 03 0 35 0 09 0 06 0 24 0 08 0 09 0 25 0 07 0 07 0 23 0 09 0 00 0 47 subm r dm 0 09 1 66 1 43 0 07 0 01 0 31 0 06 0 06 0 20 0 06 0 04 0 22 0 08 0 06 0 19 0 07 0 11 0 19 0 07 0 06 0 21 0 06 0 00 0 33 subm r v 10 0 00 1 15 0 98 0 01 0 01 0 14 0 02 0 12 0 10 0 02 0 11 0 11 0 01 0 07 0 08 0 01 0 07 0 09 0 02 0 12 0 13 0 03 0 00 0 33 subm r v 30 0 04 1 18 0 96 0 02 0 01 0 15 0 02 0 09 0 11 0 02 0 08 0 11 0 02 0 08 0 09 0 02 0 10 0 09 0 02 0 14 0 14 0 03 0 00 0 23 subm r v 50 0 06 0 69 0 64 0 04 0 01 0 33 0 06 0 10 0 17 0 05 0 10 0 20 0 08 0 11 0 23 0 08 0 11 0 22 0 07 0 09 0 19 0 05 0 00 0 35 parotid l dm 0 05 1 52 1 57 0 02 0 01 0 24 0 03 0 06 0 11 0 03 0 07 0 15 0 03 0 12 0 14 0 04 0 13 0 14 0 04 0 13 0 15 0 03 0 00 0 23 parotid l v 10 0 06 0 66 0 70 0 03 0 00 0 22 0 05 0 06 0 16 0 05 0 01 0 19 0 06 0 09 0 17 0 06 0 05 0 17 0 06 0 11 0 19 0 05 0 00 0 35 parotid l v 30 0 09 1 08 1 33 0 05 0 01 0 31 0 05 0 06 0 17 0 05 0 03 0 19 0 05 0 08 0 14 0 05 0 08 0 14 0 04 0 10 0 20 0 04 0 00 0 36 parotid l v 50 0 07 1 08 1 12 0 05 0 01 0 29 0 06 0 06 0 20 0 05 0 04 0 24 0 06 0 11 0 23 0 07 0 16 0 28 0 07 0 09 0 21 0 07 0 00 0 37 parotid r dm 0 12 1 67 1 89 0 04 0 01 0 26 0 05 0 05 0 16 0 05 0 01 0 16 0 05 0 10 0 20 0 06 0 14 0 19 0 06 0 07 0 18 0 04 0 00 0 32 parotid r v 10 0 03 0 95 0 76 0 04 0 01 0 24 0 04 0 10 0 16 0 04 0 06 0 16 0 04 0 09 0 13 0 05 0 05 0 13 0 05 0 09 0 19 0 05 0 00 0 34 parotid r v 30 0 03 1 62 1 34 0 05 0 01 0 30 0 05 0 04 0 16 0 06 0 01 0 21 0 06 0 03 0 15 0 07 0 04 0 17 0 06 0 07 0 19 0 03 0 00 0 24 parotid r v 50 0 02 1 15 1 47 0 05 0 01 0 39 0 07 0 03 0 20 0 07 0 01 0 25 0 07 0 10 0 23 0 08 0 11 0 22 0 06 0 07 0 18 0 08 0 00 0 49 pcm sup dm 0 04 1 18 1 54 0 04 0 01 0 26 0 07 0 06 0 18 0 06 0 01 0 19 0 09 0 01 0 24 0 09 0 01 0 21 0 06 0 06 0 19 0 11 0 00 0 56 pcm sup v 10 0 01 1 12 1 21 0 03 0 01 0 32 0 03 0 06 0 12 0 03 0 04 0 21 0 04 0 06 0 13 0 03 0 09 0 09 0 03 0 10 0 14 0 07 0 00 0 60 pcm sup v 30 0 11 0 72 1 08 0 05 0 00 0 32 0 06 0 04 0 16 0 06 0 01 0 24 0 07 0 00 0 17 0 07 0 00 0 16 0 05 0 06 0 17 0 05 0 00 0 32 pcm sup v 50 0 16 0 86 1 09 0 08 0 00 0 51 0 09 0 01 0 24 0 09 0 01 0 28 0 12 0 02 0 27 0 12 0 02 0 26 0 09 0 04 0 25 0 14 0 00 0 71 pcm med dm 0 22 0 66 1 07 0 13 0 01 0 46 0 09 0 03 0 24 0 10 0 01 0 38 0 10 0 06 0 23 0 11 0 06 0 30 0 11 0 03 0 25 0 11 0 00 0 49 pcm med v 10 0 02 0 76 0 85 0 03 0 02 0 24 0 04 0 09 0 20 0 04 0 10 0 23 0 02 0 09 0 17 0 03 0 08 0 21 0 05 0 11 0 21 0 06 0 00 0 41 pcm med v 30 0 12 0 69 0 97 0 05 0 01 0 25 0 05 0 06 0 14 0 05 0 05 0 17 0 02 0 10 0 10 0 03 0 11 0 10 0 06 0 07 0 19 0 06 0 00 0 39 pcm med v 50 0 02 0 66 0 74 0 06 0 00 0 35 0 06 0 07 0 22 0 06 0 02 0 25 0 10 0 10 0 26 0 10 0 12 0 27 0 06 0 10 0 20 0 06 0 00 0 38 pcm inf dm 0 30 0 35 0 95 0 08 0 01 0 35 0 10 0 03 0 25 0 09 0 02 0 26 0 10 0 01 0 30 0 11 0 03 0 31 0 10 0 09 0 26 0 14 0 00 0 48 pcm inf v 10 0 10 0 66 1 00 0 01 0 14 0 17 0 02 0 14 0 16 0 02 0 14 0 13 0 02 0 13 0 16 0 01 0 12 0 15 0 02 0 15 0 20 0 04 0 00 0 26 pcm inf v 30 0 10 0 56 0 48 0 00 0 11 0 14 0 01 0 17 0 11 0 00 0 16 0 12 0 00 0 19 0 18 0 01 0 27 0 18 0 02 0 16 0 16 0 02 0 00 0 17 pcm inf v 50 0 09 0 62 0 96 0 05 0 01 0 47 0 06 0 06 0 23 0 06 0 02 0 38 0 10 0 01 0 29 0 10 0 03 0 32 0 06 0 07 0 25 0 12 0 00 0 68 supraglottic dm 0 12 0 57 0 83 0 07 0 01 0 31 0 07 0 08 0 21 0 07 0 03 0 27 0 10 0 06 0 32 0 09 0 06 0 31 0 08 0 06 0 24 0 10 0 00 0 41 supraglottic v 10 0 02 0 74 0 77 0 03 0 01 0 24 0 03 0 12 0 16 0 03 0 03 0 18 0 04 0 11 0 24 0 04 0 11 0 26 0 02 0 16 0 15 0 04 0 00 0 34 supraglottic v 30 0 02 0 75 0 65 0 02 0 02 0 26 0 03 0 15 0 16 0 03 0 14 0 26 0 03 0 12 0 17 0 03 0 13 0 18 0 03 0 16 0 27 0 02 0 00 0 22 supraglottic v 50 0 12 0 52 0 68 0 07 0 01 0 33 0 08 0 02 0 24 0 07 0 01 0 28 0 12 0 00 0 26 0 11 0 00 0 26 0 10 0 03 0 26 0 09 0 00 0 40 oralcavity ext dm 0 23 1 26 2 17 0 17 0 00 0 65 0 13 0 03 0 30 0 13 0 00 0 44 0 15 0 04 0 35 0 14 0 04 0 34 0 13 0 00 0 28 0 15 0 00 0 71 oralcavity ext v 10 0 04 1 29 1 28 0 01 0 01 0 19 0 04 0 05 0 14 0 03 0 03 0 14 0 04 0 03 0 12 0 05 0 04 0 11 0 04 0 06 0 15 0 03 0 00 0 28 oralcavity ext v 30 0 15 1 11 1 27 0 08 0 00 0 45 0 08 0 02 0 21 0 08 0 02 0 26 0 10 0 02 0 23 0 10 0 03 0 24 0 08 0 06 0 22 0 14 0 00 0 62 oralcavity ext v 50 0 34 1 54 2 33 0 14 0 00 0 57 0 13 0 01 0 29 0 14 0 00 0 49 0 15 0 01 0 31 0 15 0 01 0 39 0 14 0 01 0 31 0 31 0 00 0 82 glotticarea dm 0 12 0 66 0 84 0 02 0 01 0 18 0 03 0 07 0 18 0 03 0 05 0 19 0 02 0 09 0 18 0 02 0 12 0 15 0 03 0 15 0 18 0 11 0 00 0 50 glotticarea v 10 0 01 0 81 0 64 0 01 0 04 0 15 0 02 0 11 0 17 0 02 0 09 0 21 0 02 0 11 0 18 0 02 0 10 0 18 0 02 0 12 0 15 0 03 0 00 0 26 glotticarea v 30 0 12 0 64 0 41 0 00 0 17 0 13 0 02 0 17 0 14 0 02 0 18 0 15 0 04 0 33 0 12 0 04 0 33 0 15 0 02 0 17 0 15 0 02 0 00 0 22 glotticarea v 50 0 03 0 81 0 81 0 02 0 02 0 17 0 03 0 09 0 15 0 02 0 06 0 18 0 05 0 08 0 18 0 04 0 07 0 20 0 03 0 11 0 18 0 11 0 00 0 52 dysfagie bsl 2 0 33 0 09 0 63 0 15 0 01 0 45 0 14 0 01 0 34 0 14 0 00 0 42 0 24 0 04 0 63 0 20 0 09 0 65 0 14 0 04 0 34 0 29 0 08 0 66 dysfagie bsl 3 0 59 0 25 0 97 0 34 0 03 0 64 0 26 0 09 0 49 0 30 0 05 0 61 0 43 0 00 0 87 0 38 0 00 0 89 0 28 0 09 0 51 0 54 0 24 0 90 leeuwenberg et al 29 table 14 mean model coefficients for d lr lasso ridge elasticnet pclr laelr dropout lrnn 0 01 12 37 17 94 7 79 0 05 0 62 0 00 0 15 0 90 0 00 0 12 0 75 0 00 0 26 1 67 0 00 0 27 1 70 0 00 0 38 1 11 0 09 0 00 0 00 0 00 0 01 14 95 10 05 19 91 1 75 1 16 2 67 2 04 1 38 3 15 1 97 1 33 3 10 2 46 1 75 4 65 2 50 1 70 4 57 2 43 1 86 3 48 2 57 1 85 3 63 p 0 01 0 46 0 38 0 55 0 02 0 00 0 10 0 08 0 00 0 25 0 06 0 00 0 23 0 10 0 00 0 30 0 10 0 00 0 38 0 18 0 05 0 33 0 00 0 00 0 00 p 0 01 0 53 0 45 0 60 0 46 0 28 0 68 0 84 0 68 0 97 0 71 0 28 0 95 0 82 0 57 1 00 0 83 0 57 1 00 0 74 0 60 0 88 0 27 0 15 0 38 intercept 3 02 3 63 2 57 2 44 2 89 2 09 2 46 2 86 2 12 2 47 2 84 2 14 2 66 3 16 2 29 2 65 3 13 2 26 2 61 3 01 2 27 2 77 3 33 2 40 age 0 01 0 32 0 37 0 01 0 16 0 11 0 02 0 15 0 13 0 02 0 15 0 12 0 02 0 29 0 27 0 01 0 21 0 24 0 02 0 22 0 15 0 01 0 31 0 35 subm l dm 0 00 2 43 1 84 0 04 0 00 0 36 0 05 0 01 0 12 0 05 0 00 0 14 0 05 0 02 0 12 0 05 0 01 0 11 0 06 0 04 0 17 0 03 0 00 0 32 subm l v 10 0 10 1 79 2 57 0 02 0 01 0 16 0 03 0 02 0 08 0 03 0 02 0 09 0 02 0 05 0 07 0 02 0 06 0 08 0 03 0 10 0 15 0 04 0 00 0 45 subm l v 30 0 05 2 33 1 97 0 02 0 00 0 11 0 03 0 04 0 09 0 02 0 03 0 09 0 02 0 16 0 08 0 02 0 16 0 10 0 02 0 09 0 14 0 03 0 00 0 41 subm l v 50 0 08 1 33 1 25 0 06 0 00 0 41 0 06 0 03 0 14 0 07 0 02 0 25 0 07 0 01 0 19 0 06 0 14 0 18 0 07 0 09 0 19 0 07 0 00 0 49 subm r dm 0 10 2 14 1 77 0 06 0 00 0 60 0 06 0 02 0 14 0 05 0 00 0 17 0 07 0 02 0 29 0 07 0 02 0 25 0 06 0 03 0 18 0 05 0 00 0 62 subm r v 10 0 01 2 02 1 96 0 01 0 00 0 03 0 03 0 05 0 10 0 02 0 04 0 10 0 02 0 04 0 17 0 02 0 04 0 09 0 03 0 11 0 14 0 01 0 00 0 15 subm r v 30 0 10 2 10 1 65 0 02 0 01 0 17 0 03 0 07 0 12 0 03 0 05 0 16 0 03 0 04 0 22 0 03 0 05 0 12 0 02 0 12 0 16 0 03 0 00 0 43 subm r v 50 0 14 1 14 1 39 0 10 0 00 0 49 0 07 0 01 0 20 0 09 0 00 0 38 0 09 0 03 0 23 0 09 0 10 0 31 0 08 0 05 0 25 0 11 0 00 0 60 parotid l dm 0 18 1 88 2 23 0 03 0 00 0 30 0 05 0 00 0 11 0 05 0 00 0 11 0 06 0 04 0 14 0 07 0 06 0 15 0 05 0 07 0 16 0 01 0 00 0 23 parotid l v 10 0 14 1 09 1 49 0 06 0 00 0 41 0 05 0 08 0 12 0 05 0 05 0 15 0 04 0 30 0 10 0 05 0 18 0 11 0 06 0 07 0 18 0 06 0 00 0 54 parotid l v 30 0 11 1 98 2 00 0 05 0 00 0 27 0 05 0 02 0 12 0 04 0 01 0 17 0 06 0 06 0 15 0 07 0 08 0 16 0 05 0 06 0 17 0 04 0 00 0 29 parotid l v 50 0 07 1 25 1 28 0 04 0 01 0 35 0 05 0 05 0 16 0 05 0 04 0 24 0 09 0 08 0 36 0 09 0 12 0 24 0 06 0 08 0 21 0 09 0 00 0 42 parotid r dm 0 31 2 32 1 94 0 03 0 00 0 20 0 06 0 01 0 13 0 05 0 00 0 13 0 08 0 05 0 17 0 09 0 09 0 18 0 06 0 04 0 15 0 01 0 00 0 18 parotid r v 10 0 08 1 72 1 23 0 05 0 01 0 39 0 05 0 12 0 14 0 04 0 11 0 17 0 06 0 07 0 11 0 06 0 06 0 12 0 05 0 07 0 15 0 06 0 00 0 48 parotid r v 30 0 04 1 57 1 82 0 06 0 00 0 38 0 06 0 01 0 14 0 06 0 00 0 18 0 08 0 07 0 18 0 09 0 07 0 18 0 06 0 04 0 18 0 04 0 00 0 36 parotid r v 50 0 02 1 09 1 26 0 07 0 01 0 45 0 07 0 04 0 21 0 07 0 00 0 20 0 10 0 07 0 20 0 10 0 10 0 23 0 08 0 08 0 24 0 12 0 00 0 53 pcm sup dm 0 20 2 79 2 57 0 02 0 00 0 10 0 06 0 02 0 11 0 05 0 00 0 10 0 07 0 03 0 11 0 07 0 02 0 16 0 06 0 03 0 17 0 02 0 00 0 26 pcm sup v 10 0 24 2 36 1 94 0 02 0 01 0 20 0 03 0 04 0 10 0 03 0 03 0 10 0 03 0 05 0 07 0 03 0 03 0 11 0 03 0 08 0 11 0 03 0 00 0 48 pcm sup v 30 0 12 1 57 2 01 0 04 0 00 0 34 0 05 0 01 0 12 0 04 0 00 0 11 0 05 0 03 0 10 0 06 0 01 0 10 0 05 0 04 0 16 0 07 0 00 0 70 pcm sup v 50 0 04 1 30 1 34 0 05 0 00 0 40 0 07 0 01 0 17 0 07 0 00 0 19 0 10 0 01 0 19 0 10 0 00 0 20 0 08 0 04 0 20 0 11 0 00 0 67 pcm med dm 0 18 2 08 1 98 0 12 0 00 0 58 0 07 0 02 0 14 0 08 0 00 0 25 0 08 0 03 0 17 0 08 0 02 0 18 0 07 0 05 0 22 0 04 0 00 0 44 pcm med v 10 0 08 1 45 1 35 0 03 0 01 0 34 0 04 0 04 0 13 0 04 0 02 0 25 0 02 0 05 0 11 0 02 0 06 0 14 0 05 0 04 0 16 0 08 0 00 0 50 pcm med v 30 0 14 1 74 1 50 0 03 0 00 0 30 0 04 0 03 0 12 0 04 0 01 0 12 0 04 0 02 0 08 0 04 0 02 0 10 0 05 0 05 0 18 0 04 0 00 0 53 pcm med v 50 0 01 1 47 1 02 0 12 0 00 0 53 0 07 0 05 0 19 0 09 0 05 0 33 0 12 0 02 0 26 0 10 0 11 0 29 0 08 0 05 0 24 0 12 0 00 0 55 pcm inf dm 0 07 1 81 1 54 0 02 0 01 0 18 0 05 0 02 0 13 0 04 0 02 0 16 0 06 0 00 0 16 0 05 0 02 0 24 0 06 0 07 0 24 0 07 0 00 0 53 pcm inf v 10 0 04 1 12 1 37 0 02 0 02 0 15 0 02 0 07 0 12 0 02 0 05 0 12 0 00 0 10 0 15 0 00 0 11 0 13 0 01 0 09 0 15 0 02 0 00 0 30 pcm inf v 30 0 04 0 92 0 85 0 01 0 02 0 20 0 01 0 10 0 10 0 01 0 11 0 13 0 04 0 21 0 13 0 04 0 21 0 13 0 02 0 17 0 16 0 02 0 00 0 21 pcm inf v 50 0 15 1 35 1 76 0 04 0 02 0 38 0 05 0 05 0 17 0 05 0 02 0 14 0 05 0 02 0 23 0 06 0 02 0 23 0 05 0 08 0 24 0 10 0 00 0 55 supraglottic dm 0 05 1 93 2 55 0 06 0 01 0 35 0 07 0 00 0 14 0 06 0 00 0 28 0 08 0 02 0 20 0 08 0 02 0 23 0 07 0 05 0 21 0 07 0 00 0 43 supraglottic v 10 0 04 1 32 1 39 0 04 0 01 0 24 0 03 0 06 0 15 0 03 0 04 0 23 0 01 0 13 0 17 0 01 0 06 0 21 0 03 0 14 0 16 0 05 0 00 0 41 supraglottic v 30 0 07 1 22 1 15 0 03 0 01 0 27 0 02 0 07 0 12 0 02 0 09 0 15 0 04 0 22 0 15 0 04 0 23 0 17 0 03 0 13 0 14 0 03 0 00 0 36 supraglottic v 50 0 24 1 28 1 72 0 07 0 00 0 35 0 08 0 01 0 20 0 08 0 00 0 32 0 09 0 01 0 21 0 09 0 02 0 23 0 09 0 02 0 25 0 10 0 00 0 43 oralcavity ext dm 0 34 1 43 2 39 0 08 0 00 0 54 0 08 0 04 0 15 0 09 0 00 0 37 0 09 0 04 0 26 0 08 0 01 0 28 0 09 0 01 0 19 0 05 0 00 0 60 oralcavity ext v 10 0 07 2 63 2 08 0 03 0 01 0 29 0 04 0 07 0 10 0 04 0 07 0 19 0 03 0 02 0 10 0 04 0 02 0 14 0 04 0 11 0 14 0 11 0 00 0 78 oralcavity ext v 30 0 10 1 64 1 66 0 07 0 00 0 58 0 07 0 02 0 15 0 07 0 00 0 20 0 08 0 04 0 19 0 08 0 04 0 21 0 07 0 05 0 19 0 12 0 00 0 87 oralcavity ext v 50 0 26 0 97 1 22 0 15 0 00 0 60 0 11 0 03 0 27 0 12 0 00 0 35 0 13 0 05 0 41 0 13 0 04 0 42 0 13 0 01 0 29 0 33 0 00 0 83 glotticarea dm 0 19 2 04 2 06 0 00 0 01 0 04 0 01 0 05 0 09 0 01 0 03 0 09 0 02 0 05 0 12 0 02 0 07 0 13 0 00 0 11 0 11 0 05 0 00 0 43 glotticarea v 10 0 05 1 12 1 11 0 01 0 03 0 12 0 02 0 08 0 11 0 02 0 07 0 13 0 01 0 11 0 20 0 01 0 11 0 19 0 03 0 11 0 21 0 03 0 00 0 23 glotticarea v 30 0 09 0 79 0 78 0 01 0 15 0 03 0 01 0 12 0 08 0 02 0 15 0 07 0 00 0 19 0 12 0 00 0 37 0 11 0 02 0 17 0 14 0 01 0 00 0 14 glotticarea v 50 0 04 1 46 1 05 0 01 0 02 0 12 0 02 0 07 0 13 0 02 0 04 0 13 0 03 0 06 0 23 0 03 0 07 0 27 0 02 0 14 0 18 0 10 0 00 0 46 dysfagie bsl 2 0 20 0 14 0 54 0 08 0 01 0 33 0 08 0 02 0 25 0 09 0 01 0 29 0 10 0 13 0 52 0 08 0 11 0 47 0 11 0 07 0 28 0 18 0 13 0 51 dysfagie bsl 3 0 36 0 03 0 74 0 19 0 00 0 48 0 15 0 04 0 36 0 17 0 04 0 44 0 21 0 02 0 61 0 15 0 02 0 55 0 19 0 01 0 41 0 34 0 04 0 65 30 leeuwenberg et al table 15 mean model coefficients for a lr lasso ridge elasticnet pclr laelr dropout lrnn 0 01 0 08 0 09 0 06 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 01 1 78 1 76 1 79 1 36 1 31 1 41 1 34 1 31 1 38 1 35 1 29 1 40 1 72 1 69 1 74 1 72 1 69 1 74 1 26 1 17 1 37 1 72 1 70 1 73 p 0 01 0 25 0 25 0 25 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 p 0 01 0 75 0 75 0 75 0 92 0 75 1 00 1 00 1 00 1 00 1 00 1 00 1 00 1 00 1 00 1 00 1 00 1 00 1 00 1 00 1 00 1 00 0 94 0 75 1 00 intercept 1 51 1 53 1 49 1 35 1 37 1 33 1 34 1 36 1 33 1 34 1 37 1 31 1 52 1 54 1 51 1 52 1 54 1 44 1 34 1 38 1 31 1 52 1 53 1 50 age 0 02 0 02 0 02 0 01 0 03 0 00 0 04 0 05 0 04 0 02 0 04 0 01 0 02 0 03 0 02 0 02 0 03 0 02 0 05 0 08 0 02 0 02 0 02 0 01 subm l dm 0 08 0 09 0 06 0 02 0 01 0 05 0 20 0 16 0 21 0 12 0 06 0 17 0 37 0 25 0 46 0 34 0 17 0 46 0 25 0 22 0 29 0 03 0 00 0 08 subm r dm 0 85 0 84 0 86 0 51 0 48 0 55 0 37 0 36 0 46 0 42 0 38 0 47 0 50 0 43 0 63 0 53 0 44 0 69 0 38 0 34 0 44 0 78 0 75 0 81 parotid l dm 0 58 0 57 0 59 0 46 0 45 0 47 0 37 0 36 0 40 0 41 0 38 0 43 0 44 0 37 0 49 0 44 0 38 0 52 0 30 0 27 0 34 0 53 0 51 0 55 parotid r dm 0 34 0 33 0 35 0 38 0 37 0 39 0 40 0 39 0 41 0 40 0 38 0 42 0 41 0 35 0 46 0 40 0 33 0 45 0 33 0 28 0 36 0 37 0 36 0 38 xer bsl 2 0 34 0 33 0 34 0 24 0 23 0 26 0 26 0 25 0 27 0 25 0 23 0 27 0 32 0 31 0 33 0 32 0 31 0 33 0 18 0 15 0 22 0 33 0 33 0 34 xer bsl 3 0 34 0 07 0 85 0 06 0 04 0 07 0 09 0 09 0 11 0 08 0 06 0 09 0 22 0 08 0 65 0 39 0 01 2 38 0 14 0 01 0 26 0 32 0 09 0 75 table 16 mean model coefficients for b lr lasso ridge elasticnet pclr laelr dropout lrnn 0 01 3 08 3 44 2 77 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 02 0 22 0 00 0 00 0 03 0 00 0 00 0 00 0 00 0 01 5 06 4 74 5 53 1 37 1 29 1 47 1 50 1 47 1 54 1 49 1 41 1 55 2 10 2 05 2 16 2 13 2 03 2 43 1 69 1 63 1 76 2 23 2 14 2 65 p 0 01 0 47 0 44 0 50 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 02 0 00 0 25 0 00 0 00 0 06 0 00 0 00 0 00 p 0 01 0 53 0 50 0 56 0 79 0 69 0 88 1 00 1 00 1 00 0 99 0 88 1 00 1 00 1 00 1 00 0 97 0 69 1 00 0 98 0 94 1 00 0 77 0 69 0 88 intercept 1 80 1 97 1 74 1 36 1 42 1 33 1 39 1 41 1 37 1 39 1 42 1 35 1 67 1 72 1 65 1 69 1 75 1 65 1 50 1 52 1 47 1 77 1 99 1 72 age 0 05 0 06 0 05 0 01 0 03 0 00 0 03 0 03 0 02 0 02 0 04 0 01 0 04 0 01 0 07 0 04 0 00 0 06 0 03 0 06 0 01 0 04 0 04 0 03 subm l dm 0 29 0 33 0 26 0 01 0 00 0 03 0 06 0 05 0 06 0 04 0 03 0 06 0 14 0 11 0 16 0 14 0 03 0 17 0 07 0 03 0 11 0 00 0 00 0 03 subm l v 10 0 62 0 51 0 81 0 02 0 00 0 08 0 08 0 07 0 08 0 07 0 04 0 08 0 16 0 14 0 19 0 19 0 14 0 33 0 12 0 08 0 15 0 10 0 02 0 28 subm l v 30 0 71 0 76 0 65 0 00 0 00 0 01 0 02 0 01 0 03 0 02 0 00 0 02 0 15 0 11 0 18 0 14 0 04 0 18 0 02 0 03 0 06 0 00 0 00 0 00 subm l v 50 0 39 0 37 0 41 0 05 0 02 0 08 0 07 0 07 0 08 0 07 0 05 0 08 0 12 0 06 0 14 0 13 0 08 0 20 0 08 0 04 0 11 0 06 0 03 0 10 subm r dm 0 26 0 23 0 29 0 13 0 07 0 20 0 12 0 11 0 12 0 12 0 10 0 15 0 17 0 16 0 20 0 18 0 14 0 27 0 15 0 11 0 19 0 18 0 12 0 26 subm r v 10 0 01 0 14 0 17 0 01 0 00 0 03 0 09 0 08 0 10 0 08 0 05 0 09 0 18 0 16 0 21 0 17 0 04 0 20 0 15 0 11 0 18 0 17 0 08 0 26 subm r v 30 0 57 0 49 0 72 0 05 0 02 0 10 0 10 0 09 0 11 0 10 0 07 0 12 0 18 0 16 0 21 0 19 0 16 0 38 0 16 0 12 0 20 0 51 0 40 0 79 subm r v 50 0 07 0 05 0 09 0 16 0 13 0 19 0 13 0 12 0 14 0 14 0 12 0 17 0 16 0 15 0 19 0 15 0 12 0 20 0 13 0 10 0 16 0 14 0 09 0 18 parotid l dm 1 25 1 09 1 40 0 13 0 08 0 18 0 11 0 10 0 11 0 11 0 08 0 15 0 10 0 09 0 11 0 12 0 08 0 35 0 11 0 07 0 14 0 15 0 07 0 22 parotid l v 10 0 32 0 38 0 28 0 07 0 04 0 13 0 12 0 11 0 12 0 11 0 09 0 14 0 14 0 13 0 17 0 13 0 03 0 16 0 13 0 10 0 16 0 06 0 01 0 13 parotid l v 30 0 08 0 03 0 14 0 14 0 10 0 19 0 11 0 11 0 12 0 12 0 10 0 15 0 09 0 08 0 10 0 09 0 08 0 11 0 10 0 06 0 13 0 18 0 12 0 23 parotid l v 50 0 51 0 56 0 46 0 01 0 00 0 02 0 05 0 04 0 05 0 04 0 02 0 05 0 04 0 02 0 06 0 03 0 08 0 05 0 04 0 01 0 07 0 01 0 00 0 04 parotid r dm 1 79 1 58 2 03 0 07 0 03 0 13 0 13 0 12 0 13 0 13 0 10 0 15 0 13 0 12 0 14 0 15 0 10 0 46 0 12 0 08 0 15 0 07 0 01 0 12 parotid r v 10 0 15 0 24 0 08 0 39 0 36 0 42 0 16 0 15 0 17 0 20 0 16 0 28 0 15 0 13 0 20 0 15 0 08 0 20 0 18 0 14 0 21 0 47 0 44 0 50 parotid r v 30 0 37 0 45 0 29 0 12 0 07 0 15 0 13 0 13 0 14 0 13 0 11 0 15 0 11 0 11 0 13 0 10 0 02 0 14 0 12 0 09 0 14 0 13 0 08 0 17 parotid r v 50 0 70 0 78 0 62 0 00 0 00 0 01 0 04 0 02 0 04 0 02 0 01 0 04 0 07 0 02 0 10 0 05 0 10 0 10 0 04 0 01 0 06 0 00 0 00 0 00 xer bsl 2 0 36 0 36 0 36 0 23 0 21 0 25 0 20 0 18 0 21 0 21 0 19 0 24 0 18 0 13 0 24 0 16 0 12 0 22 0 16 0 14 0 20 0 34 0 34 0 34 xer bsl 3 0 38 0 13 0 78 0 05 0 04 0 07 0 07 0 06 0 08 0 07 0 05 0 09 0 04 0 20 0 44 0 01 0 22 0 27 0 11 0 03 0 20 0 42 0 13 0 91 leeuwenberg et al 31 table 17 mean model coefficients for c lr lasso ridge elasticnet pclr laelr dropout lrnn 0 01 0 53 0 58 0 48 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 03 0 00 0 01 0 06 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 01 3 38 3 32 3 44 2 30 2 22 2 40 2 26 2 20 2 32 2 28 2 17 2 40 2 70 2 54 2 86 2 76 2 57 2 96 2 40 2 25 2 59 2 95 2 90 3 01 p 0 01 0 33 0 30 0 40 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 01 0 00 0 10 0 02 0 00 0 10 0 00 0 00 0 00 0 00 0 00 0 00 p 0 01 0 55 0 50 0 70 0 79 0 60 1 00 1 00 1 00 1 00 0 97 0 90 1 00 0 97 0 90 1 00 0 97 0 90 1 00 0 99 0 90 1 00 0 94 0 80 1 00 intercept 3 02 3 05 2 99 2 69 2 75 2 66 2 72 2 75 2 68 2 71 2 76 2 64 3 02 3 10 2 95 3 05 3 14 2 97 2 80 2 89 2 73 3 04 3 08 3 02 age 0 02 0 01 0 03 0 00 0 01 0 02 0 01 0 00 0 01 0 00 0 01 0 02 0 05 0 04 0 07 0 06 0 04 0 09 0 01 0 02 0 04 0 03 0 02 0 04 subm l dm 0 12 0 08 0 15 0 03 0 00 0 10 0 17 0 16 0 19 0 11 0 05 0 16 0 29 0 16 0 39 0 31 0 19 0 41 0 21 0 16 0 28 0 11 0 05 0 18 subm r dm 0 25 0 29 0 22 0 01 0 00 0 04 0 17 0 14 0 18 0 08 0 02 0 14 0 32 0 19 0 45 0 32 0 17 0 46 0 20 0 13 0 27 0 05 0 00 0 14 parotid l dm 0 05 0 06 0 03 0 04 0 01 0 06 0 12 0 10 0 13 0 07 0 04 0 10 0 21 0 10 0 30 0 21 0 10 0 29 0 10 0 06 0 15 0 02 0 00 0 05 parotid r dm 0 22 0 20 0 23 0 15 0 13 0 16 0 22 0 21 0 23 0 19 0 17 0 21 0 26 0 18 0 31 0 27 0 19 0 32 0 20 0 17 0 23 0 13 0 11 0 15 pcm sup dm 0 22 0 25 0 19 0 02 0 00 0 08 0 27 0 25 0 28 0 15 0 07 0 24 0 37 0 27 0 53 0 33 0 19 0 50 0 27 0 22 0 33 0 04 0 00 0 12 pcm med dm 0 86 0 82 0 89 0 37 0 31 0 43 0 27 0 26 0 29 0 33 0 24 0 39 0 36 0 27 0 45 0 40 0 31 0 52 0 32 0 25 0 39 0 57 0 50 0 64 pcm inf dm 0 57 0 55 0 59 0 40 0 36 0 43 0 25 0 23 0 29 0 32 0 26 0 37 0 22 0 13 0 36 0 22 0 13 0 38 0 24 0 17 0 30 0 49 0 44 0 53 supraglottic dm 0 01 0 01 0 03 0 15 0 11 0 20 0 21 0 19 0 22 0 18 0 15 0 22 0 24 0 12 0 31 0 23 0 08 0 32 0 20 0 15 0 25 0 09 0 04 0 16 oralcavity ext dm 1 61 1 58 1 64 1 15 1 09 1 19 0 56 0 51 0 62 0 84 0 65 0 98 0 40 0 29 0 64 0 45 0 31 0 70 0 59 0 46 0 75 1 37 1 31 1 41 glotticarea dm 0 00 0 03 0 03 0 01 0 00 0 03 0 03 0 02 0 05 0 02 0 00 0 04 0 03 0 03 0 13 0 02 0 06 0 12 0 05 0 00 0 09 0 07 0 01 0 13 dysfagie bsl 2 0 17 0 17 0 18 0 16 0 15 0 17 0 20 0 20 0 21 0 18 0 17 0 20 0 22 0 16 0 24 0 19 0 11 0 24 0 16 0 13 0 19 0 19 0 18 0 19 dysfagie bsl 3 0 34 0 34 0 35 0 33 0 33 0 34 0 36 0 36 0 37 0 35 0 34 0 36 0 40 0 36 0 42 0 38 0 33 0 42 0 30 0 27 0 33 0 35 0 35 0 36 32 leeuwenberg et al table 18 mean model coefficients for d lr lasso ridge elasticnet pclr laelr dropout lrnn 0 01 9 10 9 73 8 70 0 04 0 12 0 00 0 15 0 18 0 12 0 12 0 17 0 07 0 46 1 20 0 08 0 30 0 93 0 04 0 26 0 51 0 09 0 00 0 00 0 00 0 01 13 39 12 86 14 06 1 98 1 81 2 21 2 43 2 38 2 52 2 29 2 15 2 45 4 38 3 07 5 75 3 76 2 61 5 08 3 23 2 95 3 62 4 82 4 65 4 98 p 0 01 0 46 0 42 0 50 0 03 0 00 0 07 0 08 0 07 0 10 0 08 0 05 0 10 0 20 0 07 0 30 0 14 0 03 0 30 0 14 0 10 0 20 0 00 0 00 0 00 p 0 01 0 53 0 50 0 57 0 42 0 35 0 53 0 87 0 85 0 88 0 81 0 65 0 88 0 71 0 62 0 85 0 80 0 65 0 95 0 80 0 70 0 88 0 50 0 42 0 57 intercept 3 57 3 61 3 54 2 49 2 57 2 42 2 61 2 63 2 59 2 58 2 62 2 53 3 13 3 36 2 93 3 04 3 32 2 82 2 84 2 91 2 80 3 42 3 48 3 38 age 0 08 0 07 0 09 0 00 0 01 0 01 0 01 0 00 0 01 0 01 0 00 0 02 0 03 0 03 0 06 0 01 0 08 0 05 0 01 0 03 0 04 0 05 0 04 0 06 subm l dm 0 54 0 49 0 61 0 03 0 00 0 11 0 08 0 08 0 09 0 07 0 05 0 09 0 05 0 00 0 10 0 07 0 03 0 14 0 12 0 07 0 16 0 06 0 00 0 16 subm l v 10 0 16 0 28 0 05 0 00 0 00 0 01 0 02 0 02 0 02 0 01 0 01 0 02 0 01 0 10 0 08 0 03 0 07 0 12 0 01 0 03 0 06 0 10 0 00 0 23 subm l v 30 0 20 0 07 0 38 0 00 0 00 0 01 0 02 0 01 0 02 0 01 0 01 0 03 0 04 0 10 0 03 0 00 0 07 0 07 0 01 0 05 0 06 0 04 0 00 0 30 subm l v 50 0 52 0 56 0 48 0 01 0 00 0 02 0 05 0 04 0 05 0 03 0 02 0 05 0 09 0 02 0 16 0 08 0 02 0 15 0 04 0 01 0 08 0 00 0 00 0 03 subm r dm 0 49 0 57 0 41 0 01 0 00 0 02 0 07 0 06 0 07 0 05 0 03 0 07 0 08 0 04 0 11 0 07 0 03 0 11 0 09 0 05 0 15 0 02 0 00 0 11 subm r v 10 0 00 0 09 0 09 0 00 0 00 0 01 0 02 0 01 0 02 0 01 0 00 0 01 0 00 0 05 0 05 0 03 0 03 0 10 0 01 0 04 0 06 0 00 0 00 0 00 subm r v 30 0 21 0 32 0 12 0 00 0 00 0 01 0 01 0 00 0 01 0 01 0 00 0 01 0 02 0 12 0 03 0 01 0 08 0 05 0 02 0 07 0 03 0 05 0 00 0 12 subm r v 50 0 33 0 29 0 39 0 01 0 00 0 05 0 06 0 05 0 07 0 05 0 02 0 06 0 12 0 01 0 21 0 10 0 01 0 19 0 07 0 03 0 11 0 02 0 00 0 08 parotid l dm 0 03 0 18 0 12 0 01 0 00 0 02 0 04 0 03 0 04 0 03 0 01 0 04 0 01 0 02 0 05 0 04 0 03 0 09 0 04 0 01 0 08 0 00 0 00 0 01 parotid l v 10 0 46 0 40 0 52 0 04 0 01 0 08 0 08 0 07 0 08 0 07 0 05 0 09 0 01 0 07 0 17 0 02 0 05 0 11 0 09 0 05 0 14 0 04 0 00 0 08 parotid l v 30 0 01 0 06 0 07 0 01 0 00 0 03 0 01 0 00 0 02 0 01 0 00 0 02 0 01 0 09 0 04 0 02 0 05 0 09 0 01 0 04 0 05 0 00 0 00 0 03 parotid l v 50 0 10 0 16 0 05 0 01 0 00 0 03 0 02 0 01 0 03 0 02 0 01 0 03 0 05 0 01 0 08 0 06 0 01 0 10 0 02 0 01 0 05 0 01 0 00 0 03 parotid r dm 1 20 1 02 1 44 0 03 0 01 0 08 0 06 0 06 0 07 0 05 0 03 0 07 0 03 0 01 0 06 0 07 0 01 0 13 0 07 0 03 0 11 0 01 0 00 0 06 parotid r v 10 0 36 0 45 0 28 0 01 0 00 0 02 0 03 0 03 0 04 0 02 0 01 0 04 0 03 0 11 0 05 0 01 0 10 0 09 0 03 0 01 0 08 0 00 0 00 0 04 parotid r v 30 0 65 0 74 0 55 0 01 0 00 0 03 0 03 0 02 0 04 0 02 0 01 0 04 0 04 0 05 0 10 0 06 0 01 0 12 0 04 0 01 0 08 0 00 0 00 0 02 parotid r v 50 0 11 0 20 0 03 0 07 0 04 0 09 0 08 0 08 0 09 0 08 0 06 0 09 0 09 0 05 0 17 0 10 0 05 0 14 0 07 0 04 0 11 0 08 0 05 0 10 pcm sup dm 0 61 0 52 0 71 0 01 0 00 0 03 0 08 0 08 0 09 0 06 0 03 0 08 0 10 0 04 0 14 0 10 0 05 0 15 0 10 0 05 0 14 0 01 0 00 0 08 pcm sup v 10 1 63 1 81 1 45 0 00 0 00 0 01 0 01 0 02 0 01 0 00 0 02 0 01 0 03 0 17 0 02 0 02 0 16 0 05 0 06 0 15 0 01 0 00 0 00 0 00 pcm sup v 30 0 43 0 50 0 35 0 00 0 00 0 01 0 03 0 03 0 04 0 02 0 01 0 03 0 06 0 03 0 12 0 06 0 01 0 09 0 03 0 04 0 08 0 00 0 00 0 00 pcm sup v 50 0 24 0 29 0 20 0 04 0 01 0 09 0 11 0 11 0 12 0 10 0 07 0 14 0 19 0 11 0 26 0 17 0 09 0 25 0 11 0 07 0 14 0 04 0 00 0 09 pcm med dm 0 62 0 52 0 71 0 38 0 31 0 43 0 13 0 12 0 14 0 18 0 12 0 28 0 17 0 08 0 47 0 15 0 08 0 33 0 18 0 12 0 24 0 44 0 37 0 50 pcm med v 10 0 88 0 73 1 03 0 00 0 00 0 01 0 05 0 04 0 05 0 03 0 01 0 04 0 21 0 04 0 60 0 14 0 01 0 50 0 12 0 07 0 19 0 89 0 75 1 04 pcm med v 30 1 43 1 32 1 55 0 01 0 00 0 02 0 06 0 06 0 07 0 05 0 03 0 07 0 11 0 01 0 61 0 09 0 01 0 35 0 13 0 07 0 22 0 08 0 00 0 21 pcm med v 50 0 52 0 57 0 48 0 01 0 00 0 02 0 05 0 03 0 05 0 03 0 01 0 05 0 09 0 14 0 23 0 10 0 07 0 24 0 03 0 02 0 08 0 00 0 00 0 00 pcm inf dm 2 33 2 28 2 40 0 28 0 22 0 37 0 14 0 13 0 16 0 18 0 13 0 26 0 13 0 04 0 29 0 15 0 06 0 36 0 16 0 09 0 22 0 37 0 33 0 40 pcm inf v 10 0 29 0 17 0 40 0 00 0 00 0 01 0 02 0 02 0 03 0 01 0 00 0 02 0 48 0 11 0 85 0 28 0 08 0 78 0 23 0 15 0 31 0 60 0 49 0 72 pcm inf v 30 0 79 0 84 0 75 0 03 0 08 0 00 0 05 0 06 0 04 0 04 0 06 0 02 0 07 0 15 0 01 0 05 0 19 0 02 0 05 0 12 0 00 0 00 0 00 0 00 pcm inf v 50 1 15 1 21 1 09 0 01 0 00 0 03 0 07 0 06 0 07 0 05 0 02 0 07 0 15 0 01 0 23 0 11 0 06 0 22 0 06 0 02 0 10 0 01 0 00 0 05 supraglottic dm 0 41 0 33 0 48 0 07 0 03 0 12 0 12 0 12 0 13 0 12 0 09 0 17 0 12 0 02 0 30 0 12 0 03 0 29 0 14 0 10 0 20 0 05 0 01 0 15 supraglottic v 10 0 30 0 41 0 21 0 00 0 00 0 01 0 03 0 03 0 03 0 01 0 01 0 02 0 35 0 15 0 63 0 25 0 04 0 62 0 10 0 05 0 16 0 00 0 00 0 00 supraglottic v 30 0 53 0 58 0 47 0 01 0 05 0 00 0 04 0 05 0 03 0 03 0 06 0 00 0 04 0 25 0 07 0 01 0 14 0 07 0 05 0 11 0 01 0 00 0 00 0 00 supraglottic v 50 0 24 0 18 0 28 0 10 0 06 0 13 0 11 0 10 0 11 0 10 0 08 0 12 0 17 0 08 0 24 0 13 0 00 0 22 0 11 0 06 0 15 0 05 0 01 0 09 oralcavity ext dm 0 66 0 55 0 78 0 07 0 01 0 18 0 18 0 17 0 19 0 17 0 12 0 22 0 20 0 15 0 25 0 16 0 09 0 27 0 22 0 17 0 29 0 13 0 00 0 29 oralcavity ext v 10 0 56 0 45 0 71 0 00 0 00 0 01 0 02 0 02 0 03 0 02 0 00 0 02 0 00 0 09 0 05 0 03 0 06 0 08 0 03 0 02 0 07 0 01 0 00 0 07 oralcavity ext v 30 0 44 0 37 0 49 0 17 0 11 0 24 0 14 0 13 0 15 0 15 0 11 0 20 0 20 0 14 0 34 0 17 0 08 0 27 0 18 0 14 0 23 0 38 0 28 0 46 oralcavity ext v 50 0 59 0 52 0 67 0 58 0 53 0 61 0 27 0 26 0 29 0 34 0 27 0 45 0 31 0 20 0 46 0 25 0 11 0 43 0 23 0 17 0 29 0 64 0 57 0 71 glotticarea dm 0 83 0 90 0 77 0 01 0 00 0 01 0 04 0 04 0 05 0 03 0 01 0 04 0 03 0 04 0 19 0 02 0 08 0 18 0 05 0 01 0 10 0 01 0 00 0 07 glotticarea v 10 0 82 0 67 1 04 0 00 0 00 0 01 0 02 0 02 0 02 0 01 0 00 0 02 0 48 0 06 0 88 0 28 0 10 0 77 0 20 0 13 0 27 0 47 0 36 0 58 glotticarea v 30 0 18 0 14 0 22 0 02 0 06 0 00 0 05 0 07 0 04 0 05 0 07 0 03 0 10 0 21 0 03 0 09 0 18 0 01 0 06 0 10 0 01 0 00 0 00 0 00 glotticarea v 50 0 54 0 50 0 58 0 06 0 02 0 09 0 09 0 08 0 10 0 08 0 06 0 11 0 10 0 02 0 17 0 09 0 01 0 18 0 08 0 04 0 13 0 22 0 19 0 25 dysfagie bsl 2 0 16 0 16 0 17 0 14 0 12 0 15 0 15 0 15 0 16 0 15 0 14 0 16 0 21 0 16 0 24 0 17 0 07 0 24 0 11 0 08 0 15 0 19 0 18 0 19 dysfagie bsl 3 0 35 0 34 0 36 0 31 0 30 0 32 0 30 0 29 0 31 0 31 0 29 0 32 0 38 0 32 0 41 0 29 0 14 0 41 0 23 0 19 0 27 0 36 0 35 0 36 comparing methods addressing multi collinearity when developing prediction models abstract 1 introduction 2 methods for collinearity 2 1 penalization of large coefficients 2 2 dropout regularization 2 3 dimensionality reduction 2 4 constrained optimization 3 motivating example 3 1 clinical background 3 2 simulation study 3 2 1 aims 3 2 2 data generating mechanisms 3 2 3 estimators target of analysis 3 2 4 application of the methods 3 2 5 performance measures 3 2 6 coding and execution 3 3 analysis of the results 3 3 1 predictive performance 3 3 2 coefficient estimation 4 discussion acknowledgements references appendix a results for the simulated dysphagia settings b results for the real data settings c baseline tables d definition of methods d 1 the data d 2 the link function d 3 the objective function d 4 loss minimization and hyperparameter tuning d 5 lasso d 6 ridge d 7 elastic net d 8 principal component logistic regression plcr d 9 linear auto encoder logistic regression laelr d 10 dropout regularization d 11 non negative logistic regression lrnn e performed sample size calculations f correlation plots g coefficients