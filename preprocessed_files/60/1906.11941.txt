learning policies through quantile regression oliver richter and roger wattenhofer departement of electrical engineering and information technology eth zurich switzerland richtero wattenhofer ethz ch abstract policy gradient based reinforcement learning algorithms cou pled with neural networks have shown success in learning complex policies in the model free continuous action space control setting however explicitly parameterized policies are limited by the scope of the chosen parametric proba bility distribution we show that alternatively to the likeli hood based policy gradient a related objective can be opti mized through advantage weighted quantile regression our approach models the policy implicitly in the network which gives the agent the freedom to approximate any distribution in each action dimension not limiting its capabilities to the commonly used unimodal gaussian parameterization this broader spectrum of policies makes our algorithm suitable for problems where gaussian policies cannot fit the optimal pol icy moreover our results on the mujoco physics simulator benchmarks are comparable or superior to state of the art on policy methods introduction over the years algorithms in artificial intelligence have be come less restrictive in their assumptions going from expert systems to kernel engineering to deep learning approaches making the same algorithms applicable to a wider range of problems in this mindset we start from one of the least re strictive learning settings continuous action space deep re inforcement learning and show that removing the restric tion of an explicitly parameterized policy can in fact improve performance to achieve this we cut loose of policy gradient methods and show that policies can be improved by moving probability mass through quantile regression towards ade quate actions the core idea of our algorithm is to use quantile re gression koenker 2005 for an implicit approximation of the optimal policy in the network parameters modeling the policy implicitly in the network allows us to approxi mate any distribution i e we are not limited to an a priori defined parameterized distribution like multivariate gaus sian this allows us to learn complex multi modal non gaussian policies that are automatically inferred from in teraction with the environment while under full observ ability there exists an optimal deterministic policy sutton and barto 2018 this is not the case in multi agent and limited history setups here stochastic multi modal poli cies are required to reach optimality a complication that is often overlooked when applying an out of the box gaus sian policy on top of the stochastic flexibility we show that our algorithm has a trust region schulman et al 2015 schulman et al 2017 interpretation and also achieves good performance on a large set of challenging control tasks from the openai gym benchmark suite brockman et al 2016 to summarize our contributions are a derivation of a new reinforcement learning algorithm for continuous action space policies a discussion showing formal similarity between trust re gion policy gradient methods and quantile regression deep reinforcement policy learning an empirical evaluation showing the flexibility and com petitiveness of the proposed algorithm and as a byproduct a novel neural architecture for monotonic function approximation background and related work in reinforcement learning sutton and barto 2018 an agent tries to learn a task through interaction with the environment in which the task is defined by means of a reward signal more formally in each time step t the agent chooses an ac tion at based on the current state st s and gets as feed back the reward rt and next state st 1 from the environ ment s denotes the set of all possible states the goal of the agent is to maximize the discounted cumulative reward r t trt with discount factor 0 1 by adapting its policy to more rewarding trajectories for our derivation we will use the standard definitions sutton and barto 2018 of value function v st and action value function q at st v st e st t t t trt q at st est 1 at st rt v st 1 where the expectation in v is taken over trajectories col lected when acting according to policy starting from state st we use the star symbol as in to denote optimality ar x iv 1 90 6 11 94 1 v 2 cs l g 2 7 s ep 2 01 9 there are two main approaches in the literature to achieve reinforcement learning value based methods foremost q learning watkins 1989 and policy gradient based meth ods sutton et al 2000 both methods have been adapted to use deep neural networks as function approximators mnih et al 2015 mnih et al 2016 where policy gradient based methods are often combined with a critic into actor critic methods to reduce the variance in the gradients deep re inforcement learning the combination of deep learning and reinforcement learning has also found its way into the con tinuous action space setting lillicrap et al 2015 espe cially actor critic methods with proximal second order up dates schulman et al 2015 wu et al 2017 which en sure that the policy stays within a trust region have shown success in learning complex policies in the continuous ac tion space setting a now popular algorithm called proxi mal policy optimization ppo was introduced by schul man et al 2017 who showed that the trust region perfor mance boost can be achieved by optimizing a clipped first order objective recently several papers haarnoja et al 2017 haarnoja et al 2018 b schulman abbeel and chen 2017 haarnoja et al 2018 c haarnoja et al 2018 a showed that soft q learning is in fact equivalent to maximum entropy actor critic methods a setup in which the agent is encouraged to maximize the entropy of its policy besides maximizing the reward combining a stochastic actor with the sample efficiency of q learning haarnoja et al 2018 b haarnoja et al 2018 c has led to impressive results haarnoja et al 2018 a also extended this setup to a hierarchy of la tent policies where higher level policies control lower level actions through an invertible normalizing flows net work dinh sohl dickstein and bengio 2017 tang and agrawal 2018 showed that a normalizing flows network can boost the performance of on policy continuous action space algorithms as well the drawback of normalizing flow networks is however that the network needs to be invertible and thereby requires the computation of the determinant of the jacobian in each layer for the probability mass propa gation this limits normalizing flows in practice to narrow networks a limitation not faced by quantile networks as in our setup one of the early works on maximum entropy deep rein forcement learning haarnoja et al 2017 however already showed that one can train a particle based actor to approx imate the possibly multimodal maximum entropy policy defined by the soft q function with stein variational gradi ent descent liu and wang 2016 this work was extended by zhang et al 2018 which showed that policy optimiza tion can be seen as wasserstein gradient flow in the policy probability space with the 2 wasserstein distance as met ric this led them to propose an additional loss on the ac tion particles which improved performance the relation ship between policy optimization and wasserstein gradient flows was simultaneously shown by richemond and mag innis 2017 the problem with particle based methods is however that for a general sampling network the likelihood of a given sample cannot be directly recovered therefore off policy corrections for multi step learning as proposed in munos et al 2016 espeholt et al 2018 cannot be applied as we discuss in appendix a the sample likelihood can be retrieved from the quantile function our method could therefore in principle be extended with these off policy cor rections in future work fellows et al 2018 recently introduced a variational in ference framework for deep reinforcement learning they investigated the probabilistic nature of policies learned by deep reinforcement learning algorithms from a variational inference perspective but revert to gaussian policies in their experiments for implementation convenience dabney et al 2018 b 2018 a were the first to use quan tile regression in connection with deep reinforcement learn ing in their work they focused on approximating the full probability distribution of the value function in contrast we explore possibilities of using quantile regression to approxi mate richer policies by not constraining the action distribu tion to an explicitly parameterized distribution ostrovski dabney and munos 2018 showed that quantile networks can also be used for generative modeling while recently gasthaus et al 2019 used quantile regression in combina tion with a recurrent neural network for probabilistic fore casting in general we see quantile regression in combina tion with deep learning to have a lot of potential for future work as it provides a flexible way to represent an arbitrary probability distribution within the network quantile regression and quantile networks quantile regression koenker 2005 discusses approxima tion techniques for the inverse cumulative distribution func tion f 1 y i e the quantile function of some probability dis tribution y recent work dabney et al 2018 a ostrovski dabney and munos 2018 showed that a neural network can learn to approximate the quantile function by mapping a uniformly sampled quantile target u 0 1 to its corre sponding quantile function value f 1 y r thereby the trained neural network implicitly models the full probability distribution y more formally let wp u y be the p wasserstein metric wp u y 1 0 f 1 y f 1 u pd 1 p of distributions u and y also characterized as the lp metric of quantile functions mu ller 1997 dabney et al 2018 b show that minimizing the quantile regression loss koenker and hallock 2001 1 0 1 reduces the 1 wasserstein distance between a scalar target probability distribution y and a parameterized mixture of diracs u which correspond to fixed quantiles here y u with y y and u f 1 u is the quantile sample error ostrovski dabney and munos 2018 generalized this result by showing that the expected quantile loss l e u 0 1 ez z z g 2 of a parameterized quantile function g aproximating the quantile function f 1 z of some distribution z is equal to the quantile divergence q z 1 0 f 1 f 1 z fz x dx d plus some constant not depending on the parameters here is the distribution implicitly defined by g therefore training a neural network g to minimize z g with z sampled from the target probability distribution z effectively minimizes the quantile divergence q z and thereby models an approximate distribution of z implicitly in the network parameters of the neural network g another way to state this result is by noting that the quan tile regression loss appears in the continuous ranked probability score crps matheson and winkler 1976 crps f 1 z 1 0 2 z f 1 d which is a proper scoring rule gneiting and raftery 2007 i e ez z crps f 1 z z ez z crps f 1 y z for any distributions z and y minimizing the expected quan tile loss 2 is equivalent to minimizing the expected score ez z crps g z which leads to g approximating f 1 z derivation adapted from gasthaus et al 2019 note that this derivation requires g to always define a proper quantile function which we ensure in this work by modeling g through a monotonically non decreasing neural network by approximating the quantile function instead of a pa rameterized probability distribution as it is common in many deep learning models kingma and welling 2013 mnih et al 2016 schulman et al 2015 wu et al 2017 schulman et al 2017 we do not enforce any constraint on the probability distribution z e g z can be multi modal i e non gaussian as a strong function approximator like a neural network can approximate the corresponding quantile function quantile regression reinforcement learning given that a quantile network can approximate any proba bility distribution we aim at approximating the optimal pol icy in a reinforcement learning setup for this we model for each action dimension the quantile function g s f 1 s of the implicitly defined action distribution by a neural network with a state s and a target quantile 0 1 as input in practice we share the first few layers extracting features from the state into an embedding s before we pass the embedding to the individual quantile net works g s however for ease of notation we omit this shared network in the following derivations 1 from the full network g 0 1 d s rd with d being the 1 see appendix f for more details our exact implementation can be found in the supplementary material number of action dimensions an action a a rd for a given state s can be obtained by sampling u 0 1 d and taking the network output as action since the network approximates quantile functions the network output of a uniformly at random sampled quantile target is a sample from the implicitly defined action distribution the question left to address is how to train the network such that it a represents the quantile functions of the action dimensions and b the implicitly defined policy maximizes the expected discounted reward r objective a can be achieved by limiting the quantile net work to a monotonic function with respect to the quantile in put this ensures that the network represents a valid quan tile function already at the start of training note that such an architectural prior although sensible was not applied in related work dabney et al 2018 a ostrovski dabney and munos 2018 here however it is vital since it allows us to perform gradient ascent on the quantile loss detailed be low as the network remains a valid representation of a quantile function a monotonically non decreasing neural network can be achieved by restricting the neuron connect ing weights to be positive and applying a non linearity that allows for concavity as well as convexity for weight posi tivity we simply set the network weights wnet of the quantile net to the element wise exponent of unconstrained weights w i e wnet exp w as non linearity we choose a com bination of relus with neuron output y max 0 x given neuron input x and inverse relus where y min 0 x we detail and contrast our architecture design against other designs in appendix b extending the current literature on monotonic networks the more difficult objective is b achieving a policy im provement over time here we address this with advantage weighted quantile regression informally put quantile re gression is linked to the wasserstein metric which is also sometimes referred to as earth mover s distance imagine a pile of earth representing probability mass in reinforcement learning we essentially want to move probability mass to wards actions that were good and away from actions that were bad where good and bad are measured by dis counted cumulative bootstrapped reward received quan tile regression can achieve this by shaping the pile of earth according to an advantage estimation and the constraint of monotonicity a core property of quantile functions more formally we are interested in approximating the op timal policy a where we omit the implicit dependence on the state s in the following for ease of readability 2 if we were given the optimal policy this could be achieved by training on the quantile regression objective argmin ea e u 0 1 a g as this would minimize the quantile divergence between and as derived in the previous section however sam pling from the optimal policy is infeasible since we do 2 we focus our analysis on the simple case of a single action di mension with scalar input and scalar output the generalization to multidimensional independent action quantile functions follows trivially not know it a priori therefore we rewrite our objective as argmin ea e u 0 1 a a a g where is a policy with support greater or equal to the sup port of the importance ratio a a gives a measure of how much more less likely a given action a would be under the optimal policy compared to the policy that collected the experience to ease our calculations we assume the be haviour policy to be uniform i e to have constant likeli hood a for all actions a within the support of this lets us absorb the experience likelihood a into the optimiza tion appendix c discusses this approximation further following fellows et al 2018 haarnoja et al 2017 we define the optimal soft policy at temperature as a exp q a v where q and v are the soft action value and value function of the optimal policy we choose the soft formulation to retain a stochastic policy as we are interested in setups where the optimal policy is not deter ministic we can then write our objective as argmin ea e u 0 1 l a with l a exp q a v a g this shifts the perspective from estimating the likelihood un der the optimal policy to estimating the advantage a a q a v of the optimal policy over the sub optimally chosen action a while this quantity is still unknown a priori we can approximate the advantage of our current policy if we therefore approximate the optimal soft pol icy by the exponential of a the advantage of our current best policy we arrive at the iterative optimization procedure k 1 argmin ea e u 0 1 lk a 3 with lk a exp a k a a g where we used the short notation a k to denote the advan tage of policy k note that this objective has a stochastic convergence fixpoint with in practice however it is cumbersome to define a behaviour policy for experience collection a priori because an a priori defined might not cover the support of the optimal policy as required by the derivation on the other hand an a priori defined that does cover the support of might be too explorative ren dering only a few informative action samples which would result in a low sample efficiency of the algorithm what we would therefore like to do is to use our current best esti mate of the optimal policy to gather experience note how ever that the optimization 3 contracts onto the support of since the objective regresses towards samples of if we set and use an approximate iterative pro cedure like stochastic gradient descent at some point the support of would not cover the support of anymore and the policy would degenerate in the limit towards a sub optimal deterministic action in each state heuristics such as entropy regularization mnih et al 2016 have been shown to circumvent similar problems in the discrete action space setup note however that entropy regularization would be non trivial in our quantile function continuous action space setup instead we propose the following we take a linear approximation to the exponential function exp x x 1 around x 0 and multiply by to get the regularized itera tive objective function lk a a k a a g a g 4 note that this approximation is reasonable for a k a 0 which in turn is reasonable for a k given a decent approximation of the advantage this linearization however brings an interesting property if an action taken by the be haviour policy results in an outcome that is worse than the outcome predicted for k i e the advantagea k a is neg ative the objective regularizes in that we maximize the cor responding quantile loss this essentially pushes action samples a away from the bad action a thereby ex panding the support of note that ascending on the quan tile loss does not lead to divergence since we restrict our quantile network to be monotonically non decreasing and therefore any parametarization of the network results in a valid quantile function we can therefore now safely replace the behaviour policy to generate experience with our cur rent best estimate k this gives us an effective iterative on policy algorithm that regresses towards the optimal soft policy based on experience while regularizing against a pre mature collapse to a local optimum another motivation for objective 4 is its similarity to trust region methods schulman et al 2015 schulman et al 2017 while the first part in objective 4 a k a a g gives a measure for how much we will update our policy based on the current rollout the second part a g constrains the updated policy to not de viate too far from the behaviour policy which gathered the experience i e the second term defines a trust region ob jective which is controlled by a high value for keeps the policy close to the policy that collected the current batch of experience yielding update stability while a low value for allows the policy to adjust more rapidly to new experi ences we use this similarity of our algorithm to trust region methods to take advantage of algorithmic improvements em ployed by schulman et al 2017 and adapt them without adjustment in our algorithm namely in our reinforcement learning experiments we use generalized advantage estima tion schulman et al 2016 to approximate a k a normal ize the advantages and train a small number of epochs with mini batches on each collected rollout pseudo code is avail able in appendix d further note that the number of samples taken from the inner expectation in objective 3 can be chosen to trade off required computation against gradient variance in some initial experiments on the mujoco swimmer and ant task we varied the number of samplesk 1 32 128 256 and chosek 128 as we found this to be a good trade off sim ilarly we chose 2 from 0 1 2 4 based on the same 1 5 1 0 5 0 0 5 1 1 5 0 8 0 4 0 2 0 2 0 4 0 8 1 5 1 0 5 0 0 5 1 1 5 figure 1 middle decomposition of the 1 dimensional con tinuous action space into the three distinct actions of the rock paper scissors game any action outside the indicated inter vals is treated as an invalid action and results in a loss if the opponent chooses a valid action top action distribu tion learned by quantile regression training bottom action distribution learned by policy gradient training tasks the remaining hyper parameters were copied without adjustment from schulman et al 2017 and are reported in appendix f the fact that our algorithm works without any adjustment of these hyper parameters hints at its robustness and ease of applicability we leave it to future work to find a better hyper parameter set experiments and results as recent papers have raised valid concerns about the repro ducibility of deep reinforcement learning results in the con tinuous action domain islam et al 2017 henderson et al 2018 we ran all our experiments for 20 fixed random seeds 0 19 implementation details and hyper parameter choices to reproduce the results can be found in the supplementary material with our experiments we aim to answer the follow ing questions is the ability to learn more diverse probability distribu tions worthwhile to get an advantage in simple two player games do multimodal policies emerge from training on these games how does quantile regression deep reinforcement learn ing or qrdrl in short compare to other online al gorithms on well studied reinforcement learning bench marks as a first experiment we implemented a continuous ac tion version of rock paper scissors rock paper scissors can be seen as a two player multi arm bandit problem each player chooses one of the actions rock paper or scis sors and the winner is determined based on the choices rock beats scissors scissors beats paper and paper beats rock we modeled the players through algorithms choos ing their action in a 1 dimensional continuous action space where we defined intervals for the corresponding discrete ac tions as shown in figure 1 we aim to learn a non exploitable 0 100 200 300 400 500 0 3 0 2 0 1 0 0 1 quantile net gaussian net figure 2 average return of the two policy network types over the course of training rock paper scissors see main text for details the x axis denotes the training iteration a re turn of 1 corresponds to the countering policy always win ning while a return of 1 corresponds to the training policy al ways winning plotted is the average and standard deviation shaded area of 20 independent runs curves are smoothed over 10 training iterations i e 1000 games policy in this setup that is we train a policy such that a coun tering policy trained to exploit the former policy achieves the minimum possible wins specifically in each training it eration we train a countering gaussian policy from scratch on 10 000 games against the current policy and then use this countering policy as opponent in 100 games based on which the current policy is updated we trained two policy networks in this setup i a quantile network trained on the weighted quantile loss ea e u 0 1 r a g where the weight r 1 for games that were won and r 1 for games that were lost 3 and ii a gaussian policy trained to maximize the log likelihood of winning i e maximize r via policy gradi ent the exact experiment setup is described in appendix e figure 1 shows histograms of the action distributions learned by the two approaches while figure 2 shows the average return r throughout the training the results show that the uni modal nature of the gaussian network can always be exploited by the countering policy hindering any learning progress on the other hand the quantile network learns to choose close to uniform at random making the policy im possible to exploit moreover it learned that the countering gaussian policy is initialized with the mean close to 0 pa per which explains the slight tilt of the action distribution towards the right scissors and the slightly above zero return at the end of the training i e it has learned to exploit the initialization and inability to counter of the countering policy however we did not find the multi modal nature in the learned action distribution that we were hoping for to ver ify that our approach can indeed learn a multi modal policy we implemented another toy game in this game which we call choice the agent also acts in a single continuous ac 3 this is the multi arm bandit analogy to 4 with 0 1 0 5 0 0 5 1 1 0 5 0 0 5 1 figure 3 histograms of the learned action distributions on the choice toy game top our approach qrdrl bottom ppo with gaussian policy 0 0 5 1 0 1 5 2 qrdrl ppo figure 4 average return over the course of training on the choice toy game the x axis denotes steps in millions plot ted is the average and standard deviation shaded area of 20 independent runs tion dimension where an action between 0 6 and 0 4 cor responds to a button a pressed while an action between 0 4 and 0 6 corresponds to a button b pressed the agent is re warded if it presses the button that was pressed less often so far within the episode the problem is complicated in that we only model the agent as a feed forward network giv ing it no ability to remember the actions it took so far note that in this setup the best policy is to choose one of the two buttons at random implementation details can be found in appendix f as can be seen in figure 3 qrdrl is capable of recov ering the two modes needed to solve the task while prox imal policy optimization ppo schulman et al 2017 a commonly used gaussian policy gradient method learns a suboptimal compromise between the two buttons this is es pecially apparent when we look at the corresponding return throughout the training depicted in figure 4 the return of ppo stagnates around 1 5 while qrdrl continues to im prove throughout the training nevertheless qrdrl was unable to put 0 probability on the invalid actions between the modes within the given training time we believe this stems mainly from our architecture choice and the artificial discontinuous distribution setup that is difficult to approxi mate see appendix b given that the ability to express more complex stochas tic policies is indeed vital to perform well in the toy games presented we are left with our second research question whether qrdrl also performs well on commonly used re inforcement learning benchmarks to this end we run our algorithm on a diverse set of robotic tasks defined in openai gym brockman et al 2016 based on the mujoco todorov erez and tassa 2012 physics simulator these include di verse robotic figures which should learn to walk as well as robotic arms and pendulums which need to reach a cer tain point or balance themselves we compare our approach against ppo schulman et al 2017 as well as normaliz ing flows trpo dinh sohl dickstein and bengio 2017 a recently proposed on policy algorithm using normalizing flows 4 implementation details are provided in appendix f the results reported in figure 5 show that qrdrl is in deed capable of performing as well as if not superior to current state of the art on policy methods specifically we clearly outperform these methods on the pendulum tasks as well as the challenging ant task we see a similar trend in the normalizing flows trpo n trpo results suggest ing that a more flexible policy is especially helpful in these tasks however n trpo fails to match our results in many cases which we believe might be due to the limitation of a narrow flow network note also that we achieved our results without extensive hyper parameter tuning and our results might even improve for better suited hyper parameters conclusion and outlook in this work we introduce quantile regression deep rein forcement learning qrdrl a likelihood free reinforce ment learning approach to overcome the limitation of ex plicitly parameterized policies we show that the ability to learn more diverse stochastic continuous action space poli cies is essential to perform well in different situations be it to defer the exploitability of a policy or to cope with memory limitations we further show that our algorithm has perfor mance comparable to state of the art on policy methods on the well studied mujoco benchmarks given that this work introduces a new idea to approach re inforcement learning with quantile regression it offers many directions for future work off policy learning we showed in this work how quan tile regression can be used within an on policy algorithm future work could extend our approach to the off policy set ting leveraging the advantage of sample reuse note that while our training objective is likelihood free we can still recover the action likelihood through a simple gradient back propagation see appendix a this is a distinct advantage of our approach over other particle based algorithms as it enables the potential use of multi step off policy corrections as described by munos et al 2016 or espeholt et al 2018 4 we compare to their trpo version since their normalizing flow acktr implementation yielded worse inconsistent results 0 0 5 1 0 100 50 0 reacher v 2 qrdrl ppo n trpo 0 0 5 1 0 0 5 000 10 000 inverteddoublependulum v 2 0 0 5 1 0 0 500 1 000 invertedpendulum v 2 0 0 5 1 0 0 50 100 swimmer v 2 0 0 5 1 0 0 1 000 2 000 hopper v 2 0 0 5 1 0 0 1 000 2 000 3 000 4 000 walker 2 d v 2 0 0 5 1 0 0 1 000 2 000 halfcheetah v 2 0 0 5 1 0 0 1 000 2 000 ant v 2 0 0 5 1 0 200 400 600 humanoid v 2 figure 5 average return on openai gym mujoco benchmarks the x axis denotes steps in millions plotted is the average and standard deviation shaded area of 20 independent runs multivariate quantile regression we took a conserva tive approach in this work by modeling each action di mension as an independent quantile function this is along the line of gaussian policy implementations where off diagonal elements of the covariance matrix are omitted as well however insights into multivariate quantile regression chakraborty 2003 hallin et al 2010 could be adapted to give our algorithm the capability to infer the stochastic rela tions between action dimensions references brockman et al 2016 brockman g cheung v petters son l schneider j schulman j tang j and zaremba w 2016 openai gym cano et al 2019 cano j r gutirrez p a krawczyk b woniak m and garca s 2019 monotonic classifi cation an overview on algorithms performance measures and data sets neurocomputing 341 168 182 chakraborty 2003 chakraborty b 2003 on multivariate quantile regression journal of statistical planning and in ference 110 1 109 132 dabney et al 2018 a dabney w ostrovski g silver d and munos r 2018 a implicit quantile networks for dis tributional reinforcement learning in proceedings of the 35 th international conference on machine learning icml 2018 stockholmsma ssan stockholm sweden july 10 15 2018 1104 1113 dabney et al 2018 b dabney w rowland m bellemare m g and munos r 2018 b distributional reinforcement learning with quantile regression in proceedings of the thirty second aaai conference on artificial intelligence new orleans louisiana usa february 2 7 2018 daniels and velikova 2010 daniels h and velikova m 2010 monotone and partially monotone neural networks ieee transactions on neural networks 21 6 906 917 dinh sohl dickstein and bengio 2017 dinh l sohl dickstein j and bengio s 2017 density estimation using real nvp in 5 th international conference on learning representations iclr 2017 toulon france april 24 26 2017 conference track proceedings espeholt et al 2018 espeholt l soyer h munos r simonyan k mnih v ward t doron y firoiu v harley t dunning i legg s and kavukcuoglu k 2018 impala scalable distributed deep rl with impor tance weighted actor learner architectures in proceedings of the 35 th international conference on machine learning icml 2018 stockholmsma ssan stockholm sweden july 10 15 2018 1406 1415 fellows et al 2018 fellows m mahajan a rudner t g j and whiteson s 2018 virel a variational inference framework for reinforcement learning corr abs 1811 01132 gasthaus et al 2019 gasthaus j benidis k wang y rangapuram s s salinas d flunkert v and januschowski t 2019 probabilistic forecasting with spline quantile function rnns in chaudhuri k and sugiyama m eds proceedings of machine learning research vol ume 89 of proceedings of machine learning research 1901 1910 pmlr glorot bordes and bengio 2011 glorot x bordes a and bengio y 2011 deep sparse rectifier neural networks in gordon g dunson d and dudk m eds proceedings of the fourteenth international conference on artificial in telligence and statistics volume 15 of proceedings of ma chine learning research 315 323 fort lauderdale fl usa pmlr gneiting and raftery 2007 gneiting t and raftery a e 2007 strictly proper scoring rules prediction and esti mation journal of the american statistical association 102 477 359 378 haarnoja et al 2017 haarnoja t tang h abbeel p and levine s 2017 reinforcement learning with deep energy based policies in proceedings of the 34 th interna tional conference on machine learning icml 2017 syd ney nsw australia 6 11 august 2017 1352 1361 haarnoja et al 2018 a haarnoja t hartikainen k abbeel p and levine s 2018 a latent space policies for hierarchical reinforcement learning in proceedings of the 35 th international conference on machine learning icml 2018 stockholmsma ssan stockholm sweden july 10 15 2018 1846 1855 haarnoja et al 2018 b haarnoja t zhou a abbeel p and levine s 2018 b soft actor critic off policy maximum entropy deep reinforcement learning with a stochastic actor in proceedings of the 35 th international conference on ma chine learning icml 2018 stockholmsma ssan stockholm sweden july 10 15 2018 1856 1865 haarnoja et al 2018 c haarnoja t zhou a hartikainen k tucker g ha s tan j kumar v zhu h gupta a abbeel p and levine s 2018 c soft actor critic algo rithms and applications corr abs 1812 05905 hallin et al 2010 hallin m paindaveine d s iman m wei y serfling r zuo y kong l and mizera i 2010 multivariate quantiles and multiple output regression quan tiles from l 1 optimization to halfspace depth with discus sion and rejoinder the annals of statistics 635 703 henderson et al 2018 henderson p islam r bachman p pineau j precup d and meger d 2018 deep re inforcement learning that matters in proceedings of the thirty second aaai conference on artificial intelligence aaai 18 the 30 th innovative applications of artificial in telligence iaai 18 and the 8 th aaai symposium on edu cational advances in artificial intelligence eaai 18 new orleans louisiana usa february 2 7 2018 3207 3214 islam et al 2017 islam r henderson p gomrokchi m and precup d 2017 reproducibility of benchmarked deep reinforcement learning tasks for continuous control corr abs 1708 04133 jones 1992 jones m c 1992 estimating densities quan tiles quantile densities and density quantiles annals of the institute of statistical mathematics 44 4 721 727 kingma and welling 2013 kingma d p and welling m 2013 auto encoding variational bayes arxiv preprint arxiv 1312 6114 koenker and hallock 2001 koenker r and hallock k 2001 quantile regression an introduction journal of eco nomic perspectives 15 4 43 56 koenker 2005 koenker r 2005 quantile regression econometric society monographs cambridge university press lang 2005 lang b 2005 monotonic multi layer per ceptron networks as universal approximators in duch w kacprzyk j oja e and zadroz ny s eds artificial neural networks formal models and their applications icann 2005 31 37 berlin heidelberg springer berlin heidelberg lillicrap et al 2015 lillicrap t p hunt j j pritzel a heess n erez t tassa y silver d and wierstra d 2015 continuous control with deep reinforcement learning arxiv preprint arxiv 1509 02971 liu and wang 2016 liu q and wang d 2016 stein variational gradient descent a general purpose bayesian inference algorithm in advances in neural information processing systems 29 annual conference on neural in formation processing systems 2016 december 5 10 2016 barcelona spain 2370 2378 matheson and winkler 1976 matheson j e and winkler r l 1976 scoring rules for continuous probability distri butions management science 22 10 1087 1096 minin and lang 2008 minin a and lang b 2008 com parison of neural networks incorporating partial monotonic ity by structure in ku rkova v neruda r and koutn k j eds artificial neural networks icann 2008 597 606 berlin heidelberg springer berlin heidelberg mnih et al 2015 mnih v kavukcuoglu k silver d rusu a a veness j bellemare m g graves a ried miller m a fidjeland a ostrovski g petersen s beattie c sadik a antonoglou i king h kumaran d wierstra d legg s and hassabis d 2015 human level control through deep reinforcement learning nature 518 7540 529 533 mnih et al 2016 mnih v badia a p mirza m graves a lillicrap t p harley t silver d and kavukcuoglu k 2016 asynchronous methods for deep reinforcement learning in proceedings of the 33 nd international confer ence on machine learning icml 2016 new york city ny usa june 19 24 2016 1928 1937 mu ller 1997 mu ller a 1997 integral probability metrics and their generating classes of functions advances in ap plied probability 29 2 429 443 munos et al 2016 munos r stepleton t harutyunyan a and bellemare m g 2016 safe and efficient off policy reinforcement learning in advances in neural information processing systems 29 annual conference on neural in formation processing systems 2016 december 5 10 2016 barcelona spain 1046 1054 ostrovski dabney and munos 2018 ostrovski g dab ney w and munos r 2018 autoregressive quantile networks for generative modeling in proceedings of the 35 th international conference on machine learning icml 2018 stockholmsma ssan stockholm sweden july 10 15 2018 3933 3942 parzen 1979 parzen e 1979 nonparametric statistical data modeling journal of the american statistical associa tion 74 365 105 121 richemond and maginnis 2017 richemond p h and maginnis b 2017 on wasserstein reinforcement learning and the fokker planck equation corr abs 1712 07185 schulman abbeel and chen 2017 schulman j abbeel p and chen x 2017 equivalence between policy gra dients and soft q learning corr abs 1704 06440 schulman et al 2015 schulman j levine s moritz p jordan m i and abbeel p 2015 trust region policy optimization corr abs 1502 05477 schulman et al 2016 schulman j moritz p levine s jordan m i and abbeel p 2016 high dimensional con tinuous control using generalized advantage estimation in 4 th international conference on learning representations iclr 2016 san juan puerto rico may 2 4 2016 confer ence track proceedings schulman et al 2017 schulman j wolski f dhariwal p radford a and klimov o 2017 proximal policy op timization algorithms corr abs 1707 06347 sill 1998 sill j 1998 monotonic networks in advances in neural information processing systems 661 667 sutton and barto 2018 sutton r s and barto a g 2018 reinforcement learning an introduction the mit press second edition sutton et al 2000 sutton r s mcallester d a singh s p and mansour y 2000 policy gradient methods for reinforcement learning with function approximation in advances in neural information processing systems 1057 1063 tang and agrawal 2018 tang y and agrawal s 2018 boosting trust region policy optimization by normalizing flows policy corr abs 1809 10326 tarca grandjean and larachi 2004 tarca l a grand jean b p and larachi f 2004 embedding monotonicity and concavity in the training of neural networks by means of genetic algorithms application to multiphase flow com puters and chemical engineering 28 9 1701 1713 todorov erez and tassa 2012 todorov e erez t and tassa y 2012 mujoco a physics engine for model based control in 2012 ieee rsj international conference on in telligent robots and systems 5026 5033 tukey 1965 tukey j w 1965 which part of the sam ple contains the information proceedings of the national academy of sciences 53 1 127 134 velikova daniels and feelders 2006 velikova m daniels h and feelders a j 2006 mixtures of monotone networks for prediction watkins 1989 watkins c j c h 1989 learning from delayed rewards ph d dissertation king s college cam bridge wu et al 2017 wu y mansimov e liao s grosse r b and ba j 2017 scalable trust region method for deep reinforcement learning using kronecker factored ap proximation corr abs 1708 05144 you et al 2017 you s ding d canini k pfeifer j and gupta m 2017 deep lattice networks and partial monotonic functions in guyon i luxburg u v ben gio s wallach h fergus r vishwanathan s and gar nett r eds advances in neural information processing systems 30 curran associates inc 2981 2989 zhang et al 2018 zhang r chen c li c and carin l 2018 policy optimization as wasserstein gradient flows in proceedings of the 35 th international conference on ma chine learning icml 2018 stockholmsma ssan stockholm sweden july 10 15 2018 5741 5750 a likelihood in quantile functions while there are different ways to approximate an arbitrary probability distribution y we want to highlight a useful trait of approximating the quantile function specifically we note that the partial derivative of the quantile function f 1 y with respect to the quantile also known as the sparsity function tukey 1965 or quantile density function parzen 1979 has the interesting property jones 1992 f 1 y 1 py f 1 y where py is the probability density function of the distri bution y in our setup this allows us to retrieve the likeli hood of an action a g as a 1 g where is the probability density function of the policy distribution implicitly defined in our quantile function ap proximation g b monotonic neural networks while there are several methods for monotonic function approximation in the literature cano et al 2019 par tially monotonic neural networks minin and lang 2008 daniels and velikova 2010 are mainly constructed us ing either the max pool min pool architecture introduced by sill 1998 or a positive weight network with tanh activations as introduced by lang 2005 other approaches include training a monotonic network through genetic al gorithms tarca grandjean and larachi 2004 or us ing a mixture of monotonic networks for prediction ve likova daniels and feelders 2006 more recently you et al 2017 introduced a monotonic neural network with mul tidimensional lattices as non linearities we believe that this approach over complicates the problem of monotonic func tion approximation with a neural network and restrain to a novel simple yet effective way first of all we note that a neural network is a combi nation of functions most commonly a combination of lin ear embeddings i e matrix multiplication and element wise non linearities e g tanh or relu the easiest way to restrict a neural network to approximate only mono tonic functions is therefore to restrict all constituent func tions to be monotonic with respect to their inputs this can be done by restricting the matrix multiplications to have only positive weights and use monotonic element wise non linearities as was done with tanh activation by lang 2005 weight positivity is easily achieved by setting all parame ters of the network weight matrices wnet to the exponent of unconstrained weights w i e wnet exp w with exp applied element wise as was done by sill 1998 we can then optimize w using standard stochastic gradi ent decent methods in our experiments we initialize w to log u 0 fin where fin is the number of neurons in the previous layer and is a hyperparameter we set to 3 note that this positivity constraint through exponentiation is only done for the weights and not for the biases since the biases do not influence the monotinicity the more interesting architecture choice is the choice of non linearity in our work we take advantage of the good performance reported for relu non linearities glo rot bordes and bengio 2011 note that relu activation y max 0 x is monotone with respect to its input more so it is convex which would lead to a network only capable of approximating convex functions if we restrict the net work weights to be positive to overcome this limitation we use an adjusted relu activation on the hidden layers of our monotonic neural network that is we take half of the em bedding dimensions and apply a relu activation while we apply an inverse relu activation y min 0 x to the re maining embedding dimensions note that the inverse relu activation is concave and a following linear layer with pos itive weight matrix can therefore make any combination of concave and convex functions giving it the capability to ap proximate any monotonic function for a large enough hidden embedding to compare our architecture against the max pool min pool architecture of sill 1998 and an architecture with tanh non linearity as proposed by lang 2005 we train simple one hidden layer networks of each architecture with matched number of parameters 64 hidden units for the relu and tanh architecture 96 hidden units for the max min architecture to approximate the quantile function of some toy distributions based on a scalar quantile input is fed to the network as one dimensional input sampled from a scaled and shifted uniform distribution over 1 1 as 1 dimensional target distributions we choose a normal gaus sian mean 0 and standard deviation 1 a bi modal gaussian distribution two gaussians with mean 1 and 1 and both with standard deviation 0 5 a sample is drawn with equal probability from either of the two as well as a discontinuous uniform distribution samples are drawn with equal proba bility either uniformly from 1 0 5 or uniformly from 0 5 1 we train each network on 10 000 mini batches of 128 samples each on the quantile regression loss 2 the net works are trained on the learning rates 0 00001 0 0001 0 001 0 01 0 1 and the best results achieved averaged over 5 random seeds are compared in table 1 5 we note that while no architecture is significantly better than the others we see a tendency of our relu architecture to improve in normal distribution approximation further note that the max min architecture requires comparatively large learning rates we believe this is due to the fact that only a few parameters are updated with every sample due to the pooling therefore requiring a larger learning rate to be competitive with the other architectures lastly we further investigate the bad performance of our relu architecture on the discontinuous uniform distribution task in figure 6 we see that the relu network has difficulties to approxi mate the discontinuity we believe that the stochastic nature of the task prohibits any weight in the network to grow large enough to approximate the discontinuity with a steep slope 5 code to re run this experiment is available in the supplemen tary material gaussian bi modal gaussian discontinuous uniform max min 0 048 0 016 lr 0 01 0 031 0 013 lr 0 1 0 006 0 003 lr 0 1 tanh 0 055 0 015 lr 0 01 0 028 0 005 lr 0 0001 0 023 0 018 lr 0 01 relu 0 028 0 009 lr 0 01 0 019 0 011 lr 0 001 0 050 0 009 lr 0 001 table 1 comparison of different monotonic network architectures mean squared error after fitting different distributions with quantile regression shown is the mean and standard deviation of 5 random seeds as well as the learning rate with which the result was achieved 0 0 2 0 4 0 6 0 8 1 1 0 1 max min tanh relu target figure 6 learned quantile function approximations g of the different monotonic network architectures to the quan tile function of the discontinuous uniform distribution within the limited training time note that the other archi tectures provide inductive biases that help to deal with the discontinuity nevertheless we chose our relu architecture for our re inforcement learning agent since we believe that the ability to approximate multi modal normal distributions well out weights the ability to approximate discontinuities well note that we might get even better results in our toy experiments if we had chosen otherwise c constant action likelihood a in our derivation in section we initially assume the be haviour policy to be uniform such that the action like lihood a gets absorbed as constant in the optimization later we then set k which is clearly a stretch on the assumption of uniformity especially if we try to approx imate multi modal distributions with k however in early experiments we found that k stays often close to a uniform distribution and mainly adjusts the support assuming the distribution goes from a brought stochastic to a less stochas tic distribution with smaller support then the corresponding constant likelihood increases accordingly this translates in a decreased learning rate over time as the objective is multiplied by an ever smaller constant which might also help the algorithm converge we therefore retained the con stant action likelihood approximation a const note however that theoretically one can calculate the action like lihood based on the gradient of the action with respect to the quantile see appendix a store it alongside the action in the rollout data and correct the optimization objective ac cordingly we leave the investigation of such a correction to future work as action likelihoods in continuous action spaces can be non trivial to handle d pseudo code a pseudo code version of our proposed algorithm can be seen in algorithm 1 algorithm 1 learning policies through quantile regression require batch size b number of mini epochs e general ized advantage estimation discount factor inner sam ple number k randomly initialized policy network g for k 0 1 2 do gather b time steps of experience by taking actions at g st with u 0 1 d calculate the generalized advantage estimates a gae k and normalize them over the batch for e epochs do sample a mini batch of si ai a k i tuples sample k u 0 1 d for every tuple calculate lk as mean over the batch and s based on equation 4 update to minimize lk using adam end for end for e rock paper scissors in our rock paper scissors experiment we compare a gaus sian policy network against a quantile policy network both policies are trained as described in the main text against a gaussian countering policy with the same specifications as the other gaussian policy network detailed hereafter the policies opposing each other in a game choose simultane ously an action and the winner is determined as follows if both policies chose a valid action i e an action within one of the three action ranges depicted in figure 1 the winner is determined based on rock paper scissors rules that is rock wins over scissors paper over rock and scissors over paper if both policies chose the same ac tion the game results in a draw r 0 if one of the policies chose an invalid action i e an action outside the three action ranges the other policy wins with any valid action if both policies chose an invalid action the game results in a draw as gaussian policy network we implemented a simple 2 layer fully connected neural network which takes the two actions played in the last game as input projects them to a hidden layer with 64 neurons and relu activation and then back to a 2 dimensional embedding from which the first dimension is taken as mean and the second dimension as log standard deviation of the gaussian defining the action distribution our quantile policy network also consists of 2 fully con nected layers where the first takes as input the one dimen sional scaled and shifted quantile sampled from a uniform distribution over 1 1 and projects it through a positive weight matrix to the hidden representation with 64 neurons half of the neurons in the hidden representation have relu activation while the other half goes through an inverse relu non linearity as described in appendix b the hid den representation is then projected with a positive weight matrix to a single output dimension directly representing the action both gaussian and quantile network have train able unconstrained bias terms added in each layer also we did not use multiple samples for the quantile loss in this ex periment i e we set k 1 note that the gaussian network has i more parameters than the quantile network and ii more information given to it through the input the gaussian network can base its action on the last action played giving it the ability to exploit any deterministic policy nevertheless the quantile network can better fit the action space leading to better results see main text f reinforcement learning experiments for our reinforcement learning experiments the toy choice game as well as the mujoco experiments we adjusted the ppo implementation published in the openai baselines github repository https github com openai baselines that is our policy network consists of a state feature ex traction part equivalent to the ppo network architecture be fore the final projection to the gaussian parameters we re place this final projection through a quantile net per action dimension similar to the one described in appendix e with the only difference that we add the extracted features to the hidden representations of the quantile networks we summa rize the hyperparameters used in table 2 for our baselines we used the implementations provided by the authors hyperparameter value initial learning rate 3 e 4 learning rate schedule linear decay optimizer adam adam epsilon 1 e 5 discount factor 0 99 gae lambda 0 95 steps taken per update 2048 number of epochs in each update 10 batch size 32 quantile loss samples k 128 temperature 2 table 2 hyperparameters used in the reinforcement learning experiments introduction background and related work quantile regression and quantile networks quantile regression reinforcement learning experiments and results conclusion and outlook a likelihood in quantile functions b monotonic neural networks c constant action likelihood a d pseudo code e rock paper scissors f reinforcement learning experiments