ar x iv 1 90 5 01 70 7 v 5 cs l g 2 7 o ct 2 01 9 a latent variational framework for stochastic optimization philippe casgrain department of statistical sciences university of toronto toronto on canada p casgrain mail utoronto ca abstract this paper provides a unifying theoretical framework for stochastic optimiza tion algorithms by means of a latent stochastic variational problem using tech niques from stochastic control the solution to the variational problem is shown to be equivalent to that of a forward backward stochastic differential equation fbsde by solving these equations we recover a variety of existing adaptive stochastic gradient descent methods this framework establishes a direct connec tion between stochastic optimization algorithms and a secondary latent inference problem on gradients where a prior measure on gradient observations determines the resulting algorithm 1 introduction stochastic optimization algorithms are tools which are crucial to solving optimization problems arising in machine learning the initial motivation for these algorithms comes from the fact that computing the gradients of a target loss function becomes increasingly difficult as the scale and dimension of an optimization problem grows larger in these large scale optimization problems de terministic gradient based optimization algorithms perform poorly due to the computational load of repeatedly computing gradients stochastic optimization algorithms remedy this issue by replacing exact gradients of the target loss with a computationally cheap gradient estimator trading off noise in gradient estimates for computational efficiency at each step to illustrate this idea consider the problem of minimizing a generic risk function f rd r taking the form f x 1 n z n x z 1 where rd z r and where we define the set n zi z i 1 n to be a set of training points in this definition we interpret x z as the model loss at a single training point z n for the parameters x rd when n and d are typically large computing the gradients of f can be time consuming knowing this let us consider the path of an optimization algorithm as given by xt t n rather than comput ing f xt directly at each point of the optimization process we may instead collect noisy samples of gradients as gt 1 nmt z nmt x xt z 2 where for each t nmt n is an independent sample of size m from the set of training points we assume that m n is chosen small enough so that gt can be computed at a significantly lower cost 33 rd conference on neural information processing systems neurips 2019 vancouver canada http arxiv org abs 1905 01707 v 5 than f xt using the collection of noisy gradients gt t n stochastic optimization algorithms construct an estimator f xt of the gradient f xt in order to determine the next step xt 1 of the optimizer this paper presents a theoretical framework which provides new perspectives on stochastic opti mization algorithms and explores the implicit model assumptions that are made by existing ones we achieve this by extending the approach taken by wibisono et al 2016 to stochastic algorithms the key step in our approach is to interpret the task of optimization with a stochastic algorithm as a latent variational problem as a result we can recover algorithms from this framework which have built in online learning properties in particular these algorithms use an online bayesian filter on the stream of noisy gradient samples gt to compute estimates of f xt under various model assumptions on f and g we recover a number of common stochastic optimization algorithms 1 1 related work there is a rich literature on stochastic optimization algorithms as a consequence of their effectiveness in machine learning applications each algorithm introduces its own variation on the gradient esti mator f xt as well as other features which can improve the speed of convergence to an optimum amongst the simplest of these is stochastic gradient descent and its variants robbins and monro 1951 which use an estimator based on single gradient samples others such as lucas et al 2018 nesterov use momentum and acceleration as features to enhance convergence and can be inter preted as using exponentially weighted moving averages as gradient estimators adaptive gradient descent methods such as adagrad from duchi et al 2011 and adam from kingma and ba 2014 use similar moving average estimators as well as dynamically updated normalization factors for a survey paper which covers many modern stochastic optimization methods see ruder 2016 there exist a number of theoretical interpretations of various aspects of stochastic optimization cesa bianchi et al 2004 have shown a parallel between stochastic optimization and online learn ing some previous related works such as gupta et al 2017 provide a general model for adaptive methods generalizing the subgradient projection approach of duchi et al 2011 aitchison 2018 use a bayesian model to explain the various features of gradient estimators used in stochastic opti mization algorithms this paper differs from these works by naturally generating stochastic algo rithms from a variational principle rather than attempting to explain their individual features this work is most similar to that of wibisono et al 2016 who provide a variational model for continuous deterministic optimization algorithms there is a large body of research on continuous time approximations to deterministic opti mization algorithms via dynamical systems odes da silva and gazeau 2018 krichene et al 2015 su et al 2014 wilson et al 2016 as well as approximations to stochastic opti mization algorithms by stochastic differential equations sdes krichene and bartlett 2017 mertikopoulos and staudigl 2018 raginsky and bouvrie 2012 xu et al 2018 a b in partic ular the most similar of these works raginsky and bouvrie 2012 xu et al 2018 a b study con tinuous approximations to stochastic mirror descent by adding exogenous brownian noise to the continuous dynamics derived in wibisono et al 2016 this work differs by deriving continuous stochastic dynamics for optimizers from a broader theoretical framework rather than positing the continuous dynamics as is although the equations studied in these papers may resemble some of the results derived in this one they differ in a number of ways firstly this paper finds that the source of randomness present in the optimizer dynamics obtained in this paper are not generated by an exogenous source of noise but are in fact an explicit function of the randomness generated by observed stochastic gradients during the optimization process another important difference is that the optimizer dynamics presented in this paper make no use of the gradients of the objective function f which is inaccessible to a stochastic optimizer and are only a function of the stream of stochastic gradients gt 1 2 contribution to the author s knowledge this is the first paper to produce a theoretical model for stochastic opti mization based on a variational interpretation this paper extends the continuous variational frame work wibisono et al 2016 to model stochastic optimization from this model we derive optimality conditions in the form of a system of forward backward stochastic differential equations fbsdes 2 and provide bounds on the expected rate of convergence of the resulting optimization algorithm to the optimum by discretizing solutions of the continuous system of equations we can recover a number of well known stochastic optimization algorithms demonstrating that these algorithms can be obtained as solutions of the variational model under various assumptions on the loss function f x that is being minimized 1 3 paper structure in section 2 we define a continuous time surrogate model of stochastic optimization section 3 uses this model to motivate a stochastic variational problem over optimizers in which we search for stochastic optimization algorithms which achieve optimal average performance over a collection of minimization problems in section 4 we show that the necessary and sufficient conditions for optimality of the variational problem can be expressed as a system of forward backward stochastic differential equations theorem 4 2 provides rates of convergence for the optimal algorithm to the optimum of the minimization problem lastly section 5 recovers sgd mirror descent momentum and other optimization algorithms as discretizations of the continuous optimality equations derived in section 4 under various model assumptions the proofs of the mathematical results of this paper are found within the appendices 2 a statistical model for stochastic optimization over the course of the section we present a variational model for stochastic optimization the ultimate objective will be to construct a framework for measuring the average performance of an algorithm over a random collection of optimization problems we define random variables in an ambient probability space p g gt t 0 t where gt is a filtration which we will define at a later point in this section we assume that loss functions are drawn from a random variable f c 1 rd each draw from the random variable satisfies f x r for fixed x rd and f is assumed to be an almost surely continuously differentiable in x in addition we make the technical assumption that e f x 2 for all x rd we define an optimizer x x t t 0 as a controlled process satisfying x t r d for all t 0 with initial condition x 0 r d the paths of x are assumed to be continuously differentiable in time so that the dynamics of the optimizer may be written as dx t t dt where t r d represents the control where we use the superscript to express the explicit dependence of x on the control we may also write the optimizer in its integral form as x t x 0 t 0 u du demonstrating that the optimizer is entirely characterized by a pair x 0 consisting of a control process and an initial condition x 0 using an explicit euler discretization with step size 0 the optimizer can be approximately represented through the update rule x t x t t this leads to the interpretation of t as the infinitesimal step the algorithm takes at each point t during the optimization process in order to capture the essence of stochastic optimization we construct our model so that optimizers have restricted access to the gradients of the loss function f rather than being able to directly ob serve f over the path of x t we assume that the algorithm may only use a noisy source of gradient samples modeled by a c dl g semi martingale 1 g gt t 0 as a simple motivating example we can consider the model gt f x t t where t is a white noise process this particular model for the noisy gradient process can be interpreted as consisting of observing f x t plus an inde pendent source of noise this concrete example will be useful to keep in mind to make sense of the results which we present over the course of the paper to make the concept of information restriction mathematically rigorous we restrict ourselves only to optimizers x which are measurable with respect to the information generated by the noisy gradient process g to do this we first define the global filtration g as gt gu u 0 t f as the sigma algebra generated by the paths of g as well as the realizations of the loss surface f the filtration gt is defined so that it contains the complete set of information generating the optimization problem until time t 1 a c dl g continue droite limite gauche process is a continuous time process that is almost surely right continuous with finite left limit at each point t a semi martingale is the sum of a process of finite variation and a local martingale for more information on continuous time stochastic processes and these definitions see the canonical text jacod and shiryaev 2013 3 next we define the coarser filtration ft gu u 0 t gt generated strictly by the paths of the noisy gradient process this filtration represents the total set of information available to the opti mizer up until time t this allows us to formally restrict the flow of information to the algorithm by restricting ourselves to optimizers which are adapted to ft more precisely we say that the optimizer s control is admissible if a t t 0 is f adapted e t 0 t 2 f x t 2 dt 3 the set of optimizers generated by a can be interpreted as the set of optimizers which may only use the source of noisy gradients which have bounded expected travel distance and have square integrable gradients over their path 3 the optimizer s variational problem having defined the set of admissible optimization algorithms we set out to select those which are optimal in an appropriate sense we proceed similarly to wibisono et al 2016 by proposing an objective functional which measures the performance of the optimizer over a finite time period the motivation for the optimizer s performance metric comes from a physical interpretation of the optimization process we can think of our optimization process as a particle traveling through a potential field define by the target loss function f as the particle travels through the potential field it may either gain or lose momentum depending on its location and velocity which will in turn affect the particle s trajectory naturally we may seek to find the path of a particle which reaches the optimum of the loss function while minimizing the total amount of kinetic and potential energy that is spent we therefore turn to the lagrangian interpretation of classical mechanics which provides a framework for obtaining solutions to this problem over the remainder of this section we lay out the lagrangian formalism for the optimization problem we defined in section 2 to define a notion of energy in the optimization process we provide a measure of distance in the parameter space we use the bregman divergence as the measure of distance within our parameter space which can embed additional information about the geometry of the optimization problem the bregman divergence dh is defined as dh y x h y h x h x y x 4 where h rd r is a strictly convex function satisfying h c 2 we assume here that the gradients of h are l lipschitz smooth for a fixed constant l 0 the choice of h determines the way we measure distance and is typically chosen so that it mimics features of the loss function f in particular this quantity plays a central role in mirror descent and non linear sub gradient algorithms for more information on this connection and on bregman divergence see nemirovsky and yudin 1983 and beck and teboulle 2003 we define the total energy in our problem as the kinetic energy accumulated through the movement of the optimizer and the potential energy generated by the loss function f under the assumption that f almost surely admits a global minimum x argminx rd f x we may represent the total energy via the bregman lagrangian as l t x e t e t dh x e t x kinetic energy e t f x f x potential energy 5 for fixed inputs t x and where we assume that r r are deterministic and satisfy c 1 the functions can be interpreted as hyperparameters which tune the energy present at any state of the optimization process an important property to note is that the lagrangian is itself a random variable due to the randomness introduced by the latent loss function f the objective is then to find an optimizer within the admissible set a which can get close to the minimum x minx rd f x while simultaneously minimizing the energy cost over a finite time period 0 t the approach taken in classical mechanics and in wibisono et al 2016 fixes the endpoint of the optimizer at x since we assume that the function f is not directly visible to our optimizer it is not possible to add a constraint of this type that will hold almost surely instead we introduce a soft constraint which penalizes the algorithm s endpoint in proportion to its distance to 4 the global minimum f xt f x as such we define the expected action functional j a r as j e t 0 l t x t t dt total path energy e t f x t f x soft end point constraint 6 where t c 1 is assumed to be an additional model hyperparameter which controls the strength of the soft constraint with this definition in place the objective will be to select amongst admissible optimizers for those which minimize the expected action hence we seek optimizers which solve the stochastic varia tional problem arg min a j 7 remark 1 note that the variational problem 7 is identical to the one with lagrangian l t x e t e t dh x e t x e t f x 8 and terminal penalty e t f x t since they differ by constants independent of because of this the results presented in section 4 also hold the case where x and f x do not exist or are infinite 4 critical points of the expected action functional in order to solve the variational problem 7 we make use techniques from the calculus of variations and infinite dimensional convex analysis to provide optimality conditions for the variational prob lem 7 to address issues of information restriction we rely on the stochastic control techniques developed by casgrain and jaimungal 2018 a b c the approach we take relies on the fact that a necessary condition for the optimality of a g teaux differentiable functional j is that its g teaux derivative vanishes in all directions computing the g teaux derivative of j we find an equivalence between the g teaux derivative vanishing and a system of forward backward stochastic differential equations fbsdes yielding a generalization of the euler lagrange equations to the context of our optimization problem the precise result is stated in theorem 4 1 below theorem 4 1 stochastic euler lagrange equation a control a is a critical point of j if and only if l m is a solution to the system of fbsdes d l t e l x t ft dt dmt t t l t e t e f xt ft 9 where we define the processes l x t e t t h x t e t t h x t e t 2 h x t t e t f x t 10 l t e t h x t e t t h x t 11 and where the process m mt t 0 t is an f adapted martingale as a consequence if the solution to this fbsde is unique then it is the unique critical point of the functional j up to null sets proof see appendix c theorem 4 1 presents an analogue of the euler lagrange equation with free terminal boundary rather than obtaining an ode as in the classical result we obtain an fbsde 2 with backwards 2 for a background on fbsdes we point readers to carmona 2016 ma et al 1999 pardoux and tang 1999 at a high level the solution to an fbsde of the form 9 consists of a pair of processes l m which simultaneously satisfy the dynamics and the boundary condition of 9 intuitively the martingale part of the solution can be interpreted as a random process which guides l x t towards the boundary condition at time t 5 process l t and forward state processes e l x t ft t 0 u du and x t we can also in terpret the dynamics of equation 9 as being the filtered optimal dynamics of wibisono et al 2016 equation 2 3 e l x t ft plus the increments of data dependent martingale mt with mechan ics similar to that of the innovations process of filtering theory this martingale term should not be interpreted as a source of noise but as an explicit function of the data as is evident from its explicit form mt e t 0 l x u du e t f xt ft 12 a feature of equation 9 is that optimality relies on the projection of l x t onto ft thus the optimization algorithm makes use of past noisy gradient observations in order to make local gradient predictions local gradient predictions are updated using a bayesian mechanism where the prior model for f is conditioned with the noisy gradient information contained in ft this demonstrates that the solution depends only on the gradients of f along the path of xt and no higher order properties 4 1 expected rates of convergence of the continuous algorithm using the dynamics 9 we obtain a bound on the rate of convergence of the continuous optimization algorithm that is analogous to wibisono et al 2016 theorem 2 1 we introduce the lyapunov energy functional et dh x x t e t t e t f x t f x h x e t x e t t 13 where we define x to be a global minimum of f under additional model assumptions and by showing that this quantity is a super martingale with respect to the filtration f we obtain an upper bound for the expected rate of convergence from xt towards the minimum theorem 4 2 convergence rate assume that the function f is almost surely convex and that the scaling conditions t e t and t e t hold moreover assume that in addition to h having l lipschitz smooth gradients h is also strongly convex with 0 define x argminx rd f x to be a global minimum of f if x exists almost surely the optimizer defined by fbsde 9 satisfies e f xt f x o e t max 1 e e t m t 14 where e t m t represents the quadratic variation of the process e t mt where m is the martin gale part of the solution defined in theorem 4 1 proof see appendix d we may interpret the term e e t m t as a penalty on the rate of convergence which scales with the amount of noise present in our gradient observations to see this note that if there is no noise in our gradient observations we obtain that ft gt and hence mt 0 which recovers the exact deterministic dynamics of wibisono et al 2016 and the optimal convergence rate o e t if the noise in our gradient estimates is large we can expect e e m t to grow at quickly and to coun teract the shrinking effects of e t thus in the case of a convex objective function f any presence of gradient noise will proportionally hurt rate of convergence to an optimum we also point out that there will be a nontrivial dependence of e e m t on all model hyperparameters the specific definition of the random variable f and the model for the noisy gradient stream gt t 0 remark 2 we do not assume that the conditions of theorem 4 2 carry throughout the remainder of the paper in particular sections 5 study models which may not guarantee almost sure convexity of the latent loss function 5 recovering discrete optimization algorithms in this section we use the optimality equations of theorem 4 1 to produce discrete stochastic opti mization algorithms the procedure we take is as follows we first define a model for the processes f xt gt t 0 t second we solve the optimality fbsde 9 in closed form or approximate the solution via the first order singular perturbation fosp technique as described in appendix a lastly we discretize the solutions with a simple forward euler scheme in order to recover discrete algorithms 6 over the course of sections 5 1 and 5 2 we show that various simple models for f xt gt t 0 t and different specifications of h produce many well known stochastic optimization algorithms these establish the conditions in the context of the variational problem of section 2 under which each of these algorithms are optimal as a consequence this allows us to understand the prior as sumptions which these algorithms make on the gradients of the objective function they are trying to minimize and the way noise is introduced in the sampling of stochastic gradients gt t 0 5 1 stochastic gradient descent and stochastic mirror descent here we propose a gaussian model on gradients which loosely represents the behavior of mini batch stochastic gradient descent with a training set of size n and mini batches of size m by specifying a martingale model for f xt we recover the stochastic gradient descent and stochastic mirror descent algorithms as solutions to the variational problem described in section 2 let us assume that f xt w f t where 0 and w f t t 0 is a brownian motion next assume that the noisy gradients samples obtained from mini batches over the course of the optimization evolve according to the model gt w f t w e t where n m m and w e is an independent copy of w f t here we choose so thatv gt n m v f xt o m 1 which allows the variance to scale in m and n as it does with mini batches using symmetry we obtain the trivial solution to the gradient filter e f xt ft 1 2 1 gt implying that the best estimate of the gradient at the point xt will be the most recent mini batch sample observed re scaled by a constant depending on n and m using this expression for the filter we obtain the following result proposition 5 1 the fosp approximation to the solution of the optimality equations 9 can be expressed as dxt e t h h xt t 1 2 1 gt x t dt 15 where h is the convex dual of h and where t e t 0 t 0 e u u u du is a deterministic learning rate with 0 e t t 0 e u u u du when h has the form h x x mx for a symmet ric positive definite matrix m the fosp approximation is exact and 15 is the exact solution to the optimality fbsde 9 the martingale portion of the solution to 9 can be expressed as mt m 0 1 2 1 t 0 e u u u dgu proof see appendix e 1 to obtain a discrete optimization algorithm from the result of 5 1 we employ a forward euler dis cretization of the ode 15 on the finite mesh t t 0 0 tk 1 tk e tk k n this discretization results in the update rule xtk 1 h h xtk tk gtk 16 corresponding exactly to mirror descent e g see beck and teboulle 2003 using the noisy mini batch gradients gt and a time varying learning rate tk moreover setting h x 1 2 x 2 we recover the update rule xtk 1 xtk tk gtk exactly corresponding to the mini batch sgd with a time dependent learning rate this derivation demonstrates that the solution to the variational problem described in section 2 under the assumption of a gaussian model for the evolution of gradients recovers mirror descent and sgd in particular the martingale gradient model proposed in this section can be roughly interpreted as assuming that gradients behave as random walks over the path of the optimizer moreover the optimal gradient filter e f xt ft 1 2 1 gt shows that for the algorithm to be optimal mini batch gradients should be re scaled in proportion to 1 2 1 m n 5 2 kalman gradient descent and momentum methods using a linear state space model for gradients we can recover both the kalman gradient descent algorithm of vuckovic 2018 and momentum based optimization methods of polyak 1964 we assume that each component of f xt i f xt d i 1 is modeled independently as a linear dif fusive process specifically we assume that there exist processes yi yi t t 0 so that for each i i f xt b yi t where yi t r d is the solution to the linear sde dyi t ayi tdt ldwi t in particular we the notation y i j t to refer to element i j of y r d d and use the notation 7 y j t y i j t d i 1 we assume here that a l r d d are positive definite matrices and each of the wi wi t t 0 are independent d dimensional brownian motions next we assume that we may write each element of a noisy gradient process as gi t b yi t i t where 0 and where i i t t 0 are independent white noise processes noting that e i f xt h ft b e ahyi t we find that this model implicitly assumes that gradients are ex pected decrease in exponentially in magnitude as a function of time at a rate determined by the eigenvalues of the matrix a the parameters and l can be interpreted as controlling the scale of the noise within the observation and signal processes using this model we obtain that the filter can be expressed as e i f xt ft b y i t where y i t e yi t ft the process y i t is expressed as the solution to the kalman bucy 3 filtering equations dy i t ay i t dt 1 p t bdb i t p ap t p t a 2 p tbb p t ll 17 with the initial conditions y i 0 0 and p 0 e yi 0 y i 0 and where we define innovations process db i t 1 gi t b y i t dt with the property that each b i is an independent f adapted brownian motion inserting the linear state space model and its filter into the optimality equations 9 we obtain the following result proposition 5 2 state space model solution to the fosp assume that the gradient state space model described above holds the fosp approximation to the solution of the optimality equa tions 9 can be expressed as dxt e t h h xt d j 1 j t y j t x t dt 18 where t e t b e at 0 t 0 e u u ub e a t u du rd is a deterministic learning rate where ea represents the matrix exponential and where 0 e t eat t 0 e u u u eau du can be chosen to have arbitrarily large eigenvalues by scaling t the martingale portion of the solution of 9 can be expressed as mt m 0 1 t 0 e u u ub e a t u p ubdb u proof see appendix e 2 5 2 1 kalman gradient descent in order to recover kalman gradient descent we discretize the processes x t and y over the finite mesh t defined in equation 18 applying a forward euler maruyama discretization of 18 and the filtering equations 17 we obtain the discrete dynamics yi tk 1 i e tk a yi tk le t wi k gi tk b yi tk e t i k 19 where each of the i k and wi k are standard gaussian random variables of appropriate size the filter y i k e ytk gtk k k 1 for the discrete equations can be written as the solution to the discrete kalman filtering equations provided in appendix b discretizing the process x over t with the forward euler scheme we obtain discrete dynamics for the optimizer in terms of the kalman filter y as xtk 1 h h xtk d j 1 j tk y j k 20 yielding a generalized version of kalman gradient descent of vuckovic 2018 with d states for each gradient element setting h x 1 2 x 2 d 1 and b 1 recovers the original kalman gradient descent algorithm with a time varying learning rate just as in section 5 1 we interpret each gtk as being a mini batch gradient as with equation 2 the algorithm 20 computes a kalman filter from these noisy mini batch observations and uses it to update the optimizer s position 3 for information on continuous time filtering and the kalman bucy filter we refer the reader to the text of bensoussan 2004 or the lecture notes of van handel 2007 8 5 2 2 momentum and generalized momentum methods by considering the asymptotic behavior of the kalman gradient descent method described in sec tion 5 2 1 we recover a generalized version of momentum gradient descent methods which includes mirror descent behavior as well as multiple momentum states let us assume that t 0 remains constant in time then using the asymptotic update rule for the kalman filter as shown in proposi tion b 2 and equation 20 we obtain the update rule xtk 1 h h xtk d j 1 j tk y j k y i k a k b a y i k k gi k 21 where a i e 0 a and where k r d is defined in the statement of the proposition b 2 this yields a generalized momentum update rule where we keep track of d momentum states with y i j k d j 1 and update its position using a linear update rule this algorithm can be seen as be ing most similar to the aggregated momentum technique of lucas et al 2018 which also keeps track of multiple momentum states which decay at different rates under the special case where d 1 b 1 and h 1 2 x 2 we recover the exact momentum algorithm update rule of polyak 1964 as xtk 1 xtk tk y k y i k p 1 y k p 2 gtk 22 where we have a scalar learning rate tk where p 1 a k b a p 2 k are positive scalars and where gtk are mini batch draws from the gradient as in equation 2 the recovery of the momentum algorithm of polyak 1964 has some interesting consequences since p 1 and p 2 are functions of the model parameters a and 0 we obtain a direct relationship between the optimal choice for the momentum model parameters the assumed scale of gradient noise l 0 and the assumed expected rate of decay of gradients as given by e at this result gives insight as to how momentum parameters should be chosen in terms of their prior beliefs on the optimization problem 6 discussion and future research directions over the course of the paper we present a variational framework on optimizers which interprets the task of stochastic optimization as an inference problem on a latent surface that we wish to optimize by solving a variational problem over continuous optimizers with asymmetric information we find that optimal algorithms should satisfy a system of fbsdes projected onto the filtration f generated by the noisy observations of the latent process by solving these fbsdes and obtaining continuous time optimizers we find a direct relationship between the measure assigned to the latent surface and its relationship to how data is observed in particular assigning simple prior models to the pair of processes f xt gt t 0 t recovers a number of well known and widely used optimization algorithms the fact that this framework can naturally recover these algorithms begs further study in particular it is still an open question whether it is possible to recover other stochastic algorithms via this framework particularly those with second order scaling adjustments such as adam or adagrad from a more technical perspective the intent is to further explore properties of the optimization model presented here and the form of the algorithms it suggests in particular the optimality fb sde 9 is nonlinear high dimensional and intractable in general making it difficult to use existing fbsde approximation techniques so new tools may need to be developed to understand the full extent of its behavior lastly numerical work on the algorithms generated by this framework can provide some insights as to which prior gradient models work well when discretized the extension of simplectic and quasi simplectic stochastic integrators applied to the bsdes and sdes that appear in this paper also has the potential for interesting future work references laurence aitchison a unified theory of adaptive stochastic gradient descent as bayesian filtering arxiv preprint arxiv 1807 07540 2018 9 amir beck and marc teboulle mirror descent and nonlinear projected subgradient methods for convex optimization operations research letters 31 3 167 175 2003 alain bensoussan stochastic control of partially observable systems cambridge university press 2004 ren carmona lectures on bsdes stochastic control and stochastic differential games with finan cial applications volume 1 siam 2016 philippe casgrain and sebastian jaimungal mean field games with partial information for algorith mic trading arxiv preprint arxiv 1803 04094 2018 a philippe casgrain and sebastian jaimungal mean field games with differing beliefs for algorithmic trading arxiv preprint arxiv 1810 06101 2018 b philippe casgrain and sebastian jaimungal trading algorithms with learning in latent alpha models arxiv preprint arxiv 1806 04472 2018 c nicolo cesa bianchi alex conconi and claudio gentile on the generalization ability of on line learning algorithms ieee transactions on information theory 50 9 2050 2057 2004 andr belotto da silva and maxime gazeau a general system of differential equations to model first order adaptive algorithms arxiv preprint arxiv 1810 13108 2018 john duchi elad hazan and yoram singer adaptive subgradient methods for online learning and stochastic optimization journal of machine learning research 12 jul 2121 2159 2011 vineet gupta tomer koren and yoram singer a unified approach to adaptive regularization in online and stochastic optimization arxiv preprint arxiv 1706 06569 2017 jean jacod and albert shiryaev limit theorems for stochastic processes volume 288 springer science business media 2013 svetlana jankovic miljana jovanovic and jasmina djordjevic perturbed backward stochastic dif ferential equations mathematical and computer modelling 55 5 6 1734 1745 2012 diederik p kingma and jimmy ba adam a method for stochastic optimization arxiv preprint arxiv 1412 6980 2014 walid krichene and peter l bartlett acceleration and averaging in stochastic descent dynamics in advances in neural information processing systems pages 6796 6806 2017 walid krichene alexandre bayen and peter l bartlett accelerated mirror descent in continuous and discrete time in advances in neural information processing systems pages 2845 2853 2015 james lucas shengyang sun richard zemel and roger grosse aggregated momentum stability through passive damping arxiv preprint arxiv 1804 00325 2018 jin ma j m morel and jiongmin yong forward backward stochastic differential equations and their applications number 1702 springer science business media 1999 panayotis mertikopoulos and mathias staudigl on the convergence of gradient like flows with noisy gradient input siam journal on optimization 28 1 163 197 2018 arkadii semenovich nemirovsky and david borisovich yudin problem complexity and method efficiency in optimization 1983 yu nesterov a method of solving a convex programming problem with convergence rate o 1 k 2 in sov math dokl volume 27 etienne pardoux and shanjian tang forward backward stochastic differential equations and quasi linear parabolic pdes probability theory and related fields 114 2 123 150 1999 boris t polyak some methods of speeding up the convergence of iteration methods ussr compu tational mathematics and mathematical physics 4 5 1 17 1964 maxim raginsky and jake bouvrie continuous time stochastic mirror descent on a network vari ance reduction consensus convergence in 2012 ieee 51 st ieee conference on decision and control cdc pages 6793 6800 ieee 2012 herbert robbins and sutton monro a stochastic approximation method the annals of mathemati cal statistics pages 400 407 1951 sebastian ruder an overview of gradient descent optimization algorithms arxiv preprint arxiv 1609 04747 2016 10 weijie su stephen boyd and emmanuel candes a differential equation for modeling nesterov s accelerated gradient method theory and insights in advances in neural information processing systems pages 2510 2518 2014 ramon van handel stochastic calculus filtering and stochastic control course notes url http www princeton edu rvan acm 217 acm 217 pdf 2007 james vuckovic kalman gradient descent adaptive variance reduction in stochastic optimization arxiv preprint arxiv 1810 12273 2018 jean walrand and antonis dimakis random processes in systems lecture notes department of electrical engineering and computer sciences university of california berkeley ca 94720 august 2006 andre wibisono ashia c wilson and michael i jordan a variational perspective on accelerated methods in optimization proceedings of the national academy of sciences 113 47 e 7351 e 7358 2016 ashia c wilson benjamin recht and michael i jordan a lyapunov analysis of momentum methods in optimization arxiv preprint arxiv 1611 02635 2016 pan xu tianhao wang and quanquan gu accelerated stochastic mirror descent from continuous time dynamics to discrete time algorithms in international conference on artificial intelligence and statistics pages 1087 1096 2018 a pan xu tianhao wang and quanquan gu continuous and discrete time accelerated stochastic mirror descent for strongly convex functions in international conference on machine learning pages 5488 5497 2018 b 11 a obtaining solutions to the optimality fbsde a 1 a momentum based representation of the optimizer dynamics using a simple change of variables we may represent the dynamics of the fbsde 9 in a simpler fashion which will aid us in obtaining solutions to this system of equations let us define the momentum process p pt t 0 t as pt l t e t h x t e t h x t 23 noting that since h is convex we have the property that h x h 1 x we may use equation 23 to write in terms of the momentum process as e t h h xt e t pt xt 24 the introduction of this process allows us to represent the solution to the optimality fbsde 9 and by exten sion the optimizer in a much more tractable way re writing 9 in terms of pt we find that d pt e t t te f x t ft e t 2 h xt t e t pt dt dmt pt e t e f x t ft 25 where the dynamics of the forward process x can be expressed as dx t e t h h x t e t pt x t dt 26 this particular change of variables corresponds exactly to the hamiltonian representation of the optimizer s dynamics which we show in appendix a 3 writing out the explicit solution to the fbsde 25 we obtain a representation for the optimizer s dynamics as pt e t t e u e u u f x u 2 h xu u e u u pu du e t f x t ft 27 showing that optimizer s momentum can be represented as a time weighted average of the expected future gradients over the remainder of the optimization and the term e t 2 h xt t e t pt where the weights are determined by the choice of hyperparameters and noting that 2 h xt t e t t pt 2 h xt t h xt e t t h xt e t 28 we find that the additional correction term in 27 can be interpreted as the remainder in the first order taylor expansion of the term h xt e t the representation 27 demonstrates optimizer does not only depend on the instantaneous value of gradients at the point x t rather we find that the algorithm s behaviour depends on the expected value of all future gradients that will be encountered over the remainder of the optimization process projected onto the set of ac cumulated gradient information ft this is in stark contrast to most known stochastic optimization algorithms which only make explicit use of local gradient information in order to bring the optimizer towards an optimum a 2 first order singular perturbation approximation when h does not take the quadratic form h x 1 2 x mx for some positive definite matrix m the nonlinear dynamics of the fbsde 9 or in the equivalent momentum form 25 make it difficult to provive a solution for general h more precisely the taylor expansion term 28 constitutes the main obstacle in obtaining solutions in general in cases where the scaling parameter t is sufficiently large we can assume that the taylor expansion remainder term of equation 28 will become negligibly small hence we may approximate the optimality dynamics of the fbsde 25 by setting this term to zero this can be interpreted as the first order term in a singular perturbation expansion of the solution to the momentum fbsde 25 under the assumption that the taylor remainder term vanishes we obtain the approximation p 0 p 0 t 0 t for the momentum which we present in the following proposition proposition a 1 first order singular perturbation fosp the linear fbsde d p 0 t e t t t e f xt ft dt dm 0 t p 0 t e t e f x t ft 29 12 admits a solution that can be expressed as p 0 t e t t e u u u f xu du e t f x t ft 30 provided that e t 0 e u u u f xu du proof noting that the remainder term in the expression 28 vanishes we get that p 0 t e t t e u u u f xu du e t f x t fu 31 under the assumption that are continuous over 0 t and that e f x 2 the right part of 31 is bounded now note that the integral on the left side of 31 is upper bounded for all t by the integral provided in the integrability condition of proposition a 1 and therefore this condition is a sufficient condition for the expression 31 to be finite and well defined although a general model independent bound for the accuracy of such approximations is beyond the scope of this paper it can still serve as a reasonable and computationally cheap alternative to attempting to solve the original problem dynamics directly with a bsde numerical scheme for more information on singular perturbation methods in the context of fbsdes see jankovic et al 2012 a 3 hamiltonian representation of the optimizer dynamics just as in hamiltonian classical mechanics it is possible to express the optimality fbsde of theorem 4 1 with hamiltonian equations of motion we define the hamiltonian h as the legendre dual of l at which can be written as h t x p p l t x 32 where p l x using the identity dh x y dh h x h y where h is the legendre dual of h and inverting the expression for l x in terms p we may compute equation 32 as 4 h t x p e t t dh h x e t p h x e t t f xt 33 using this definition of h and using the fbsde 9 we obtain the following equivalent representation for the dynamics of the optimizer using the simple substitution pt l x t and noting from equations 10 and 11 that pt e t h xt e t t h xt 34 a straightforward computation applied to the definition of h shows that the dynamics of the optimality fb sde 9 admit the alternate hamiltonian representation of the optimizer dynamics dxt h p t dt d pt e h x t ft dt dmt 35 along with the boundary condition pt 0 b the discrete kalman filter here we present the reader to the kalman filtering equations used in section 5 2 consider the model presented in equations 19 yi tk 1 a kyi tk l kwi k gi tk b yi tk e t i k 36 where we use the notation a k i e tk a and l k le t and where wi k and i k are all independent standard gaussian random variables we provide the kalman filtering equations for this model in the following proposition 4 see wibisono et al 2016 appendix b 4 for the full details of the computation 13 proposition b 1 walrand and dimakis 2006 theorem 10 2 let y i k e ytk gtk k k 1 then y i k satisfies the recursive equation y i k a k y i k kk gi k b a ky i k 37 where the matrices kk are obtained via the independent recursive equations pk k 1 a kpk 1 k 1 a k l k l k 38 sk 2 b pk k 1 b 39 kk pk k 1 bs 1 k 40 pk k i kkb pk k 1 41 for more information on the discrete kalman filter its derivation and for asymptotic properties we refer the reader to the lecture notes walrand and dimakis 2006 next we provide a result on the asumptotic properties of the kalman filter in the proposition that follows proposition b 2 walrand and dimakis 2006 theorem 11 2 assume that tk t 0 is constant so that a k a and l k l become constant and assume that there exists a positive definite solution k r d d to the algebraic matrix equation k a k a l l 42 then we may write the asymptotic dynamics of the filter y i as y i k a y i k k gi k b a y i k 43 where k is the solution to the system of algebraic matrix equations k i rc s r sb b sb 2 1 s a k a l l 44 for more information on the kalman filter its derivation and theoretical properties see walrand and dimakis 2006 14 c proofs relating to theorem 4 1 before going forward with the main part of the proof we first present a lemma for the computation of the g teaux derivative of j lemma c 1 the functional j is everywhere g teaux differentiable in a the g teaux at a point a in the direction for a takes the form dj e t 0 t l t x t t e t t l u x u u x du e t f x t ft dt 45 proof if we assume that the conditions of leibniz rule hold we may compute the g teax derivative as j e t 0 l t x t t t dt e t f x t f x e t 0 l t x t t t dt e t f x t e t 0 l t x t t x t 0 u du l t x t t t dt t 0 u du f x t 46 where we have l t x x e t t h x e t h x e t 2 h x e t f x 47 l t x e t h x e t h x 48 note here that the derivative in f is path wise for every fixed realization of the function f since f c 1 we have that f is also well defined for every realization of f to ensure that this computation is valid and that the conditions of the leibniz rule are met due to the continuity of 46 in is sufficient for us to show that the integrals in equation 46 are bounded for any and first note that by the young and jensen inequalities e t 0 u du f x t 1 2 e t 0 u 2 du f x t 2 49 where the boundedness holds from the fact that a and that e f x 2 for all x rd next we focus on the left part of equation 46 by the cauchy schwarz and young inequalities we have l t x t t x t 0 u du l t x t t t l t x t t x t 0 u du l t x t t t 50 1 2 t 0 u du 2 l t x t t x 2 l t x t t 2 t 2 51 using the l lipschitz property of the gradients of h we can also bound the partial derivatives of the lagrangian with the triangle inequality as l t x t t x e t t h x e t h x e t 2 h x e t t t f x l e t t e t e t t t f x c 0 f x l t x e t h x e t h x e t l c 0 where c 0 supt 0 t e t t e t e t t t is bounded by the assumption that are continuous in 0 t 15 using the above result and applying young s inequality to the previous result we can upper bound equa tion 51 as 51 32 1 c 1 t 0 u 2 du t 2 t 2 f xt 2 52 64 1 c 1 t 0 u 2 du t 0 u 2 du t 2 t 2 f xt 2 53 where the number 32 is chosen to be much larger than what is strictly necessary by young s inequality notice here that by the definition of a this forms an integrable upper bound to the left integral of equation 46 validating our use of leibniz s rule and showing that j is indeed g teaux integrable now that integrability concerns have been dealt with we can proceed with the computation of the g teaux derivative by applying integration by parts to the left side of equation 54 and moving the right hand side into the integral we obtain j e t 0 t l t x t t t t l u x u u x du e t f x t dt using the tower property and fubini s theorem on the right we get dj e t 0 t l t x t t e t t l u x u u x du e t f x t ft dt 54 as desired c 1 proof of theorem 4 1 using the representation of the g teux derivative of j brought forth by lemma c 1 we may proceed with the proof of theorem 4 1 proof of theorem 4 1 the goal is to show that the bsde 9 is a necessary and sufficient condition for to be a critical point of j for any g teaux differentiable function j a necessary and sufficient condition for a point a to be a critical point is that its g teaux derivative vanished in any valid direction lemma c 1 shows that the g teaux derivative takes the form of equation 45 therefore all that remains is to show that the fbsde 9 is a necessary and sufficient condition for equation 45 to vanish sufficiency we will show that equation 45 vanishes when the fbsde 9 holds assume that there exists a solution to the fbsde 9 satisfying a we may then express the solution to the fbsde explicitly as l t e t t l x u du e t f x t ft inserting this into the right side of 45 we find that dj vanishes for all a demonstrating sufficiency necessity conversely let us assume that dj 0 for all a and for some a for which the fbsde 9 is not satisfied we will show by contradiction that this statement cannot hold by choosing a direction in which the g teax derivative does not vanish consider the choice t t l t x t t e t t l u x u u x du e t f x t ft 55 for some sufficiently small 0 we will first show that a for some 0 first note that clearly must be ft adapted and we have 0 t moreover note that since a we have that e t 0 t 2 f x 2 dt that 0 notice that by the continuity of f and the definition of x the expression e t 0 t 2 f x 2 dt 56 is continuous in since 56 is bounded for 0 by continuity there exists some 0 for which 56 is bounded and by extension where a for this same value of inserting 55 into the g teaux derivative 45 we get that dj e t 0 l t x t t e t t l u x u u x du e t f x t ft 2 dt 57 which is strictly positive unless the fbsde 9 is satisfied thus forming a contradiction and demonstrating that the condition is necessary 16 d proof of theorem 4 2 proof the proof of this theorem is broken up into multiple parts the idea will be to first show that the energy functional e is a super martingale with respect to ft and then to use this property to bound the expected distance to the optimum lastly we bound a quadratic co variation term which appears within these equations to obtain the final result before delving into the proof we introduce standard notation for semi martingale calculus we use the noation dyt dy c t yt to indicate the increments of the continuous part y c of a process y and its discontinuities yt yt yt where we use the notation t to indicate the left limit of the process we use the notation y z t to represent the quadratic co variation of two processes y and z this quadratic variation term can be decomposed into d y z t d y z c t yt zt where y z c t represents the quadratic covariation between y c and zc and where yt zt represents the inner product of their discontinuities at t for more information on semi martingale calculus and the associated notation see jacod and shiryaev jacod and shiryaev 2013 sections 3 5 dynamics of the bregman divergence the idea will now be to show that the energy functional e defined in equation 13 is a super martingale with respect to the visible filtration ft using it s formula and it s product rule for c dl g semi martingales jacod and shiryaev 2013 theorem 4 57 as well as the short hand notation yt xt e t t we obtain ddh x yt h yt dy c t 1 2 d i j 1 2 h yt xi xi d yi yj c t h yt d h yt x yt h yt dyt d h y y t h yt yt 1 2 d i j 1 2 h yt xi xi d yi yj c t h yt d h yt x yt d i j 1 2 h yt xi xi d yi yj c t h yt yt h yt h yt yt d h yt x yt 1 2 d i j 1 2 h yt xi xi d yi yj c t h yt yt where from line 1 to 2 we use the identity d g y y t i j 2 g yt xi x j d yi y j c t g yt yt for any c 2 function g note that since h is convex 2 h must have positive eigenvalues and hence 1 2 d i j 1 2 h yt xi xi d yi y j c t 0 the convexity of h also implies that h x h y x y 0 and therefore we get h yt yt 0 the convexity of h also implies that h yt h yt yt 0 combining these observations we find that ddh x yt d h yt x yt d i j 1 2 h yt xi xi d yi y j c t h yt yt 58 d h yt x yt h y y t 59 super martingale property of e applying the scaling conditions to the optimality fbsde 9 we obtain the dynamics d h x t e t e t te f x t ft dt dm t 60 inserting this in to the dynamics of for the energy functional and applying the upper bound 59 we find that det d h yt x yt te t f xt f x dt e t f xt t dt 61 e t te f xt ft dt dmt x yt t e t f xt f x dt e t f xt t dt 62 d f x yt e t t e t f xt f x dt dm t 63 where we use the notation m t to represent the ft martingale defined as dm t e t t e f xt ft f xt dt dmt x yt 64 now note that due to the assumed convexity of f we have that d f x yt is almost surely non negative second by the scaling conditions e t t is positive hence the drift in equation 63 is almost surely negative and et is a super martingale using the super martingale property we find that e et e e 0 e dh x x 0 e 0 0 e 0 f x 0 f x c 0 where c 0 0 using the definition of e and using the fact that dh 0 if h is convex we obtain e te f xt f x e dh x xt e t t e t f xt f x c 0 e h y y t 65 17 upper bound on the quadratic co variation now we upper bound the quadratic co variation term appear ing on the right hand side of 65 using the further change of variable zt h yt and noting that by the assumed convexity of h that h x h 1 x we get h y y t z h z t assuming that h is strongly convex we get that h must have 1 lipschitz smooth gradients this implies that i the eigenvalues of 2 h must be bounded above by 1 ii from the cauchy schwarz inequality we have h x h y x y 1 x y 2 using these two observations and writing out the expression for z h z t we get z h z t d i j 1 2 h yt xi xi d yi y j c t h z zt 66 1 z t 67 moreover note that since zt h x t e t t and since h x t is a process of finite variation the opti mality dynamics 9 imply that z t e t m t e t m t inserting the quadratic co variation bound into equation 65 and using the super martingale property we obtain the final result e f xt f x e t c 0 1 2 e h x e t t e t c 0 1 2 e 2 te m t c 0 1 2 e t max 1 e 2 te m t o e t max 1 e t 2 te m t as desired e proofs of propositions 5 1 and proposition 5 2 both of the proofs contained in this sections are applications of the momentum representation of the optimizer dynamics and the fosp approximation to the solution of the optimality fbsde 9 e 1 proof of proposition 5 1 proof using proposition a 1 we find that the solution to the fosp takes the form p 0 t e t t e t t t f xu du e t f x t ft applying fubini s theorem and the martingale property of e f xu fu gu 1 2 we find that p 0 t e t t e u u u f xu du e t f x t ft t t e u u u e f x u ft du e t e f x t ft t t e u u u e f x t ft du e t e f x t ft gt 1 2 1 t t e u u u du e t inserting expression above into equation 24 and re arranging terms we obtain the desired result e 2 proof of proposition 5 2 proof using proposition a 1 we find that the solution to the fosp takes the form p 0 t e t t e t t t f xu du e t f x t ft 18 applying fubini s theorem and noting that e i f xt h yi t d j 1 b e ah j y j t we obtain p 0 t e t t e u u u f xu du e t f x t ft t t e u u u e f x u ft du e t e f x t ft t t e u u u d j 1 b e a u t j y j t du e t d j 1 b e a t t j y j t d j 1 t t e u u u b e a u t j du e t b e a t t j y j t inserting expression above into equation 24 and re arranging terms we obtain the desired result 19 1 introduction 1 1 related work 1 2 contribution 1 3 paper structure 2 a statistical model for stochastic optimization 3 the optimizer s variational problem 4 critical points of the expected action functional 4 1 expected rates of convergence of the continuous algorithm 5 recovering discrete optimization algorithms 5 1 stochastic gradient descent and stochastic mirror descent 5 2 kalman gradient descent and momentum methods 5 2 1 kalman gradient descent 5 2 2 momentum and generalized momentum methods 6 discussion and future research directions a obtaining solutions to the optimality fbsde a 1 a momentum based representation of the optimizer dynamics a 2 first order singular perturbation approximation a 3 hamiltonian representation of the optimizer dynamics b the discrete kalman filter c proofs relating to theorem 4 1 c 1 proof of theorem 4 1 d proof of theorem 4 2 e proofs of propositions 5 1 and proposition 5 2 e 1 proof of proposition 5 1 e 2 proof of proposition 5 2