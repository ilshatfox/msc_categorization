a walk in the statistical mechanical formulation of neural networks alternative routes to hebb prescription elena agliari 1 adriano barra 1 andrea galluzzi 2 daniele tantari 2 and flavia tavani 3 1 dipartimento di fisica sapienza universita di roma 2 dipartimento di matematica sapienza universita di roma 3 dipartimento sbai ingegneria sapienza universita di roma elena agliari adriano barra roma 1 infn it galluzzi tantari mat uniroma 1 it flavia tavani sbai uniroma 1 it keywords statistical mechanics spin glasses random graphs abstract neural networks are nowadays both powerful operational tools e g for pattern recognition data mining error correction codes and complex theoretical models on the focus of scientific investigation as for the research branch neural networks are handled and studied by psychologists neurobiologists engineers mathematicians and theoretical physicists in particular in theoretical physics the key instrument for the quantitative analysis of neural networks is statistical mechanics from this perspective here we first review attractor networks starting from ferromagnets and spin glass models we discuss the underlying philosophy and we recover the strand paved by hopfield amit gutfreund sompolinky one step forward we highlight the structural equiv alence between hopfield networks modeling retrieval and boltzmann machines modeling learning hence realizing a deep bridge linking two inseparable aspects of biological and robotic spontaneous cognition as a sideline in this walk we derive two alternative with respect to the original hebb proposal ways to recover the hebbian paradigm stemming from ferromagnets and from spin glasses respectively further as these notes are thought of for an engineering audience we highlight also the mappings between ferromagnets and operational amplifiers and between antiferromagnets and flip flops as neural networks built by op amp and flip flops are particular spin glasses and the latter are indeed combinations of ferromagnets and antiferromag nets hoping that such a bridge plays as a concrete prescription to capture the beauty of robotics from the statistical mechanical perspective 1 introduction neural networks are such a fascinating field of sci ence that its development is the result of contribu tions and efforts from an incredibly large variety of scientists ranging from engineers mainly involved in electronics and robotics hagan et al 1996 miller et al 1995 physicists mainly involved in statistical mechanics and stochastic processes amit 1992 hertz and palmer 1991 and mathematicians mainly working in logics and graph theory coolen et al 2005 saad 2009 to neuro biologists harris warrick 1992 rolls and treves 1998 and cog nitive psychologists martindale 1991 domhoff 2003 tracing the genesis and evolution of neural net works is very difficult probably due to the broad meaning they have acquired along the years 1 scien tists closer to the robotics branch often refer to the 1 seminal ideas regarding automation are already in the w mcculloch and w pitts model of perceptron mc culloch and pitts 1943 2 or the f rosenblatt ver sion rosenblatt 1958 while researchers closer to the neurobiology branch adopt d hebb s work as a starting point hebb 1940 on the other hand scien tists involved in statistical mechanics that joined the community in relatively recent times usually refer to the seminal paper by hopfield hopfield 1982 or to the celebrated work by amit gutfreund sompolinky amit 1992 where the statistical mechanics analysis of the hopfield model is effectively carried out whatever the reference framework at least 30 works of lee during the xiix century if not even back to descartes while more modern ideas regarding spontaneous cognition can be attributed to a turing turing 1950 and j von neumann von neumann 1951 or to the join efforts of m minsky and s papert minsky and papert 1987 just to cite a few 2 note that the first transistor crucial to switch from analogical to digital processing was developed only in 1948 millman and grabel 1987 ar x iv 1 40 7 53 00 v 1 co nd m at d is n n 2 0 ju l 20 14 years elapsed since neural networks entered in the theoretical physics research and much of the former results can now be re obtained or re framed in mod ern approaches and made much closer to the engi neering counterpart as we want to highlight in the present work in particular we show that toy mod els for paramagnetic ferromagnetic transition ellis 2005 are natural prototypes for the autonomous stor age retrieval of information patterns and play as op erational amplifiers in electronics then we move further analyzing the capabilities of glassy systems ensembles of ferromagnets and antiferromagnets in storing retrieving extensive numbers of patterns so to recover the hebb rule for learning hebb 1940 in two different ways the former guided by ferromag netic intuition the latter guided by glassy counter part both far from the original route contained in his milestone the organization of behavior finally we will give prescription to map these glassy systems in ensembles of amplifiers and inverters thus flip flops of the engineering counterpart so to offer a concrete bridge between the two communities of theoretical physicists working with complex systems and engi neers working with robotics and information process ing as these notes are intended for non theoretical physicists we believe that they can constitute a novel perspective on a by now classical theme and that they could hopefully excite curiosity toward statistical me chanics in nearest neighbors scientists like engineers whom these proceedings are addressed to 2 statistical mechanics microscopic dynamics obeying entropy maximization hereafter we summarize the fundamental steps that led theoretical physicists towards artificial intel ligence despite this parenthesis may look rather dis tant from neural network scenarios it actually allows us to outline and to historically justify the physicists perspective statistical mechanics aroused in the last decades of the xix century thanks to its founding fathers lud wig boltzmann james clarke maxwell and josiah willard gibbs kittel 2004 its solely scope at that time was to act as a theoretical ground of the already existing empirical thermodynamics so to rec oncile its noisy and irreversible behavior with a de terministic and time reversal microscopic dynamics while trying to get rid of statistical mechanics in just a few words is almost meaningless roughly speaking its functioning may be summarized via toy examples as follows let us consider a very simple system e g a perfect gas its molecules obey a newton like microscopic dynamics without friction as we are at the molecular level thus time reversal as dis sipative terms in differential equations capturing sys tem s evolution are coupled to odd derivatives and instead of focusing on each particular trajectory for characterizing the state of the system we define or der parameters e g the density in terms of micro scopic variables the particles belonging to the gas by averaging their evolution over suitably probabil ity measures and imposing on these averages energy minimization and entropy maximization it is possible to infer the macroscopic behavior in agreement with thermodynamics hence bringing together the micro scopic deterministic and time reversal mechanics with the macroscopic strong dictates stemmed by the sec ond principle i e arrow of time coded in the en tropy growth despite famous attacks to boltzmann theorem e g by zermelo or poincare castiglione et al 2012 statistical mechanics was immediately recognized as a deep and powerful bridge linking mi croscopic dynamics of a system s constituents with emergent macroscopic properties shown by the sys tem itself as exemplified by the equation of state for perfect gases obtained by considering an hamiltonian for a single particle accounting for the kinetic contri bution only kittel 2004 one step forward beyond the perfect gas van der waals and maxwell in their pioneering works fo cused on real gases reichl and prigogine 1980 where particle interactions were finally considered by introducing a non zero potential in the microscopic hamiltonian describing the system this extension implied fifty years of deep changes in the theoretical physics perspective in order to be able to face new classes of questions the remarkable reward lies in a theory of phase transitions where the focus is no longer on details regarding the system constituents but rather on the characteristics of their interactions indeed phase transitions namely abrupt changes in the macroscopic state of the whole system are not due to the particular system considered but are primarily due to the ability of its constituents to perceive inter actions over the thermal noise for instance when considering a system made of by a large number of water molecules whatever the level of resolution to describe the single molecule ranging from classical to quantum by properly varying the external tunable parameters e g the temperature 3 this system even 3 we chose the temperature here as an example of tun able parameter because in neural networks we will deal with white noise affecting the system analogously in tually changes its state from liquid to vapor or solid depending on parameter values of course the same applies generally to liquids the fact that the macroscopic behavior of a system may spontaneously show cooperative emergent prop erties actually hidden in its microscopic description and not directly deducible when looking at its compo nents alone was definitely appealing in neuroscience in fact in the 70 s neuronal dynamics along axons from dendrites to synapses was already rather clear see e g the celebrated book by tuckwell tuckwell 2005 and not too much intricate than circuits that may arise from basic human creativity remarkably simpler than expected and certainly trivial with re spect to overall cerebral functionalities like learning or computation thus the aptness of a thermodynamic formulation of neural interactions to reveal possible emergent capabilities was immediately pointed out despite the route was not clear yet interestingly a big step forward to this goal was prompted by problems stemmed from condensed mat ter in fact quickly theoretical physicists realized that the purely kinetic hamiltonian introduced for perfect gases or hamiltonian with mild potentials allowing for real gases is no longer suitable for solids where atoms do not move freely and the main energy contri butions are from potentials an ensemble of harmonic oscillators mimicking atomic oscillations of the nu clei around their rest positions was the first scenario for understanding condensed matter however as ex perimentally revealed by crystallography nuclei are arranged according to regular lattices hence motivat ing mathematicians in study periodical structures to help physicists in this modeling but merging statis tical mechanics with lattice theories resulted soon in practically intractable models 4 as a paradigmatic example let us consider the one dimensional ising model originally introduced to investigate magnetic properties of matter the generic out of n nucleus labeled as i is schematically rep condensed matter disorder is introduced by thermal noise namely temperature there is a deep similarity between them in stochastic processes prototype for white noise generators are random walkers whose continuous limits are gaussians namely just the solutions of the fourier equa tion for diffusion however the same celebrated equation holds for temperature spread too indeed the latter is related to the amount of exchanged heat by the system under con sideration necessary for entropy s growth reichl and pri gogine 1980 kac 1947 hence we have the first equiva lence white noise in neural networks mirrors thermal noise in structure of matter 4 for instance the famous ising model baxter 2007 dated 1920 and curiously invented by lenz whose prop erties are known in dimensions one and two is still waiting for a solution in three dimensions resented by a spin i which can assume only two values i 1 spin down and i 1 spin up nearest neighbor spins interact reciprocally through positive i e ferromagnetic interactions ji i 1 0 hence the hamiltonian of this system can be written as hn ni ji i 1 i i 1 h ni i where h tunes the external magnetic field and the minus sign in front of each term of the hamiltonian ensures that spins try to align with the external field and to get parallel each other in order to fulfill the minimum energy principle clearly this model can trivially be extended to higher dimensions however due to prohibitive difficulties in facing the topological constraint of considering near est neighbor interactions only soon shortcuts were properly implemented to turn around this path it is just due to an effective shortcut namely the so called mean field approximation that statistical mechan ics approached complex systems and in particular artificial intelligence 3 the route to complexity the role of mean field limitations as anticipated the mean field approximation al lows overcoming prohibitive technical difficulties ow ing to the underlying lattice structure this consists in extending the sum on nearest neighbor couples which are o n to include all possible couples in the system which are o n 2 properly rescaling the coupling j j n in order to keep thermodynami cal observables linearly extensive if we consider a ferromagnet built of by n ising spins i 1 with i 1 n we can then write hn j 1 n n n i j ji j i j 1 2 n n n i j i j 1 where in the last term we neglected the diagonal term i j as it is irrelevant for large n from a topolog ical perspective the mean field approximation equals to abandon the lattice structure in favor to a complete graph see fig 1 when the coupling matrix has only positive entries e g p ji j ji j j this model is named curie weiss model and acts as the sim plest microscopic hamiltonian able to describe the paramagnetic ferromagnetic transitions experienced by materials when temperature is properly lowered an external magnetic field h can be accounted for by adding in the hamiltonian an extra term h ni i according to the principle of minimum energy the two body interaction appearing in the hamilto nian in eq 1 tends to make spins parallel with each other and aligned with the external field if present sabato 3 maggio 14 figure 1 example of regular lattice left and complete graph right with n 20 nodes in the former only nearest neighbors are connected in such a way that the number of links scales linearly with n while in the latter each node is connected with all the remaining n 1 in such a way that the number of links scales quadratically with n however in the presence of noise i e if tempera ture t is strictly positive maximization of entropy must also be taken into account when the noise level is much higher than the average energy roughly if t j noise and entropy driven disorder prevail and spins are not able to feel reciprocally as a result they flip randomly and the system behaves as a para magnet conversely if noise is not too loud spins start to interact possibly giving rise to a phase tran sition as a result the system globally rearranges its structure orientating all the spins in the same direc tion which is the one selected by the external field if present thus we have a ferromagnet in the early 70 a scission occurred in the statis tical mechanics community on the one side pure physicists saw mean field approximation as a merely bound to bypass in order to have satisfactory pic tures of the structure of matter and they succeeded in working out iterative procedures to embed statis tical mechanics in quasi three dimensional reticula yielding to the renormalization group developed by kadanoff and wilson in america wilson 1971 and di castro and jona lasinio in europe di castro and jona lasinio 1969 this proliferative branch gave then rise to superconductivity superfluidity bean 1962 and many body problems in condensed matter bardeen et al 1957 conversely from the other side the mean field ap proximation acted as a breach in the wall of complex systems a thermodynamical investigation of phe nomena occurring on general structures lacking eu clidean metrics e g erdos renyi graphs bolloba s 1998 agliari et al 2008 small world graphs watts and strogatz 1998 agliari and barra 2011 diluted weighted graphs agliari et al 2012 was then pos sible in general as long as the summations run over all the indexes hence mean field is retained rather complex coupling patterns can be solved see e g the striking parisi picture of mean field glassy sys tems me zard et al 1987 and this paved the strand to complex system analysis by statistical mechanics whose investigation largely covers neural networks too 4 serial processing hereafter we discuss how to approach neural net works from models mimicking ferromagnetic transi tion in particular we study the curie weiss model and we show how it can store one pattern of infor mation and then we bridge its input output relation called self consistency with the transfer function of an operational amplifier then we notice that such a stored pattern has a very peculiar structure which is hardly natural but we will overcome this fake flaw by introducing a gauge variant known as mattis model this scenario can be looked at as a primordial neural network and we discuss its connection with bi ological neurons and operational amplifiers the suc cessive step consists in extending through elementary thoughts this picture in order to include and store sev eral patterns in this way we recover via the first al ternative route w r t the original one by hebb both the hebb rule for synaptic plasticity and as a corol lary the hopfield model for neural networks too that will be further analyzed in terms of flip flops and in formation storage 4 1 storing the first pattern curie weiss paradigm the statistical mechanical analysis of the curie weiss model cw can be summarized as follows start ing from a microscopic formulation of the system i e n spins labeled as i j their pairwise couplings ji j j and possibly an external field h we derive an explicit expression for its macroscopic free energy a the latter is the effective energy namely the difference between the internal energy u divided by the temperature t 1 and the entropy s namely a s u in fact s is the penalty to be paid to the second principle for using u at noise level we can therefore link macroscopic free energy with microscopic dynamics via the fundamental relation a lim n 1 n ln 2 n exp hn j h 2 where the sum is performed over the set of all 2 n possible spin configurations each weighted by the boltzmann factor exp hn j h that tests the likelihood of the related configuration from expres sion 2 we can derive the whole thermodynamics and in particular phase diagrams that is we are able to discern regions in the space of tunable parameters e g temperature noise level where the system be haves as a paramagnet or as a ferromagnet thermodynamical averages denoted with the symbol provide for a given observable the expected value namely the value to be compared with measures in an experiment for instance for the magnetization m ni 1 i n we have m m e hn j e hn j 3 when the system is noiseless zero tempera ture hence spins feel reciprocally without errors and the system behaves ferromagnetically m 1 while when 0 the system behaves completely ran dom infinite temperature thus interactions can not be felt and the system is a paramagnet m 0 in between a phase transition happens in the curie weiss model the magnetization works as order parameter its thermodynamical av erage is zero when the system is in a paramagnetic disordered state m 0 while it is different from zero in a ferromagnetic state where it can be either positive or negative depending on the sign of the external field dealing with order parameters al lows us to avoid managing an extensive number of variables i which is practically impossible and even more important it is not strictly necessary now an explicit expression for the free energy in terms of m can be obtained carrying out summations in eq 2 and taking the thermodynamic limit n as a ln 2 lncosh j m h j 2 m 2 4 in order to impose thermodynamical principles i e energy minimization and entropy maximization we need to find the extrema of this expression with re spect to m requesting m a 0 the resulting expression is called the self consistency and it reads as m a 0 m tanh j m h 5 this expression returns the average behavior of a spin in a magnetic field in order to see that a phase tran sition between paramagnetic and ferromagnetic states actually exists we can fix h 0 and pose j 1 for simplicity and expand the r h s of eq 5 to get m j 1 6 0 0 5 1 1 5 2 0 0 2 0 4 0 6 0 8 1 t m 60 10020 0 05 0 025 0 1 t tc 1 m figure 2 average magnetization m versus temperature t for a curie weiss model in the absence of field h 0 the critical temperature tc 1 separates a magnetized region m 0 only one branch shown from a non magnetized region m 0 the box zooms over the critical region notice the logarithmic scale and highlights the power law behavior m t tc where 1 2 is also referred to as critical exponent see also eq 6 data shown here are obtained via monte carlo simulations for a system of n 105 spins and compared with the theoretical curve solid line thus while the noise level is higher than one c 1 or t tc 1 the only solution is m 0 while as far as the noise is lowered below its criti cal threshold c two different from zero branches of solutions appear for the magnetization and the sys tem becomes a ferromagnet see fig 2 the branch effectively chosen by the system usually depends on the sign of the external field or boundary fluctuations m 0 for h 0 and vice versa for h 0 clearly the lowest energy minima correspond to the two configurations with all spins aligned either upwards i 1 i or downwards i 1 i these configurations being symmetric under spin flip i i therefore the thermodynamics of the curie weiss model is solved energy minimization tends to align the spins as the lowest energy states are the two ordered ones however entropy maximization tends to randomize the spins as the higher the en tropy the most disordered the states with half spins up and half spins down the interplay between the two principles is driven by the level of noise intro duced in the system and this is in turn ruled by the tunable parameter 1 t as coded in the definition of free energy a crucial bridge between condensed matter and neural network could now be sighted one could think at each spin as a basic neuron retaining only its ability to spike such that i 1 and i 1 represent firing and quiescence respectively and as sociate to each equilibrium configuration of this spin system a stored pattern of information the reward is that in this way the spontaneous i e thermodynam ical tendency of the network to relax on free energy minima can be related to the spontaneous retrieval of the stored pattern such that the cognitive capability emerges as a natural consequence of physical princi ples we well deepen this point along the whole paper 4 2 the curie weiss model and the saturable operational amplifier let us now tackle the problem by another perspec tive and highlight a structural mathematical similarity in the world of electronics the plan is to compare self consistencies in statistical mechanics and trans fer functions in electronics so to reach a unified de scription for these systems before proceeding we recall a few basic concepts the operational ampli fier namely a solid state integrated circuit transis tor uses feed back regulation to set its functions as sketched in fig 3 insets there are two signal in puts one positive received and one inverted thus negative received two voltage supplies vsat vsat where sat stands for saturable and an out put vout an ideal amplifier is the linear approxi mation of the saturable one technically the voltage at the input collectors is thought constant so that no cur rent flows inside the transistor and kirchoff rules ap ply straightforwardly keeping the symbols of fig 3 where rin stands for the input resistance while r f rep resents the feed back resistance i i 0 and as suming rin 1 without loss of generality as only the ratio r f rin matters the following transfer func tion is achieved vout gvin 1 r f vin 7 where g 1 r f is called gain therefore as far as 0 r f thus retro action is present the device is amplifying let us emphasize deep structural analogies with the curie weiss response to a magnetic field h first we notice that all these systems saturate whatever the magnitude of the external field for the cw model once all the spins become aligned increasing further h will no longer produce a change in the system analo gously once in the op amp reached vout vsat larger values of vin will not result in further amplification also notice that both the self consistency and the transfer function are two input output relations the input being the external field in the former and the input voltage in the latter the output being the mag netization in the former and the output voltage in the latter and once fixed 1 for simplicity expand ing m tanh j m h 1 j h we can compare 10 6 2 2 6 10 1 0 6 0 2 0 2 0 6 1 h m 10 5 0 5 10 1 0 5 0 0 5 1 vin v o u t vsat vsat figure 3 average magnetization m versus the external field h and response of a charging neuron solid black line compared with the transfer function of an operational am plifier red bullets tuckwell 2005 agliari et al 2013 in the inset we show a schematic representation of an oper ational amplifier upper and of an inverter lower term by term the two expression as vout 1 r f vin 8 m 1 j h 9 we see that r f plays as j and consistently if r f is absent the retroaction is lost in the op amp and the gain is no longer possible analogously if j 0 spins do not mutually interact and no feed back is allowed to drive the phase transition such a bridge is robust as operational amplifiers per form more than signal amplifying for instance they can perform as latches namely analog to digital con verters latches can be achieved within the curie weiss theory simply working in the low noise limit as the sigmoidal function m versus h see fig 3 of the self consistency approaches a heavyside hence while analogically varying the external field h as soon as it crosses the value h 0 say from nega tive values the magnetization jumps discontinuously from m 1 to m 1 hence coding for a digi talization of the input 4 3 the route from curie weiss to hopfield actually the hamiltonian 1 would encode for a rather poor model of neural network as it would ac count for only two stored patterns corresponding to the two possible minima that in turn would repre sent pathological network s behavior with all the neu rons contemporarily completely firing of completely silenced moreover these ordered patterns seen as information chains have the lowest possible entropy and for the shannon mcmillan theorem in the large n limit 5 they will never be observed this criticism can be easily overcome thanks to the mattis gauge namely a re definition of the spins via i 1 i i where 1 i 1 are random entries extracted with equal probability and kept fixed in the network in statistical mechanics these are called quenched variables to stress that they do not con tribute to thermalization a terminology reminiscent of metallurgy me zard et al 1987 fixing j 1 for simplicity the mattis hamiltonian reads as hmattisn 1 2 n n n i j 1 i 1 j i j h n i 1 i i 10 the mattis magnetization is defined as m 1 i 1 i i to inspect its lowest energy minima we perform a comparison with the cw model in terms of the stan dard magnetization the curie weiss model reads as hcwn n 2 m 2 hm and analogously we can write hmattisn in terms of mattis magnetization as hmattisn n 2 m 21 hm 1 it is then evident that in the low noise limit namely where collective prop erties may emerge as the minimum of free energy is achieved in the curie weiss model for m 1 the same holds in the mattis model for m 1 1 however this implies that now spins tend to align parallel or antiparallel to the vector 1 hence if the latter is say 1 1 1 1 1 1 1 in a model with n 6 the equilibrium configurations of the network will be 1 1 1 1 1 1 and 1 1 1 1 1 1 the latter due to the gauge symmetry i i enjoyed by the hamil tonian thus the network relaxes autonomously to a state where some of its neurons are firing while others are quiescent according to the stored pattern 1 note that as the entries of the vectors are chosen randomly 1 with equal probability the re trieval of free energy minimum now corresponds to a spin configuration which is also the most entropic for the shannon mcmillan argument thus both the most likely and the most difficult to handle as its informa tion compression is no longer possible two remarks are in order now on the one side according to the self consistency equation 5 and as shown in fig 3 m versus h displays the typi cal graded sigmoidal response of a charging neuron tuckwell 2005 and one would be tempted to call the spins neurons on the other side it is definitely inconvenient to build a network via n spins neurons which are further meant to be diverging i e n in order to handle one stored pattern of information 5 the thermodynamic limit n is required for both mathematical convenience e g it allows saddle point stationary phase techniques and in order to neglect observable fluctuations by a central limit theorem argument only along the theoretical physics route overcoming this limitation is quite natural and provides the first derivation of the hebbian prescription in this paper if we want a network able to cope with p patterns the starting hamiltonian should have simply the sum over these p previously stored 6 patterns namely hn 1 2 n n n i j 1 p 1 i j i j 11 where we neglect the external field h 0 for sim plicity as we will see in the next section this hamil tonian constitutes indeed the hopfield model namely the harmonic oscillator of neural networks whose coupling matrix is called hebb matrix as encodes the hebb prescription for neural organization amit 1992 4 4 the route from sherrington kirkpatrick to hopfield despite the extension to the case p 1 is formally straightforward the investigation of the system as p grows becomes by far more tricky indeed neural networks belong to the so called complex systems realm we propose that complex behaviors can be dis tinguished by simple behaviors as for the latter the number of free energy minima of the system does not scale with the volume n while for complex systems the number of free energy minima does scale with the volume according to a proper function of n for in stance the curie weiss mattis model has two minima only whatever n even if n and it constitutes the paradigmatic example for a simple system as a counterpart the prototype of complex system is the sherrington kirkpatrick model sk originally intro duced in condensed matter to describe the peculiar behaviors exhibited by real glasses hertz and palmer 1991 me zard et al 1987 this model has an amount of minima that scales exp cn with c 6 f n and its hamiltonian reads as hskn j 1 n n n i j ji j i j 12 where crucially coupling are gaussian distributed 7 as p ji j n 0 1 this implies that links can be ei 6 the part of neural network s theory we are analyzing is meant for spontaneous retrieval of already stored informa tion grouped into patterns pragmatically vectors clearly it is assumed that the network has already overpass the learning stage 7 couplings in spin glasses are drawn once for all at the beginning and do not evolve with system s thermalization namely they are quenched variables too ther positive hence favoring parallel spin configura tion as well as negative hence favoring anti parallel spin configuration thus in the large n limit with large probability spins will receive conflicting sig nals and we speak about frustrated networks in deed frustration the hallmark of complexity is fun damental in order to split the phase space in several disconnected zones i e in order to have several min ima or several stored patterns in neural network lan guage this mirrors a clear request also in electronics namely the need for inverters that once mixed with op amps result in flip flops crucial for information storage as we will see the mean field statistical mechanics for the low noise behavior of spin glasses has been first described by giorgio parisi and it predicts a hierarchical orga nization of states and a relaxational dynamics spread over many timescales for which we refer to specific textbooks me zard et al 1987 here we just need to know that their natural order parameter is no longer the magnetization as these systems do not magne tize but the overlap qab as we are explaining spin glasses are balanced ensembles of ferromagnets and antiferromagnets this can also be seen mathemati cally as p j is symmetric around zero and as a re sult m is always equal to zero on the other hand a comparison between two realizations of the sys tem pertaining to the same coupling set is meaning ful because at large temperatures it is expected to be zero as everything is uncorrelated but at low temper ature their overlap is strictly non zero as spins freeze in disordered but correlated states more precisely given two replicas of the system labeled as a and b their overlap qab can be defined as the scalar prod uct between the related spin configurations namely as qab 1 n n i a i b i 8 thus the mean field spin glass has a completely random paramagnetic phase with q 0 and a glassy phase with q 0 split by a phase transition at c tc 1 the sherrington kirkpatrick model displays a large number of minima as expected for a cognitive system yet it is not suitable to act as a cognitive system because its states are too disordered we look for an hamiltonian whose minima are not purely random like those in sk as they must represent or dered stored patterns hence like the cw ones but the amount of these minima must be possibly exten sive in the number of spins neurons n as in the sk and at contrary with cw hence we need to retain 8 note that while in the curie weiss model where p j j 1 the order parameter was the first momen tum of p m in the sherrington kirkpatrick model where p j n 0 1 the variance of p m which is roughly qab is the good order parameter 0 0 05 0 1 0 0 2 0 4 0 6 0 8 1 1 2 1 4 0 0 05 0 1 0 0 2 0 4 0 6 0 8 1 1 2 1 4 0 0 05 0 1 0 0 2 0 4 0 6 0 8 1 1 2 1 4 0 0 05 0 1 0 0 2 0 4 0 6 0 8 1 1 2 1 4 t pm sg sg rr figure 4 phase diagram for the hopfield model amit 1992 according to the parameter setting the system be haves as a paramagnet pm as a spin glass sg or as an associative neural network able to perform information re trieval r the region labeled sg r is a coexistence re gion where the system is glassy but still able to retrieve a ferromagnetic flavor within a glassy panorama we need something in between remarkably the hopfield model defined by the hamiltonian 11 lies exactly in between a curie weiss model and a sherrington kirkpatrick model let us see why when p 1 the hopfield model recovers the mattis model which is nothing but a gauge transformed curie weiss model conversely when p 1 n p i j n 0 1 by the standard central limit theorem and the hopfield model recovers the sherrington kirkpatrick one in between these two limits the system behaves as an as sociative network barra et al 2012 b such a crossover between cw or mattis and sk models requires for its investigation both the p mat tis magnetization m 1 p for quantifying retrieval of the whole stored patterns that is the vo cabulary and the two replica overlaps qab to con trol the glassyness growth if the vocabulary gets en larged as well as a tunable parameter measuring the ratio between the stored patterns and the amount of available neurons namely limn p n also re ferred to as network capacity as far as p scales sub linearly with n i e in the low storage regime defined by 0 the phase dia gram is ruled by the noise level only for c the system is a paramagnet with m 0 and qab 0 while for c the system performs as an attractor network with m 6 0 for a given selected by the external field and qab 0 in this regime no dan gerous glassy phase is lurking yet the model is able to store only a tiny amount of patterns as the capacity is sub linear with the network volume n conversely when p scales linearly with n i e in the high storage regime defined by 0 the phase di agram lives in the plane see fig 4 when is 10 6 2 2 6 10 1 0 6 0 2 0 2 0 6 1 h m 10 5 0 5 10 1 0 5 0 0 5 1 vin v o u tr 2 vin vout r 1 vin vout j 0 figure 5 average magnetizations m for each party of a bipartite antiferromagnetic model in the presence of the ex ternal field h solid black line compared with the transfer function of flip flop red bullets tuckwell 2005 agliari et al 2013 in the left inset we show a schematic repre sentation of a flip flop while in the right inset we show a schematic representation of an antiferromagnetic coupling small enough the system is expected to behave simi larly to 0 hence as an associative network with a particular mattis magnetization positive but with also the two replica overlap slightly positive as the glassy nature is intrinsic for 0 for large enough c c 0 14 however the hop field model collapses on the sherrington kirkpatrick model as expected hence with the mattis magneti zations brutally reduced to zero and the two replica overlap close to one the transition to the spin glass phase is often called blackout scenario in neu ral network community making these predictions quantitative is a non trivial task in statistical mechan ics and nowadays several techniques are available among which we quote the replica trick originally used by the pioneers amit gutfreund sompolinsky amit et al 1985 the martingale method origi nally developed by pastur sherbina and tirozzi pas tur et al 1994 and the cavity field technique re cently developed by guerra and some of us in barra et al 2010 4 5 frustration and flip flops as we saw in order to store multiple patterns the free energy landscape must be split into several valleys and this can be achieved only by introducing negative couplings frustrating the network this request mir rors the need for inverters and in turn flip flops in synthetic information storage systems as we are going to sketch because so far ferromagnetism has been al ready accounted with its formal equivalence to the behavior of op amps now we turn to discuss antifer romagnetic behavior which mathematically and con ceptually mimics in structure of matter the behavior of flip flops in electronics as a general scenario will then be the result of merging ferromagnetic and antiferromagnetic cou plings in statistical mechanics and operational ampli fiers and flip flops in electronics let us finally con sider just a small subset of the whole hopfield net work namely two groups of spins a b antiferromag netically interacting i e with a negative coupling say j 1 instead of j 1 of ferromagnetism as shown in fig 5 right inset and coupled with an external field h not surprisingly remembering eq 5 and that here j 1 the self consistencies coupled to the evolu tion of these two interacting spin groups read as ma tanh mb h 13 mb tanh ma h 14 the behavior of this system is shown still in fig 5 where the transfer function of a flip flop is pasted too for highlighting the structural equivalence an ensemble of spins that interact with both positive and negative couplings is a frustrated network whose free energy decomposes in multiple valleys namely a spin glass a particular spin glass is the hopfield model where the coupling matrix has both positive and negative entries but they obey the hebb prescrip tion this system can be seen as an ensemble of fer romagnetic positive coupling and antiferromagnetic negative coupling interactions thus from an engi neering perspective as a linear combination of op amps to amplify external signals and flip flops to store the information they convey 4 6 hopfield networks and boltzmann machines the world of neural networks is nowadays very broad with several models for several tasks hopfield net works are key models for retrieval while boltzmann machines are fundamental systems for learning ack ley et al 1985 however learning and retrieval are not two independent operations but rather two com plementary aspects of cognition remarkably the re lated models reflect this hidden link in fact as we are going to show hopfield networks and boltzmann ma chines obey two inseparable aspects of the same com plex thermodynamics hence it must be possible to re cover the hebb rule for learning also starting from re stricted boltzmann machines the latter in their simplest development are by layered networks with no links within any layer and only links among neurons of different layers see fig 6 links can be either positive or negative figure 6 examples of boltzmann machines encoded by bipartite spin glasses with 1 left panel and 0 5 right panel hence also boltzmann machines hide frustration and belongs to complex systems barra et al 2012 a if we use the symbol i i 1 n for neurons of one layer z 1 p for neurons of the other layer and i to label the link between the spin neuron or node i and the spin neuron or node we can write an hamiltonian for the boltzmann machine as hn z 1 n n i 1 p 1 i iz 15 the hamiltonian 15 represents in the jargon of sta tistical mechanics a bipartite spin glass in order to study the related phase diagram we work out the sta tistical mechanics machinery and write the free en ergy of the boltzmann machine as a 1 n log 2 n 2 p z exp n n i 1 p 1 i iz 16 remarkably as there are no links within each party from a statistical mechanics perspective these networks are simple to deal with because the sums are factorized in particular we can carry out the sum over z to get a 1 n log 2 n exp 2 2 n n n i j p 1 i j i j hence the leading contribution of the boltzmann ma chine is nothing but the hopfield network 9 we re mark that as a sideline we re obtained the hebb prescription for retrieval also from an artificial intel ligence perspective starting from a model meant for learning and from a structure of matter perspective from a spin glass instead that from a ferromagnet 9 actually with some additional mathematical efforts it can be shown that not only the leading term is an hopfield network but the whole machine behaves as the hopfield network figure 7 example of a non mean field model where spins occupy nodes of a graph endowed with a metric distance in this way although each spin is connected with any other the coupling strength decay as a power of distance here represented with different shades darker links correspond to stronger couplings it can be useful a visual comparison with the mean field topology shown in fig 1 right panel 5 conclusions proved by countless historically examples re search strongly benefits from interdisciplinary dia logues for at least a couple of reasons one is clearly the unrestrainable formalization of soft sciences by hard ones as it is happening nowadays in systems bi ology the other is that genuine ideas stemmed in a research branch can be suitably extended to cover as pects in a usually closer or related different research branches these notes are intended to contribute to the second route indeed from the statistical mechanics perspec tive the strand paved by engineers toward a theory for neural networks appears as the most natural way to proceed or in other words the solely in perfect agreement with thermodynamic prescriptions beyond tacking an historical perspective on such an evolution framed within a modern theoretical physics scaffold we gave here two alternative ways to re obtain the celebrated hebb rule and we high lighted the thermodynamical equivalence between hopfield networks and boltzmann machines remark ably those models appear as a unique joint framework from theoretical physics as learning and retrieval their outcomes are two inseparable aspects of the same phenomenon spontaneous cognition lastly with the aim of driving engineer s curiosity to ward statistical mechanics we performed a one to one bridge between statistical mechanics outcomes and behaviors of electronic components in particular we showed how the ferromagnetic scenario may play as a theory for signal amplification and how the anti ferromagnetic scenario may play for information stor age as happens in flip flops as ferromagnets and an tiferromagnets synergically cooperate in glassy sys tems and glassyness is intrinsic in neural networks theory in order to split the free energy into several minima thanks to frustration because we want to cor respond each free energy minima with a stored pat tern op amp and flip flops must be crucial devices in practical realizations of neural networks indeed they are we conclude with a remark about possible per spectives we started this historical tour highlighting how thanks to the mean field paradigm engineering e g robotics automation and neurobiology have been tightly connected from a theoretical physics per spective now however as statistical mechanics is starting to access techniques to tackle complexity hid den even in non mean field networks see e g fig 7 namely a hierarchical graph whose thermodynamics for the glassy scenario is almost complete castel lana et al 2010 we will probably witness another split in this smaller community of theoretical physi cists working in spontaneous computational capabil ity research from one side continuing to refine tech niques and models meant for artificial systems well lying in high dimensional mean field topologies and from the other beginning to develop ideas models and techniques meant for biological systems only strictly defined in finite dimensional spaces or even worst on fractal supports this work was supported by gruppo nazionale per la fisica matematica gnfm istituto nazionale d alta matematica indam references ackley d h hinton g e and sejnowski t j 1985 a learning algorithm for boltzmann machines cognitive science 9 147 169 agliari e asti l barra a burioni r and uguzzoni g 2012 analogue neural networks on correlated random graphs journal of physics a 45 365001 agliari e and barra a 2011 a hebbian approach to complex network generation europhysics letters 94 10002 agliari e barra a burioni r di biasio a and uguz zoni g 2013 collective behaviours from bio chemical kinetics to electronic circuits scientific re ports 3 agliari e barra a and camboni f 2008 criticality in diluted ferromagnets journal of statistical mechan ics theory and experiment p 10003 amit d j 1992 modeling brain function cambridge university press amit d j gutfreund h and sompolinsky h 1985 spin glass models of neural networks physical re view a 32 1007 bardeen j cooper l n and schrieffer j r 1957 the ory of superconductivity physical review 108 1175 barra a bernacchia a contucci p and santucci e 2012 a on the equivalence of hopfield networks and boltzmann machines neural networks 34 1 9 barra a genovese g and guerra f 2010 the replica symmetric approximation of the analogical neural net work journal of statistical physics 140 784 796 barra a genovese g guerra f and tantari d 2012 b how glassy are neural networks jour nal of statistical mechanics theory and experiment p 07009 baxter r j 2007 exactly solved models in statistical mechanics courier dover publications bean c p 1962 magnetization of hard superconductors physical review letters 8 250 bolloba s b 1998 modern graph theory volume 184 springer castellana m decelle a franz s mezard m and parisi g 2010 the hierarchical random energy model physical review letters 104 127206 castiglione p falcioni m lesne a and vulpiani a 2012 chaos and coarse graining in statistical me chanics cambridge university press coolen a c c ku hn r and sollich p 2005 the ory of neural information processing systems oxford university press di castro c and jona lasinio g 1969 on the micro scopic foundation of scaling laws phys lett 29 322 323 domhoff g w 2003 neural networks cognitive devel opment and content analysis american psychologi cal association ellis r 2005 entropy large deviations and statistical mechanics volume 1431 taylor francis hagan m t demuth h b and beale m h 1996 neural network design pws pub boston harris warrick r m editor 1992 dynamic biological networks mit press hebb d o 1940 the organization of behavior a neu ropsychological theory psychology press hertz john a k and palmer r 1991 introduction to the theory of neural networks lecture notes hopfield j j 1982 neural networks and physical sys tems with emergent collective computational abilities proc natl a sc 79 8 2554 2558 kac m 1947 random walk and the theory of brownian motion american mathematical monthly pages 369 391 kittel c 2004 elementary statistical physics courier dover publications martindale c 1991 cognitive psychology a neural network approach thomson brooks cole publishing co mcculloch w s and pitts w 1943 a logical calculus of the ideas immanent in nervous activity the bulletin of mathematical biophysics 5 4 115 133 me zard m parisi g and virasoro m a 1987 spin glass theory and beyond volume 9 world scientific singapore miller w t werbos p j and sutton r s editors 1995 neural networks for control mit press millman j and grabel a 1987 microelectronics mcgraw hill minsky m l and papert s a 1987 perceptrons ex panded edition mit press boston ma pastur l shcherbina m and tirozzi b 1994 the replica symmetric solution without replica trick for the hopfield model journal of statistical physics 74 1161 1183 reichl l e and prigogine i 1980 a modern course in statistical physics volume 71 university of texas press austin rolls e t and treves a 1998 neural networks and brain function rosenblatt f 1958 the perceptron a probabilistic model for information storage and organization in the brain psychological review 65 6 386 saad d editor 2009 on line learning in neural net works volume 17 cambridge university press tuckwell h c 2005 introduction to theoretical neuro biology volume 8 cambridge university press turing a m 1950 computing machinery and intelli gence mind pages 433 460 von neumann j 1951 the general and logical theory of automata cerebral mechanisms in behavior pages 1 41 watts d j and strogatz s h 1998 collective dynamics of small world networks nature 393 440 442 wilson k g 1971 renormalization group and critical phenomena physical review b 4 3174 1 introduction 2 statistical mechanics microscopic dynamics obeying entropy maximization 3 the route to complexity the role of mean field limitations 4 serial processing 4 1 storing the first pattern curie weiss paradigm 4 2 the curie weiss model and the saturable operational amplifier 4 3 the route from curie weiss to hopfield 4 4 the route from sherrington kirkpatrick to hopfield 4 5 frustration and flip flops 4 6 hopfield networks and boltzmann machines 5 conclusions