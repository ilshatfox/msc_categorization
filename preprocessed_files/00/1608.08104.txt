constraint matrix factorization for space variant psfs field restoration f ngole 1 j l starck 1 k okumura 1 j amiaux 1 and p hudelot 2 1 laboratoire aim cea dsm cnrs universite paris diderot irfu service d astrophysique cea saclay orme des merisiers 91191 gif sur yvette france 2 institut d astrophysique de paris umr 7095 cnrs upmc 98 bis boulevard arago f 75014 paris france e mail ngolefred yahoo fr november 2015 abstract context in large scale spatial surveys the point spread function psf varies across the instrument field of view fov local measurements of the psfs are given by the isolated stars images yet these estimates may not be directly usable for post processings because of the observational noise and potentially the aliasing aims given a set of aliased and noisy stars images from a telescope we want to estimate well resolved and noise free psfs at the observed stars positions in particular exploiting the spatial correlation of the psfs across the fov contributions we introduce rca resolved components analysis which is a noise robust dimension reduction and super resolution method based on matrix factorization we propose an original way of using the psfs spatial correlation in the restoration process through sparsity the introduced formalism can be applied to correlated data sets with respect to any euclidean parametric space results we tested our method on simulated monochromatic psfs of euclid telescope launch planned for 2020 the proposed method outperforms existing psfs restoration and dimension reduction methods we show that a coupled sparsity constraint on individual psfs and their spatial distribution yields a significant improvement on both the restored psfs shapes and the psfs subspace identification in presence of aliasing perspectives rca can be naturally extended to account for the wavelength dependency of the psfs keywords dimension reduction spatial analysis super resolution matrix factoriza tion sparsity ar x iv 1 60 8 08 10 4 v 3 cs c v 3 1 a ug 2 01 6 constraint matrix factorization for space variant psfs field restoration 2 1 introduction in many applications such as high precision astronomical imaging or biomedical imaging the optical system introduces a blurring of the images that needs to be taken into account for scientific analyses and the blurring function also called point spread function psf is not always stationary on the observed field of view fov a typical example is the case of the euclid space mission 1 to be launched in 2020 where we need to measure with a very high accuracy the shapes of more than one billion of galaxies an extremely important step to derive such measurements is to get an estimate of the psf at any spatial position of the observed images this makes the psf modeling a critical task in first approximation the psf can be modeled as a convolution kernel which is typically space and time varying several works in image processing 2 and specifically in astronomy 3 4 address the general problem of restoring images in presence of a space variant blur assuming that the convolution kernel is locally known in astronomical imaging unresolved objects such as stars can provide psf measurements at different locations in the fov nevertheless these images can be aliased given the ccd sensors sizes which makes a super resolution sr step necessary this is the case for instance for the euclid mission the sr is a widely studied topic in general image processing literature 5 in astronomy softwares imcom 6 and psfex 7 which propose an sr option are widely used the imcom provides an oversampled output image from multiple under sampled input images assuming that the psf is perfectly known it does not deal with the psf restoration itself the psfex treats sr as an inverse problem with a quadratic regularizer in 8 a sparsity based super resolution method was proposed assuming that several low resolution lr measurements of the same psf are available in practice we generally don t have such multiple measurements in this paper we consider the case where the psf is both space variant and under sampled and we want to get an accurate modeling at high resolution of the psf field assuming we have under sampled measurements of different psfs in the observed field we assume that the psfs vary slowly across the field intuitively this implies a compressibility of the psfs field which leads us to the question of what would be a concise and easily understandable representation of a spatially indexed set of psfs 2 notations we adopt the following notation conventions bold low case letters are used for vectors bold capital case letters are used for matrices we treat vectors as column vectors unless explicitly mentioned otherwise for a matrix m we note mij the j th coefficient of the ith line m c j or m j its j th column and m l i or m i its i th line that we treat as a line vector more generally constraint matrix factorization for space variant psfs field restoration 3 for j 1 j 2 we note m j 1 j 2 the matrix obtained by extracting the columns of m indexed from j 1 to j 2 for i 1 i 2 m i 1 i 2 is defined analogously with respect to m s lines for a vector u u k refers to its kth component for a given integer m we note im the identity matrix of size m m let e be a euclidean space e can be r 2 or r 3 if we consider spatial or spatio temporal data respectively we note u uk 1 k p a set of vectors in e in this paper we only consider the case e r 2 u will be a set of positions in a plan 3 the psfs field 3 1 the observation model we assume that we have an image i which contains p unresolved objects such as stars which can be used to estimate the psfs field noting yk one of these p objects at spatial position uk yk is therefore a small patch of i with ny pixels around the spatial position uk we will write yk as a 1 d vector the relation between the true psf xk and the noisy yk observation is yk mkxk nk 1 where mk is a linear operator and nk is a noise that we assume to be gaussian and white we will consider two kinds of operators in this paper the first one is the simple case where mk inx and we have the number of pixels nx in xk is equal to ny and the second one is a shift downsampling degradation operator and nx m 2 dny where md is the downsampling factor in lines and columns with md 1 noting y y 1 yp the matrix of ny lines and p columns of all observed patches x x 1 xp the matrix nx p of all unknown psfs we can rewrite eq 1 as y f x n 2 where f x m 1 x 1 mpxp this rewriting is useful because as we discuss in the following the different psfs xk are not independent which means that the problems of eq 1 should not be solved independently for each k in other terms the vectors xk 1 k p belong to a specific unknown manifold that needs to be learned by using the data globally 3 2 the data model let be a r dimensional subspace of rnx embedding the psfs field we assume that there exists a continuous function f e 7 so that f uk xk k j 1 pk the regularity of f translates the correlation of the data in space and time let si 1 i r be a basis of by definition we can write each xk as a linear combination of the si xk r i 1 aiksi k 1 p or equivalently x sa 3 constraint matrix factorization for space variant psfs field restoration 4 where s s 1 sr and a is a r p matrix containing the coefficients a k of the vectors xk k 1 p in the dictionary s each column of the matrix s that we also refer to as an atom can be seen as an eigen psf i e a given psf s feature distributed across the field 3 3 the inverse problem we need therefore to minimize y f x 2 f which is an ill posed problem due to both the noise and the operator f f denoting the frobenius norm of a matrix there are several constraints that may be interesting to use in order to properly regularize this inverse problem positivity constraint the psf xk should be positive low rank constraint as described above we can assume that xk r i 1 aiksi which means that we can instead minimize min a s y f sa 2 f 4 we assume that r min n p this dimension reduction has the advantage that there are much less unknown to find leading to more robustness but the problem is now that the cost function is not convex anymore smoothness constraint we can assume that the vectors xk are structured the low rank constraint does not necessarily impose xk to be smooth or piece wise smooth adding an additional constraint on s atoms such as a sparsity constraint allows to capture spatial correlations within the psfs themselves an additional dictionary s can therefore be introduced which is assumed to give a sparse representation of the vectors sk proximity constraint we can assume that a given xk at a position uk is very close to another psf xk at position uk if the distance between uk and uk is small this means that the field f must be regular this regularity can be forced by adding constraints on the lines of the matrix a indeed the p values relative to a line a i correspond to the contribution of the ith eigen psf to locations relative to the spatial positions u we show in section 5 how these four constraints can be jointly used to derive the solution let first review existing methods susceptible to solve this problem 4 related work in all this section y refers to the observed data set yk 1 k p in the first part the aforementioned degradation operator f is simply the identity therefore we review some dimension reduction methods in the second part f is a shifting and downsampling operator we present a psf modeling software dealing with this more constraining setting constraint matrix factorization for space variant psfs field restoration 5 4 1 dimension reduction the principal components analysis is certainly one of the most popular mathematical procedure in multivariate data analysis and especially dimension reduction in our case we want to represent y s elements using r vectors with r max p ny a pca gives an orthonormal family of r vectors in rny so that the total variance of y along these vectors directions is maximized by definition the pca looks for redundant features over the whole data set therefore in general the principal components neither capture localized features in sense of e nor have a simple physical interpretation in 9 a regularized pca is proposed to address this shortcoming for spatial data analysis in atmospheric and earth science indeed as a pca the method solves the following problem min a y yata 2 f s t aa t ir 5 for some chosen small r moreover it jointly imposes a sparsity constraint and a smoothing penalties with respect to the space e on the matrix a lines this way with the right balance between those two penalties one favors the extraction of localized spatial features making the interpretation of the optimal a easy yet there is no obvious way of setting the sparsity and smoothness parameters which are crucial moreover unless the data actually contain spatially localized and non overlapping features the coupled orthogonality and sparsity constraint is likely to yield a biased approximation of the data in the context of remote sensing and multi channel imaging two ways of integrating spatial information into pca are proposed in 10 the set y is made of multi channel pixels in the first way the author introduces a weighting matrix indicating the relative importance of each pixel for instance the weight of a given pixel can be related to its distance to some location of interest in e then the computation of the covariance matrix of image bands is slightly modified to integrate this weighting this idea is close to the methodology proposed in 11 as a consequence one expects to recover spectral features spatially related to some location of interest within the most important eigen pixels yet we do not have any specific location of interest in e and we rather want to recover relevant features across the whole data set the second approach aims at taking into account the spatial associations and structural properties of the image to do so modified versions of the image bands covariance matrices are calculated with increasing shifts between the bands up to a predetermined maximum shifting amplitude these covariance matrices including the regular one are averaged and the principal components are finally derived intuitively one expects the spectral features present in structured images regions to be strengthened and therefore captured into the principal components however we consider a general setting where the data are randomly distributed with respect to e which makes the shifted covariances matrices ill defined a review of pca applications and modifications for spatial data analysis can be found in 12 constraint matrix factorization for space variant psfs field restoration 6 in case the data lie on or are close to a manifold m of dimension r embedded in rn one can consider using one of the numerous non linear dimension reduction algorithms published in the manifold learning literature such as gmra 13 14 the idea is to partition the data in smaller subsets of sample close to each other in the sense of the manifold geometry from this partionning the manifold tangent spaces are estimated at subsets locations estimates are then simply given by the best regressions of these subsets with r dimensional affine subspaces the method includes some multiresolution considerations that are not relevant to our problem this procedure provides a dictionary in which each of the original samples need at most r elements to be represented moreover the local processing of the data which is necessary in this setting because of the manifold curvature makes this approach somehow compatible with the considered problem indeed by hypothesis the closer two samples will be in sense of e the closer they will be in rn and the more likely they will fall into the same local cluster another interesting alternative to the pca can be found in 15 this construction called treelets extracts features by uncovering correlated subsets of variables across the data samples it is particularly useful when the sample size is by far smaller than the data dimensionality p ny which does not hold in the application we consider in the following 4 2 super resolution in this subsection f takes the following form f x m 1 x c 1 mpx c p 6 where mi is a warping and downsampling matrix since we consider a set of compact objects images the only geometric transformation one has to deal with for registration is the images shifts with respect to the finest pixel grid which can be estimated using the images centroids 8 to the best of our knowledge the only method dealing with this specific setting is the one used in the psf modeling software psfex 7 this method solves a problem of the form min s 1 2 y f s s 0 a 2 f s 2 f 7 s 0 is a rough first guess of the model components each line of the weight matrix a is assumed to follow a monomial law of some given field s parameters the number of components is determined by the maximal degree of the monomials for instance let say that we want to model the psfs variations as a function of their position in the field with monomials with degrees up to 3 then one needs 6 components corresponding to the monomials 1 x x 2 y xy and y 2 assuming that the ith psf in y s columns order is located at ui uix uiy then the ith column of a is given by a c i 1 uix u 2 ix uiy uixuiy u 2 iy t up to a scaling factor constraint matrix factorization for space variant psfs field restoration 7 this method is used for comparisons in the numerical experiments part 5 resolved components analysis 5 1 matrix factorization we have seen that we can describe the psfs field f as f u 1 f up x sa 8 the matrix s is independent of the spatial location and the ith line of a gives the contribution of the vector si to each of the samples as discussed in section 3 3 the field s regularity can be taken into account by introducing a structuring of the matrix a we can write a i t n l 1 il l i 1 r 9 where l 1 l n is a set of vectors spanning rp equivalently we can write a vt where v 1 n and is a r n matrix see fig 1 figure 1 data matrix factorization the jth sample which is stored in the jth column of x is linear combination of s columns using a s jth column coefficients as the weights similarly the jth line of a is a linear combination vt s lines using s jth line coefficients as the weights constraint matrix factorization for space variant psfs field restoration 8 physical interpretation an interesting way to well interpret a is to consider the ideal case where the measurements are distributed following a regular grid of locations u in this case we can expand the vector a i t using the discrete cosine transform dct and vectors i in eq 9 are regular cosine atoms and the column index of the matrix is related the frequency hence lines relative to high frequencies will be related to quicky varying psf components in the field while lines related to low frequencies will be related to psfs stable components in practice the sampling is not regular and the dct cannot be used and v has to be learned in a way to keep the harmonic interpretation valid we want some lines a i to describe stable psfs components on the fov and other to be more related to local behavior 5 2 the proximity constraint on a as previously mentioned we want to account for the psfs field s regularity by constraining a s lines specifically we want some lines to determine the distribution of stable features across the psfs field while we want other lines to be related to more localized features in order to build this constraint let first consider the simple case of a one dimensional field of regularly spaced psfs 5 2 1 regularly distributed observations we first assume that e r we suppose that p 2 k 1 for some integer k and we consider the 1 d vector e a i 1 i p defined as follows i p i 1 1 ui uk 1 e if i 6 k 1 10 i p j 1 j 6 k 1 a uj uk 1 e otherwise 11 for some positive reals e and a we suppose that e a is normalized in l 2 norm we refer to this family of signals parametrized by e and a as notch filters in reason of their frequency responses shapes some examples can be found in fig 2 one can observe that 1 1 is essentially a high pass filter as e increases the notch structure clearly appears with an increasing notch frequency it is clear that for a vector v minimizing the functional e a v v e a 22 promotes vectors with spectra concentrated around the notch frequency corresponding to the chosen values of e and a we can directly use this family of filters to constraint a as follows we define the functional mrp r 7 r a r i 1 ei a a i 12 where ei i is a set of reals verifying 0 e 1 e 2 er and a 0 2 because the notch frequency increases with ei minimizing promotes varying level of smoothness of a s lines which is what we wanted to achieve the filter e a and the constraint matrix factorization for space variant psfs field restoration 9 a direct domain samples b discrete fourier transform dft entry wise moduli figure 2 notch filters examples for different values of the parameter e in eq 10 and 11 the parameter a is set to 1 functional definitions can be extended to higher dimensions of the space e by involving a multidimensional convolution 16 therefore if the psfs are distributed over a regular grid with respect to e one can implement the proximity constraint by solving min a s 1 2 y f sa 2 f a 13 for some positive yet in practical applications the observations are in general irregularly distributed in the next section we propose a slightly different penalty which is usable for arbitrary observations distributions constraint matrix factorization for space variant psfs field restoration 10 5 2 2 general setting let define the functional e a rp 7 r v p k 1 p i 1 i 6 k avk vi uk ui e 2 2 14 where e and a are positive reals minimizing e a v tends to enforce the similarity of close features with respect to e in other terms the more uk ui 2 is large the less important is avk vi in e a v and e somehow determines the radius of similarity for e 1 e a e a because of the uniform spacing of the values ui and the decay of 1 uk ui e 2 for sufficiently high p we give more details on this approximation in appendix a in the 1 d case however unlike e a the functional e a is still relevant without the uniform sampling hypothesis and we expect qualitatively the same behavior as e a with respect to the frequency domain if the data sampling is sufficiently dense therefore we use e a instead of e a in the functional of eq 13 besides we use the term frequency even for randomly distributed samples 5 2 3 flexible penalization the redundant frequencies dictionary v the efficiency of the regularization of the problem 13 relies on a good choice of the parameters e 1 er and a indeed if the associated notch frequencies does not match with the data set frequency content the regularization will more or less bias the psf estimation depending on the lagrange multiplier besides setting this parameter might be tricky we propose an alternate strategy for constraining a which leads to the factorization model introduced in section 5 1 and still builds over the idea of notch filters for v rp we can write e a v pe av 22 15 where pe a is a p p matrix defined by pe a i j 1 ui uj e 2 if i 6 j 16 pe a i i p j 1 j 6 i a ui uj e 2 17 i j j 1 pk 2 therefore e a v v tqe av 18 where qe a p t e ape a and is symmetric and positive we consider the singular values decomposition svd of qe a qe a ve ade av t e a the diagonal values of de a are sorted in decreasing order we note de a the vector made of these diagonal values so that de a 1 de a p 0 considering the reduced form e a v p i 1 de a i v ve a i 2 it is clear that minimizing e a v promotes vectors correlated with qe a last eigenvectors in the case of regular sampling with respect to e these eigenvectors are the harmonics close to the notch frequency of e a we can rewrite the functional constraint matrix factorization for space variant psfs field restoration 11 accordingly a r i 1 p j 1 dei a j v vei a j 2 19 it is clear from this expression that minimizing a enforces the selection of the eigenvectors associated with the lowest eigenvalues in the set dei a j i j for describing a s lines this can be seen as a sparsity constraint over a s lines with respect to the atoms vei a j i j yet the small subset of atoms which will carry most of the information is somehow predefined through the eigenvalues dei a j i j this is unsuitable if the notch filters parameters are poorly selected on the contrary one would like to select in a flexible way the atoms which fit the best the data let suppose that we have determined a set of parameters ei ai 1 i r so that the filters ei ai notch frequencies would cover the range of significant frequencies with respect to e present in the data as previously we note vei ai 1 i r the eigenvector s matrices associated with the operators ei ai we note v ve 1 a 1 ver ar considering the preceding remark we introduce the following problem min s 1 2 y f s vt 2 f s t l 0 l l 1 r 20 now a vt each line of a is a sparse linear combination of vt s lines and the active atoms are optimally selected according to the data the choice of the parameters ei ai 1 i r and l 1 i r is discussed in a forthcoming section 5 2 4 a connection with graphs theory in case a 1 the matrix pe a is the laplacian of an undirected fully connected and weighted graph with p nodes 1 p such that the weight of the vertex connecting a node i to a node j is 1 ui uj e 2 17 as proposed in spectral graph theory 18 this gives a natural interpretation of pe a and qe a eigenvectors as harmonic atoms in the graph s geometry each line of the matrix a can be seen as a function defined on a family of graphs determined by the observations locations so that we enforce the regularity of a s lines according to the graphs geometry our approach is thereby close to the spectral graphs wavelets framework 19 however the graphs wavelets are built on a single graph and a scaling parameter allows one to derive wavelets atoms corresponding to spectral bands of different sizes in our case the scales diversity is accounted for by building a dictionary of harmonics corresponding to different graphs indeed as e increases the weight associated to the most distant nodes in the sense of ui uj 2 becomes negligible which implies that the corresponding graph laplacian is determined by nearby nodes yielding higher frequencies harmonics 5 3 the smoothness constraint on s as previously mentioned each psf is a structured image we can account for this through a sparsity constraint this has proven effective in multiple frame psfs super resolution 8 constraint matrix factorization for space variant psfs field restoration 12 since we do not estimate individual psfs directly we instead constraint the eigen psfs which are s s columns specifically we promote s s columns sparsity with respect to a chosen dictionary s by definition a typical imaging system s psf concentrates most of its power in few pixels therefore a straightforward choice for s is in in other words we will enforce the sparsity of s s columns in the pixels domain on the other hand we take s as the second generation starlet forward transform 20 without the coarse scale the power of sparse prior in wavelet domain for inverse problems being well established we shall online emphasize the fact that this particular choice of wavelet is particularly suitable for images with nearly isotropic features 5 4 algorithm we define the sets 1 mr n r l 0 l l 1 r and 2 s mnr r mr n r s vt mnp r 0 the aforementioned constraints leads us to the following optimization problem min s 1 2 y f s vt 2 f r i 1 wi ssi 1 1 2 s 21 where denotes the hadamard product and c denotes the indicator function of a set c see appendix b the l 1 term promotes the sparsity of s columns with respect to s the vectors wi i weight the sparsity against the other constraints and allow some adaptivity of the penalty with respect to the uncertainties propagated to each entry of s 8 the parametric aspects of this method are made clear in the subsequent sections the problem 21 is globally non convex because of the coupling between s and and the l 0 constraint in particular the feasible set s mnr r mr n r s vt 0 with n rp is non convex therefore one can at most expect to find a local minimum to do so we consider the following alternating minimization scheme i initialization 0 1 with n rp s 0 argmin s 1 2 y f s 0 vt 2 f r i 1 wi ss i 1 s t s 0 v t 0 ii for k 0 kmax a k 1 argmin 1 2 y f sk vt 2 f s t l 0 l l 1 r b sk 1 argmin s 1 2 y f s k 1 vt 2 f r i 1 wi ss i 1 s t s k 1 v t 0 the problem a remains non convex yet there exists heuristic methods allowing one to approach a local minimum 21 23 the problem b is convex and can be solved efficiently one can note that there is no positivity constraint in the sub problem a this choice is motivated by two facts constraint matrix factorization for space variant psfs field restoration 13 the feasible set of b is non empty for any choice of k 1 allowing to be outside of the global problem feasible set for s fixed brings some robustness regarding local degenerated solutions there is an important body of work in the literature on alternate minimization schemes convergence and in particular in the non convex and non smooth setting see 24 and the references therein in the proposed scheme the analysis is complicated by the asymmetry of the problems a and b we define the function h s 1 2 y f s vt 2 f r i 1 wi ssi 1 22 and the matrix s k argmin s 1 2 s sk 22 s t s k 1 vt 0 one immediate sufficient condition for the sequence h k sk k to be decreasing and thereby convergent is h k 1 s k h k sk 23 which occurs if sk k 1 stays sufficiently close to 2 although we do not prove this always holds true we observe on examples that the matrix sk k 1 v t in general only has a few and small negative entries for k 1 this follows from the adequacy of the dictionary v for sparsely describing a s lines the complete method is given in algorithm 1 the resolution of the minimization sub problems is detailed in appendices algorithm 1 resolved components analysis rca 1 parameters estimation and initialization harmonic constraint parameters ei ai 1 i r v a 0 noise level a 0 w 0 0 2 alternate minimization 3 for k 0 to kmax do 4 for j 0 to jmax do 5 sk argmin s 1 2 y f sak 2 f r i 1 wk j i ss i 1 s t sak 0 6 update wk 0 sk update wk j 1 7 end for 8 k 1 argmin 1 2 y f sk vt 2 f s t l 0 l 9 update noise level k 1 wk 1 0 10 ak 1 k 1 v t 11 ak 1 i ak 1 i ak 1 i 2 for i 1 r 12 end for 13 return skmax akmax constraint matrix factorization for space variant psfs field restoration 14 5 5 parameters setting 5 5 1 components sparsity parameters we consider the terms of the form wk j ss 1 where k is the alternate minimization index and j is the re weighted l 1 minimization index we first suppose that s in we decompose wk j as wk j k j k 24 let consider the minimization problems in s in algorithm 1 assuming that we simply minimize the quadratic term using the following steepest descent update rule sm 1 sm f y f smak atk 25 for a well chosen step size f being the adjoint operator one can estimate the entry wise standard deviations of the noise which propagates from the observations to the current solution sm 1 for a given matrix x in mnp r we assume that f takes the following general form f x m 1 x 1 mpx p we define f 2 as f 2 x m 1 m 1 x 1 mp mp x p 26 we note b the observational noise or model uncertainty that we assume to gaussian white and centered the propagated noise entry wise standard deviations are given by k f 2 var b atk a t k 27 where var returns entry wise variances and f 2 is the adjoint operator of f 2 now one can proceed to a hypothesis testing on the signal presence in each entry of sm 1 based on k 25 and denoise sm 1 accordingly for instance we define the noise free version of sm 1 as follows s m 1 i 1 i 2 0 if sm 1 i 1 i 2 k i 1 i 2 sm 1 i 1 i 2 sm 1 i 1 i 2 sm 1 i 1 i 2 k i 1 i 2 otherwise 28 where controls the false detection probability the noise being gaussian we typically choose 3 or 4 for the sequence s m converges to a solution of the problem argmin s 1 2 y f s kut 2 f r i 1 k i s i 1 29 for k k one can find some material on minimization schemes in appendice appendix c this choice yields a noise free but biased solution because of the thresholding this is a well known drawback of l 1 norm based regularizations the purpose of the vector k j is to mitigate this bias 26 k 0 is a vector with ones at all entries at the step 6 in algo 1 k j is calculated as follows k j 1 1 sk k 30 constraint matrix factorization for space variant psfs field restoration 15 where all the operations are entry wise and sk is the vector made of element wise absolute values of sk entries qualitatively this removes the strongest features from the l 1 norm terms by giving them small weights which makes the debiasing possible conversely the entries dominated by noise get weights close to 1 so that the penalty remains unchanged for s 6 in we follow the same rational to set the sparsity in the transform domain according to the noise induced uncertainty we need to further propagate it the noise through the operator s formally we need to estimate the element wise standard deviations of sf b atk let consider the intermediate random matrix yf f b assuming that f f id 31 yf s lines are statistically independent therefore within a given column of yfa t k the entries are statistically independent from one another we deduce that the element wise standard deviations of sf b atk are given by k s s f 2 var b atk a t k 32 then k is obtained as previously and k j is calculated as k j 1 1 ssk k 33 the property 31 is approximately true in the case of super resolution 5 5 2 number of components we do not propose a method to choose the number of components r yet we observe that because of the sparsity constraint some lines of the matrix k 1 at the step 8 in algorithm 1 are equal to the null vector when the number of components is overestimated the corresponding lines in ak 1 and subsequently the corresponding columns in sk are simply discarded this provides an intrinsic mean to select the number of components thus in practice one can choose the initial r as the data set dimensionality from the embedding space point of view which can be estimated based on a principal component analysis 5 5 3 proximity constraint parameters in this section we consider the functionals ei ai and especially the choice of the parameters ei and ai let assume that we have determine a suitable range for the parameters ei ai s emin emax amin amax for i 1 r for a particular e a we consider the matrix qe a and its eigenvectors matrix ve a introduced in section 5 2 3 as previously stated we want the weights matrix a lines to be sparse with respect to qe a s eigenvectors in order to choose the parameters and initialize the weights matrix we use the following greedy procedure we consider a sequence of matrices ri 1 i r with r 1 y for i j 1 rk we define je a ri max k j 1 pk rive a k 2 34 constraint matrix factorization for space variant psfs field restoration 16 and we note v e a the optimal eigenvector we choose the i th couple of parameters as ei ai argmax e a s je a ri 35 a 0 i v ei ai and ri 1 ri rive avte a regarding the set s we choose the interval amin 0 and amax 2 this range allows the notch structure assuming that emin 0 for a 0 he a behaves as a low pass filter for a 0 we observe that he a becomes a notch filter with a notch frequency close to the null frequency for a 2 as previously stated e determines the influence of two samples on one another corresponding coefficients in the matrix a in the algorithmic process according to section 5 2 2 we set emin 1 let consider the graph ge introduced in section 5 2 4 the higher is e the lower is ge connexity considering that we are looking for global features yet localized in the field frequency domain the highest possible value of e should guarantee that the graph ge is connected this gives us a practical upper bound for e once s is determined we discretize this set with a logarithmic step in such a way to have more samples close to emin amin which correspond to low notch frequencies we solve approximately problem 35 by taking the best couple of parameters in the discretized version of s this step is the most computationally demanding especially for large data samples 5 5 4 weights matrix sparsity parameters the parameters l are implicitly set by the minimization scheme used at step 8 in 1 this is detailed in appendix c 6 numerical experiments in this section we present the data used to test the proposed method the simulation realized and comparisons to other existing methods for both dimensionality reduction and super resolution aspects 6 1 data the data set consists of simulated optical euclid psfs as in 8 for a wavelength of 600 m the psfs distribution across the field is shown on fig 3 these psfs account for mirrors polishing imperfections manufacturing and alignment errors and thermal stability of the telescope 6 2 simulation we applied different dimension reduction algorithms to a set of 500 psfs located in the blue box on fig 3 we applied the algorithms to different observations of the fields with varying level of white gaussian noise for a discrete signal s of length n corrupted with a white gaussian noise b we define the signal to noise ratio snr as snr s 22 n 2 b 36 constraint matrix factorization for space variant psfs field restoration 17 figure 3 simulated psfs distribution across the fov 6 3 quality assessment in astronomical surveys the estimated psf s shape is particularly important precisely one has to be able to capture the psf anisotropy we recall that for an image x xij i j the central moments are defined as p q x i j i ic p j jc qxij 37 with p q n 2 ic jc are the image centroid coordinates the moments 2 0 and 0 2 quantifies the light intensity spreading relatively to the lines ic y y r and x jc x r respectively now we consider the moment 1 1 we introduce the centered and rotated pixels coordinates xi yj defined by the system of equations xi cos yj sin i ic 38 xi sin yj cos j jc 39 for some 0 2 then we have 1 1 i j sin 2 2 x 2 i y 2 j 2 cos 2 1 xi yj xij 40 and in particular 1 1 i j 1 2 x 2 i 4 y 2 j 4 xij it becomes clear that 1 1 quantifies the light intensity spreading with respect to the pixels grid diagonals the ellipticity parameters are defined as e 1 x 2 0 x 0 2 x 2 0 x 0 2 x 41 e 2 x 2 1 1 x 2 0 x 0 2 x 42 we define the vector x e 1 x e 2 x t this vector characterizes how much x departs from an isotropic shape and indicates its main direction of orientation constraint matrix factorization for space variant psfs field restoration 18 it plays a central theoretical and practical role in weak lensing based dark matter characterization 27 another important geometric feature is the so called psf size it has been shown that the size error is a major contributor to the systematics in weak gravitational lensing surveys 28 we characterize the size of a psf x as follows s x i j i ic 2 j jc 2 xij i j xij 1 2 43 assuming that a given psf is a 2 d discrete probability distribution this quantity measures how much this distribution is spread around its mean ic jc t let note xi 1 i p the set of original psfs and x i 1 i p the set of corresponding estimated psfs with one of the compared methods at a given snr the reconstruction quality is accessed through the following quantities the average error on the ellipticity vector e p i 1 xi x i 2 p noting x 1 x 1 xp x p the dispersion of the errors on the ellipticity vector is measured through the nuclear norm b the average absolute error on the size es p i 1 s xi s x i p in pixels the dispersion of the errors on the size s std s xi s x i i in pixels 6 4 results 6 4 1 dimension reduction in this section we compare rca to pca gmra and the software psfex we ran a pca with different number of principal components between 0 and 15 10 was the value which provided the best results gmra input was the data set intrinsic dimension 29 two since the psfs only vary as a function of their position in the field we provided the absolute squared quadratic error allowed with respect to the observed data based on the observation noise level for psfex we used 15 components finally rca used up to 15 components and effectively 2 and 4 components respectively for the lowest snr fields realization as previously mentioned we assess the components sparsity s constraint on the one hand we consider s in which enforces the components sparsity in pixels domain this is referred to as rca in the plots on the other hand we take s as the second generation starlet forward transform 20 without the coarse scale this is referred to as rca analysis in the plots one can see on the left plot in fig 4 that the proposed method is at least 10 times more accurate on the ellipticity vector than the other considered methods moreover the right plot shows that the accuracy is way more stable this is true for both choice of the dictionary s fig 5 shows that the estimated size s x i is very sensitive to the choice of the dictionary s the results are by far more accurate with a sparsity constraint on the components in wavelet domain than in direct domain constraint matrix factorization for space variant psfs field restoration 19 a average error on the ellipticity vector b dispersion of the ellipticity vector figure 4 x axis snr see section 6 2 y axis log 10 e for the left plot log 10 b for the right plot a average absolute error on the size b dispersion of the errors on the size figure 5 x axis snr y axis es for the left plot s for the right plot for a given estimate of the psf at a given location the error on the size parameter is more sensitive to errors on the core of the psf main lobe and first rings and less sensitive to errors on the outer part of the psf than one would expect regarding the error on the ellipticity vector the error on the outer part of the psf is essentially related to the observational noise whereas the error on core of the psf which has a high snr is more related to the method induced bias this explains why the pca performs quite well for this parameter on the other hand the bias induced by the sparsity is not only related to the dictionary choice but also to the underlying data model with respect to the chosen dictionary as previously explained the components sparsity term is set in such a way to penalize any feature which does not emerge from the propagated noise which is a source constraint matrix factorization for space variant psfs field restoration 20 of bias by using wavelets we might recover features which are dominated by noise in pixel domain as long as the wavelet filters profile at given scale and direction matches those features spatial structure thus we expect less error on the reconstructed psf s core by using wavelets we might also consider two distinct ways of using sparsity for the components we can model each component as s ts with sparse which is known in the sparse recovery literature as synthesis prior we can alternately constraint ss to be sparse this priors are equivalent if the dictionary is unitary 30 therefore the pixel domain sparsity constraint can be considered as falling into both framework however the two priors are no longer equivalent and potentially yields quite different solutions for overcomplete dictionaries we observe in practice that unless the simulated psfs are strictly sparse with respect to the chosen dictionary this includes redundant wavelet dictionaries the synthesis prior yields a bias on the reconstructed psf size since the estimated psfs are sparse linear combinations of atoms which are in general sharper than a typical psf profile the analysis prior is somehow weaker and appears to be more suitable for approximately sparse data we do not observe a significant difference between these methods with respect to the mean squared error except for gmra which gave noisier reconstructions we applied the aforementioned methods to the psfs field previously used with additional 30 corners psfs and 30 localized psfs as shown on fig 3 at an snr of 40 this assess the behavior of the algorithms with respect to spatial clustering and sparse data distribution one can see in fig 6 examples of simulated observed psfs from different areas in the fov a observation 1 center psf b observation 2 center psf c observation 3 corner psf d observation 4 corner psf e observation 5 local psf f observation 6 local psf figure 6 input psfs at different locations in the fov for a snr 40 the corresponding reconstructed psfs can be seen in fig 7 for each of these observed psfs the reconstructed psfs for each method are shown in fig 7 one can observe that the proposed method gives noiseless and rather accurate psfs reconstruction for both the center the corners and the localized area of the field constraint matrix factorization for space variant psfs field restoration 21 figure 7 psfs reconstructions from the left to the right original gmra pca psfex rca from the bottom to the top 2 local psfs reconstructions 2 corner psfs reconstructions 2 center psfs reconstructions the observed corresponding psfs can be seen in fig 6 constraint matrix factorization for space variant psfs field restoration 22 see fig 3 one can also see that we fail to capture accurately the rings pattern in the corners and the localized area the dictionary s considered are not specifically adapted to curve like structures the ring patterns varies across the fov but are locally correlated therefore they can only be recovered where the psfs are sufficiently dense and numerous which is the case at the fov s center pca and psfex yield a significant increase of the snr in their estimated psfs at the center and in the localized area yet they fail to do so in the corners because of the lack of correlation for the pca and local smoothness for psfex finally the poor results obtained with gmra can be explained by the fact that the underlying manifold sampling is not sufficiently dense for the tangent spaces to be estimated reliably 6 4 2 super resolution in this section the data are additionally downsampled to euclid telescope resolution pca and gmra does not handle the downsampling therefore we only consider psfex and rca in this section for each method we estimate an upsampled version of each psf with a factor 2 in lines and columns in case of euclid this is enough to have a nyquist frequency greater than half the signal spatial bandwidth 31 as previously rca analysis refers to the proposed method with the dictionary s chosen as the second generation starlet forward transform 20 without the coarse scale rca lsq refers to the proposed method with the dictionary s chosen as the identity matrix and the weight matrix a simply calculated as a argmin a 1 2 y f s a 2 f 44 s being the current estimate of the components matrix among all the methods previously considered for comparison psfex is the only one handling the undersampling as for the dimension reduction experiment the proposed method with s chosen as a wavelet dictionary is at least one order of magnitude more accurate over the shape parameters and the mean square error besides fig 10 shows that the proximity constraint over the matrix a allows one to select a significantly better optimum than a simple least square update of a indeed regularizing the weight matrix estimation reinforces the rejection of f s null space as previously we restored the complete field of fig 3 for a linear snr of 40 using rca analysis with undersampled input psfs as shown in fig 11 the figure 12 shows consistent results with the dimension reduction experiment in particular the corners psfs restoration is obviously more accurate 7 reproducible research in the spirit of participating in reproducible research the data and the codes used to generate the plots presented in this paper will be made available at http www http www cosmostat org software rca constraint matrix factorization for space variant psfs field restoration 23 a average error on the ellipticity vector b dispersion of the error on the ellipticity vector figure 8 x axis snr see section 6 2 y axis log 10 e for the left plot log 10 b for the right plot a average absolute error on the size b dispersion of the errors on the size figure 9 x axis snr y axis es for the left plot s for the right plot cosmostat org software rca 8 conclusion we introduced rca which is a dimension reduction method for continuous and positive data field which is noise robust and handles undersampled data as a linear dimension reduction method rca computes the input data as linear combinations of few components which are estimated as well as the linear combination coefficients through a matrix factorization the method was tested over a field of simulated euclid telescope psfs we show that constraining both the components matrix and the coefficients matrix using sparsity http www cosmostat org software rca http www cosmostat org software rca constraint matrix factorization for space variant psfs field restoration 24 figure 10 average normalized least square error a observation 1 center psf b observation 2 center psf c observation 3 corner psf d observation 4 corner psf e observation 5 local psf f observation 6 local psf figure 11 input psf at different locations in the field for a snr 40 yield at least one order of magnitude more accurate psfs restoration than existing methods with respect to the psfs shapes parameters in particular we show that the analysis formulation of the sparsity constraint over the components is particularly suitable for capturing accurately the psfs sizes we also show that constraining the coefficients matrix yields a significantly better identification of the psfs embedding subspace when the data are undersampled an important extension of rca for astronomical imaging would be to account for the wavelength dependency of the psfs indeed an unresolved star image is a linear combination of the psfs at different wavelengths weighted by the star s spectrum hence rca can be naturally extended by replacing the matrix s with a tensor for which each element would be a polychromatic eigen psf acknowledgements this work is supported by the european community through the grants physis contract no 640174 and dedale contract no 665044 within the h 2020 framework program the authors acknowledge the euclid collaboration the european space agency and the support of the centre national d etudes spatiales constraint matrix factorization for space variant psfs field restoration 25 figure 12 psfs reconstructions from the left to the right original psfex rca from the bottom to the top 2 local psfs reconstructions 2 corner psfs reconstructions 2 center psfs reconstructions the observed corresponding psfs can be seen in fig 11 constraint matrix factorization for space variant psfs field restoration 26 references 1 esa sre 2011 euclid mapping the geometry of the dark universe tech rep esa 2 escande p and weiss p 2014 arxiv e prints preprint 1404 1023 3 miraut d balle j and portilla j 2012 eurasip j adv sig proc 2012 193 url http dblp uni trier de db journals ejasp ejasp 2012 html mirautbp 12 4 denis l thie baut e and soulez f 2011 fast model of space variant blurring and its application to deconvolution in astronomy icip ed macq b and schelkens p ieee pp 2817 2820 isbn 978 1 4577 1304 0 url http dblp uni trier de db conf icip icip 2011 html denists 11 5 farsiu s robinson d elad m and milanfar p 2004 international journal of imaging systems and technology 14 47 57 issn 1098 1098 url http dx doi org 10 1002 ima 20007 6 rowe b hirata c and rhodes j 2011 preprint arxiv 1105 2852 v 2 7 bertin e 2011 automated morphometry with sextractor and psfex astronomical data analysis software and systems xx astronomical society of the pacific conference series vol 442 ed evans i n accomazzi a mink d j and rots a h p 435 8 ngole mboula f m starck j l ronayette s okumura k and amiaux j 2015 astronomy astrophysics 575 a 86 preprint 1410 7679 9 wang w t and huang h c 2015 arxiv e prints preprint 1501 03221 10 cheng q 2006 spatial and spatially weighted principal component analysis for images processing geoscience and remote sensing symposium 2006 igarss 2006 ieee international conference on pp 972 975 11 harris p brunsdon c and charlton m 2011 international journal of geographical information science 25 1717 1736 preprint http dx doi org 10 1080 13658816 2011 554838 url http dx doi org 10 1080 13658816 2011 554838 12 dems ar u harris p brunsdon c fotheringham a s and mcloone s 2013 annals of the association of american geographers 103 106 128 preprint http dx doi org 10 1080 00045608 2012 689236 url http dx doi org 10 1080 00045608 2012 689236 13 allard w k chen g and maggioni m 2011 arxiv e prints preprint 1105 4924 14 maggioni m minsker s and strawn n 2014 arxiv e prints preprint 1401 5833 15 lee a b nadler b and wasserman l 2008 ann appl stat 2 435 471 url http dx doi org 10 1214 07 aoas 137 16 rakhuba m v and oseledets i v 2014 arxiv e prints preprint 1402 5649 17 anderson w n and morley t d 1985 linear and multilinear algebra 18 141 145 18 chung f r spectral graph theory vol 92 american mathematical soc 19 hammond d k vandergheynst p and gribonval r 2009 arxiv e prints preprint 0912 3848 20 starck j l murtagh f and bertero m 2011 starlet transform in astronomical data processing handbook of mathematical methods in imaging ed scherzer o springer new york pp 1489 1531 isbn 978 0 387 92919 4 21 soussen c idier j duan j and brie d 2015 ieee transactions on signal processing 63 3301 3316 cran de pt sbs url https hal archives ouvertes fr hal 00948313 22 blumensath t and davies m 2008 journal of fourier analysis and applications 14 629 654 issn 1069 5869 url http dx doi org 10 1007 s 00041 008 9035 z 23 cartis c and thompson a 2015 information theory ieee transactions on 61 2019 2042 issn 0018 9448 24 bolte j sabach s and teboulle m 2014 mathematical programming 146 459 494 25 starck j l murtagh f and fadili j 2010 sparse image and signal processing wavelets curvelets morphological diversity new york ny usa cambridge university press isbn 0521119138 9780521119139 26 cande s e wakin m and boyd s 2008 journal of fourier analysis and applications 14 877 905 27 dodelson s 2003 10 weak lensing and polarization modern cosmology ed dodelson s burlington academic press pp 292 iii isbn 978 0 12 219141 1 url http www sciencedirect com 1404 1023 http dblp uni trier de db journals ejasp ejasp 2012 html mirautbp 12 http dblp uni trier de db journals ejasp ejasp 2012 html mirautbp 12 http dblp uni trier de db conf icip icip 2011 html denists 11 http dx doi org 10 1002 ima 20007 arxiv 1105 2852 v 2 1410 7679 1501 03221 http dx doi org 10 1080 13658816 2011 554838 http dx doi org 10 1080 13658816 2011 554838 http dx doi org 10 1080 00045608 2012 689236 http dx doi org 10 1080 00045608 2012 689236 http dx doi org 10 1080 00045608 2012 689236 1105 4924 1401 5833 http dx doi org 10 1214 07 aoas 137 http dx doi org 10 1214 07 aoas 137 1402 5649 0912 3848 https hal archives ouvertes fr hal 00948313 http dx doi org 10 1007 s 00041 008 9035 z http www sciencedirect com science article pii b 9780122191411500292 constraint matrix factorization for space variant psfs field restoration 27 science article pii b 9780122191411500292 28 paulin henriksson s amara a voigt l refregier a and bridle s l 2008 astronomy astrophysics 484 67 77 preprint 0711 4886 29 little a lee j jung y m and maggioni m 2009 estimation of intrinsic dimensionality of samples from noisy low dimensional manifolds in high dimensions with multiscale svd statistical signal processing 2009 ssp 09 ieee sp 15 th workshop on pp 85 88 30 elad m milanfar p and rubinstein r 2007 inverse problems 23 947 url http stacks iop org 0266 5611 23 i 3 a 007 31 cropper m e a 2013 monthly notices of the royal astronomical society 431 preprint arxiv astro ph im 1210 7691 32 moreau j 1965 bulletin de la socie te mathe matique de france 93 273 299 url http eudml org doc 87067 33 combettes p l condat l pesquet j c and cong vu b 2014 arxiv e prints preprint 1406 5439 34 beck a and teboulle m 2009 siam journal on imaging and sciences 2 183 202 35 mancera l and portilla j 2008 non convex sparse optimization through deterministic annealing and applications icip ieee pp 917 920 url http dblp uni trier de db conf icip icip 2008 html mancerap 08 appendix a notch filter approximation in this appendix we explain why the functional e a introduced in subsection 5 2 can be approximated with the functional e a we use the subsection 5 2 notations we consider the 1 d case the samples ui 1 i p are uniformly spaced scalar we assume that u 1 up we note u 2 u 1 thus i p i 1 1 k 1 i e e if i 6 k 1 and a 1 k 1 2 k n 1 a ne e a 2 using the centered definition of the convolution with a zero boundary condition for a vector v vi 1 i p the vector h v e a is given by h j p i 1 vi j k 1 i a 3 for j j 1 pk and with the convention that j k 1 i 0 if j k 1 i 1 or j k 1 i p combining eq a 1 a 2 and a 3 we can write h j 2 k n 1 a ne e vj i max 1 j k min p j k i 6 j 1 j i e e vi a 4 http www sciencedirect com science article pii b 9780122191411500292 http www sciencedirect com science article pii b 9780122191411500292 0711 4886 http stacks iop org 0266 5611 23 i 3 a 007 http stacks iop org 0266 5611 23 i 3 a 007 arxiv astro ph im 1210 7691 arxiv astro ph im 1210 7691 http eudml org doc 87067 http eudml org doc 87067 1406 5439 http dblp uni trier de db conf icip icip 2008 html mancerap 08 http dblp uni trier de db conf icip icip 2008 html mancerap 08 constraint matrix factorization for space variant psfs field restoration 28 we recall that e a v h 22 on the other hand e a v tv 22 with tv defined as tv j 2 min j 1 p j n 1 1 ne max j 1 p j n min j 1 p j 1 1 ne a e vj p i 1 i 6 j 1 j i e e vi if j 6 k 1 and tv k 1 2 k n 1 a ne e vj p i 1 i 6 k 1 1 k 1 i e e vi a 5 thus tv k 1 h k 1 0 and for j 6 k 1 tv j h j max j 1 p j n k 1 1 ne k n min j 1 p j 1 1 ne a e vj max 1 j k i 1 i 6 j 1 j i e e vi p i min p j k i 6 j 1 j i e e vi a 6 given the symmetry of e a with respect to k 1 we focus on the above difference for j k we further assume that j 6 1 then eq a 6 simplifies to tv j h j p j n k 1 1 ne k n j 1 ne a e vj 1 j 1 e e v 1 p j n k 1 ne e vn a 7 now using the inequalities for n 1 n n 1 1 t 1 e dt 1 ne n n 1 1 te dt a 8 and assuming that e 1 we get the following upper bounding tv j h j 1 e 1 max p j 1 1 e j 1 1 e k 1 1 e k 1 e p j 1 e j 1 e k 1 1 e k 1 e a e 1 j 1 e k 1 e p j 1 e v e a 9 we see that the higher is k we recall that p 2 k 1 and the closer j is to k the smaller is the error therefore we use tv as an approximation for h up to boundaries errors appendix b convex analysis in this appendix we give the general convex analysis material relevant to our work we consider a finite dimensional hilbert space h equipped with the inner product and associated with the norm constraint matrix factorization for space variant psfs field restoration 29 appendix b 1 proximity operator definition let f be a real valued function defined on h f is proper if its domain as defined by domf x h f x is non empty f is lower semicontinuous lsc if lim infx x 0 f x f x 0 we define 0 h as the set of proper lsc convex real valued function defined on h for a function f 0 h the function y 1 2 y 2 f y achieves its minimum at a unique point denoted by proxf h 32 the operator proxf is the proximity operator of f examples let c be a convex closed set of h the indicator function of c is defined as c x 0 if x c otherwise b 1 it is clear from the definition that the proximity operator of c is the orthogonal projector onto c for h r r and f x x proxf y softthresh y where softthresh denotes the soft thresholding operator with a threshold properties separability if h h 1 hn for f 0 h and if f x f 1 x 1 fn x n where fi 0 hi for i 1 n then proxf y proxf 1 y 1 proxfn y n translation for f 0 h and a h we define fa x f x a then proxfa y a proxf y a appendix b 2 convex conjugate definition let f be a real valued function defined on h the function f y max x x y f x is the convex conjugate of f it is also known as the legendre fenchel transformation of f properties moreau identity for f 0 h and r prox f x prox 1 f x x fenchel moreau theorem if f 0 h f f appendix c minimization schemes this appendix details the practical resolution of the proposed method optimization problems constraint matrix factorization for space variant psfs field restoration 30 appendix c 1 components estimation problem we consider the step 5 in the alg 1 if s in the problem of estimating components takes the following generic form min s f s g 1 l 1 s h s c 1 with f s r i 1 wi s c i 1 g 1 rn p l 1 s sa and h s 1 2 y m s 2 f for some bounded linear operator m f 0 rn r g 1 0 rn p and l 1 is a bounded linear operator moreover h is convex differentiable and has a continuous and lipschitz gradient this problem can be solved efficiently using the primal dual algorithms introduced in 33 for instance one only need to be able to compute f and g 1 proximity operators for some given positive reals and and h s gradient prox f s s ij 1 i n 1 j p with s ij softthresh wj i sj i prox g 1 z z z h s m y m s where m is the adjoint operator of m for an arbitrary dictionary s we instead consider the following generic formulation of the problem min s g 1 l 1 s g 2 l 2 s h s c 2 where g 2 z r i 1 wi z c i 1 and l 2 s ss c 1 ss c r one can use the algorithms suggested before and minimization will require the computation of g 2 proximity operator for some given positive real which is simply given by prox g 1 z z z with z i j softthresh wj i z i j appendix c 2 coefficients estimation we consider the step 8 in the alg 1 the problem takes the generic form min j s t l 0 l l 1 r c 3 where j is convex differentiable and has a continuous and lipschitz gradient and rr q this problem is combinatorial and its feasible set is non convex for typical data sizes in image processing applications and tractable processing time one can at best reach a good local optimum there is an extensive literature on optimization problems involving the l 0 pseudo norm we propose an heuristic based on quite common ideas now and which appears to be convenient from a practical point of view let be a global minimum of problem c 3 for a vector m rr q we define its support as supp m i j j 1 rk j 1 qk m i j 0 c 4 we note e the set of r q real matrices sharing the support of e m rr q supp m supp c 5 constraint matrix factorization for space variant psfs field restoration 31 e is a vector space in particular e is a convex set therefore is a solution of the following problem min j s t e c 6 the proposed scheme is motivated by the idea of identifying approximately e along with the iterative process one can think of numerous algorithms to solve problem c 6 all involving the orthogonal projection onto e we build upon the fast proximal splitting algorithm introduced in 34 for a vector u rq we note a permutation of j 1 qk verifying u 1 u q for an integer k q we define suppk u i j 1 qk u i u k c 7 finally for a vector rr q we define the subspace ek m rr q suppk m i suppk i i 1 r c 8 the proposed scheme is given in algorithm 2 f is a positive valued concave increasing function and projebf k c uk denotes the orthogonal projection onto ebf k c uk algorithm 2 beck teboulle proximal gradient algorithm with variable proximity operator 1 initialization 0 0 rr q 0 0 t 0 1 res 1 0 res 0 0 tol k 0 2 minimization 3 while k kmax and resk resk 1 resk do 4 uk k 1 j k 5 k 1 projebf k c uk uk 6 tk 1 1 4 t 2 k 1 2 7 k 1 tk 1 tk 1 8 k 1 k k k 1 k 9 resk 1 j k 10 k k 1 11 end while 12 return kstop the solution support size is constraint at step 5 and the size is gradually increased as shown in fig c 1 the convergence analysis this scheme is out of the scope of this paper however fig c 2 suggests that once an index is included in an iterate support this index is included in all the forthcoming iterates supports this implies that at each support size s step in fig c 1 the algorithm approximately solves a problem of the following form min j s t e c 9 constraint matrix factorization for space variant psfs field restoration 32 figure c 1 support size function x axis iteration index k in algorithm 2 y axis bf k c for f x x 1 figure c 2 algorithm 2 main iterate evolution x axis k 1 0 for the top image and k 1 1 for the bottom image y axis iterate index k for a given subspace e which is a convex problem this scheme can be viewed as an iterative hard thresholding 22 with a decreasing threshold 35 yet it is quite easy to get an upper bound of the support size related to the parameters l in problem c 3 from the data depending on the time one is willing to spend on the coefficients computation this yields convenient choices for the function f 1 introduction 2 notations 3 the psfs field 3 1 the observation model 3 2 the data model 3 3 the inverse problem 4 related work 4 1 dimension reduction 4 2 super resolution 5 resolved components analysis 5 1 matrix factorization 5 2 the proximity constraint on a 5 2 1 regularly distributed observations 5 2 2 general setting 5 2 3 flexible penalization the redundant frequencies dictionary v 5 2 4 a connection with graphs theory 5 3 the smoothness constraint on s 5 4 algorithm 5 5 parameters setting 5 5 1 components sparsity parameters 5 5 2 number of components 5 5 3 proximity constraint parameters 5 5 4 weights matrix sparsity parameters 6 numerical experiments 6 1 data 6 2 simulation 6 3 quality assessment 6 4 results 6 4 1 dimension reduction 6 4 2 super resolution 7 reproducible research 8 conclusion appendix a notch filter approximation appendix b convex analysis appendix b 1 proximity operator appendix b 2 convex conjugate appendix c minimization schemes appendix c 1 components estimation problem appendix c 2 coefficients estimation