a computational algorthm for the hardy function utlising sub sequences of generalised quadratic gauss sums with an overall operational complexity 1 3 2 1 d m lewis department of mathematics university of liverpool m o building peach st liverpool l 69 7 zl tel 44 151 794 4014 email d m lewis liverpool ac uk abstract this paper describes a practical methodology for computing the hardy function using just 1 3 2 1 standard computational operations to a tolerance of in the relative error the methodology is analogous to work published relatively recently by g h hiary although the details are very different initially the hardy function is formulated into sub sequences of generalised quadratic gaussian sums of ever increasing length adapting a theoretical framework formulated by r b paris an algorithm for the computation of quadratic gaussian sums using just operations is developed and tested this algorithmic methodology is itself incorporated as a sub program into a somewhat larger algorithm algorithm zt 13 designed for the computation of which is considerably more efficient than classical methods as exemplified by variants of the riemann siegel formula sample computations using algorithm zt 13 are presented the results of which conform to technical theoretical predictions made regarding the limits of its performance both in terms of its operational count and degree of precision speculative ideas briefly floated in the conclusions suggest that adaptations to this methodology have the potential to deliver significant further computational savings in the future 1 introduction numerous evaluation methodologies have been put forward for calculating the riemann zeta function formally defined by 1 1 1 1 1 with extension for all apart from the simple pole at 1 by means of analytic continuation 41 12 23 an excellent modern review of various computationally efficient means of calculating over different parts of the complex plane is given in 7 the connection between and the primes through the euler product formula in 1 enabled riemann in his paper of 1859 35 to establish an exact formulation for the prime counting function not to be confused with the number 1 1 1 2 1 1 2 a li li li 1 0 2 2 1 2 b where li is the logarithmic integral 15 eq 8 240 2 and is the m bius mu function e g 22 p 32 for an account of this work see 12 the first proof of 2 b for the function was published in 29 the process of m bius inversion inherent in 2 a is discussed in a general form by 9 section 10 the numbers represent the non trivial zeros of the first proofs of the prime number theorem li published by 16 and 43 demonstrated that 0 1 which is sufficient to show the error term in 2 b is li as for more modern proofs of the pnt see 9 the famous riemann hypothesis rh asserts that 1 2 which would imply li 1 2 3 however the current best estimate for the error li 3 1 5 0 4 see 22 is very much larger than 3 given the fundamental importance of the rh to the distribution of the primes much of the computational work devoted to has focused on calculating along the critical line 1 2 i 0 where see 12 1 2 i i i 2 i rsi 1 2 i 5 1 4 i 2 2 2 2 1 8 1 48 6 in the above is hardy s z function 23 formally defined by 1 2 i 1 4 i 2 1 4 i 2 i 1 2 7 while is the riemann siegel theta function the riemann siegel integral rsi see 9 can be used in place of 1 as an alternative means of defining 1 it is relatively straightforward 23 to show that that and the rh is equivalent to statement that 0 the riemann siegel formula rsf discovered by c l siegel from riemann s notes in the g ttingen university library 39 14 7 is a computationally efficient method for calculating the hardy function for large it states that 2 1 2 1 4 1 0 1 2 2 r 8 where 2 2 0 2 2 1 16 2 and r are combinations of the derivatives of the function 0 bounds on the cut off remainder term are discussed in 14 but typically 2 0 011 7 4 200 this formula forms the basis of most of the numerical calculations of on the critical line and has been used to show that at least the first 1013 zeros with imaginary parts up to 2 5 1012 do indeed satisfy the rh 13 this type of ongoing work employs the ingenious idea of 31 of utilising a non uniform fft algorithm 10 11 to compute multiple evaluations of along 1 2 i with 0 1 1 and this allows one to make the necessary evaluations of 8 in just 1 2 operations each to within an accuracy of o 1 as opposed to the 1 2 operations needed if the rsf was applied sequentially an alternative methodology has recently been devised by 34 to isolate the first 103 800 788 359 zeros on the line with imaginary parts 30 610 046 000 to an accuracy of 2 102 the rsf has been known for more than eighty years and it along with its related variants 5 had long been thought to provide the most computationally efficient means of computing and hence 1 2 i along the critical line indeed its characteristics led e bombieri to propose in 6 that the evaluation of a single arbitrary value of 1 2 i requires a calculation which is fundamentally of o 1 2 1 in operational complexity in order to achieve an error bounded by a fixed however the much more recent ground breaking work of g a hiary 18 19 proposes a number of algorithmic methods that effectively rebut bombieri s proposition in particular 18 re formulates 1 2 i into collections of either quadratic or cubic gaussian sums of lengths 1 6 and 5 26 respectively using the principle of quadratic reciprocity the former can be estimated recursively using operations as discussed in section 3 and algorithm qgs for arbitrary exponents whilst the latter can be estimated in a somewhat analogous manner provided the cubic coefficient in the exponent is not too large this means that in order to compute 1 2 i to an accuracy of one requires only 1 3 1 operations in the quadratic case and in principle as low as 4 13 1 operations in the cubic case both methods incur a necessary pre computational costing of 1 3 1 and 4 13 1 operations respectively and both require similar orders of bits of storage before any actual calculations can commence the cubic method utilises a standard fast fourier transform fft subroutine hiary s paper 18 consists mainly of theoretical formulations and does not contain much analysis of the actual performance of the final algorithms however some sample computations of 1 2 i for 1024 31 do appear on a related website 20 this paper seeks to build upon the algorithmic ideas of 18 although the actual methodology is quite distinctive in order to produce essentially an 1 3 1 operational variant for the computation of 1 2 i but with more emphasis on the details underlying the 1 terms to be more specific it is the aim of this paper to demonstrate that a single evaluation of 1 2 i for arbitrarily large requires no more than 1 3 2 1 elementary computational operations specifically elementary cosine evaluations see section 3 6 in order to yield an approximation with absolute relative error not a fixed bound less than here 0 is a parameter satisfying 0 although doing so more slowly than any power of specifically 1 o for any 0 for practical calculations there is a good deal of flexibility about the precise choice centring around the trade off between the greater accuracy obtained when is small at the expense of a higher contribution to the operation count and less accurate calculations larger obtained at greater speed the final algorithm algorithm zt 13 of section 5 does not require any pre computational costings or the use of fft but does require 2 bits of storage for numbers of bits formulation of the methodology is founded upon the alternative definition 5 of 1 2 i on the critical line in terms of the riemann siegel integral the integral first found by bessel hagen from riemann s notes 39 is defined by the formula rsi i 2 i i 1 2 i i 2 sin 0 1 9 0 1 where the symbol 0 1 denotes a path of integration along a line of slope 1 crossing the real axis between 0 and 1 directed from upper left to lower right this integral was first studied in detail by turing 42 with a view to utilising it to calculate turing s method as discussed in 23 was to shift the line of integration to the right to cross the real axis between and 1 to create a semi infinite parallelogram the integral along this new line is much easier to estimate than the original because the main bulk of the integrand is confined to a relatively small region of the integration range the drawback is that the poles of the integrand at the integers 1 2 inside the parallelogram yield the same residues as the main sum of the rsf 8 hence turing s method offers no significant computational improvements over the rsf and so has rarely been utilised for that purpose the remainder terms are also more difficult to formulate however the rsi contains far more information about 1 2 i hidden within its structure than can be discerned using turing s methodology in an unpublished manuscript 27 this author carried out a thorough analysis of 9 which revealed a new and distinctive representation for this was achieved by integrating 9 directly along the line 0 1 specifically along the line crossing the real axis at 1 2 with 1 2 i for 0 the main difficulty with this direct approach is that the term grows in competition with the i 2 term when 0 which forces one to consider 9 as integral over an essentially infinite range the bulk of the integral covers a range 1 3 as to calculate it effectively but with some lengthy analysis 27 appendix a it is possible to circumvent this problem the result is a representation of the hardy function in terms of a sum of the confluent hypergeometric or kummer s function 32 chap 13 given by 2 i 3 4 i 8 5 4 i 1 4 i 2 1 2 3 4 i 2 3 2 2 i 4 2 1 2 2 i 8 1 2 2 5 2 1 4 1 32 2 3 4 i 2 3 2 2 i 4 2 1 10 essentially 10 is equation a 40 of 27 for rsi 1 2 i combined with 5 to obtain here 1 1 is kummer s function and 5 256 4 6 arising from sterling s series for the gamma function formally the sum is taken over all positive odd integers 1 however because it turns out that its terms decay only as 1 the infinite sum does not converge and in practice it must be terminated at some odd integer 2 the notation will be used to denote the round and floor functions to the nearest odd and even integers respectively beyond this cut off the remainder of the series can be continued analytically and estimated to very high accuracy using euler maclaurin summation in just the same way as 1 can be continued analytically for 1 these observations concerning the truncation of 10 can be established from estimates 27 of the respective kummer s functions generated using some standard methods of contour integration an adapted version of this estimation methodology will be discussed in some detail in section 2 because it is fundamental to the proposition that the calculation 1 2 i is an 1 3 2 1 computational operation for large but for the moment applying the methodology on a term by term basis 27 in which each kummer function in 10 is estimated separately one obtains the following somewhat simpler expression 2 2 cos 2 1 2 8 2 2 1 4 1 1 11 where 1 32 2 4 2 12 2 3 4 1 2 1 1 4 2 1 4 1 24 2 4 1 1 32 2 11 here 8 the relative error 1 8 3 4 and represents the euler maclaurin remainder terms see specifically eqs a 73 74 of 27 the portcullis variable so designated because it acts as a gateway from the summands in 11 to the summands in 8 see below is defined for all real by 2 2 1 1 2 1 2 1 1 2 2 1 2 1 2 12 actually fixes the position of a saddle point of a certain integral see eq 21 representation of the respective kummer functions note that for reasons discussed in section 2 1 the approximate sum 11 commences at 2 the nearest odd integer above not at 1 as in the exact sum 10 a slight modification to 11 is required whenever itself happens to be very close to an odd integer specifically when where 1 6 if 0 the main sum begins at 4 and a transition term representing the contribution at 2 must be included for 1 6 0 and small 1 4 this takes the form q v 27 appendix a section a 3 6 eq a 65 23 4 1 3 32 3 1 4 3 2 3 32 3 1 4 1 12 cos 2 1 3 24 13 for larger 1 4 1 and for the case when 0 it is straightforward to calculate numerically in the latter scenario the main sum recommences at 2 however it is difficult to formulate any relatively simple approximations for that smoothly transform themselves into the first summation term of 11 in these instances dropping all the relatively small o 1 contributions in 11 and concentrating on the sum of the main terms alone leads to the following theorem theorem 1 let 0 30 define 2 1 2 let be the riemann siegel theta function 6 and 2 2 1 8 o 1 in 6 then there exists an upper bound 0 satisfying the lim 0 such that 2 1 2 2 cos 2 1 2 8 2 2 1 4 6 15 1 12 14 the proof of this theorem is the main focus of 27 in which 2 for convenience however there is nothing special about this value and the second sum can be terminated at any 2 and then continued analytically using euler maclaurin summation e g 59 eq 2 10 1 the proof for any general proceeds virtually identically provided the integration contour of 27 eq 8 is defined to be the circle centred at 2 with maximum radius 2 notice also the appearance of rather than which appears in the definition of 8 a precise definition of the difference 1 48 is given by 23 eq 5 2 it is also important to point out that the transition term 13 is not included in this estimate of the discrepancy between the main sums of 8 11 in fact it is this omission which is the main source of error in 14 because 1 12 it the difficulty of establishing the precise transition between 13 and the main terms of 11 that is reflected in the conservative value assigned to but the most interesting result is not 14 per se but rather the connection established in the course of its proof between the respective summands and by means of the portcullis variable 12 the terms in 14 corresponding to the main sum of the rs formula arise from the saddle points of certain integrals defined by 27 eq 26 situated at 4 2 2 eliminating from 12 establishes a direct connection for real between the summands in 8 and in 11 given by 2 2 2 1 4 1 1 2 15 this connection leads to some fascinating results utilising the ideas used to prove theorem 1 27 was able to demonstrate essentially the idea is simply to reduce the radius of systematically to exclude the terms 2 4 one at a time and reapply the methodology to the smaller circle that moving between from to 2 to corresponds to a change from to a point where 8 1 4 hence the first term alone of 11 the calculation of which an 1 computational operation corresponds to the result of summing the last 8 1 4 terms of the main sum rs formula an 1 4 operation more specifically let 1 2 2 1 2 and denote the corresponding nearest integer values satisfying 15 by 1 and 2 respectively then 2 2 cos 2 1 1 1 2 8 1 2 2 1 4 2 1 2 2 16 so for example if 1024 then 1 1595769121607 1 2 2 398941625041 and 398942280401 the sum of the 655 360 terms on the right of 16 computes to 4 727 10 4 compared to the predicted value of 4 744 10 4 on the left the relative error of 0 0034 2 8 1 lies in the range specified by 11 this kind of dramatic computation saving cannot be expected to continue indefinitely because using the entire sum 11 to estimate is manifestly an computational operation compared to the operations needed for the rsf as increases each individual term of 11 corresponds to ever smaller portions of the main sum of the rsf until one reaches a boundary point situated at 2 3 1 2 when the terms are in a rough one to one correspondence moving on into the 2 regime one finds the roles of terms making up 8 and 11 swap over so that now each individual term of the rsf now corresponds to ever larger portions of the sum over but the swap over boundary at 2 is still sufficiently large to realise a significant reduction in the operational count necessary to compute this can be achieved by utilising a hybrid of the two main sums of 8 and 11 the crux of this idea is to substitute the last 1 2 terms of the rsf with the first 2 3 terms of 11 to give 2 2 2 2 8 4 2 2 cos 2 1 2 8 2 2 1 4 3 2 17 setting the cut off at 2 3 1 2 as in 17 reduces the total number of terms needed compute from the using 8 alone to a minimum of 2 2 1 using 8 and 17 in combination a reduction of 17 16 in computer time the saving is closer to 15 because the terms on the right of 17 are slightly more complicated to evaluate but still elementary compared to those of 8 although theoretically the error in 17 is no better than found in 14 in practice millions of computations lew show that it bounded above by a term 1 01 64 1 4 the size of this bound corresponds to the maximum magnitude of the terms of both 8 and 11 at the cut off point 2 in light of these computational savings an obvious question to ask is whether one can do any better there seems to be scope for such a scenario simply because 11 is a sum of elementary terms and as such contains huge tranches of computational redundancy in the regime when 2 as well as the computational efficiencies highlighted in 16 and 17 understanding the nature of the computational redundancy hidden within the mathematical structure of 11 provides the first step towards the goal reducing the computational complexity of much further 2 formulating hardy s function into generalised quadratic gauss sums to make progress one must first appreciate how the relatively simple approximation 11 arises from the exact representation 10 for all the key details are discussed in 27 appendix a so what follows is only a brief summary kummer s function 3 4 i 2 3 2 2 i 4 can be expressed in terms of euler s integral formulation 28 eq 4 2 1 3 4 i 2 3 2 2 i 4 3 2 3 4 i 2 3 4 i 2 i 2 4 exp i 2 1 1 1 4 1 0 1 2 2 2 12 2 3 4 4 2 4 1 16 1 4 1 24 2 4 i 2 4 exp i 2 1 1 1 4 18 1 0 after applying stirling s asymptotic expansion to both complex gamma functions substituting into 10 gives i 8 25 1 4 i 2 4 exp i 2 1 1 1 4 1 0 2 1 19 the integral in 19 can be split into two and written as i 2 4 where i 2 4 exp i 2 1 1 1 4 1 2 0 exp i 2 4 1 i 2 1 4 1 3 2 1 20 the second form of is obtained following the substitution 1 1 and provides the easiest means of estimating its asymptotic value 2 1 asymptotic analysis of the integral this section summarises the main points of 27 eq a 3 3 concerning the asymptotic behaviour of needed for the improvements of sections 2 2 4 the basic idea is to treat 20 as a contour integral let and devise an appropriate path that is equivalent to the real line 1 and then invoke cauchy s theorem this approach is valid provided the entire path lies in the re 0 plane where the integrand in 21 is analytic two suitable paths are shown in figs 1 a b one for and one for respectively writing i with 1 and 2 2 the phase of the integrand 20 is given by i 2 4 1 2 2 sin 4 2 1 2 cos 2 21 so if then 21 is negative 1 and 0 hence integrating along any path in this region starting from 1 and finishing at an arbitrary i 0 gives a negligibly small contribution to 20 on the circle the integrand is 7 4 which means the integral vanishes as the radius the consequence of these two observations is that for the only significant contribution to 20 along the path in fig 1 a will be the result of the integration on the small portion of the unit circle beginning at 1 but in 27 eq a 45 46 it was established that this contribution always takes the form i i 2 8 22 irrespective of the direction of rotation from 1 the value of the real number is immaterial because when 22 is substituted into i 2 4 the result is automatically zero so all the terms in 19 associated with odd integers can only be of in size and the sum of these is negligible in comparison with the terms this is the reason for commencing the summation at in 11 and 14 when the integration contour is as shown in fig 1 b one moves around the unit circle to the point i which again contributes nothing to 19 because of 22 and then along two mutually perpendicular lines the second of which crosses the real axis at the point given by 12 this defines the saddle point of the integrand s 20 phase and it is at this saddle point when the overwhelming bulk of the integral is concentrated using standard saddle point methods 2 one finds that i 4 2 3 4 i 2 4 1 2 1 1 1 1 i 2 4 i 2 1 1 1 4 1 0 i 8 3 44 cos 2 4 1 2 8 1 1 1 1 23 a b substituting these results into 19 along with the simplification an exact consequence of 12 25 4 1 4 1 4 1 4 1 2 2 2 2 1 4 24 gives 11 the case when where 1 6 is discussed extensively in 27 eq a 3 6 but is not relevant for this investigation 2 2 combining together collections of integrals that form the sum 19 for the fact that the sum 19 effectively commences at 2 rather than 1 provides a means of stripping away the computational redundancy inherent in the formulation 11 for example suppose we have two neighbouring terms at 1 1 and 2 1 where is the corresponding even integer between them in what follows the terminology pivot integer will be used for then it straightforward to show that 2 1 i 2 4 exp i 2 1 1 1 4 1 0 1 2 1 i 1 2 4 exp i 2 1 1 1 4 1 0 2 i 2 4 exp i 2 1 1 1 4 cos 2 isin 2 i 4 1 0 26 now because so that integral 26 for the two combined terms has much the same asymptotic properties as that of 19 for a single term the main phase is of 2 whilst the terms in involve phases of o which change relatively slowly in comparison hence estimates for integral 26 are computable using very much the same methodology as described in section 2 1 and of course it is only the cosine term in 26 that really matters since the amplitude of the sine term is at least a factor smaller pairing off a couple of neighbouring terms in this way is fine but computationally this makes only a small difference to operational count necessary for the evaluation of 19 but suppose one is more ambitious and attempts to pair off a whole collection of such terms 1 3 around a single pivot where is some as yet undetermined odd integer fixing the collection size the equivalent expression to 26 for such a collection is given by 2 i 2 4 exp i 2 1 1 1 4 cos 2 i sin 2 i 2 4 1 0 1 27 now provided 2 2 is satisfied then all the remarks concerning 26 for the case 1 should still apply to 27 for 3 5 as well to see if this supposition is true one must examine a generic integral of the type 27 in more detail 2 3 estimation of an upper bound for the collection size the first step is to rewrite integral 27 first by dividing the range of integration into two and then substituting 1 for the integral over the range 1 2 1 which gives i 2 2 4 exp i 2 1 1 1 4 cos 2 i 2 4 1 0 28 a b i 2 2 4 exp i 2 1 1 1 4 sin 2 i 2 4 1 0 making the further substitution 1 1 0 1 2 cf eqs 19 20 one obtains exp i 2 4 1 i 2 1 4 1 3 2 1 cos 2 1 i 2 4 1 exp i 2 4 1 i 2 1 4 1 3 2 1 sin 2 1 i 2 4 1 29 a b these two integrals can be estimated using similar methods to those used to obtain 23 a for but as will be seen there are some important differences associated with the error terms the integration contour is the same as fig 1 b the contribution to 28 originating from integrating around the unit circle remains zero as explained under 22 hence the only significant contribution to 28 from 29 arises from the integration through the saddle point at associated with the most rapidly varying 2 and terms of the exponential phase by contrast the phase terms in and 2 vary relatively slowly near since 2 2 integrating along the contour i 4 where 1 2 gives the following expression for the case for is essentially identical i 4 2 2 1 2 i 2 4 1 i 4 cos 2 1 i 4 30 setting 0 in 30 recovers in 30 the real coefficient 2 1 4 2 1 0 3 i 2 4 3 2 2 2 2 2 2 1 8 1 2 2 2 1 3 4 where 2 4 1 2 1 2 2 2 1 2 2 2 4 31 these expressions 31 were derived in 27 eqs a 49 50 a 50 b which also lists the values of 3 but these are irrelevant to the argument that follows now in general if and 1 then and it is this term that dominates the behaviour of the derivatives of in particular one would expect 0 0 and 0 0 0 2 2 however since both 0 0 as a consequence of the definition of given by 12 and 0 0 by differentiation it means that both 0 and 0 are terms of 1 just like 0 itself in fact it is easy to show from these results that 0 2 0 expanding the term in square brackets in 30 around the point 0 gives the following i 2 4 1 i 4 0 i 2 4 1 0 i 2 i 2 4 1 4 1 2 2 2 32 which puts 30 in the form i 4 0 i 2 4 1 2 2 1 2 cos 2 1 i 4 i 4 0 i 2 i 2 4 1 4 1 2 o 2 2 2 1 2 cos 2 1 i 4 33 the next step is to find an approximation of the first integral in 33 with a prescribed relative error bound consider the following generic integral defined by 2 i i 4 with 0 and 0 34 the following assignments makes the integrands in 33 34 identical 2 1 4 2 1 2 2 1 1 35 expansion of the term i 4 1 1 1 i 4 i 2 3 3 in 34 gives 2 i 1 i 4 i 2 3 3 36 now define 1 1 2 1 6 where max in 27 and employ the assignments 35 then the term 3 4 3 4 3 2 0 041 and the exponential of this factor will be very close to unity now suppose one could fix so that when the 2 i i 4 0 then the terms 3 3 in 36 could be ignored because they would only become significantly different from unity in the region when the value of 2 i i 4 would render the integrand a totally insignificant in magnitude subject to this supposition could then be approximated using the standard result i 3 2 i i 4 2 i 3 i 2 2 4 4 3 37 utilising 15 eq 3 323 2 with the proviso 3 if this proviso were satisfied then one can define 3 1 as a small parameter in which case 37 itself could be written in the form i i 2 2 4 4 i 2 2 4 4 o 2 1 2 o 2 38 studying equation 38 carefully one is struck by the fact that if all the terms involving are indeed small the term inside could be written as 1 this would leave relatively simple expressions for both and the integral required for 28 a b together with 35 the term inside would indeed lie close to unity provided 2 2 4 4 3 3 4 2 7 3 3 4 2 7 3 2 1 2 2 3 5 1 2 1 2 3 1 6 2 3 5 1 6 39 is chosen as a suitable upper bound on the value of 2 4 the upper bound on the collection size its implications and validity at first glance the surprising feature of 39 is the sheer scale of this upper bound here are some simple observations if 1 1 1 1 then 1 6 whilst if 1 2 1 2 1 and 1 6 1 3 hence 39 suggests that provided then pairs of integrals which make up the terms of 19 can be first be combined as in 27 and then estimated to some prescribed accuracy say of using the very much simpler approximation given by the leading part of 38 taken together all these estimates form a collection of at least 1 6 in size characterised by a common fixed pivot integer the implications of this last point regarding the sum of such a collection 27 will only become clear in the next section but given all the assumptions made in section 2 3 to reach this point are such large collection sizes truly feasible consider the following proposition let satisfy the following approximation a precise definition of will be prescribed in section 4 2 2 when assessing the computational aspects of the proposition 1 3 2 1 2 3 1 6 2 3 5 1 6 1 40 here is a small parameter which for the moment will simply prescribed to satisfy 0 but do so more slowly than any power of with 0 with this prescription for backtrack and consider the validity of all the various assumptions that were necessary to formulate 31 39 in the remainder of this section let 1 constant the first necessary condition for 38 was that 3 1 using approximation 40 one has 3 3 1 3 16 2 2 1 1 3 o 1 3 1 6 1 4 when 1 2 2 1 4 41 o 1 3 otherwise and hence 1 across the entire range of possible pivots this means that the proviso 3 is satisfied equation 40 also implies that 2 2 4 4 3 3 4 2 7 from 39 which means in 38 the term 1 the variable 1 1 2 1 6 1 introduced after 36 satisfies 1 1 9 21 4 2 15 4 2 1 2 9 o 1 9 2 9 1 6 when o 1 9 2 11 9 otherwise 2 i i 4 o 2 9 5 18 5 12 when o 2 9 5 9 otherwise 42 since this real part is 0 the requirement that integral 34 is negligible outside the range will also be met taken together these results demonstrate that the supposition regarding exclusion of the terms 3 3 from 36 is justified and one can write i i 2 2 4 4 1 i 2 i 2 2 2 1 2 1 1 1 43 with and replaced by 35 and hence the first integral appearing in 33 can be approximated by 2 2 1 2 cos 2 1 i 4 2 2 9 5 9 i 2 2 2 1 2 1 1 cos 2 1 44 the final step is to show that the estimate 44 for the first integral in 33 dominates the second integral appearing in 33 the latter is clearly identical to the former except for an extra factor of in the integrand and so one would expect it to be much smaller than 44 however the second integral is also multiplied by a factor of magnitude 2 4 1 2 the 2 factor is smaller still and does not alter the conclusions which has the potential to outweigh the reduction in the integral itself all the analysis discussed in deriving approximation 44 remains applicable so the resulting generic analogue of 37 satisfies 1 i 3 2 i i 4 2 i i 4 2 2 3 45 this is obtained from the exact result 2 2 2 3 exp 2 4 2 which is the derivative w r t of 15 eq 3 323 2 so the term involving the second integral in 33 differs in magnitude from 44 by a factor 2 4 1 2 2 2 3 3 2 2 2 1 1 2 3 3 2 2 2 1 1 2 1 4 46 using 35 and 40 hence the second integral in 33 is smaller than the first integral by at least a factor of 4 for all this is no more significant than the correction already accounted for in 44 one could continue the expansion of 32 to higher powers of but this only gives rise to leading order corrections to the integral of at most 4 which are insignificant compared to the corrections already obtained hence using 44 46 and the value of 0 found from 31 one finds i 4 i 2 4 1 i 2 1 4 1 3 2 i 2 4 1 i 2 2 2 1 2 1 1 cos 2 1 2 3 4 1 1 exp i 4 2 1 2 1 1 i 2 cos 2 1 47 provided this approximation for provides a similar estimate for integral defined by 29 b except the cosine term is replaced by a sine term substituting these results into 28 and noting that because is even and is odd the term i 2 4 i 4 one obtains upper terms cosine lower terms sine i 2 2 4 exp i 2 1 1 1 4 cos sin 2 1 0 i 8 3 4 1 1 4 cos 4 i sin 2 4 1 2 4 1 2 8 cos sin 2 1 48 note that when 0 one automatically recovers 23 although the relative error in 48 is as highlighted after 29 much larger since it arises from estimates at 2 5 composed in terms of generalized quadratic gauss sums using approximation 48 for the integral 28 one can now consider the sum of such integrals as prescribed in 27 now there is one small caveat to the results established in 2 2 2 4 the definition of given by 40 stipulates that 1 now if then this may not happen for instance if 1 2 2 1 4 this means that one must have 1 16 to ensure that 1 depending upon how is actually defined there are a arbitrarily large number of for which 40 is not satisfied for values lying in this range the integrals making up the terms of in 11 cannot be collected together and estimated asymptotically in the manner described above without producing relative errors greater than however this is not a serious problem because the range forms only an infinitesimal proportion of the total range of in 11 hence all one needs to recognize is that there exists a constant to be denoted by 1 16 as will be discussed later it is computationally expedient to set very much larger than this minimum value for which the terms of 11 associated with can only be be added together one by one rather than as amalgamative collections of length with this caveat one can now substitute expression 48 into 27 n b note the additional factor of 2 in front which in turn can be utilised in 19 the sum in 19 is cut off beyond 2 and then continued analytically as in 10 11 this gives 2 2 cos 2 1 2 8 2 2 1 4 1 1 25 1 4 8 3 4 1 1 2 2 1 1 49 here cos 2 4 1 2 4 1 2 8 cos 2 and the corresponding term with cosine replaced by sine now since 1 3 and o the relative error brought about by the term in 49 is no more than 1 3 substituting 12 for the amplitude in the second sum and incorporating identity 24 one arrives at 2 2 cos 2 1 2 8 2 2 1 4 1 1 4 2 2 2 1 4 2 2 cos 2 1 2 8 2 4 1 cos 2 1 1 50 note the step size for the first sum in 49 50 is 2 2 so as the scale of increases increases too and the gaps between the pivots widen the number of pivots is simply 1 3 effectively this means the single sum 11 carried out from to has been rewritten as double sum 50 with total summands 1 3 to so apart from significantly increasing the relative error from 1 to seemingly nothing of any significance achieved but this is not the case the sum over the collections 1 3 in 50 can rewritten in the form 1 2 re i 1 1 2 8 i 2 4 1 i 4 i 4 1 51 at each pivot define 4 1 1 1 1 2 8 and fix 1 2 substituting 2 1 for into the above with 0 1 2 a little algebra transforms 51 to 1 2 re i 1 1 1 2 1 4 i 1 1 1 2 1 4 52 where 1 i 2 2 i 2 0 1 i 2 2 53 and is the standard notation for the generalised quadratic gauss sum the prime on the summation sign indicates the first and last terms are halved 0 1 0 1 2 hence the two additional endpoint terms in 53 substituting 53 into 50 gives 2 2 cos 2 1 2 8 2 2 1 4 1 1 2 2 2 2 1 4 re i i 4 4 1 54 where 1 1 and 2 4 it is now apparent that there is an asymptotic if 0 as prescribed structural connection between the hardy function and a whole series of sub sequences based upon pairs of generalised quadratic gauss sums each of length 1 2 corresponding to half the collection size a long established principle of such gaussian sums is that their evaluation involves a computational process of only in complexity see below not the as might be assumed from the number of summands given that the number of pivots in 54 is 1 3 one is led to the tentative conclusion that a single arbitrary evaluation of is potentially an 1 3 2 computational operation for the problem now is to construct an algorithmic scheme which can be formulated into a suitable code for the estimation the performance of which comes as close as possible to realising this conclusion in his paper 18 hiary after re formulating some original ideas published in 41 reached very much the same conclusion however in his work the connection between and quadratic gaussian sums was established directly from 8 rather than 11 by performing a truncated taylor expansion of the logarithmic part of the phase about some suitable pivot integer up to the quadratic terms the scaling factors in the associated error term introduced at this level of truncation means that the resulting sub sequences of gauss sums can extend to a length 1 6 comparable to the collection size utilised here so in retrospect the scale on the upper bound predicted by 39 is not so surprising as the formulations are analogous to one another 3 efficient evaluation of generalized quadratic gauss sums the topic of quadratic and higher order gauss sums has been the subject of study by many authors over a long period of time the paper of 3 gives a comprehensive review of the many of the main results restricting the discussion to quadratic gauss sums the most famous result is that of gauss himself who first proved that for any positive integer 1 2 0 2 i 2 1 0 1 i 1 i 2 55 many alternative proofs were established later by direchlet kronecker schur and others this result was subsequently extended by 37 for rational where and are relatively prime to give 0 i 4 0 56 relatively simple proofs of both results 55 and 56 are presented in 30 this in turn led by utilising the methods of contour integration to the discovery of the remarkable quadratic reciprocity formula 40 for generalized quadratic gauss sums 1 2 i 2 4 1 2 57 where and are integers with 0 and even expression 56 is essentially a special case of 57 with 0 which is known as schaar s formula 3 3 1 a quadratic reciprocity approximation for a generalized quadratic gauss sum expressions 55 57 apply for quadratic gaussian sums with restricted to certain rational values so they cannot be utilised to compute the sums which appear in 54 because are usually irrational however a more recent paper by paris 33 provides a means of computing generalised quadratic gaussian sums for essentially arbitrary parameters the method based on the abel plana form of the euler maclaurin summation formula discussed in 32 section 2 10 produces an asymptotic expansion for in terms of another shorter quadratic gaussian sum an approximate quadratic reciprocity relation and a series of terms based around the complementary error function 32 chap 7 in the crucial result is theorem 2 of 33 which is reproduced in full below theorem paris let be quadratic gaussian sum 0 1 1 2 1 2 i 4 2 and i 2 then satisfies the following asymptotic approximation i 2 i 2 1 i 2 1 2 i 1 0 i 2 2 erf erf sgn 2 2 2 erfc sgn 2 58 here erf and erfc are the error and complimentary error functions respectively and sgn is the signum function the on the first sum denotes the last term is halved if 0 this sum vanishes if 1 if 0 then the erfc term also vanishes the coefficients in the second sum are defined by 1 2 1 0 1 2 1 2 59 if 0 then however these coefficients are singular at all non zero integer values so for 0 the corresponding regularised coefficients have to be substituted into 58 these are defined by removing the singular values of to give 1 2 1 1 2 1 0 60 the second sum in 58 is not actually convergent because 1 2 factor ensures that the terms grow without bound so this sum must be truncated at some small integer 1 much as stirling s series for the logarithm of the gamma function must be truncated when the remainder terms stop decreasing finally the remainder term are comprises the following with 1 2 2 2 2 where 1 2 1 61 n b there is an additional 2 factor in 33 eq 2 10 1 which appears to be inconsistent with the earlier eqs 2 2 2 4 5 of that paper this is corrected for here by the inclusion of the factor 2 in the bound for this point is addressed again in section 4 4 2 where the remainder term is subject to greater scrutiny the function has a maximum value when 1 2 and 0 is rounded to nearest even integer for half integers values given by 2 2 1 2 2 2 3 2 1 0 62 for all 1 this result is readily established from the integral representation of the lerch function see 15 eq 9 556 the integral in 62 dies off rapidly giving successive maximum values 1 8 2879 2 32 2895 3 128 1207 and 4 512 0525 which approach 2 2 1 as gets large now if 1 2 in 58 then the is at its smallest when 2 however if on average 1 4 then the is at its smallest when 3 providing a suitable cut off value the technical details of the proof of this theorem are presented in 33 along with some illustrative calculations from a computational point of view the importance of 58 lies in the recursive structure of this approximate quadratic reciprocity formula it means that any can be written in terms of another shorter quadratic gauss sum plus a few correction terms so as 33 points out although postulated much earlier by 17 repeated application of 58 together with the utilisation of simple symmetry properties inherent in all such sums will potentially allow the evaluation of in far fewer computational operations than the incurred simply by the summation of separate terms to bring this out it is useful to rewrite equation 58 in the form i 2 1 63 where i 2 2 1 2 i 2 1 erf erf sgn 2 2 2 erfc sgn 2 i 2 1 2 i 1 0 64 n b the factor i 2 vanishes if 0 or 1 hence the appearance of the heaviside step function 1 2 if 0 the alterations discussed in 59 60 still apply the aim now is to construct a recursive algorithm based on 63 which will reduce the length of the gauss sum quickly from 1 to 1 1 2 by formulating successive pairs of coefficients until reaches some small 1 termination value the best way to achieve this is to exploit the symmetry properties inherent in and shift the range of from 0 1 as specified in 58 to 1 2 1 2 and then apply 63 64 for 0 1 2 with 1 the length of the gauss sum can be reduced from 1 to 1 1 2 in at most 2 iterations however the cost of computing successive correction values of down to a value of 1 1 2 for instance would be rather inefficient a much better strategy is to estimate the average computational cost of a single valuation in terms of basic arithmetical operations link this operational count to the equivalent cost of computing a gaussian sum of length and then terminate the iteration procedure when 1 termination would then occur after at most 2 1 iterations the value of will be a constant in practice 10 100 because the cost of computing of comes down to the cost of computing the various error function terms which are essentially independent the factors influencing the best choice of termination constant are discussed in section 3 6 having computed the successive 1 1 1 parameters for 1 one initially computes 1 1 1 exactly and then repeatedly applies 63 to reach an estimate for the sum 1 1 1 actually desired so to achieve a workable and reliable algorithm one needs to devise a specific iterative scheme for the calculation of 1 1 1 parameters establish bounds both for the value of and the resulting error in the estimate for 1 1 1 3 2 a recursive scheme for the efficient computation of a generalized quadratic gauss sum in what follows let 1 standard quadratic gauss sum and 1 its complex conjugate consider the calculation of where initially are just an arbitrary pair of real numbers let 1 and define 1 1 2 1 2 now if is even one can simply substitute 1 for in the sum but if is odd an extra quadratic factor i 2 is effectively introduced into the exponent of all the summands of this can be dealt with by replacing the quadratic factor by a corresponding linear factor since i 2 1 i 2 2 which can be achieved by adjusting the value of 1 by an appropriate half factor at the same time one wishes to be able to utilise theorem paris which specifies that 0 and 1 2 1 2 which can be easily achieved by defining 1 sgn 1 1 1 4 1 1 2 1 2 65 and the choice of is always made so that 1 lies inside the range 0 1 2 then 1 1 1 1 1 1 1 2 1 2 66 where 1 sgn 1 now from 63 one can see that the quadratic parameter of the primary gauss sum is replaced by a factor 1 in the secondary gauss sum the change of sign can be easily handled by incorporating a switch from a standard complex conjugate gauss sum into the recursion procedure this suggests computing a hierarchical chain of primary and secondary gauss sums linked together formulaically 1 1 1 1 1 if 1 0 1 1 1 1 1 if 1 0 1 67 to keep track of the sign changes introduced into the linear parameter by this procedure it is easiest to switch from depending upon whether 0 so starting from 66 one can apply the following algorithm to estimate 3 3 an algorithm for the recursive computation of a generalized quadratic gauss sum using theorem paris algorithm qgs replace by 1 1 1 1 1 1 and 1 defined by equation 65 above fix the parameters 10 with a small positive integer while compute for 1 2 1 1 1 1 1 2 1 2 1 1 sgn 1 1 1 1 1 1 4 1 0 1 2 provided either i 1 10 or ii 3 10 1 compute 1 1 1 1 exactly if 1 0 1 1 then 1 1 2 if iii 3 10 1 compute exactly from this starting point compute for 1 1 the following iterations i 2 1 1 1 sgn 1 the final iteration 1 1 1 1 gives an estimate to 1 1 1 1 where is the error term the magnitude of this and the corresponding relative error denoted by are discussed in section 3 5 note that although each iterate 1 2 1 2 the algorithm invokes theorem paris at step 4 for which conforms with specified condition that 0 1 obviously if ever happened to be exactly zero one would terminate the algorithm immediately and compute the corresponding linear gauss sum as a geometric series in step 3 an upper bound on the size of 1 10 is enforced where realistically possible although theorem paris still gives a valid approximation for gaussian sums with 1 10 it is in nature an asymptotic approximation most suitable large summation values consequently it is better to avoid commencing with an initial sum of length 1 10 as this can produce a relatively large error in the first iteration which then percolates through into a relatively large error in the final answer provided is not too large say 3 then it is better to commence with an exact calculation of the initial sum with length sometimes this bound is impossible to enforce as when the final iteration jumps from a potentially very large 3 to a very small 1 then commencement with a very small initial sum becomes unavoidable the algorithm requires a storage capacity of 2 bits for the contents of the arrays 1 and 3 4 sample computations using algorithm qgs in order to get an idea for the workings of algorithm qgs it is worth performing some sample calculations of quadratic gauss sums the issues highlighted will help to estimate its computational efficiency and accuracy all the following calculations of are for 0 129901233 this is an unremarkable 9 digit composite integer large enough to highlight the main features of algorithm qgs but not so large to make an accurate term by term check computation of 0 a really time consuming computational exercise values for the termination constant 20 and cut off integer 3 were also used throughout case a 0 with 1 45 0 149071198 and 1 23 71 0 43083951 the results of applying the computational algorithm for this sum are presented in table 3 4 a at the top of each table are shown the respective calculations of from steps 1 and 2 underneath are shown the respective iteration calculations of the intermediate gaussian sums starting from the exact initial value for the sum over 1 until a final estimate for 0 is reached steps 3 to 5 next to these values are the corresponding term by term calculations accurate to the number of decimal places shown in the table the associated absolute and relative errors in these estimates are also shown in this first example 1 is chosen to be a quadratic irrational which means by the euler lagrange theorem see for example 8 and references therein that it possesses a positive continued fraction representation that eventually becomes periodic in this instance 1 45 0 6 1 2 2 2 1 12 where in the standard notation 0 1 2 3 4 0 1 1 1 2 1 3 1 4 1 3 1 4 1 1 2 3 1 68 and 3 4 denotes the composition of the periodic sequence of positive partial quotients this means that the quadratic sum coefficients 2 3 4 will also demonstrate periodic behaviour with a period less than or equal in length to that of the continued fraction representation of 1 the properties of the 2 3 4 can be deduced from their nearest integer continued fraction representations because they themselves represent differences from a nearest integer these nearest integer representations can be computed quite easily from any positive continued fraction representation of 1 1 2 1 2 by employing the following procedure 1 move along the representation 0 1 1 2 or 1 1 2 if 1 0 from left to right and whenever a 1 is encountered replace it by a 0 and add one to the two adjacent digits either side so 0 6 1 2 2 2 1 12 becomes 0 7 0 3 2 3 0 14 2 from the initial zero again move along the representation left to right on encountering the second zero change the sign of all the subsequent non zero digits bracketed by zeros two and three between zeros three and four leave the signs alone but between zeros four and five change the signs again and repeat this pattern then concertina out all the zeros except the first which are irrelevant so 0 7 0 3 2 3 0 14 becomes 0 7 0 3 2 3 0 14 0 7 3 2 3 14 which is the nearest integer continued fraction representation of 1 45 notice that in any nearest integer continued fraction the partial quotients satisfy 2 the procedure can also be applied to non periodic positive representations e g 1 1 1 2 7 15 1 292 1 1 1 3 1 14 2 0 3 7 16 294 3 5 15 3 or 2 1 2 1 1 3 4 1 2 2 1 2 the nearest integer continued fraction representations of the subsequent 2 3 iterates can now be read off by multiplying by sgn 1 deleting 1 and shifting the remaining one place to the left so 2 becomes 1 etc in this case 2 0 3 2 3 14 3 0 2 3 14 3 4 0 3 14 3 2 and 5 0 14 3 2 3 before the pattern repeats itself the period length of four is a result of two zeros being concertinaed out of the original period six pattern exhibited by the positive continued fraction for 1 the values of 2 5 are shown in their quadratic irrational forms in table 3 4 a by contrast the iterates display no such pattern and essentially form a sequence of pseudo random numbers although as 1 is also a quadratic irrational it seems likely a periodic pattern might eventually establish itself the sequence terminates after 11 iterations when 12 22 from the exact calculation of 22 1 12 12 one achieves an estimate for 0 in a cpu time 10 2 sec compared to 103 sec for the corresponding term by term computation note that computation of the latter is highly prone to round off error arising from the computation of the angular phase so it essential that the values are stored to an accuracy of at least 3 10 digits special care must also be taken when naively applying the iteration scheme for the as it is numerically unstable and insignificant initial errors build up rapidly after a few repetitions if were an integer consisting of millions of digits the indirect algorithm of 38 devised for computing large scale continued fraction representations could be utilised to calculate the values speedily and accurately for the very much more modest calculations presented here it is sufficient to apply algorithm qgs as it stands with computed to an accuracy of 30 digits notice how the magnitude of the error builds up after each successive iteration but only does so in conjunction with the overall size of the sum this is illustrated by the behaviour of the relative error which after some initial fluctuations stabilises the basis of this result is due to the relative sizes of the terms appearing in the approximate quadratic reciprocity formula 63 at the th iteration the intermediate gaussian sum is 1 1 2 whilst the additive factor 64 is no more than 1 2 and so is much smaller hence any error in the correction term quickly becomes negligible in comparison with the error in the intermediate gaussian sum as both the intermediate gaussian sum and its error are subsequently scaled by factors of 1 2 the relative error plateaus to a constant after just a few iterations see next section for example multiplying 18 946 the final error in table 3 4 a by 1 45 1 4 gives 7 3150 the error in the final intermediate gaussian sum likewise 7 3150 2 3 9514 etc a pattern that only starts to break down when 8 is reached case b 0 with 1 0 13474402 and 1 0 36787944 the results for this calculation are shown in table 3 4 b in this instance the values of are not quadratic irrationals and so their corresponding positive continued fractions representations are non periodic hence the corresponding iterates form a series of pseudo random numbers never conforming to any pattern again as in table 3 4 a the sequence terminates when 11 with 12 60 on this occasion this is an example where the next iteration would give rise to a relatively steep drop from 12 60 to 13 1 as 12 0 01506 1 12 since the computation of the exact initial sum of length 12 60 3 with no error can be achieved almost as quickly as an estimate derived by iteration from a sum of length 13 1 it makes sense to terminate step 2 of the algorithm at this point the average number of iterations required by the algorithm is related to the growth of the product of partial quotients 1 2 of the nearest integer continued fraction of 1 for the positive continued fraction the behaviour of this product for large has been known since 24 see also 36 and 1 who proved that for generic irrationals 0 1 2 more specifically those irrationals that have unbounded partial quotients which follow no specific pattern so excluding countable subsets of such as the quadratic irrationals and numbers like 1 2 1 k 1 1 2 2 1 2 685452001 69 where k is khintchine s constant this result is based on the fact that in the limit of large the number of occurrences of the positive integer in the positive continued fraction representation of follows the gauss kuzmin distribution 25 26 see also 8 1 1 2 2 1 2 70 the above formula can be used to approximate the corresponding distribution for nearest integer continued fractions the conversion procedure from positive to nearest continued fractions outlined above removes all the 1 digits and converts most of them to zeros not all because a small proportion become 3 and an even smaller proportion convert to 2 assuming that all 1 digits were to convert to zeros entirely the partial quotients of a nearest integer continued fraction would satisfy 1 1 2 3 2 2 3 1 2 1 k 2 3 2 5 41265167 71 hence the average number of iterations needed by the algorithm qgs to reduce a gaussian sum of length to a length starting from a generic irrational should be about where 1 5 41265 0 592 to a first order approximation a more precise calculation of for nearest integer continued fractions gives a figure of 5 3081 when 0 59907 for the sum of length 0 129901233 and 20 computed here this gives a termination value of 9 which agrees well with the value of 11 found for 1 the behaviour of the error estimates presented in table 3 4 b are also very similar to those shown in table 3 4 a the error grows with each iteration but as before the relative error after some small initial fluctuations settles down to a constant value the scale of the final relative error differs little from that of the initial value case c 0 with 2 10 0 141213562 and 10 71 0 375293312 the two previous cases illustrate that the final relative error in the estimate of a gaussian sum of this length is typically of 10 3 however it is quite easy to find values for which the algorithm gives more precise results if is chosen to be to the quadratic irrational 2 10 0 7 14 all the following coefficients 2 3 0 14 and the length of each intermediate sum is reduced rapidly by a factor close to 14 bringing about termination after just 6 iterations here the initial intermediate sum computation produces an estimate which has a relative error of only about 10 6 rather than the 10 3 found in the previous examples the size of this initial error is due to the fact that the value of 0 071 is smaller than the corresponding 0 427 0 496 values found earlier which naturally reduces the error term 61 this comparatively small initial error percolates through the rest of the calculation to produce a final estimate still accurate to six significant figures this example illustrates that the accuracy of the gaussian sum estimate will be improved if it turns out that the algorithm terminates immediately after a relatively small value in one off calculations it could be worthwhile to allow small changes in to achieve this case d 0 with 0 3326133909287256850174 and 1 2 0 183939720 case e 0 with 1 2 0 2 and 1 0 in both these cases the initial value of is a non generic irrational specially chosen so that its nearest integer continued fraction representation has a large partial quotient relatively early in its expansion in case d 0 3 154 275596610848 8 3 2 and in case e 0 2 2380080351076487 3 more specifically a quotient satisfying occurring within the first 0 59 terms or so of the expansion statistically the chances of encountering such a large partial quotient so early in the expansion are very small since the probability that over such a small range is only 1 when and such occurrences become more likely e g case b above nevertheless it is a possibility and the behaviour of the algorithm under such scenarios is interesting in case d the presence of the very large quotient 3 275596610848 0 causes the algorithm to immediately terminate table 3 4 d after just 3 iterations notice too that the first intermediate sum does not scale up as 1 as in the previous cases this is because the additive factor sgn given by 64 approaches 1 2 times the multiplicative factor in 63 since 3 3 3 3 the two erf terms offset to 1 whilst the term involving erfc vanishes when 0 so the usual 1 scaling cancels out completely hence the first intermediate sum is very much smaller in magnitude than would be normally be expected given 3 280564 this small intermediate value percolates through the remaining iterations to give an unusually small value for the full sum much less than the behaviour seen normally case e is slightly different although as in case d the algorithm terminates very quickly table 3 4 e after just 2 iterations but on this occasion the 1 scaling is present which means that the final sum is very much larger than the standard behaviour the reason lies in the fact that although 2 2 is very small 2 is also close to zero and which means that 2 hence the cancellation effect seen in case d does not come about because the two erf terms no longer offset the final sum is as a consequence of the original 1 2 0 and so its magnitude lies close to the exact value 0 1 2 0 0 i 4 2 in both these examples the regular behaviour of the relative error seen in cases a c is disrupted because of the abrupt termination of the algorithm in each case the relative error commences at very small value see section 3 5 and does not get the chance to stabilise in case d the magnitude of the intermediate sums has not built up sufficiently to dominate the additive factors sgn and so stabilise the error if the same initial was used in conjunction with a larger value the regular behaviour of the relative error and the estimate for the sum size would gradually reassert itself in case e the effect of increasing would be offset by tighter approximation of 1 2 so the scaling and error disruption would remain features of the results 3 5 error bounds application of algorithm qgs gives an approximation to the original quadratic gaussian sum 1 1 1 1 with an error which must be bounded initially consider the simplest case when 1 is a generic irrational characterised by a lack of any large partial quotients early on in its nearest integer continued fraction representation specifically early on means a partial quotient within the first 1 0 59 iterations needed to reduce to a number less than so the initial sum computed at either steps 3 i or 3 iii of the algorithm is either of length 1 or respectively in practice that means most irrationals with expansions following gauss kuzmin statistics most quadratic irrationals unless they have a very large early in their periodic pattern and most irrationals with regular but non periodic expansions such as where the really large partial quotients are only encountered at positions but irrationals similar to the example discussed in cases d e above would be excluded both generic and non generic irrationals form uncountable sets however members of the former will tend to be much more common than the latter one can make a rough estimate of the probability that an irrational is generic in the specific sense set out above using the fact that almost all irrationals obey gauss kuzmin statistics choose values for and with construct an irrational 0 1 2 by means of a series of independent random selections of partial quotients satisfying distribution 71 then apply the constructed to reduce 1 2 in the manner prescribed in step 2 of the algorithm and cease reduction if the probability that this will occur 1 3 2 can be estimated from 71 if one further assumes that each iteration has up to this point reduced by an average factor 1 0 59 0 183 commensurate with the estimate that it takes 0 59 iterations to go from to then 1 in which case the probability that the constructed will be generic and the reduction process reaches satisfies 1 1 1 3 2 0 59 1 1 1 3 2 3 02 72 so if the termination constant 100 then this generic probability would be close to one if 1 is a generic irrational then the initial sum in the iteration process computed exactly at step 3 will be of length from this exact initial sum step 4 of the algorithm computes successive approximations to the various intermediate sums let represent the respective error term associated with the 1 iteration defined by i 2 1 1 1 sgn 1 73 for 1 2 1 if the initial sum starts at step 3 iii then replace by 1 in the above and what follows the first iteration when approximates the sum to within an error 0 the second iteration uses this approximation to estimate the next intermediate sum 1 1 1 1 to within an error 0 1 1 2 1 continuing in this vein one finds that the final error satisfies 0 1 2 1 1 2 1 2 1 1 1 2 1 1 1 1 2 1 2 1 74 since 1 2 the sum in the curly brackets of 74 will always be less than 2 2 1 3 41 and one deduces that 3 41 1 2 1 3 41 1 1 1 1 3 41 1 1 1 1 75 the last result comes about because and hence so one concludes that the modulus of the relative error will satisfy 1 1 1 1 3 41 76 each iteration error is given by 61 and the will never exceed the maximum modulus 3 1 2 1 2 15 4 if a cut off value of 3 is employed cf discussion after eq 61 hence one concludes that the final relative error cannot exceed and indeed must be very much less than 3 41 15 4 1 2 77 all the relative errors for the calculations shown in table 3 4 with 30 are orders of magnitude smaller than this rather crude upper bound what is clear is that the size of both the actual and relative errors are regulated by the choice of the case when 1 is a non generic irrational is only slightly different in such an instance the algorithm terminates immediately the small quadratic parameter 1 associated with the large partial quotient is encountered however because is so small the first source of error from 61 with 3 is given by 0 3 3 which is completely negligible compared to what comes next for example the error in the first intermediate sum computed in table 3 4 d is beyond the 9 significant digits shown in the display whilst in table 3 4 e the relative error only 10 10 these relatively high precision initial estimates mean that the main source of error in the quadratic gaussian sum arises from the error 1 in the second intermediate sum percolating through the rest of the calculation an upper bound for this error can be found in an analogous manner to the upper bound established in 74 77 the magnitude of the error satisfies 3 41 1 2 2 1 3 41 1 1 1 1 1 1 1 1 78 whilst the relative error is less than 1 1 1 1 1 2 1 1 2 1 2 79 so the actual and relative errors in the computation of a general quadratic gaussian sum are always smaller when 1 is a non generic irrational as opposed to a generic irrational by at least a factor of 1 3 6 the choice of termination constant the most appropriate choice of comes down to a trade off between greater computational accuracy large and computational speed small however if one is motivated purely to minimize the computational speed of the algorithm one can obtain a useful estimate for a lower bound on that can be employed for all here is any arbitrarily large integer and 1 is any irrational generic or non generic computing directly from 53 requires 4 basic multiplication and addition operations to find each phase 2 the value of 2 would be computed once and then stored and a sine and cosine evaluation to find each term there are terms and these must all be added together which required a further 2 complex addition operations so this gives a total of 2 6 operations where is the operational count to compute the sine cosine function for an arbitrary phase and is the operational count for any of the basic arithmetic operations however for any computing platform the operational count for a single basic arithmetic operation is at least or should be fifty times faster than for a single intrinsic sine cosine evaluation 50 hence to compute directly requires an operational count 2 12 when applying algorithm qgs the main computational problem concerns the evaluation of the appropriate term 64 at each iteration step this term contains three error function calls within it which are key to assessing its computational speed now each of these calls is of the form erf since erfc sgn 2 1 erf and hence arg 4 for every erf evaluation this being the case it is relatively simple to construct a purpose built erf routine which takes advantage of inherent symmetry properties that apply when arg 4 since is not restricted the routine would have to employ both the standard power series 32 eq 7 6 1 and asymptotic expansions 32 eq 7 12 1 valid for small and large respectively as well as adopting rapidly converging power series local to the range 3 4 9 4 when neither of the standard representations are particularly efficient all these series methods involve the evaluation of a 2 i 2 term plus the equivalent of a further operation to compute the various power asymptotic series that arise such a purpose built routine can easily be constructed so that the relative error lies below 0 5 for all however most modern mathematical packages such as matlab or maple contain an intrinsic routine built into their software designed to compute erf for arbitrary arg these routines are somewhat slower but much more precise generally requiring the equivalent of 9 to achieve an accuracy to within 10 significant figures the operational count for remainder of the term is easier to estimate the short sum over takes about 4 40 depending upon whether exact expressions cf definitions 59 60 0 cot 1 1 2 2 0 2 1 1 cot sin 2 1 3 etc 80 or power series expansions the exact expressions very prone to round off error if 0 2 0 3 1 2 15 2 4 315 1 3 1 5 4 2 63 4 75 etc 81 are used to compute the coefficients the remaining computation of and its amalgamation into iteration step 63 requires about 10 80 so with 3 the total operational count comes to approximately 22 200 26 plus the three necessary erf evaluations hence one obtains an estimate between 35 53 for the evaluation of depending upon whether a purpose built or an intrinsic routine is used to calculate erf suppose the algorithm were to be employed with termination constant 1 then the number of iterations required to truncate the initial sum size down to a value 1 1 1 would vary from an average value 1 0 59 up to an maximum value 1 2 1 1 44 in what follows it is useful to define 10 constant representing the mean value of operational count of where is the iteration count factor derived from 71 the absolute maximum of this quantity will be denoted by and representative values using an intrinsic erf routine of 0 59 53 and 1 44 53 will be adopted for calculation purposes so in the worst case scenario the algorithm would require to compute compared to the 2 12 needed for the direct term by term calculation the value of at which these two operational counts become comparable provides a suitable means of choosing in the worst case scenario solving 2 12 gives a value of 189 however if the average number of iterations is substituted then would satisfy 2 12 61 so in order to maximise the speed of the algorithm a prescribed value for 61 189 would be appropriate the round figure of 100 suggests itself which is equivalent to a value 0 87 53 values for 61 189 are motivated purely by the desire to maximize computational speed of the algorithm if more precise estimates of are required then would need to be made larger the corresponding 2 operational count would then be increased slowing the algorithm down 4 the computational complexity of the hardy function in the limit at the conclusion of section 2 equation 54 expresses as a sum of 1 3 subsidiary pairs of quadratic gaussian sums each of ever increasing length 1 2 given by 40 the algorithm developed in section 3 from the theorem paris demonstrates that a general quadratic gaussian sum of arbitrary length can be evaluated using an operational count no larger than 2 to within a relative error 1 2 for a suitable choice of 10 100 in combination these two results allow one to reduce the computational complexity of significantly below the of 8 or its variants this assertion is framed in the following theorems 2 3 the proofs of which are the subject of this section 4 1 theorem 2 let 0 and set 0 50 for any 5 0 6 0 define 8 28 and choose as a fixed parameter prescribed so that 0 and 1 for any 0 define and choose to lie in the range 0 0 2 1 6 where 55 4 1 48 2 0 127 fix 0 and let denote the odd integers in this range fix 2 1 3 and let denote elements of a subset of the even pivot integers within the given interval define by the following sum 2 2 cos 2 1 2 8 2 2 1 4 re i i 2 2 1 4 4 4 82 the various parameters of the sub sums that make up are given by or 2 2 1 1 2 1 2 1 1 1 2 4 4 1 1 2 8 1 2 0 1 1 3 1 83 a b 2 1 6 2 2 2 where 1 1 suppose the quadratic gaussian sums are estimated using algorithm qgs with termination constant i if 0 3 4 4 1 12 2 the calculation of requires only 1 3 computational operations in the limit ii otherwise if 3 4 4 1 12 2 2 1 6 then the calculation of requires computational operations 4 2 the cut off and operational conversions before embarking on the analysis necessary to prove theorem 2 a couple of points are worth elucidating with regards to the implications for estimating the computational complexity of itself equation 54 gives an expression for the hardy function of which forms only a part because the cut off set in 82 namely 2 1 3 is very much less than the cut off 2 in 54 the idea behind this choice of is to use 82 to estimate a portion of the terms that appear in the main sum of 8 and then compute the remainder directly structurally this will give rise to a second hybrid formula for analogous to 17 in that case the last 1 2 1 2 terms of 8 were replaced by the first 2 3 odd terms of 11 reducing the total summands by 17 16 but this proposed second hybrid formula is more ambitious since is representative of all but the first 1 1 3 1 3 terms of 8 the size of the relative error encapsulated by the term representative in the previous sentence is the subject theorem 3 the implication of theorem 2 is that the sum of the overwhelming majority of the terms in the rsf can be computed via in something close to 1 3 operations this is what lies at the heart of reducing the computational complexity of the hardy function the other point is a small technical issue concerning some conversions linking the standard sine cosine operational count with and the operational counts required to compute a typical rsf term and a typical principal term of the new series 11 respectively a typical rsf term including the amplitude consists of and evaluations so estimates devised from the maple mathematical package show that 0 62 and 0 23 so 1 85 which for simplicity will be rounded up to define a conversion relation 2 in 27 eq 72 an optimal method for computing 2 is presented based on the estimates above this implies that where 1 12 large scale sample computations of the two respective sums in 17 suggest this estimate is a little too low with 1 17 1 30 depending upon the programming scheme employed so for the purpose of analysing theorem 2 let 2 define a second conversion relation with lying inside the range given above 4 3 proof of theorem 2 4 3 1 operational count of first sum making up the first part of the proof is very straight forward assuming all the bounds concerning set out in theorem 2 are met then define 1 4 0 this particular choice of b is motivated by the desire that the number which commences the second sum making up is large enough to ensure that 1 as discussed at the start of section 2 5 this is a necessary pre requisite which allows the principal terms of the new series 11 for to be collected together into the quadratic gaussian sums of 54 in such a way that the relative error introduced is always with this choice of one finds that at the corresponding value of defined by 83 a will have reached 1 3 1 12 o 1 3 1 24 84 the bounds imposed on the definition of means that the 1 12 factor dominates 84 and 1 5 0 6 1015 computation of the first sum in 82 is then equal to 2 the number of typical principal terms times the operational count of a typical term using the conversions discussed in 4 2 this is equivalent to 1 4 2 2 1 4 operations 4 3 2 the precise definition of for pivots the next part of the proof is to establish the computations needed to evaluate the second sum starting from the first pivot integer where 1 to some suitable pivot where 1 1 constant termed the step up parameter at which point 1 1 specifically the value of is deemed suitable provided it is large enough so that at some point within the pivots the length of the quadratic gaussian sums exceeds the set value of in the range specified in theorem 2 when substitution of the algorithm qgs for their computation becomes applicable now equation 40 prescribes an upper bound on the collection size which ensures the process of combining the principal terms of the new series 11 into quadratic gaussian sums always yields relative errors however the upper bound 40 is a little awkward to deal with as the 1 factor causes to move rapidly through a number of different scale regimes as increases from unity these rapid changes in the size of must be carefully be accounted for in order to estimate the operational count suppose and 1 3 8 so that 1 with 8 1 1 1 if were to be defined using 40 then 2 1 3 2 3 1 6 1 2 11 2 24 2 1 1 3 1 85 it is tricky to keep track of rapidly changing scale of the small term in 85 so ideally one would like to forget about it by setting it to zero however because the small term is negative the resulting value of would then exceed the upper bound to avoid this problem is defined to equal 1 3 for in theorem 2 i e approximately one half of the upper bound 40 85 fixing in this way also ensures it never exceeds its upper bound even if lies considerably above unity for example if 7 4 10 15 then 83 a gives 0 72 1 3 1 6 1 59 1 3 1 6 the corresponding upper bound value 40 this means that the length of the quadratic gaussian sums is continually constrained to keep the relative errors for all in practical calculations see section 5 one prescribes a suitable fixed value of and then steps up the size of the values utilising the second definition given in theorem 2 which are computationally more efficient when is no longer close to however the suitability of is difficult to pin down precisely since it is determined by the respective values of and in theorem 2 so instead for the purposes of proving the theorem the parameter 28 is chosen and utilised in such a way as to guarantee is suitable for all possible choices of and whilst at the same time ensuring the stepping up always occurs before can grow beyond the 1 1 scale regime this is the reason behind the appearance of the 28 factor in as will become clear shortly 4 3 3 operational count of second sum of for pivots define the constant 1 12 3 1 08690 with let the interval be divided up into a series of slowly increasing subintervals or pivot blocks numbered 1 2 each block consists of 1 4 different pivots and associated with each block is a fixed step parameter this prescribes the lengths of the gaussian sums defined by 53 83 and means that within the block each pivot is separated from its two nearest neighbours by a factor 2 2 4 4 let 1 1 and 0 1 the choice of 1 1 is conservative but serves to smooth out the transition from the first to the second sum of starting at 0 2 compute the second sum 82 over block 1 which consists of the 1 4 pivots 0 1 2 0 3 2 0 5 2 0 2 1 4 1 2 the first even integer 1 bounding the pivots residing in block 1 is given by 1 0 2 1 4 1 1 0 4 1 4 86 now define the next step parameter for block 2 to be 2 1 block 2 will consist of the 2 1 4 pivots 1 1 2 1 1 2 2 1 4 1 2 1 the lowest even integer 2 bounding the pivots residing in block 2 is given by 2 1 2 2 1 4 2 1 87 the step parameter for block 3 is then 3 2 and the process repeated for all the subsequent blocks so for block one has step parameter 1 where 1 1 2 1 4 1 1 1 88 with pivots 1 1 1 1 2 1 4 1 1 next one needs to estimate some lower bounds on as 1 2 since 0 1 4 substituting 86 into 83 a gives 2 1 1 3 41 3 1 3 1 12 1 1 4 1 3 4 1 3 1 3 1 12 89 where 1 1 4 substituting 89 into 87 gives 2 1 4 4 1 4 2 2 1 4 2 1 3 1 3 2 1 3 1 12 1 2 2 1 3 2 1 3 1 4 1 1 3 1 3 2 1 o 1 4 90 dropping the 1 4 terms underestimates 2 so on substituting 90 into 83 a one obtains a lower bound for 3 2 2 1 9 1 9 2 1 3 2 1 3 1 12 1 1 3 this leads to an estimate 3 2 2 1 9 2 1 3 1 9 1 4 1 1 3 1 9 1 9 2 1 3 3 1 1 3 which to first order can be used to find lower bound for 4 etc the pattern for 1 2 1 that emerges is given by 1 2 1 3 2 1 3 1 12 1 1 3 1 9 1 3 1 1 3 1 1 91 this bound has important implications from the definition of prescribed at the start of this section one can compute 1 3 1 1 3 1 3 1 3 1 3 1 2 1 3 4 1 3 1 1 2 1 4 1 4 3 2 1 3 48 1 4 1 2 1 3 2 1 2 1 8 1 1 3 2 1 4 1 4 3 92 now consider the final block 28 which has step parameter if 1 in 92 the factor 3 1 factor is negligible in comparison with the other terms e g if 5 0 6 1016 then 9 3 1 0 00015 and 1 8 3 1 1 hence 2 1 3 1 6 55 1 4 48 2 1 3 1 6 0 254 1 3 1 6 93 theorem 2 specifies 1 for any 0 and so the factor 55 12 1 although potentially rather slowly theorem 2 also specifies that 1 3 1 6 so the implication of 93 is that there must exist a 1 1 such that 1 hence for those pivots lying inside blocks computation of their respective quadratic gaussian sums can be achieved using algorithm qgs each to within a relative error 1 2 by means of a significantly reduced operational count for example suppose 2 0 9 is chosen to be somewhat larger than the minimum value 5 0 6 specified in theorem 2 then substituting 2 into 92 gives 3 5 2 1 9 2 4 9 1 9 7 1 4 108 0 97 7 27 4 9 1 9 94 hence one could select so that it satisfies 0 7 27 4 1 9 2 3 in which case the value of 2 for still larger values of 2 0 12 it would be possible to select so that it lies between 0 2 from the bound on 2 given by eq 89 when would fall to unity in summary for any lying in the range specified in theorem 2 and if is restricted to values near the lower end of its prescribed range e g either 2 or 2 3 then approaches 1 or 2 respectively as if the lower bound 92 is substituted into 88 by the end of block one has reached a pivot value 1 4 1 4 2 1 4 2 2 1 3 1 2 1 2 1 8 1 1 3 1 1 2 1 4 1 4 3 1 1 4 1 4 2 3 4 2 1 2 3 8 2 2 1 2 1 4 1 8 1 3 1 3 2 2 95 now the vast majority of the values in 95 lie in the range 8 which means that the factor in square brackets 1 8 1 3 1 1 for the smallest values this does not hold but these terms themselves are completely dominated by those associated with large values close to hence an excellent lower bound estimate on the sum in 95 can be found from 3 2 2 3 2 3 2 3 2 3 2 1 3 2 3 2 1 3 log 2 42 14 7 2 3 2 1 8 3 2 1 96 so by the end of block one must have reached at least 1 4 1 4 14 7 2 3 4 3 2 1 with 1 14 7 2 3 4 3 2 1 1 2414 97 the factor 28 in the definition for 28 introduces the 7 2 term into this lower bound estimate for ensuring that the stepping up of the values occurs at a reasonable small 1 1 value numerical calculations for up to 101500 and appropriate choices of show that the value 1 2401 is usually achieved by the time and reaches 1 6 at since 97 gives only a lower bound estimate for this higher value is to be expected but it too remains well within the 1 1 regime the operational cost of all these calculations is as follows for block 1 1 1 1 0 so one effectively has two terms to compute for each of the 1 4 pivots at every pivot within blocks 2 3 one has to compute two quadratic gaussian sums of length longhand that is equivalent to 2 1 terms for each of the 1 4 pivots for blocks 1 computation of the two quadratic gaussian sums of length can be achieved to within a relative error 1 2 using algorithm qgs as discussed in section 3 6 this will take about 2 2 1 2 certainly no more than the maximum possible number of 2 2 1 2 one must also add in a further 2 to account for the calculation of the i 2 2 1 4 amplitude factors that appear in 82 so across all the blocks the average number of computations required will amount to 2 1 4 1 4 2 1 2 1 4 1 2 1 2 4 1 4 2 1 4 2 1 4 4 2 2 1 4 1 2 1 4 1 1 98 using the 2 conversion factor now it is just a matter of estimating the three sums in 98 the second sum is trivial to estimate since 1 1 1 1 9 9 4 1 12 1 3 1 0 99 where 0 1 the third sum is a bit more difficult because it requires an upper bound estimate for not yet established one can achieve this by demonstrating that the value of will certainly be bounded above by the lower bound 92 established for 1 to show this consider how the difference grows with from 88 one can see that to second order this difference is given by 2 1 1 4 1 2 hence to second order q v eqs 91 92 1 2 1 3 2 1 2 1 8 1 1 3 2 1 4 1 4 3 1 1 1 3 1 1 100 where 0 0 1 1 2 2 2 3 and 1 otherwise this means that for 2 the ratio 1 1 3 2 2 2 1 4 1 3 1 2 1 1 3 1 1 1 3 101 the ratio 101 is initially very small and then gradually increases approaching but never exceeding the value 1 as hence 1 1 08690 1 as postulated using this result one can now estimate an upper bound on the third sum in 98 as follows 1 1 lower bound on 1 1 2 1 2 1 8 1 2 1 4 3 1 4 2 1 1 3 1 102 since 3 the sums involving 3 are negligibly small the sums involving just are given by 99 which leaves 1 1 1 1 1 1 1 28 9 9 4 1 12 1 3 1 1 103 where 1 1 0 1 hence 1 1 9 9 4 1 12 2 1 3 1 28 1 0 2 4 2 0 104 finally one must estimate the first sum in 98 running from 2 since for large indeed 1 for 2 0 12 in which case this sum will vanish procuring an upper bound by simply replacing by 1 leads to a rather crude approximation which will vastly overestimate 98 if is small e g if 8 a much better estimate can be obtained by first modifying expression 100 for 1 to include corrections of all order of magnitude and then employing 1 1 from 88 one obtains for 0 1 2 1 3 2 1 2 1 8 1 1 3 2 1 4 1 4 3 1 1 1 1 1 3 2 1 3 2 1 2 1 8 1 1 3 2 1 4 1 4 3 1 3 2 1 0 1 3 2 1 3 2 1 2 1 8 1 1 3 2 1 4 1 4 3 3 2 3 2 1 1 3 105 hence the first sum in 98 is bounded by 2 1 3 2 3 2 1 1 3 1 4 2 1 2 1 8 3 2 2 2 1 4 2 1 2 1 8 1 3 2 3 2 3 2 1 1 3 2 1 3 1 4 2 1 2 1 8 1 1 3 3 2 2 2 2 1 3 1 4 2 1 2 1 8 1 1 3 2 3 2 3 2 3 2 1 4 3 1 106 substituting bounds 99 104 and 106 for the three sums into 98 one obtains an upper bound to the operational count necessary to compute the second sum of for pivots one finds that this computation requires no more than 2 1 4 2 2 1 3 1 4 2 1 2 1 8 1 1 3 2 3 2 3 2 3 2 1 4 3 1 9 9 4 1 3 1 3 1 1 0 4 1 0 2 2 28 1 log 4 0 107 4 3 4 the precise definition of for pivots in section 4 3 2 the precise definition 83 a for the step parameter based around 40 and 85 was motivated by considerations of the first pivot regime in which both and are close to unity from the estimates leading up to 97 in the previous section all the pivots in this second regime satisfy where 1 2401 this equates to a value of 3 894 which is sufficiently above unity to approximate the upper bound 40 for by the following 2 2 3 1 6 2 1 2 3 5 1 6 2 2 3 1 6 2 3 3 2 5 2 108 the simpler definition 83 b proposed for in theorem 2 is asymptotically similar to 108 since 2 1 6 2 2 2 2 1 6 2 1 1 2 1 6 2 1 3 2 5 2 109 however because the term 3 2 in 109 is positive as opposed to negative in 108 an extra factor of 2 1 3 is included in 109 to ensure it never exceeds 108 this condition is guaranteed provided 2 463 1 103 a fact already established by the analysis leading to 97 4 3 5 operational count of second sum of for pivots computation of this element of the sum proceeds in an analogous manner to that described in section 4 2 3 the interval is subdivided into more pivot blocks numbered 1 2 only in this computational region the number of pivots within each block is fixed at 1 4 at the end of each block the step parameter is increased this time regulated by equation 109 much as before the computation ceases when 2 1 3 the designated maximum specified in theorem 2 the first question to consider is the actual size of necessary to ensure continuation of equation 88 gives 2 1 4 1 1 2 28 7 3 1 3 1 3 1 1 110 the next step is to establish a lower bound estimate for the various step parameters 1 initially 1 2 1 6 2 2 2 2 1 6 2 2 1 1 4 2 1 28 7 3 1 1 4 2 1 111 where 1 1 28 7 3 1 1 4 2 for the next block 2 1 2 1 6 2 2 1 2 1 2 1 6 2 2 1 1 1 4 2 1 2 2 1 28 7 3 1 1 1 4 2 1 2 1 2 112 where 2 1 28 7 3 1 1 4 2 1 2 this pattern continues and one finds that 1 2 1 6 2 2 1 2 1 1 1 4 2 1 2 2 2 1 2 1 2 1 2 113 where 1 28 7 3 1 1 4 2 1 2 2 2 1 2 since 1 2 by the end of the th block one is guaranteed to have reached at least summand 1 this value of 1 is certain to surpass the value prescribed in theorem 2 for some satisfying 6 1 1 6 1 3 8 114 now the exact value of the parameter is dependent upon the precise definition of however it is possible to get some idea of its value in the limit as since the term in curly brackets will tend to unity in this limit 1 2414 see eq 97 and 28 3 1 1 1 7 3 1 1 4 2 log 1 0812 0 0781 which in turn 2 133 the numerical results mentioned earlier show that 1 6 2 the values of associated with these blocks are all sufficiently large for the associated quadratic gaussian sums to be calculated using algorithm qgs so the operational count of with will be similar to the operational count for blocks 1 when algorithm qgs was first utilised only on this occasion the number of pivots is fixed at 1 4 incorporating this change into the final two terms of 98 one obtains an operational count 1 4 4 4 2 2 1 1 2 1 1 115 since 1 28 7 3 2 1 6 2 1 28 7 3 1 an upper bound on the logarithmic sum is given by 1 1 2 1 6 2 1 1 1 28 7 3 1 1 2 1 6 2 1 2 1 28 7 3 116 substituting 116 into 115 gives an upper bound on the operational count as follows 28 7 3 1 3 1 3 4 4 2 2 2 2 1 6 2 2 1 2 1 28 7 3 2 1 3 2 1 3 7 28 3 1 6 2 1 28 7 3 1 2 1 3 2 2 1 2 1 28 7 3 117 4 3 6 total operational count for the evaluation of the sum as defined in theorem 2 the total operational count for the evaluation of sum of theorem 2 using the algorithm qgs to estimate the quadratic gaussian sums in the manner described in sections 4 3 2 5 cannot exceed the combination of results 107 and 117 in full the evaluation of requires no more than 2 1 4 2 2 1 3 1 4 2 1 2 1 8 1 1 3 2 3 2 3 2 3 2 1 4 3 1 1 3 2 1 3 7 28 3 1 3 1 28 7 3 1 4 2 2 3 2 4 1 28 7 3 1 3 1 3 9 4 9 1 1 0 4 1 0 2 2 28 1 log 4 1 118 standard sine cosine function suppose that the termination parameter is chosen lie below 2 4 1 3 4 4 1 12 2 3 4 4 1 12 2 as specified by postulate i of theorem 2 and consider the limit of 118 as as 2 this implies 1 see discussion after eq 94 and hence the first term in 118 collapses to a negligible 4 x 1 4 value in the remaining two terms the all the factors 0 1 1 1 12 whilst the terms of the form 2 whilst not necessarily tending to zero in this limit would be dominated by their corresponding terms hence the evaluation of with 3 4 4 1 12 2 requires no more than 1 3 2 1 3 7 3 1 3 1 7 3 1 4 1 12 3 1 1 2 3 14 1 3 2 0 247 4 02 1 8 38 1 3 2 0 099 3 68 1 119 the approximate result in 119 is obtained by substituting in the average values for 0 59 53 and 2 established at the end of section 3 and after 114 respectively the upper bound for 118 is obtained using the respective maximum values 1 44 53 and 2 133 the value of 1 17 1 30 these operational counts are consistent with and so prove postulate i of theorem 2 the numerical factors that appear here obviously depend upon the conservative conversion factors 2 and 2 prescribed in section 4 2 in reality these will vary slightly depending upon the operating system being used but cannot change the overall conclusion section 5 2 compares 118 to actual operational counts derived from sample computations for larger values of things are a little more complicated if 3 4 4 1 12 2 then 1 which means that the first term in 118 can contribute significantly to the overall operational count in this instance 2 in 119 and so the most significant terms in the overall operational count 118 would comprise o 1 4 1 8 1 1 3 3 2 o 1 3 120 when 3 4 4 1 12 2 2 1 6 the value of is relatively small and the second term is dominates the overall operational count which remains as this is consistent with postulate ii of theorem 2 it is only when starts to approach the value 2 1 6 at which point 1 do the two terms in 120 start to coalesce to around o of course fixing at such a large value effectively means that all the quadratic gaussian sums making up are being evaluated long hand since c f eq 112 with no computational benefits accruing from the use of algorithm qgs this completes the proof of theorem 2 the implications of this result for the operational complexity of in the limit are summarised in the following theorem 4 4 theorem 3 let 0 and set 0 50 for any 5 0 6 0 define 8 and choose as a fixed parameter prescribed so that 0 and 1 for any 0 define and choose to lie in the range 0 0 2 1 6 where 55 4 1 48 2 fix 0 and let denote the odd integers in this range fix 2 1 3 1 1 2 4 and let denote elements of a subset of the even pivot integers within the given interval define the sum as given by equations 82 3 of theorem 2 then utilising a refined version see 4 5 2 b of algorithm qgs with parameters to evaluate the sub sums of as discussed in sub sections 4 3 1 4 3 5 one can obtain the following approximation for the majority of terms making up the hardy function 2 1 121 for an additional operational cost of 7 3 times the bounds specified in theorem 2 in particular section 4 3 6 corollary suppose is chosen so that 0 0 3 4 4 1 12 2 then the calculation of the hardy function to a precision of requires only 1 3 7 3 as 4 5 proof of theorem 3 the statement of theorem 3 expresses two distinct points firstly it links the sum defined in theorem 2 to a partial sum of the terms in the rsf 8 secondly it makes a statement about the size of the relative errors that can accrue if is used as an approximation for this partial sum these points will be discussed separately 4 5 1 connection of the sum to the riemann siegel formula this is relatively straightforward since it follows almost automatically from theorem 1 quoted in the introduction q v eq 14 links the sum of terms arising from the first order approximations of the various confluent hypergeometric functions in 10 18 to the terms of 8 to within a guaranteed tolerance of 1 12 since forms a proportion of the full sum 54 which itself arises when one approximates the same confluent hypergeometric functions 18 into sized collections section 2 albeit with larger relative errors than the absolute error of theorem 1 then too is linked to a proportion of the terms making up the main sum of 8 the actual number depends upon the cut off value or more precisely the corresponding value of which is the exact number of terms from the full sum represented by any choice cut off satisfying is linked to a corresponding cut off in 8 by means of equation 15 this can be easily established using the exactly same methodology employed by 27 eq 8 to proof theorem 1 for the full sum but over a smaller circle centred at 2 with radius 2 since it can be ignored when inverting 15 this gives a first order approximation for 1 1 2 4 2 8 1 3 1 3 122 accurate to 3 hence the sum corresponds to an approximation of the sum of terms of the riemann siegel formula which represents the overwhelming majority of those needed to compute 4 5 2 error analysis this raises some more difficult issues there are two sources of error that arise in the approximation 121 these must be examined in much more detail than hitherto 4 5 2 a errors arising from the derivation of the approximation the first source of error concerns those terms loosely denoted by in section 2 arising from the derivation of the connection between and the series of gaussian quadratic sums expressed by 54 of which forms a part the first such error arises from the estimation of the integral defined by 34 35 using approximation 38 with the exponential factor i 2 2 4 4 set equal to unity this is justified by fixing to lie below the upper bound given by 40 which ensures that 3 satisfies 2 2 4 4 3 3 4 2 7 suppose instead that the exponential factor is written as 1 i 2 2 4 4 the effect of including the i 2 2 4 4 term explicitly in the analysis spelt in section 2 cf eqs 27 30 and 47 54 is to produce a first order correction to equation 54 which is structurally similar except for the quadratic gaussian sums i also the substitution of the imaginary part as opposed to the real part which are replaced by i 3 2 1 2 2 3 5 i 2 1 3 0 2 123 the multiplicative factor in front arises from 39 and is from 40 whilst 2 1 has been substituted for in the summation part the structural similarity stated above pertains to the exponential phases in 123 which are identical to the corresponding phases of the i terms in 54 this is important for the following reason one would expect the sum 123 to be somewhat smaller than the corresponding which occurs in the main part of 54 since all the amplitudes 2 1 1 however if the phase structure of 123 were to differ from 54 then when collections of these smaller sub sums each characterised by a different pivot value are themselves added together they could give rise by some extremely fortuitous correlation in the signs of the sub sums to a first order correction to 54 and hence very much larger than the predicted so it is necessary to check this cannot occur the correction term 123 can to 1 be written in terms of triple derivative of with respect to o i 1 i 3 3 3 124 if the pivot is such that 2 then 0 1 1 1 alternatively if 1 2 replace 0 1 and adjust appropriately so that 1 2 1 2 in each case theorem paris is then applicable suppose is a generic irrational in the sense elucidated in section 3 5 in which case the quadratic gaussian sum in 124 is dominated by the first term of 58 63 substituting this term into 124 allows one to evaluate the derivatives explicitly giving to 1 o i i 2 3 2 3 2 i 2 1 3 0 125 with 1 the 0 term in 125 corresponds to the first in the hierarchal chain of shorter quadratic gaussian sums 1 generated from by repeated application of 58 the 0 terms correspond to derivatives of this first shorter sum just as 125 follows from 123 these derivatives can themselves be written in terms of derivatives of the next shorter sum simply by applying theorem paris a second time this process can be repeated recursively just as in algorithm qgs until termination is reached when 1 hence the hierarchal chain of sums originating from in 58 would also be a feature of the derivatives 124 5 and the phase structure of the original sum would be repeated in its derivatives complicating things are the whole series of extra amplitudes consisting of products of factors commencing with powers of 2 subsequently followed by powers of down the hierarchal chain but these extra amplitudes can only increase the size of the error term if their various constituent factors have moduli greater than unity suppose the pivot is such that 1 2 and is generic then the factor 2 1 since 1 alternatively if 2 then the factor 2 2 1 using the general definition 40 for this is bounded above by 2 1 2 1 2 3 5 1 6 2 1 2 3 1 3 2 1 6 2 1 3 2 2 2 1 3 2 2 2 1 3 2 2 1 126 hence all the initial factors in the chain satisfy 2 1 it is an interesting coincidence to note that the cut off 2 1 3 originally proposed in theorem 2 with a view to optimizing the computation of also bounds the error by guaranteeing the initial amplitude factors in 125 never exceed unity moving on down the hierarchal chain all the subsequent factors satisfy 1 since 1 and hence to the termination point of the chain hence 124 consists of an term multiplied by a term which cannot be any larger in magnitude than the size of itself this fact combined with the observation that the sub sum 123 4 shares the same phase structure as means that when collections of such sub sums are added together as prescribed by 82 the final total cannot give anything larger than the postulated correction to when happens to be a non generic irrational things are slightly more awkward in such instances at some point in the hierarchal chain a large partial quotient is encountered resulting in a very small value this abruptly terminates the chain of gaussian sums since 1 for simplicity suppose this termination happens immediately if the termination occurs further down the hierarchal chain the analysis is entirely analogous then the behaviour of the sum in 124 will no longer be governed the leading order term in 63 as this will be cancelled out by the first term in square brackets of 64 instead it is the second set of terms erf erf i 2 2 in 64 which will be relevant the behaviour of these terms is complicated but there are two main branches valid for small i 2 1 i 2 for 1 and 1 127 a b i 2 2 for 1 and 1 the most common case 127 a pertains when is not itself close to zero an example of this is the termination exhibited in 3 4 case d substituting 127 a into 124 and differentiating gives rise to to first order extra amplitudes 4 3 3 1 2 1 the less common case 127 b occurs when is also close to zero small enough to ensure an example of this kind of termination is exhibited in 3 4 case e substituting 127 b and differentiating gives rise to first order extra amplitudes 2 3 3 1 similar to the generic case so in both cases the extra amplitudes have modulus less than one which means that just as in the generic case the correction terms 123 4 are no larger in magnitude than itself since the associated phase structures are also preserved one arrives at the same conclusion as for the generic case namely that when the various sub sums 123 are added together their total will not exceed the postulated correction to another relative error term arising from the derivation analysis of section 2 pertains to the secondary integral appearing in 33 but as was demonstrated in 44 this secondary integral is no more than the main integral of 33 multiplied by a factor io so the inclusion of such secondary integrals in the computation of still only produces an correction to the estimate for the partial sum of 8 in concordance with theorem 3 4 5 2 b errors associated with the computation of the second source of error arises in the computation of the various quadratic gaussian sums by means of algorithm qgs in section 3 5 it was shown that for a single general quadratic gaussian sum the relative error was always less than the crude bound 1 2 for a termination constant q v eqs 76 77 in itself this is fine but what is envisaged in theorem 3 is the computation of which is a sum of multiple combinations of gaussian sums all estimated using algorithm qgs again this means there is the potential to obtain through some extremely fortuitous correlation in the signs of the errors of the various sub sums a much larger overall error in the calculation of than one would envisage from 77 the problem is that unlike for the derivation errors discussed above structural similarity is not preserved here the phase of the overall error in algorithm qgs is determined by combinations of the phases of the various 0 terms arising from each iteration step 73 74 and these phases will be quasi random hence one cannot guarantee that some fortuitous correlation in signs will not occur to overcome this problem one needs a methodology to reduce the overall error in a systematic way should the need arise the value of is determined from the remainder terms 61 which arise from asymptotic expansions of the complimentary error function erfc and as is pointed out in 32 sections 2 11 iii 7 12 i such expansions can be exponentially improved by employing similar methods to those of 4 it is possible to derive very much more accurate estimates for the remainder terms than those presented in 33 unfortunately these methods on their own are not enough for the purposes of proving theorem 3 because although yielding a superior estimate for the remainder term it still suffers from algebraic not exponentially small relative errors and the presence of these algebraic errors means that the resulting reduction in is still insufficient to guarantee that the sum of all the relative errors arising from the 1 4 1 7 3 1 3 gaussian sub sums making up cannot combine to give an unacceptably large value nevertheless refining the estimates of the remainder terms provides the key because the resulting analysis highlights the means of ensuring that each individual associated with a particular sub sum can be set to any specified precision without at the same time drastically increasing the order of the operational count estimate in theorem 2 in which case the relative error in the computation of can be reduced to the same scale as the derivation errors discussed previously irrespective of the details of the constituent phase structure in the various the first step towards attaining this desired level of accuracy is to make a more detailed examination of the remainder terms 61 of theorem paris the remainder terms of 58 64 come about through the method employed by 33 eq 2 5 to estimate the following integral 2 2 erfc 2 2 2 2 1 2 2 2 128 the second expression follows from the identity 7 11 3 of 32 which links erfc to 1 2 the generalized exponential integral 32 section 8 19 it applies for all in this instance 33 eq 2 3 2 i 4 0 1 2 129 since arg 4 and hence arg 2 2 for most values it will prove useful to establish the asymptotic expansion for 1 2 in the particular case when i with 0 this is easily done by means of results given in 32 reproduced below 1 2 1 2 1 2 8 19 2 t 1 2 t t t 0 8 6 4 arg 1 1 t 1 2 t t 0 1 t 1 2 t t t 0 1 0 1 1 2 1 1 t 1 2 t t t 0 1 0 130 the last expression follows from t 1 1 1 t 1 and the application of 32 eq 2 11 9 now in the special case arg i 2 one can use the substitution t i 2 2 to transform the integral in 130 into a more convenient form giving 1 2 i 1 1 2 1 2 i i 4 1 2 2 i 2 1 i 2 0 1 0 131 now 131 is suitable for and as it stands since both satisfy arg 2 2 however since satisfies arg 2 3 2 the use of equation 8 6 4 in the derivation of 130 is invalid in this instance to overcome this problem one must invoke the analytical continuation formula 32 eq 8 19 18 for the general function 1 2 with this gives 1 2 i 2 2 2 2 2 i i 2 2 i 4 1 2 i 2 2 2 132 since the argument is now appropriate one can use 131 to replace the exponential function on the right of 132 using 128 131 132 one obtains i 2 2 2 i 3 4 2 2 i 5 4 1 2 i 2 2 2 i 2 2 2 i 3 4 2 1 2 i 2 2 2 2 i 2 2 i 3 4 2 1 1 2 i 2 2 1 2 i i 4 i 2 2 1 2 2 2 2 2 1 i 2 0 1 0 2 i 2 2 1 1 1 2 i 2 2 1 2 1 0 2 i i 4 2 2 2 2 1 i 2 0 133 equation 133 is actually equation 2 6 b of 33 but with the remainder term now written explicitly in terms of a specific integral the case for and is much easier since 131 can be substituted into 128 directly giving 33 q v eq 2 6 a 1 1 1 2 i 2 2 1 2 1 0 2 i i 4 2 2 2 2 1 i 2 0 134 the remainder terms in 133 134 denoted by and respectively in 33 eq 2 6 but not actually defined there satisfy 2 i i 4 2 2 2 2 1 i 2 0 2 i i 4 2 2 2 2 1 i 2 0 135 the relatively crude estimates for the integral in 135 used in 33 are what underlie the error bounds calculated from 61 62 for the general application of algorithm qgs these bounds are more than adequate but for computation of envisaged in theorem 3 something more precise is needed both integrals in 135 can be written in the generic form 2 2 1 i 2 0 with 0 136 simple calculus reveals that the exponential phase 2 2 possesses a saddle at and 4 substituting into 136 gives 2 2 o 3 1 i 2 2 2 1 i 2 2 137 since 1 1 1 2 the series of terms denoted by o 1 12 2 only converges provided restricting the limits of the first integral now around 0 the denominator in 137 is given by 1 1 i 2 1 1 i 2 i 2 3 i 3 2 1 1 2 2 2 3 138 where the terms denoted by 1 2 2 real part to first order in this means 1 i 2 i 2 2 3 i 3 2 1 1 2 2 2 3 2 2 1 i 2 2 139 with 1 2 1 2 real part to first order now consider the behaviour of the real part of the exponential phase of integrand 139 as in both the limits 0 and re 2 2 1 1 1 3 0 140 2 2 1 2 1 1 1 3 equation 140 shows that provided 2 the real part of the phase is already significantly negative as irrespective of the actual value of consequently the magnitude of second integral in 139 will be exponentially smaller than the first integral since 1 i 2 1 1 2 it is relatively easy to show that the second integral in 139 is no more than 4 1 2 4 2 this means that integral satisfies 1 i 2 i 2 2 3 4 1 2 4 2 where 2 2 2 3 i 3 2 1 1 2 2 141 in this form can be approximated by elementary functions using the tabulated standard integral 15 eq 3 462 3 there is one slight drawback because the standard integral 15 eq 3 462 3 covers the infinite range rather than the finite range in 141 so employing 15 eq 3 462 3 introduces a further error of the form 1 i 2 i 2 2 3 1 i 2 142 the presence of the term in 142 restricts the use of 15 eq 3 462 3 to the range 2 with this caveat one can establish estimates for to increasing degrees of precision terminating the computation on reaching the terms of 3 in 141 gives 1 i 2 4 1 2 4 2 3 3 1 i 2 3 with 1 1 3 1 2 i 2 and 3 2 1 3 1 2 2 i 4 12 2 3 i 2 3 2 5 1 2 3 143 further explicit expressions for the various 4 arising from the contributions of the o 4 in 141 can be found by repeated application of 15 eq 3 462 3 however these expressions become increasingly complicated whilst producing progressively smaller increases in precision since 1 3 2 irrespective of the limit 0 or a concise and accurate estimate for the generic integral is obtained by setting the constant 1 for this application 2 and direct numerical computation of shows that estimate 143 is accurate up to a maximum relative error of 5 inclusion of both the 1 and 3 terms reduces this to 1 using 143 it is now possible to obtain more refined estimates for the remainder terms 135 in these specific instances 2 2 2 and 2 since the value is excluded from these remainders that erfc function is found separately in 64 2 and 2 with 1 this gives an improved estimate for the remainder terms of the form sgn i 4 i 1 2 1 2 1 1 i 2 1 2 3 i 3 2 1 2 1 2 2 1 2 144 to first 1 equation 144 recovers as it should the same upper bound on as that given by 33 eq 2 7 which is the basis of the error estimates 61 62 now the exact expression for the total remainder term in 58 64 is given by q v 61 i 4 2 1 1 1 1 i 4 2 1 1 145 equation 145 b is the same as equation 2 10 of 33 but with the deletion of the term 33 c f discussion pp 585 6 and an extra i 4 included for consistency with 33 eq 2 4 in equation 145 c the terms are given by 144 but with the only other feature of note is the inclusion of the extra unaccounted for 2 factor a point previously raised after equation 61 which seems inconsistent with early equations of 33 it is included in 145 for pragmatic reasons discussed in a moment the computation of requires a means of estimating the infinite sums arising from 145 following the substitution of 144 for 2 these sums converge very rapidly and are dominated by the terms at which the factor 2 1 reaches its maximum this maximum could become relatively large when 1 2 3 2 but this is only possible for the 1 terms in 145 b and or the 1 terms in 145 c the term in is always much smaller than its companions since these potentially relatively large terms given explicitly by 1 1 i 4 2 1 1 1 1 146 are much the most significant of all the various factors making up the remainder not specifically accounted for in algorithm qgs so incorporation of 146 directly into 64 using estimates for the various computed from 144 should produce a refined version of algorithm qgs with significantly improved precision capabilities the performance of this refined algorithm qgs applied to the five quadratic gaussian sums cases 3 4 a e previously examined in section 3 4 is illustrated in table 4 5 2 only the final answers are shown in table 4 5 2 with 3 and 20 as before along with the absolute error and relative error values the corresponding values for these errors obtained previously using the basic version of algorithm qgs see tables 3 4 a e are shown in parenthesises as anticipated inclusion of 146 gives a much higher degree of precision with a more than five fold reduction in the error terms in all cases even examples 3 4 d e where the errors were already extremely small the extra operational count needed to compute 146 using 144 is comparatively small many of the parameters such as have already computed previously whilst factors such as i 4 i 1 need only be computed once and then stored roughly equivalent to adding an extra term in the short sum evaluation to 1 4 40 see discussion before eq 80 this means that for 3 the maximum operational count for one evaluation of goes up from 53 to about 60 the resulting changes in the values of 0 59 60 and 1 44 60 are much too small to effect the conclusions of theorem 2 the stray 2 factor is incorporated into 146 simply because without it the precision performance of algorithm qgs is unchanged so it should appear although its origin remains a small mystery following the incorporation of the most significant correction terms given by 146 directly into algorithm qgs the general remainder term can now be written as i 4 2 1 1 1 1 i 4 2 4 2 3 2 1 2 1 2 3 2 2 2 1 2 2 1 2 147 for with 2 in this specific case this comes about because 1 2 and the 1 2 1 2 irrespective of the values of this means that the relative error in each individual gaussian sum calculated from algorithm qgs using termination constant should satisfy see 75 76 3 41 2 3 2 1 2 2 1 2 148 for the calculations shown in table 4 5 2 with 20 3 and 2 this gives a value of 1 2 10 4 comparison with the results shown in table 4 5 2 reveal this bound to be comparable to but somewhat below the actual values 4 5 10 4 and 2 3 10 4 found for the cases 3 4 a b respectively the reason for these larger than expected values lies in the various 1 2 terms disregarded if the approximation 1 is adopted to estimate 146 explicit computation of the terms 1 and 3 that make up see 143 provides a complicated and operationally expensive means of obtaining extra but incremental reductions in rather than continuing any further with this incremental approach a better strategy would be to modify equation 64 to include more and more of the various erfc terms explicitly each computed using the intrinsic erf routines common to modern software packages this provides a much more efficient means of reducing 145 and hence to the desired precision necessary to satisfy the constraints of theorem 3 however implementation of such a strategy would come at the expense of increasing the operational count needed to compute this increase is estimated below currently 64 already contains the erfc sgn 2 term associated with explicitly since the corresponding asymptotic expansions 132 133 are not very accurate for 1 2 33 pp 585 6 the proposed strategy is to gradually add in further such erfc 2 terms q v eq 129 into 64 starting with sgn then sgn followed by 2 sgn 2 sgn etc in conjunction with this process one would also augment 64 with the erfc 2 terms associated with the variables 2 for sgn sgn 2 sgn whenever a erfc 2 term is added into 64 one would need to modify the expressions 59 60 for and by removing the corresponding factor of from the overall sum these are relatively minor modifications to algorithm qgs and for the purposes of proving theorem 3 the underlying details of no great importance what really matters is the speed of the reduction in as each successive erfc 2 term is incorporated directly into 64 is it fast enough to prove theorem 3 without increasing the operational count beyond the limits imposed by theorem 2 to answer this suppose all the erfc 2 terms for sgn sgn sgn sgn are computed explicitly then the general remainder term will be given by 147 and the corresponding relative error will be indeed be bounded by 148 notice that as increases in 148 the value of can also be increased further reducing the task now is to choose and to guarantee that does not produce an increase in the relative error of beyond that of the current level already present due to the various derivation errors discussed earlier to achieve this first prescribe 2 1 2 2 in which case 2 3 2 1 2 1 2 149 now in there are 7 3 1 3 gaussian sums to evaluate most of whom will be of in size but each of these must be divided by their appropriate amplitude factor 2 2 1 4 the value for 2 2 1 4 1 1 6 from 24 40 which means that if in the extremely unlikely event that the phases of all the respective were to correlate perfectly the sum of the relative errors could reach a maximum 1 3 1 6 1 6 to be certain that this term is no larger than the current relative error scale one requires 1 6 2 1 2 2 1 1 6 7 6 1 2 1 6 7 6 150 so the number of extra erfc 2 terms which must be incorporated directly into 64 to ensure the relative error in cannot exceed is 7 3 5 as at the same time the corresponding cut off integer increases as the square of this value implementing this strategy leads to an increase in the operational count over and above that predicted in theorem 2 currently a constant value of 0 59 53 has been employed to estimate the average operational count of computing the iteration count factor based on a value of cut off integer 3 with set implicitly equal to one see the discussion at the end of section 3 6 this is used in 118 119 to obtain the upper bound on the operational count necessary to evaluate based on the estimates of section 3 6 would now become a linear function of and of the form 0 59 12 5 9 2 asymptotically this means that in order to evaluate and be absolutely certain of achieving an approximation to the partial sum 121 of the last terms of which is accurate to in the relative error the overall operational count would have to rise from 1 3 2 of theorem 2 assuming 10 to 1 3 2 7 3 this relatively small increase in the operational count guarantees the desired level of accuracy specified by theorem 3 however for actual computations see section 5 the implementation such a complex error refinement strategy is a proviso of only theoretical as opposed to a practical necessity since both sources of error that arise when linking to from the derivation of the formulation of and from its computation via algorithm qgs have with the refinements discussed above been shown to give rise to corrections no larger than this completes the proof of theorem 3 4 6 corollary of theorem 3 this follows directly from the results of theorems 2 3 and the connection formula 15 linking terms of the rsf to the terms of the new approximation series 11 for from 8 and 121 one has 2 1 1 4 1 2 1 1 1 1 151 with 1 1 2 4 and 2 1 3 clearly calculation of first sum on the right hand side of 151 requires just 1 following the computation methodology of sections 4 3 3 4 theorem 2 proves that the calculation of second sum requires no more than 1 3 in turn theorem 3 demonstrates subject to the proviso discussed at the end of section 4 5 2 b that computing by means of this methodology yields an approximation to the sum of the latter terms of which is accurate to in the relative error since the cut off value 1 3 1 3 from 122 this means that is possible to compute an approximation for in just 1 3 7 3 1 152 accurate to in the limit as with the definitions for and specified in the corollary the final result follows automatically one is tempted to say that the precision specified is accurate to in the absolute relative error of itself however the analysis only supports an accuracy in the absolute relative error of to say these two statements are equivalent requires the condition that to be satisfied employing the methodology of classical exponent pairs one can find a bound for the exponent 1 11081719 78340470 0 14145586 in the estimate 1 less than the well known huxley bound 32 205 0 15609 for the estimate of itself 21 the bound on 1 is easily established from the exponent pair which results from the operations 2 2 2 3 4 starting from an initial exponent pair 2 7 4 7 the processes are defined in 22 ch 2 so it would appear that which would imply that as one would expect intuitively in practical terms if one wishes to estimate for by computing instead then the calculation will be accurate to 5 sample computations the computational methodology discussed in section 4 3 in the proof of theorem 2 can be adapted to generate a suitable algorithm for practical calculations of however the benefits in terms of a significantly reduced operational count compared to the rsf of estimating by computing and then employing 151 will only start to manifest themselves for relatively large values the analysis leading to 119 shows that the computation of requires 1 3 2 1 compared to the needed for 8 hence significant savings should start to accrue beyond the value 1020 when the latter operational count overtakes the former to investigate further how this comes about some sample calculations of for 1018 1023 were carried out using the new methodology and compared to the results obtained using the rsf 5 1 a new algorithm for the computation of in just 1 3 2 1 operations algorithm zt 13 assuming storage capacity 2 bits initialise 1015 0 1 1 05 1 15 0 and integer values for 2 4 and 30 80 compute 8 2 2 1 4 2 1 3 and 1 12 3 initialise block no 0 if 1 6 then compute the transition term see eq 13 and add to zp for summands 2 2 step 2 compute first sum contribution to see eq 82 directly and add to zp initialise first pivot 2 1 while do block no 1 if then 2 1 1 3 else 2 1 6 2 2 2 endif 1 2 1 for 1 to do compute gs parameters and 2 2 1 4 see eq 83 if then compute directly else compute using the refined version of algorithm qgs endif of see eq 82 2 1 if 1 exit p loop endif enddo 1 enddo 1 1 2 4 see eq 122 compute the cut off riemann siegel sum see eq 151 estimate for end the computational structure of algorithm zt 13 is based on the methodology discussed in detail in section 4 with a few minor alterations in practice it is difficult to predict exactly the integer values at which point the step up procedure can be safely initiated see section 4 3 2 and estimate 114 the number of blocks needed for the calculation to reach in section 4 3 2 the choice of 28 ensured 1 24 as see 97 however waiting for 1 24 before instigating the step up to larger values is somewhat inefficient for practical calculations instead it is better to initialise a value for the step up parameter 1 05 1 15 fixing to a slightly smaller value speeds up the algorithm since the switch over to the larger values comes earlier in the calculation with no detrimental effect on its accuracy see section 5 2 the other point to note is that the computation of the two gaussian sums in step 4 does not require two separate calls to algorithm qgs since the quadratic parameter in each gaussian sum is identical certain replicative computations can be eliminated utilising this fact it is easy to create a slightly modified version of algorithm qgs which computes both and simultaneously at an operational cost of only about one and three quarters times that of a single call to the standard version the refinements to the basic version of algorithm qgs discussed in section 3 go no further than the incorporation of the improved error terms given by 146 into 64 5 2 computational performance and accuracy of algorithm zt 13 in order to test out some the theoretical predictions made in the previous three sections a fortran code implementation of algorithm zt 13 was developed to carry out some sample calculations of in the range 1018 23 across these calculations fixed default settings of 0 2 10 18 0 0482 for the relative error 30 for the termination constant cut off integer 3 and step up parameter 10 9 were employed table 5 1 gives a detailed breakdown of all the intermediate calculations necessary to obtain an estimate for 1018 to give an insight into what is involved the table breaks down each part of the overall calculation into the contributions given by the various blocks the block size 1 4 which gives the number of pivots within the block is shown first here 1 1137 as defined in section 4 3 3 followed by the associated collection size the calculations encompassed by the 0 block comprise the sum of those terms in 82 too large and distinctive to be effectively parcelled together into quadratic gaussian sums step 3 in zt 13 the size of this block was set to 2 1 4 double that suggested in section 4 3 1 doubling the initial block size allows for significant parcelling to commence immediately from block 1 finessing the prescription set out in section 4 3 3 which postulated the parcelling should begin in pair wise fashion with 1 1 in this example the prescribed value of means one can safely set 1 9 speeding up the computations a touch the third column gives the range of integers encompassed by each block so for instance block 1 consists of all the odd integers 1595832369 1596536727 whilst the even integer 0 1595832368 forms the boundary of blocks 0 and 1 each set of gaussian sum parameters in 82 is computed from the pivots 0 10 0 30 0 50 that lie within block 1 this conforms to the prescription set out before 86 87 but with 1 9 rather than 1 1 the fourth column shows the total value of the partial sum over all the integers for each block the exact value of partial sum derived from the corresponding terms of the rsf summing backwards of course appears in column six followed by the relative error in the early blocks the quadratic gaussian sums are relatively short and are computed exactly however upon reaching the sixth block one finds 6 61 2 which means that the quadratic gaussian sums are now sufficiently large to trigger the implementation of algorithm qgs to facilitate their computation step 4 in zt 13 the strict restrictions imposed on the size of the successive means that although these computations are no longer exact the relative errors associated with the partial sums of the blocks are no larger than by the time the computation reaches the end of block 14 the pivot values 1771303725 at this stage it is safe to step up to the larger of the two scales proposed in 83 b hence the sharp jump in the designated sizes seen between blocks 14 15 in the tabulated results the step up occurs close to the asymptotically predicted block value of 28 13 beyond this point of the calculation all the block sizes remain fixed at 14 1 4 142854 value reached by block 14 on reaching block 26 one encounters an anomaly in the form of a relative error value of 0 0850 somewhat larger than the upper bound the cause of this anomaly is down to fact that magnitude of the partial sum itself just happens to lie close to zero rather than any fault with the methodology the corresponding absolute error 5 24 10 4 for this block is of the same order as the absolute errors found in the partial sums of the other blocks such anomalous behaviour will only be problematic if the value of is such that 0 when any discrepancy would give rise to a large relative error in such instances one would need to repeat the calculation to greater precision by refining the choices of and to establish estimates for the absolute error from block 27 onwards the calculation continues in the prescribed manner until one reaches block 37 close to the beginning of this block the pivots exceed the cut off value 2 1 3 4955842210 which terminates the procedure in practice termination always occurs after a complete gaussian sum computation which means that the final integer of the final block 49558423767 here will always fractionally exceed the value of and will be odd because is odd in this instance termination at the 37 th block corresponds to a value of 37 23 1 2 somewhat smaller than the estimate 114 in the limit as increases one would expect the 1 2 factor to move a little closer to the predicted upper bound 114 at around 2 combining all the separate partial sums from each of the 37 blocks gives a value for 1018 0 376110 since 4955843767 corresponds to a cut off integer 65986402 using 15 122 1018 approximates the sum of the final 398942280 main terms of the rsf which equals 0 375570 to six decimal places the remaining 1 1 terms of the formula plus the small scale o 1 4 remainders specified in 8 amounts to 0 565274 adding this to 1018 gives an estimate for 1018 of 0 189164 compared to the actual value 1018 0 189704 to six d p this equates to an overall relative absolute error of 2 85 10 3 5 40 10 4 an order of magnitude below prescribed valuation of 0 0482 indeed the smallness of this overall error is indicative that there is no sign of any correlations which might cause the computational errors associated with the multiple calls of algorithm qgs to aggregate detrimentally necessitating the implementation of the proviso discussed in section 4 5 2 b table 5 2 presents a much briefer summary of the performance of algorithm zt 13 for values of 1018 19 20 21 22 23 compared to the corresponding rsf calculations all the relative errors lie an order of magnitude below the prescribed valuation of 0 with agreement to about three significant figures in the 1023 calculation algorithm qgs is utilised in all but blocks zero and one i e 1 is stepped up after block number 22 and the computation terminates near the beginning of block 91 so in this case 69 corresponding to an increase in 1 3 as anticipated across the six calculations average parameter values for 0 94 3 18 45 pertained table 5 3 gives an illustration the kind of accuracy attainable utilising algorithm zt 13 to calculate 1020 for three different values of all three estimates fall well inside their prescribed error bounds with the most accurate estimate pertaining for the smallest value of as expected this higher accuracy is achieved at the expense of a longer cpu run time changes to the settings of the termination constant and the step up parameter produce only minor changes to the accuracy and run times table 5 2 also shows the sequential cpu time necessary to complete the 1018 23 calculations as one can see algorithm zt 13 starts to overtake the rsf in terms of computational speed from 1019 onwards however a better reflection of the performance of algorithm zt 13 in terms of the operational count framework utilised in section 4 is obtained by considering the respective timing ratios fig 2 a shows the ratio of the timings of algorithm zt 13 compared to the corresponding rsf calculation for with 18 23 an estimate for the upper bound on the operational count for the former was derived sections 4 3 1 6 to second order the bound 118 can be expressed as 1 3 2 153 with 1 3 1 1 7 28 3 4 2 2 3 2 4 1 1 1 12 3 1 0 1 4 153 the constants and can be estimated from the prescribed values of and the averages of found above provided the operational conversion factors and are known estimates from 27 discussed in section 4 2 show that 1 17 1 30 whilst in section 3 6 it was established that 0 59 1 44 53 counts using an efficient intrinsic erf routine equating 1 24 and 1 01 53 one obtains the following mean values 1 012 and 28 744 for the constants dividing 153 a by 2 the operational count of the rsf gives an almost equivalent theoretical prediction for the timings ratio found from the data in table 5 2 for the equivalence to be precise this prediction must include an extra parameter 0 1 to compensate for the fact that 153 a is an upper bound rather than a direct estimate on the operational count of algorithm zt 13 fig 2 a shows a plot the theoretical ratio derived from 153 against the actual computational data of table 5 2 with a fitted 0 312 to minimize least squares errors as one can the agreement between prediction and computations is excellent the fitted value of 0 312 suggests that 118 153 overestimates the operational count of algorithm zt 13 by a factor of about 3 2 across this particular range of another way of comparing the performance of algorithm zt 13 vis a vis the rsf is to compare the changes in the computational timings ratio 10 10 1 as increases these results are shown in fig 2 b in the case of 8 the ratio remains constant at 10 3 16 for all as shown by contrast for algorithm zt 13 it declines as increases the asymptotic prediction derived from 153 a is represented by the lower green line on the figure whilst the upper red line shows the ratio s predicted behaviour based upon the values 1 012 and 28 744 established earlier as can be seen the corresponding data points for the actual calculations shown in table 5 2 fall increasingly close to this latter result this ratio analysis provides a somewhat better test of the predicted bounds 118 153 since it is independent of the choice of which cancels out one concludes that in this computational range increasing by a factor of ten requires only 2 6 increase in the operational count using algorithm zt 13 compared to the 3 16 required by the rsf asymptotically the ratio is predicted to fall to around 101 3 2 15 but this would require values well beyond anything currently computational feasible one final interesting feature of the performance of algorithm zt 13 is the evolution of the computational time required to complete an individual block of calculations fig 3 which pertains for the 1023 calculation is typical as one can see from the figure the longest computational times are required for those blocks numbered marked by the vertical dashed line close to where the step up of takes place this is somewhat surprising since in the later blocks is much larger and one might expect the associated quadratic gaussian sums would take longer to estimate however this does not occur because the rate of decrease in the quadratic parameter 1 1 is faster than the corresponding increase in and by the time the cut off point is reached 1 this means that near the cut off point algorithm qgs only requires a single as opposed to iterations to estimate each sum hence the computations for these later blocks are somewhat faster than those for the intermediate numbered blocks this is tied in with the fact that beyond the cut off point the gaussian sums start to lose their quadratic character and behave like ordinary geometric series this is a distinctive feature of this particular representation of in terms of gaussian sums quite unlike the representation presented in 18 all the detailed calculations presented here were carried out sequentially using a single processor in order to verify the operational estimates made in theorem 2 however very much faster computational speeds can be achieved by means of parallelisation a parallel coding version of algorithm zt 13 using the facilities of the archer uk national supercomputing service http www archer ac uk is currently under development 6 conclusions the paper 18 conclusively showed that it should be possible reduce the computational complexity of the rsf main sum down from o 1 2 1 proposal of 6 down to only 1 3 where is an absolute constant by reformulating in terms generalised quadratic gaussian sums typically of 1 6 in length the approximate quadratic reciprocity property exemplified by eq 58 of the latter which means they can be estimated recursively using operations provides the basis for the computational reduction the algorithmic methodology of 18 was formulated directly from the terms of the rsf main sum this paper demonstrates that the same kind of methodology can also be utilised to calculate based upon the approximate formulation given by 11 14 derived by 27 under the conditions set out in theorem 1 from the sum of kummer s functions 10 the analysis carried out in section 2 shows that from this starting point can be rewritten in terms sub sequences of generalised quadratic gaussian sums 54 accurate to within a fixed bound on the relative error although structurally analogous to representation derived by 18 the detailed specification is distinctive and more subtle the remainder of this work is devoted in the development of a computational algorithm for the estimation utilising approximation 54 the first task is to formulate a practical methodology for the computation of a quadratic gaussian sum of length at a logarithmical operational expense the result is algorithm qgs of section 3 which utilises the inherent quadratic reciprocity property 3 of gaussian sums to make such a computation estimate at a cost of just 2 operations where 100 termination constant the work of 33 is crucial in this regard both for the asymptotic quadratic reciprocity approximation 58 encapsulated by theorem paris and the initial error analysis which leads to error bounds formulated in section 3 5 section 4 is devoted to two main results in the first half is devoted to the sum and its computation in no more than 1 3 operations for 0 1 10 100 using algorithm qgs and the methodology established in the proof of theorem 2 the second half is devoted to the accuracy of the approximation provided by to a partial sum of the rsf for with some augmentations to the error analysis of 33 which leads to the formulation a refined algorithm qgs one can prove that this approximation will be accurate to in the relative error in the limit as a proviso to the analysis underlying the proof of theorem 3 specifies that in order to achieve this level of accuracy as the operational count for the computation of needs to rise theoretically by factor of 7 however the degree of correlation necessary in what are essentially vast sequences of quasi random error terms to make such a refinement a necessity is so remote as to render it superfluous for all practical large scale calculations the results of sections 3 4 facilitate the development of the new algorithm zt 13 section 5 1 which produces an evaluation of the hardy function in just 1 3 2 1 operations excluding the http www archer ac uk proviso above the detailed analysis of the behaviour of algorithm qgs enable one to make a much more specific estimate for the value of than given by 18 sample computations utilising algorithm zt 13 for values in the range 1018 23 illustrate it behaves very much as predicted comparisons with exact computations demonstrate that algorithm zt 13 leads to estimates for which are well within the specified maximum relative error see tables 5 2 3 tolerance in fact for these specific examples the accuracy is an order of magnitude more precise the speed of the computations conforms to the operational prediction of theorem 2 fig 2 b and is faster than the rsf beyond 1019 as shown in fig 2 a it is interesting to speculate on future developments of this work as mentioned in the introductory remarks 18 theorem 1 1 formulates a procedure for the computation in just 4 13 operations in which it is likely that can be taken around 4 this formulation substitutes collections of 5 26 cubic in place of the 1 6 quadratic gaussian sums analogous to those utilised here the former can then be estimated in much the same way as the latter provided the cubic coefficient is sufficiently small however the implementation of any such a cubic procedure is problematic since its computational speed would remain inferior to any 1 3 3 quadratic procedure until 1090 a figure well beyond anything currently practical but this may be too pessimistic in the methodology presented here the analogous cubic gaussian sum terms appear explicitly in 38 and the upper bound on the collection size 39 was based upon ensuring these terms remain sufficiently small some preliminary examination of the effects of including these terms directly suggests that one might be able to formulate a representation for in which the cubic sums can take collection sizes as large as 1 4 and still remain computable in just operations using analogous methods to those of algorithm qgs such a result would imply that one could compute using as little as 1 4 1 2 operations accurate to some fixed relative error scale one suspects that further advancements along these lines should prove possible in future references 1 bailey d h borwein j m crandall r h 1997 on the khintchine constant math comp 66 417 431 2 bender c m orszag s a 1999 advanced mathematical methods for scientists and engineers new york springer verlag 3 berndt b c evans r j 1981 the determination of gauss sums bull amer math soc 5 5 107 129 4 berry m v howls c j 1991 hyperasymptotics for integrals with saddles proc r soc lond a 434 657 675 5 berry m v keating j p 1992 a new asymptotic representation for 1 2 i and quantum spectral determinants proc r soc lond a 437 151 173 6 bombieri e remarks on the analytic complexity of zeta functions in y motohashi ed analytic number theory london mathematical society lecture note series vol 247 cambridge university press cambridge 1998 pp 21 30 7 borwein j m bradley d m crandell r e 2000 computational strategies for the riemann zeta function j comp appl math 121 247 296 8 borwein j van der poorten a shallit j zudilin w 2014 never ending fractions an introduction to continued fractions australian mathematical society lecture series 23 cambridge university press 9 cohen h 2000 a course in computational algebraic number theory graduate texts in mathematics volume 138 springer verlag 10 crandall r pomerance c 2005 prime numbers a computational perspective springer science business media inc usa 11 dutt a rokhlin v 1993 fast fourier transforms for nonequispaced data siam j sci comput 14 1368 1393 12 edwards h m 1974 riemann s zeta function new york dover publications 13 gourdon x 2010 the 1013 first zeros of the riemann zeta function and zeros computation at very large height http numbers computation free fr constants miscellaneous zetazeros 1 e 13 1 e 24 pdf 14 gabcke w 1979 neue herleitung und explizite restabsch tzung der riemann siegel formel ph d thesis g ttingen 15 gradshteyn i s ryzhik i m 2007 table of integrals series and products 7 th edition jeffrey a zwillinger d eds elsevier academic press 16 hadamad j 1896 sur la distribution des zeros de la fonction et ses consequences arithm tiques bull soc math france 24 199 220 17 hardy g h littlewood j e 1914 some problems of diophantine approximation acta mathematica 37 193 238 18 hiary g a 2011 fast methods to compute the riemann zeta function annals of mathematics 174 891 946 19 hiary g a 2011 a nearly optimal method to compute the truncated theta function its derivatives and integrals annals of mathematics 174 859 889 20 hiary g a 2017 fast methods to compute the riemann zeta function https people math osu edu hiary 1 fastmethods html 21 huxley m n 2005 exponential sums and the riemann zeta function v proc lond math soc 90 1 41 22 ivi a 2003 the riemann zeta function theory of hardy s z function dover publications inc new york republication of work published in 1985 by john wiley sons new york 23 ivi a 2013 the theory of hardy s z function cambridge tracts in mathematics cambridge university press 24 khintchine a 1935 metrische kettenbruchprobleme compositio math 1 361 382 25 kuzmin r o 1928 on a problem of gauss dokl acad sci ussr 375 380 26 l vy p 1929 sur les lois de probabilit don t dependent les quotients complets et incomplets d une fraction continue bull soc math france 57 178 194 27 lewis d m 2015 the development of a hybrid asymptotic expansion for the hardy function consisting 2 2 2 2 main terms some 17 less than the celebrated riemann siegel formula http arxiv org abs 1502 06903 28 luke y l 1969 the special functions and their approximations vol 1 new york london academic press 29 von mangoldt h 1895 zu riemann s abhandlung ueber die anzahl der primzahlen unter einer gegebenen gr sse j reine angew math 114 255 305 30 montgomery h l vaughan r c 2006 multiplicative number theory i classical theory ch 9 primitive characters and gauss sums pp 282 325 cambridge studies of advanced mathematics 97 cambridge university press 31 odlyzko a m sch nhage a 1988 fast algorithms for multiple evaluations of the riemann zeta function trans amer math soc 309 797 809 32 olver f w j lozier d w boisvert r f clark c w 2010 nist handbook of mathematical functions cambridge university press 33 paris r b 2008 an asymptotic expansion for the generalised quadratic gauss sum applied mathematical sciences 2 12 577 592 34 platt d j 2017 isolating some non trivial zeros of zeta math comp 86 307 2449 2467 35 riemann b 1859 ber die anzahl der prinzahlen unter eine gegebener gr sse monatsber akad berlin 671 680 https people math osu edu hiary 1 http arxiv org abs 1502 06903 33 ryll nardzewski c 1951 on the ergodic theorems ii ergodic theory of continued fractions studia mathematica 12 74 79 37 schaar m 1850 m moire sur la th orie des r sidus quadratiques acad roy soc lettres beaux arts belgique 24 14 pp 38 shui p 1995 computation of continued fractions without input values math comp 64 211 1307 1317 39 siegel c l 1932 ber riemanns nachla zur analytischen zahlentheorie quellen studien zur geschichte der math astron und phys abt b studien 2 45 80 40 siegel c l 1960 uber das quadratische reziprozit tsgesetz zahlk rpern nachr akad wiss g ttingen math phys kl no 1 1 16 41 titchmarsh e c 1986 the theory of the riemann zeta function 2 nd edn oxford clarendon press 42 turing a m 1943 a method for the calculation of the zeta function proc london math soc ser 2 48 180 19 43 de la vall e poussin c j 1896 recherches analytiques sur la th orie des nombres premi re partie ann soc sci bruxelles i 20 183 256 tables 1 129901233 1 45 1 1 23 71 2 19364532 7 45 1 0 390159303539095 3 5650494 5 45 4 1 0 162904175231039 4 2413049 1 45 5 1 0 381463061012124 5 824395 7 45 4 1 0 383438172238648 6 60139 2 1 0 256248660552216 7 17548 3 1 0 378177224069896 8 7493 4 1 0 114444787592628 9 2559 5 1 0 165014271963481 10 186 2 1 0 262049291848319 11 54 3 1 0 398056283255953 12 22 4 1 0 067895171805313 estimate of exact value of relative error 12 as exact value 0 95384635 1 66611062 i 0 0 0 0 11 1 00607535 3 90371044 i 1 00333214 3 89043286 i 0 0135 3 374 10 3 10 1 28630419 6 88855557 i 1 29322876 6 87130724 i 0 0185 2 658 10 3 9 14 0775845 25 30169184 i 14 0772259 25 2328684 i 0 0688 2 382 10 3 8 6 12662881 48 9362647 i 6 05515130 48 8420622 i 0 1182 2 402 10 3 7 27 6518893 69 5560559 i 27 5007943 69 3982979 i 0 2184 2 926 10 3 6 62 3228952 123 849884 i 62 0122200 123 581538 i 0 4105 2 969 10 3 5 479 859544 185 654054 478 340786 185 691189 1 5192 2 961 10 3 4 531 440375 701 485722 i 529 212066 700 177406 i 2 5840 2 944 10 3 3 78 9762197 1344 14844 i 77 2560754 1340 59086 i 3 9516 2 943 10 3 2 1777 43497 1746 87547 i 1774 57271 1740 14359 i 7 3151 2 943 10 3 1 4535 00190 4594 68373 i 4527 85134 4577 13867 i 18 946 2 943 10 3 table 3 4 a output generated by algorithm qgs illustrating in detail the computation of the quadratic gaussian sum 0 1 45 1 23 71 a set of recursive iterations are performed in which the original sum is rewritten in terms of a series of shorter intermediate quadratic gauss sums each with their own parameters when 1 the exact value of the th much shorter sum is computed from this starting point the previous recursive iterations are reversed and an estimate of the original sum built up from equations 63 64 the exact values of the intermediate sums and the associated errors are also shown 1 129901233 1 1 1 2 17503414 0 42147960099874 1 0 230209768280677 3 7377331 0 37259406536018 1 0 453805669989322 4 2748749 0 31611398846808 1 0 282037310361788 5 868918 0 16341584517053 1 0 392201296527760 6 141994 0 11935763607527 1 0 400019998785776 7 16948 0 37818201568035 1 0 351440359739468 8 6409 0 35577061166962 1 0 429288927468500 9 2279 0 18920009916778 1 0 293354725243173 10 431 0 28540949184794 1 0 050499849278713 11 122 0 49626228782619 1 0 176938226376921 12 60 0 01506345440907 1 0 356541753660095 estimate of exact value of relative error 12 as exact value 5 99626094 6 21622453 i 0 0 0 0 11 2 47725770 12 2276531 i 2 46267551 12 2290382 i 0 0146 1 174 10 3 10 11 8634890 20 0400796 i 11 8850320 20 02306601 i 0 0274 1 179 10 3 9 48 22186610 20 4135977 i 48 2343029 20 3499866 i 0 0648 1 238 10 3 8 80 8009685 38 3042272 i 80 721100 38 3815374 i 0 1111 1 244 10 3 7 113 271534 92 1515455 i 113 109940 92 238325 i 0 1834 1 256 10 3 6 386 779631 164 145822 i 386 400652 164 517221 i 0 5306 1 263 10 3 5 876 083627 557 413123 i 876 314137 556 113049 i 1 3203 1 272 10 3 4 1552 63044 998 33174 1553 04478 996 036357 i 2 3324 1 264 10 3 3 147 310219 3021 03492 i 150 742369 3019 38816 i 3 8067 1 259 10 3 2 1559 83456 4388 23283 i 1553 97933 4388 90792 i 5 8642 1 259 10 3 1 5290 66224 11536 2495 i 5301 47806 11524 4924 i 15 975 1 259 10 3 table 3 4 b as table 3 4 a but for the quadratic gaussian sum 0 1 1 1 129901233 2 10 1 10 71 2 18370808 7 10 2 1 0 153724462171376 3 1305572 2 1 0 163067331555985 4 92784 2 1 0 294531480224220 5 6593 2 1 0 144372430964127 6 468 2 1 0 031474266260106 7 33 2 1 0 442876534874666 estimate of exact value of relative error 7 as exact value 6 36379881 0 158944615 i 0 0 0 0 6 19 1558359 13 4017658 i 19 1558145 13 4017626 i 2 163 10 5 9 214 10 7 5 64 1769995 59 1391565 i 64 176950 59 139096 i 7 732 10 5 8 859 10 7 4 222 241162 241 875551 i 222 241036 241 875239 i 3 373 10 4 1 027 10 6 3 428 153221 1155 83662 i 428 153209 1155 83534 i 1 280 10 3 1 039 10 6 2 2663 73406 3781 54500 i 2663 73285 3781 54032 i 4 484 10 3 1 046 10 6 1 12144 44498 1943 67131 i 12144 43440 1943 66515 i 1 224 10 2 9 958 10 7 table 3 4 c as table 3 4 a but for the quadratic gaussian sum 0 2 10 10 71 1 129901233 0 332613390928 1 1 2 2 43206889 0 00649350649 1 0 05301357552 3 280564 3 628491 10 12 1 0 16409063118 4 1 irrelevant irrelevant irrelevant estimate of exact value of relative error 4 as exact value 0 5 0 0 i 0 0 0 0 3 0 865720322 0 711435501 i 0 865720322 0 711435501 i 10 10 10 10 2 4 920381960 3 937306408 i 4 920381970 3 937306410 i 9 728 10 8 1 542 10 9 1 10 05737802 2 726908802 i 10 05611070 2 724765960 i 2 489 10 3 2 389 10 4 table 3 4 d as table 3 4 a but for the quadratic gaussian sum 0 0 3326339 1 2 1 129901233 1 2 0 2 1 1 0 2 64950616 4 201538 10 16 1 4 900798 10 9 3 0 irrelevant irrelevant irrelevant estimate of exact value of relative error 3 as exact value 0 5 0 0 i 0 0 0 0 2 2 95151374 1 90568655 i 107 2 95151374 1 90568655 i 107 4 432 10 3 1 261 10 10 1 4 85720022 1 04582716 i 107 4 85720022 1 04582716 i 107 7 052 10 2 1 419 10 9 table 3 4 e as table 3 4 a but for the quadratic gaussian sum 0 1 2 0 2 1 0 0 0 0 0 3 4 a 129901233 1 45 1 1 23 71 3 4 b 129901233 1 1 1 3 4 c 129901233 2 10 1 10 71 3 4 d 129901233 0 332613390928 1 1 2 3 4 e 129901233 1 2 0 2 1 1 0 estimate of 0 0 0 0 exact value of relative error 3 4 a 4529 38172 4579 59925 i 4527 85134 4577 13867 i 2 897 18 946 4 501 10 4 2 943 10 3 3 4 b 5299 74860 11526 7988 i 5301 47806 11524 4924 i 2 883 15 975 2 273 10 4 1 259 10 3 3 4 c 12144 43639 1943 66622 i 12144 43440 1943 66515 i 2 264 10 3 1 224 10 2 1 841 10 7 9 958 10 7 3 4 d 10 0563258 2 72513736 i 10 05611070 2 724765960 i 4 292 10 4 2 489 10 3 4 120 10 5 2 389 10 4 3 4 e 4 85720022 1 04582716 i 107 4 85720022 1 04582716 i 107 1 283 10 2 7 052 10 2 2 581 10 10 1 419 10 9 table 4 5 2 higher precision gaussian sum computations for the examples shown in tables 3 4 a e incorporating the improvements 146 to the remainder terms the resulting refined algorithm qgs produces reduced errors compared to those shown in parentheses found previously computation of 1018 for 1595769123 398942280 30 3 0 block collection sizes final value of each block partial sum corresponding final value partial sum relative error 0 63244 0 1595832367 0 033078 395406152 0 033033 1 37 10 3 1 35218 9 1596536727 0 223712 386758725 0 226116 1 06 10 2 2 39223 21 1598262539 0 226875 377255248 0 227283 1 80 10 3 3 43684 33 1601233051 0 233534 367266439 0 233831 1 27 10 3 4 48652 43 1605514427 0 028613 357221582 0 029194 1 99 10 2 5 54185 53 1611366407 0 366864 346927266 0 366501 9 90 10 4 6 60347 61 1618849435 0 085892 336615735 0 085828 7 46 10 4 7 67211 69 1628258975 0 058909 326152678 0 058845 1 09 10 3 8 74854 79 1640235615 0 132097 315225569 0 132370 2 06 10 3 9 83367 87 1654908207 0 195195 304113503 0 195779 2 98 10 3 10 92849 95 1672735215 0 075149 292793411 0 075219 9 31 10 4 11 103408 105 1694657711 0 391719 281057961 0 391807 2 24 10 4 12 115168 113 1720916015 0 120074 269163637 0 119802 2 27 10 3 13 128266 123 1752725983 0 058025 256941103 0 058231 3 53 10 3 14 142854 133 1791010855 0 138548 244460930 0 138875 2 35 10 3 15 142854 261 1865866351 0 641492 224730918 0 642255 1 19 10 3 16 142854 277 1945293175 0 116383 208194901 0 117073 5 89 10 3 17 142854 293 2029291327 0 317633 193919717 0 319356 5 40 10 3 18 142854 311 2118432223 0 127847 181283854 0 127376 3 70 10 3 19 142854 329 2212715863 0 043415 169966199 0 042267 2 72 10 2 20 142854 349 2312713663 0 444972 159686675 0 445641 1 50 10 3 21 142854 369 2418425623 0 192445 150300383 0 192441 2 07 10 5 22 142854 391 2530423159 0 098546 141652491 0 098288 2 62 10 3 23 142854 413 2648706271 0 255487 133666527 0 255414 2 86 10 4 24 142854 437 2773846375 0 244961 126245552 0 245158 8 04 10 4 25 142854 463 2906414887 0 145447 119316290 0 144246 8 33 10 3 26 142854 489 3046411807 0 005642 112847133 0 006166 8 50 10 2 27 142854 515 3193837135 0 189155 106807432 0 189422 1 41 10 3 28 142854 545 3349833703 0 059023 101128560 0 059113 1 52 10 3 29 142854 575 3514401511 0 422062 95795370 0 421981 1 92 10 4 30 142854 605 3687540559 0 240625 90791122 0 240133 2 05 10 3 31 142854 639 3870393679 0 389069 86070339 0 389131 1 59 10 4 32 142854 673 4062960871 0 262033 81623926 0 261562 1 80 10 3 33 142854 711 4266384967 0 270332 77418503 0 270616 1 05 10 3 34 142854 749 4480665967 0 524108 73448753 0 523929 3 42 10 4 35 142854 789 4706375287 0 051565 69698128 0 051865 5 78 10 3 36 142854 831 4944084343 0 367282 66152213 0 367593 8 46 10 4 37 6712 875 4955843767 0 139107 65986402 0 139114 5 03 10 5 total 1018 0 376110 2 0 375570 2 1 1 1 4 0 565274 0 565274 estimate for 1018 0 189164 1018 0 189704 2 85 10 3 table 5 1 detailed breakdown of calculations leading to the hybrid estimate of 1018 1018 1019 1020 1021 1022 1023 rsf cpu time 0 189704 5 27 103 28 270243 1 67 104 3 345199 5 36 104 2 610424 1 67 105 5 227095 5 27 105 1 608632 1 67 106 hybrid estimate cpu time 0 189164 5 93 103 28 266343 1 65 104 3 339213 4 46 104 2 613327 1 16 105 5 221425 2 97 105 1 617695 7 50 105 absolute error 0 000540 0 003900 0 005986 0 002903 0 005670 0 009065 relative error 2 85 10 3 1 38 10 4 1 79 10 3 1 11 10 3 1 08 10 3 5 64 10 3 table 5 2 hybrid estimates of 1018 23 utilising algorithm zt 13 with settings for the relative error 0 termination constant 30 and step up parameter 10 9 comparisons of accuracy and sequential cpu time are made to the rsf hybrid estimate of 1020 cpu time absolute error relative error 0 2 30 1 11 3 340945 4 90 10 4 0 004254 1 27 10 3 0 30 1 11 3 339213 4 46 10 4 0 005986 1 79 10 3 2 0 30 1 11 3 334418 4 01 10 4 0 010781 3 22 10 3 0 20 1 11 3 339977 4 53 10 4 0 005221 1 53 10 3 0 50 1 11 3 339812 4 61 10 4 0 005387 1 61 10 3 0 30 1 08 3 346098 4 37 10 4 0 000090 2 68 10 4 0 30 1 14 3 346866 4 49 10 4 0 000166 4 98 10 4 table 5 3 hybrid estimates of 1020 utilising algorithm zt 13 for different values of the relative error termination constant and step up parameter figures 1 a b schematics of the general contours of integration in space used to estimate integral given by eq 20 fig 1 a is for the case when whilst fig 1 b is when the integration through the saddle point situated at in fig 1 b is what gives rise to the main terms in the formulation for specified by eq 11 1 1 fig 1 a im w re w pc r 1 4 4 1 i 1 2 2 1 4 fig 1 b im w re w i 3 figures figure 2 a timings ratio of algorithm zt 13 compared to the rsf the red diamonds represent actual calculations the blue dashed line is the fit obtained from the predicted upper bound operational count 153 with compensation factor 0 312 2 b timings ratio for the computation of 10 10 1 using algorithm zt 13 the red diamonds represent actual calculations the red dashed curve is the predicted behaviour of 153 across the range of 19 23 the green dashed curve is the asymptotic prediction as figure 3 sequential computational times necessary to complete the calculations for each pivot block data taken from the 1023 computation which is typical the vertical dashed line represents block no 23 the point at which the step up of the collection size is initiated in this case