estimation of dynamical systems in noisy conditions and with constraints krishan mohan nagpal wells fargo co krishan nagpal yahoo com november 16 2021 abstract when measurements from dynamical systems are noisy it is useful to have estimation algorithms that have low sensitivity to measurement noises and outliers in the first set of results described in this paper we obtain optimal estimators for linear dynamical systems with insensitive loss functions the insensitive loss function which is often used in support vector machines provides greater robustness when the measurements are biased and very noisy as the algorithm tolerates small errors in prediction which in turn makes the estimates less sensitive to measurement noises apart from insensitive quadratic loss function estimation algorithms are also derived for insensitive huber m loss function which provides robustness in presence of both small noises as well as outliers robustness in presence of outliers is achieved with huber cost function based estimator as the error penalty function switches from quadratic to linear for errors beyond certain threshold the second set of results in the paper describe algorithms for estimation when apart from general description of dynamics of the system one also has additional information about states and exogenous signals such as known range of some states or prior information about the maximum magnitude of noises disturbances while the proposed approaches have similarities to kalman bucy or h 2 smoothing algorithm the algorithms are not linear in measurements but are easily implemented as optimal estimates are obtained by solving a standard quadratic optimization problem with linear constraints for all cases algorithms are proposed not only for filtering and smoothing but also for prediction of future states 1 introduction the problems of prediction filtering and smoothing involve estimating states of a dynamical system based on noisy measurements of the system in prediction the goal is to predict state of the system at a future time without knowledge of the future measurements in filtering the goal is to estimate the current state of the system using all the past measurements while in smoothing one estimates both the current and past states of the system when the dynamical system is linear and known kalman bucy estimation framework also sometimes described ash 2 framework provides optimal way to estimate states of the system when the power spectrum density of the noises and disturbances is known 1 and 9 kalman bucy framework also has certain optimal worst case properties for example krener 8 showed that kalman bucy filter is a minimax filter with quadratic norm of exogenous signals in some applications such as in finance the data can be very noisy and unpredictable with unstable statistical properties of the exogenous signals noises for such situations when there is less information about noises worst case approaches such ash have been proposed see for example 5 13 and 15 nagpal and khargonekar 13 showed that the optimal fixed intervalh 2 smoother is also the optimal smoother for the worst caseh criteria in presence of outliers and uncertainty about statistical properties of noises and disturbances other statistical approaches such as estimation with huber m estimator cost function have also been proposed see for example 7 4 and 2 krishan nagpal is a managing director in corporate risk in wells fargo co the opinions expressed here are those of the author and do not represent those of his employer wells fargo co ar x iv 2 01 1 02 64 8 v 3 ee ss s y 1 4 n ov 2 02 1 the proposed approach here is motivated by the well known l 2 worst case optimality property of fixed interval smoother see for example 8 and 13 it is shown in 13 that the optimal smoother minimizes l 2 norm of the exogenous signals for given measurements and also minimizes theh norm from exogenous signals to error precise description of this property is in the lemma 2 1 in the next section here we extend the l 2 cost criteria to two other cost functions which provide additional robustness properties in presence of noisy measurements and outliers the insensitive loss function one of the two loss functions considered in this paper was introduced by vapnik and coworkers see for example 18 and 19 for support vector machine svm algorithms in machine learning and regression in insensitive svm learning algorithms the cost function may be linear or quadratic and involve problem specific kernels but the common theme is that one ignores small errors which has been shown to provide more robustness and better generalizability of the algorithms support vector machines with such loss functions have also been applied in least squares regression and time series analysis see for example 14 12 and 16 as illustrated in an example presented in section 4 insensitive loss function provides good performance for both low frequency measurement noises such as unknown biases as well as high frequency noises additionally the algorithm produces smoother estimates of states making the estimates also less sensitive to high frequency measurement noises in presence of noisy data with outliers estimation algorithms based on quadratic cost function can be overly influenced by outliers resulting in overall poor performance in such cases algorithms with huber type cost functions 7 may be more appropriate as the cost function switches from quadratic to linear when errors become sufficiently large an iterative approach for estimation for linear systems using huber cost function was proposed in 4 the second cost function considered in this paper is a hybrid of huber loss function and insensitive loss function as the one considered in 14 for arma system identification the proposed cost function a tolerates small errors is insensitive and b has linear instead of quadratic penalty for large errors making the estimates less sensitive to bias and small noises as well as outliers there are instances when additional information is available about the system beyond the description of the the dynamical model examples of such additional information are maximum or minimum value of certain states for example price of an asset can never be negative or physical constraints that limit movement of an object or knowledge about possible range of changes in some states for example a constraint on change in position over time from knowledge about maximum velocity or acceleration one could also have some knowledge about the magnitude of disturbances and measurement noises such as an upper bound on measurement noise incorporating such additional information can be helpful not only in identification of outliers but can also lead to improved estimates as such information puts constraints on exogenous signals and possible trajectories of states thresholds for such constraints could be constants or related to given observations of the system such as average of some states is close to the average of the measurements one approach for estimation under such constraints is by obtaining sets constraining possible state values see for example 11 and 6 estimation algorithms have also been proposed when there are with equality constraints in dynamics of the system see for example 21 and 17 in this paper we assume the additional information about the system can be described in terms of inequality constraints that are linear with respect to states and exogenous signals this allows for a wide variety of constraints including the examples mentioned above under the proposed framework the constraints on states are not limited to ranges at a particular time but could link states at multiple periods such as constraints on average value of states over certain time interval the constraints could also be time dependent where the constraints are different in different time intervals though not explicitly considered equality constraints can also be incorporated as they can be represented as a pair of inequality constraints the second set of results presented here provide algorithms for obtaining optimal estimates for both cost functions insensitive quadratic as well as huber that also incorporate these additional constraints the algorithms proposed here can be applied for filtering fixed interval smoothing as well as for prediction of future states in all cases the estimates are obtained by solving quadratic optimization problem with linear constraints however they cannot be recursively implemented as is the case for optimalh 2 andh filtering algorithms one other difference is that while optimal h 2 and h estimates for linear systems are linear in measurements the estimates from the proposed algorithms are not linear in measurements however there are close similarities in the approaches for example the proposed algorithm for insensitive quadratic loss function reduces to the optimalh 2 andh smoothing algorithm when approaches zero this paper is organized as follows in the next section we describe the problems addressed in this paper and their connection to optimalh 2 smoothing algorithm main results are described in section 3 while the appendix contains all the proofs of these results section 4 contains an illustrative example and the last section contains a summary of the results 2 2 problem formulation throughout the paper n represents a positive integer which will be used to describe the number of measurements available for estimation for vectors v w rn v w implies that all the components of the vector v w are non negative in particular for a real valued vector v 0 would imply that all elements of the vector v are non negative for a matrix c rm n c will indicate its transpose for a vector xk rn xkj denotes the j th element of xk diagonal matrix composed of r in diagonal blocks and zero everywhere else would be denoted by diag r im will denote identity matrix of dimension m for all the estimation problems we will assume that the underlying system is known and finite dimensional linear system of the following form xk 1 axk bwk initial condition x 0 is not known with x 0 its best estimate yk cxk vk 1 where xk rn is the state yk rm are the noisy measurements wk and vk are unknown exogenous signals and measurement noises respectively given measurements y 1 yn the prediction problem involves estimating xk for k n while for filtering the goal is to estimate xn in the problem of fixed interval smoothing the goal is to estimate xk where 1 k n the proposed approach applies equally to linear time varying systems when a b and c depend on time index k in equation 1 but for ease of transparency we will assume the system parameters a rn n b rn l and c rm n are known constant matrices for any k 0 x k will denote the estimate of xk based on the given measurements y 1 yn 2 1 l 2 optimality ofh 2 andh smoothers the proposed approach is motivated by the well known l 2 optimality of fixed interval smoothing approach for both h 2 andh see for example 13 lemma 2 1 l 2 optimality of h 2 and h smoothing let p q and r be positive definite weighting matrices for uncertainties in initial conditions disturbances and measurement noise for given measurements y 1 yn of the system 1 consider the following optimization problem of minimizing l 2 norm of disturbances and noises a scaling parameter of 1 2 is added for convenience argmin x 0 x k w k v k 1 2 x 0 x 0 p x 0 x 0 n 1 k 0 w kqw k n k 1 v krv k 2 subject to constraint that the estimated states match the given observations based on 1 i e x k 1 ax k bw k 3 a yk cx k v k 3 b the solution to the optimization problem 2 subject to constraints 3 a and 3 b is given by the following two point boundary value problem x k 1 k 1 a bq 1 b c rc a x k k 0 c r yk where x 0 x 0 p 1 a 0 and n 0 4 a w k q 1 b k 4 b v k yk cx k 4 c proof of lemma for completeness a proof is provided in the appendix see also 13 for equivalent proof for continuous system and h optimality of the smoothing algorithm 4 a a similar minimax optimality criteria of kalman bucy estimation was shown by krener 8 the algorithm described in 4 a has the same form as the optimal h 2 fixed interval smoothing algorithm see for example 9 and 20 the above algorithm shows that the optimal smoother provides the state estimates based on initial conditions and disturbances that have the minimum l 2 norm and would give rise to the observed measurements yk the role of weights p q and r can be seen by considering the following unit ball of uncertainty in initial condition exogenous signals and measurement noise x 0 w k v k such that x 0 x 0 p x 0 x 0 n 1 k 0 w kqw k n k 1 v krv k 1 3 a quadratic insensitive loss function with 1 and r 1 b insensitive huber loss function with 1 r 1 and 3 the dashed line represents the region where the cost function is linear figure 1 comparison of the two loss functions considered in this paper with 1 both loss functions are the same for 4 error 4 and both are zero for 1 error 1 for larger errors when error 4 huber loss function right plot is linear and has smaller value than the quadratic loss function left plot if p is very large when its smallest eigenvalue approaches then for x 0 to be in the above unit ball x 0 x 0 or equivalently there is very little uncertainty in initial state of the system similarly r is chosen to be very large when the measurements are fairly accurate as for v k to be in the above unit ball of uncertainty with large r its l 2 norm has to be quite small in summary the smaller the weights greater is the uncertainty and vice versa remark 2 after theorem 3 1 provides the interpretation of these matrices for the fixed interval smoothing approach when noises and disturbances are white h 2 smoothing 2 2 insensitive quadratic and huber loss functions motivated by the above l 2 optimality of theh 2 smoothing algorithm we will consider estimation problems with two objective functions in this paper the two cost functions are insensitive quadratic loss function and insensitive huber loss function and are illustrated in figure 1 and described below the loss functions are parameterized by user specified positive parameters and r insensitive quadratic loss function f z r 0 if z 1 2 r z 2 if z 5 insensitive huber loss function f h z r 0 if z 1 2 r z 2 if z r z r 2 2 r if z r 6 in both cases the cost function is zero for sufficiently small z for z the difference between the above two cost functions is for larger values of z when z r in this case the huber cost function is linear rather than quadratic in z this feature of linear as opposed to quadratic cost function for large errors makes the algorithm based on huber cost function less sensitive to outliers the parameters and thresholds in the above cost functions are chosen so that the cost function is continuous has the same value at the switching points note that the above insensitive huber loss function is not completely general as choices of quadratic and linear weights r and fix the level r where the function switches from quadratic to linear the advantage of this functional form is that it makes the optimization problems more tractable while achieving the desired objective of linear penalty for large errors if is very large the threshold level for switching from quadratic to linear r would also be large and thus one would expect minimizing the above huber cost function would lead to the same results as minimizing the insensitive quadratic loss function this is indeed the case as described in the remark after theorem 3 2 utilizing results from 10 the insensitive huber loss function can be described in terms of a convex function with linear constraints 4 lemma 2 2 insensitive huber cost function as a convex optimization with linear constraints the insensitive huber cost function in 6 is given by the following optimization problem f h z r min 1 2 1 2 r 21 2 1 7 a subject to constraints z 2 and 2 0 7 b proof of lemma consider the huber cost function fh defined below without any fh z r 1 2 r z 2 if z r z r 2 2 r if z r mangasarian and musicant 10 show that fh can be computed from the following optimization problem fh z r min 1 1 2 r 21 z 1 8 a moreover optimal 1 z when z r optimal 1 r when z r and optimal 1 r when z r 8 b the relationship between insensitive huber function 6 and fh can be seen as f h z r fh 2 r where 2 max 0 z the lemma 2 2 follows from the above observation and 8 a notice that the optimization problem 7 a can be written as the following quadratic optimization problem with linear constraints f h z r min 1 2 t r 21 t subject to constraints t 2 1 t z 2 and 2 0 9 the following result that shows how lemma 2 2 can be used to convert regression problems with insensitive huber loss function 6 into a quadratic optimization this result may be of independent interest and is similar to many of the optimization problems we will encounter later for the next result on regression with insensitive huber cost function we will use the following definitions rh r 1 0 0 0 r 2 0 0 0 rm 1 m 1 m 10 lemma 2 3 regression with insensitive huber cost function for given a rm n positive definite q rn n and b rm where m n and positive parameters ri i and i consider the regression problem of obtaining x rn that minimizes the following insensitive huber cost function min x rn 1 2 x qx m i 1 f h ax b i ri i i 11 where ax b i is the i th element of the vector ax b let be the solution of the following quadratic optimization problem with linear constraints argmax 1 2 aq 1 a r 1 h b 12 a subject to constraints and 12 b then optimal x for the problem 11 can be obtained as xopt q 1 a 13 for completeness proof of the above lemma is provided in the appendix it can be seen that if one removed the constraints on in 12 b and set 0 the optimal solution x above would be the same as the least squares solution 5 2 3 estimation problem descriptions we next describe the problems addressed in this paper in all the problems below we will assume that vector rm with 0 is user specified constant vector problem 1 estimation with insensitive quadratic loss function let p q and r be given positive definite weighting matrices for initial conditions disturbances and measurement noise given measurements y 1 yn of the system 1 obtain the estimate from the following optimization problem argmin x 0 x k w k k 1 2 x 0 x 0 p x 0 x 0 n 1 k 0 w kqw k n k 1 yk cx k k r yk cx k k 14 subject to constraint that the estimated states match the given observations based on 1 and the slack variable k is less than i e x k 1 ax k bw k k in 0 n 15 a k where 1 m and i 0 are specified constants 15 b the key difference in the above optimization problem from the one in 2 is in the third term where v k yk cx k is replaced by yk cx k k if yk cx k the choice of k yk cx k satisfies constraint 15 b while also making the last term in the optimization problem 14 zero this implies that there is no difference in the last term of the cost function in 14 as long as cx k stays within a tube of thickness around yk this ability to tolerate small errors in prediction estimate make the estimation algorithm produce smoother estimates that are less sensitive to noises and exogenous signals in both high frequencies as well as well as low frequencies such as bias terms for a simple example of low sensitivity to measurement noises consider a situation where x 0 0 wk 0 xk 0 yk vk 6 0 and vk in this case while the optimal estimate from the above algorithm x k will be 0 as optimal k yk the optimal smoother described in lemma 2 1 will be nonzero since yk 6 0 an example in section 4 illustrates this robustness property in presence of bias and high frequency measurement noises the next problem considered is the estimation with insensitive huber loss function of the form described in 6 that also provides robustness against outliers problem 2 estimation with insensitive huber loss function let p q be positive definite matrices and r 1 rm 1 m and 1 m be positive scalar parameters associated with the huber cost function given measurements y 1 yn obtain the estimate from the following optimization problem argmin x 0 x k w k 1 2 x 0 x 0 p x 0 x 0 1 2 n 1 k 0 w kqw k n k 1 m j 1 fhuber ykj cx kj rj j j 16 where fhuber is the insensitive huber loss function defined in 6 and the above optimization is subject to the constraints 15 a describing the system dynamics note that the last term of the cost function 16 has m terms at each time step k with each corresponding to one of the m measurements recall yk rm this separate treatment for each measurement is in contrast to one term involving the the prediction error for each time step in 14 where there is no restriction on r being diagonal the last term with weighting matrix r as shown in the comment after theorem 3 2 the optimization problems with quadratic and huber cost function result in the same solution if r is diagonal and r rh and j is sufficiently large this is due to the fact that when j is very large the threshold error level of r when the cost function switches from quadratic to linear is also very large and thus the huber cost function effectively becomes insensitive quadratic cost function there are instances when additional information is available beyond the system dynamics described in 1 that can help in identifying noisy outliers and also enhance the accuracy of the estimate examples of additional information are maximum or minimum value of certain states for example price of an asset can never be negative or physical constraints on movement of an object or knowledge about possible range of changes in some states for example a bound on change in position from knowledge about maximum velocity or acceleration one could also have some knowledge about the magnitude of disturbances and measurement noises such as wk or vk we will assume that the additional information is in form of linear inequality constraints where the constraints could be known constants or dependent on measurements such as average of some states is within a band around the average of measurements 6 some such examples of additional information are described below a 1 n n i 1 lxi b average value between a and b where these are constants or functions of measurements yk 17 a cxi a yi cxi a yi measurement noise vi yi cxi bounded by a 17 b l xi l xi l al i xi l 1 j 0 al j 1 bwi j a yi l yi b constraint on changes over a period l 17 c we will assume that the additional information about the system can be written as linear inequality constraints involving xk and wk as is the case in all the constraints described above more specifically we will assume that the additional constraints are of the form n k 1 ukxk n 1 k 0 vkwk a where a uk and vk are known given measurements y 1 yn 18 in the above uk rp n and vk rp l are known matrices and each of the p rows of the above inequality describes a different inequality constraint for example if some state constraints are known in terms of average measurements such as lxk b 1 n n 1 yk c for all k we would have u 1 l 0 0 u 2 0 l 0 un 0 0 l vk 0 k and a b b 1 n n 1 yk c c similarly if the constraints are l xk 1 xk l a i xk lbwk a for all k 1 to n we would have u 1 un l a i 0 0 0 l a i 0 0 0 l a i v 0 vn 1 lb 0 0 0 lb 0 0 0 lb the above is an example of constraints involving state values at different times as another example of constraints linking states at different times consider a constraint on average value such as 1 n n 1 lxk 1 n n 1 yk b in this case we would have uk 1 n l and vk 0 for all k since uk and vk are dependent on time k the above formulation also allows for different constraints at different times one also notes that though vk does not appear in the general form of constraint described in 18 constraints involving vk can also be written as above for example vk a can be written as in 17 b since vk yk cxk finally though written in form of inequalities the proposed approach can also be applied to equality constraints by either additional equality constraint or writing equality constraint as a pair of inequalities for example representing equality constraint lx a as two inequality constraints lx a and lx a for the next set of problems we consider estimation with the dual objective of minimizing insensitive loss function both quadratic and huber while also incorporating such additional information about the states and exogenous signals problem 3 estimation with insensitive quadratic loss function and constraints let p q and r be given positive definite weighting matrices for uncertainties in initial conditions disturbances and measurement noise given measurements y 1 yn of the system 1 obtain the estimate from the following optimization problem argmin x 0 x k w k 1 2 x 0 x 0 p x 0 x 0 n 1 k 0 w kqw k n k 1 yk cx k k r yk cx k k 19 subject to constraints in problem 1 15 a and 15 b and the additional constraints about the system n k 1 ukx k n 1 k 0 vkw k a where a uk and vk are known given measurements y 1 yn 20 7 the next problem considers estimation with the same constraints but with huber loss function problem 4 estimation with insensitive huber loss function and constraints let p q and r be given positive definite matrices r 1 rm 1 m and 1 m be positive scalar parameters associated with the huber cost function given measurements y 1 yn of the system 1 obtain the estimate from the following optimization problem argmin x 0 x k w k 1 2 x 0 x 0 p x 0 x 0 1 2 n 1 k 0 w kqw k n k 1 m j 1 fhuber ykj cx kj rj j j 21 where fhuber is the insensitive huber loss function defined in 6 the above optimization is subject to constraint related to given dynamics 15 a and additional constraints about the system 20 in all the problems described above the goal is to estimate x 0 x n given the measurements y 1 yn in the problem of prediction we are interested in estimating future states for which we do not yet have any measurements that is estimate x n j for some j 1 while the available measurements are only y 1 yn problems 5 and 6 described below are a natural extension of the the proposed framework for estimating future states with the same two cost functions considered above problem 5 prediction with insensitive quadratic loss function and constraints let p q and r be given positive definite weighting matrices for uncertainties in initial conditions disturbances and measurement noise given measurements y 1 yn of the system 1 determine the estimate x n j for a given j 1 from the following optimization problem argmin x 0 x k w k 1 2 x 0 x 0 p x 0 x 0 n j 1 k 0 w kqw k n k 1 yk cx k k r yk cx k k 22 subject to constraints 15 a 15 b and the following n j k 1 ukx k n j 1 k 0 vkw k a where a uk and vk are known given measurements y 1 yn 23 the next problem considers the problem of prediction with constraints and insensitive huber loss function problem 6 prediction with insensitive huber loss function and constraints let p q and r be given positive definite matrices r 1 rm 1 m and 1 m be positive scalar parameters associated with the huber cost function given measurements y 1 yn of the system 1 determine the estimate x n j for a given j 1 from the following optimization problem argmin x 0 x k w k 1 2 x 0 x 0 p x 0 x 0 1 2 n j 1 k 0 w kqw k n k 1 m j 1 fhuber ykj cx kj rj j j 24 where fhuber is the insensitive huber loss function defined in 6 the above optimization is subject to constraints 15 a and 23 there are two main differences in problems 5 and 6 compared to problems 1 to 4 a the objective functions 22 and 24 include contribution of exogenous signal w k for all k n j 1 instead of just k n 1 and b the constraints 23 incorporates all constraints involving w k and x k that influence all the states up to x n j in other words last two problems also include constraints beyond the last measurement period n 3 main results we next describe solutions to the six problems described above the proposed algorithms have similarities with the kalman bucy smoothing algorithm there are however differences as the proposed algorithms require an additional step of solving a quadratic optimization problem moreover while kalman andh filtering algorithms can be implemented recursively the proposed algorithm for filtering is not recursive as at each time step one has to estimate all the states from the initial time to the final step thus as the available data or n grows the size of the optimization problem also grows and to keep the algorithm tractable for real time applications with increasing number of measurements one may need to implement the algorithm with the most recent n measurements in the results below one would notice similarities in algorithms for insensitive quadratic and huber cost functions and they are discussed after theorem 3 2 8 the following definitions will be used in describing the results f cb 0 0 cab cb 0 can 2 b cb 0 can 1 b cab cb y y 1 cax 0 yn can x 0 qinv q 1 0 0 0 q 1 0 0 0 q 1 25 a rinv r 1 0 0 0 r 1 0 0 0 r 1 rhinv r 1 h 0 0 0 r 1 h 0 0 0 r 1 h where rh is defined in 10 25 b im im k im im where and are defined in 10 and k rmn 25 c m fqinvf rinv ca can p 1 a c a nc 25 d mh fqinvf rhinv ca can p 1 a c a nc 25 e the result below shows that the optimal solution to problem 1 described by 14 subject to constraints 15 a and 15 b can be obtained by solving a standard quadratic optimization with linear constraints theorem 3 1 problem 1 let rnm be the solution of the following quadratic optimization problem with linear constraints argmax 1 2 m y 26 a subject to constraints and 26 b let k be obtained from optimal as follows k 1 a k c k n 0 where 1 n and k rm 27 with k obtained as above the optimal values for x k w k that minimize 14 are obtained as follow x k 1 ax k bq 1 b k x 0 x 0 p 1 a 0 28 a w k q 1 b k 28 b proof of the above theorem is provided in the appendix remark 1 one notes that in the optimal kalman bucy orh 2 smoother described in 4 a k 1 a k c r yk cx k comparing 4 a to the equation 27 it is easily observed that both algorithms have the same k and x k if r yk cx k k thus k can be thought of as the scaled innovation term equivalent of r yk cx k in 4 a for insensitive loss function remark 2 to see the the similarity of the above result to the minimum variance interpretation of optimal smoother we will make the following assumptions where x 0 0 is assumed for simplicity as the impact of non zero x 0 0 can be easily incorporated due to linearity of the dynamical system x 0 0 e x 0 x 0 p 1 e wkw j q 1 kj e vkv j r 1 kj 9 where x 0 wk and vk are also assumed to be zero mean and independent of each other under these assumptions it is well known that the kalman bucy or optimalh 2 smoothing algorithm is the optimal linear minimum variance estimate of x given y see for example 20 or equivalently x e xy e yy 1 y is the same estimate x as that obtained from 4 a where x x 1 xn y y 1 yn x x 1 x n as one might expect if 0 the smoothing algorithm described in theorem 3 1 should also reduce to the same estimate of x to see that that is indeed the case first note that when 0 optimal solution of 26 a is m 1 y m 1 y since we are assuming x 0 0 since xk akx 0 k 1 i 0 a k 1 ibwi and yk cakx 0 c k 1 i 0 a k 1 ibwi vk under the covariance assumptions on initial conditions and disturbances and independence of x 0 wk and vk one obtains e yy ca can p 1 a c a nc fqinvf rinv e yy m e xy a an p 1 a c a nc b 0 0 ab b 0 an 1 b ab b qinvf from 27 one notes that f b 0 b n 1 and c a c a n 1 c 0 this together with the fact that m 1 y e yy 1 y when 0 one observes that e xy e yy 1 y e xy a an p 1 a 0 b 0 0 ab b 0 an 1 b ab b qinv b 0 b n 1 finally one observes that the optimal estimate x described in 28 a is recursive implementation of the right hand side above the next result provides solution to problem 2 which is the estimation problem with insensitive huber loss function theorem 3 2 problem 2 let rnm be the solution of the following quadratic optimization problem with linear constraints argmax 1 2 mh y 29 a subject to constraints k and k 29 b where and k are as defined in 25 c let k be obtained from optimal as follows k 1 a k c k n 0 where 1 n and k rm 30 with k obtained as above the optimal values for x k w k that minimize 16 are obtained as follow x k 1 ax k bq 1 b k x 0 x 0 p 1 a 0 31 a w k q 1 b k 31 b 10 the primary difference between theorem 3 1 and 3 2 is the additional constraint of k which is kj j for all k and j one key observation from comparing theorems 3 1 and 3 2 is that the optimal and thus the optimal estimates x k are the same for both if r rh diag ri and k or equivalently is very large to see this note that m mh when r rh and when k is very large the constraint k is not active and thus optimal in theorems 3 1 and 3 2 are the same for intuition behind this observation note that insensitive huber cost function effectively becomes insensitive quadratic cost function when is very large as the threshold j j rj for switch from quadratic to linear is not reached in the subsequent results as well we will notice that the estimates from both insensitive quadratic and huber cost functions are the same if r rh and k or equivalently is very large for the next result define v v 0 vn 1 g b b a b a n 1 0 b b a n 2 0 0 b u 1 u n h ca can n i 1 uia i 32 t f g v qinv f g v rinv 0 0 0 hp 1 h 33 th f g v qinv f g v rhinv 0 0 0 hp 1 h 34 the following result for problem 3 describes the algorithm to obtain the optimal estimate for the insensitive quadratic loss function 19 subject to additional knowledge about the system in terms of constraints defined in 20 theorem 3 3 problem 3 let rnm and rp be the solution of the following quadratic optimization problem with linear constraints argmax 1 2 t y a n i 1 uia ix 0 35 a subject to constraints and 0 35 b let k be obtained from optimal and as follows k 1 a k c k u k n 0 where 1 n and k rm 36 with k obtained as above the optimal values for x k w k that minimize 19 subject to constraints 15 a 15 b and 20 are obtained as follow x k 1 ax k bq 1 b k bq 1 v k x 0 x 0 p 1 a 0 37 a w k q 1 b k q 1 v k the next result for problem 4 describes the algorithm to obtain the optimal estimate for the insensitive huber loss function subject to additional knowledge about the system in terms of constraints defined in 20 theorem 3 4 problem 4 let rnm and rp be the solution of the following quadratic optimization problem with linear constraints argmax 1 2 th y a n i 1 uia ix 0 38 a subject to constraints k k k k and 0 38 b 11 where and k are as defined in 25 c let k be obtained from optimal and as follows k 1 a k c k u k n 0 where 1 n and k rm 39 with k obtained as above the optimal values for x k w k that minimize 21 subject to constraints 15 a and 20 are obtained as follow x k 1 ax k bq 1 b k bq 1 v k x 0 x 0 p 1 a 0 40 a w k q 1 b k q 1 v k for describing the results of the prediction problems problems 5 and 6 where the goal is to estimate x n j where j 1 given available measurements only up to n we will use the following notation f f 0 nm jl where 0 nm jl rnm jl where each element of the matrix is 0 41 a g b b a b a n j 1 0 b b a n j 2 0 0 b u 1 u n j v v 0 vn j 1 h ca can n j i 1 uia i 41 b q inv q 1 0 0 0 q 1 0 0 0 q 1 a diagonal matrix with q 1 in diagonals 41 c t f g v q inv f g v rinv 0 0 0 h p 1 h 42 t h f g v q inv f g v rhinv 0 0 0 h p 1 h 43 the following result for problem 5 describes the algorithm to obtain the optimal prediction for the insensitive quadratic loss function 22 subject to additional knowledge about the system 23 theorem 3 5 problem 5 let rnm and rp be the solution of the following quadratic optimization problem with linear constraints argmax 1 2 t y a n j i 1 uia ix 0 44 a subject to constraints and 0 44 b let k be obtained from optimal and as follows k 1 a k u k with n j 0 for n 1 k n j 45 a k 1 a k c k u k for 1 k n where 1 n and k rm 45 b with k obtained as above the optimal values for x k w k that minimize 22 subject to constraints 15 a 15 b and 23 are obtained as follow x k 1 ax k bq 1 b k bq 1 v k x 0 x 0 p 1 a 0 46 a w k q 1 b k q 1 v k 12 the next result describes the solution to the prediction problem with insensitive huber cost function problem 6 theorem 3 6 problem 6 let rnm and rp be the solution of the following quadratic optimization problem with linear constraints argmax 1 2 t h y a n j i 1 uia ix 0 47 a subject to constraints k k and 0 47 b where and k are as defined in 25 c let k be obtained from optimal and as follows k 1 a k u k with n j 0 for n 1 k n j 48 a k 1 a k c k u k for 1 k n where 1 n and k rm 48 b with k obtained as above the optimal values for x k w k that minimize 24 subject to constraints 15 a and 23 are obtained as follow x k 1 ax k bq 1 b k bq 1 v k x 0 x 0 p 1 a 0 49 a w k q 1 b k q 1 v k for all three results described above the dimension of the variables and thus the computational requirements continue to grow as the size of the data n grows unlike recursive kalman bucy and h filtering algorithms to obtain the estimate x n 1 with the addition of new data yn 1 one has to recompute all the states x 0 x n 1 in real time applications of filtering and prediction this might be computationally prohibitive in such cases a practical way to use the proposed approach would be to use the most recent n observations where n is chosen as large as computationally feasible in such cases the initial condition prior estimate the variable defined as x 0 here would be replaced with the estimate of states n time units ago 4 illustrative example we consider the linear dynamical system of the form 1 where a 0 8 1 0 3 0 4 b 0 5 2 c 1 0 x 0 1 1 x 0 0 0 the weights for initial condition exogenous signal and parameters for the two cost functions considered in the paper are q 1 r 1 p 1 0 0 1 5 4 for the simulation exercises it is assumed that exogenous signals and measurement noises are of the form wk 5 r 1 k where r 1 k n 0 1 vk 5 r 2 k 5 1 k 6 where r 2 k n 0 1 vk vk 20 for k 8 9 10 11 additional large noise to represent outliers where n 0 1 denotes normal random variable with mean of zero and standard deviation of one note that the measurement noise vk has bias high frequency components as well as outliers all measurements have a bias of 6 high frequency oscillatory term of 5 1 k and an additional measurement noise of 20 for k between 8 and 11 representing outliers several simulations were performed under these assumptions for twenty data points n 20 table 1 shows the comparison of average root mean square and absolute errors for the proposed estimation algorithms 13 kalman bucy smoothing insensitive quadratic insensitive huber root mean square error x 1 13 28 12 77 12 24 root mean square error x 2 8 44 7 41 7 35 average absolute error x 1 10 16 9 75 9 35 average absolute error x 2 6 5 5 77 5 65 table 1 comparison of average root mean square and absolute errors a x 1 k actual value and its estimates using different algo rithms b x 2 k actual value and its estimates using different algo rithms figure 2 comparison of actual value estimates using optimal kalman bucy smoother optimal insensitive quadratic and huber loss function estimators the plot on the left is for the first state while the one on the right is for the second state estimates based on 20 measurements n 20 the insensitive estimators have slightly lower error and have smoother trajectories less abrupt changes with kalman bucy h 2 smoothing algorithm one notes from table 1 that both insensitive quadratic as well as huber cost function approaches algorithms described in theorems 3 1 and 3 2 perform slightly better than kalman bucy smoothing algorithm in terms of lower root mean square and absolute errors as one might expect performance of algorithm based on huber cost function is slightly better due to presence of very noisy measurements for k from 8 and 11 figure 2 shows the comparison of actual value as well as estimates using different algorithms for one sample path next we extend this example to estimation in presence of constraints while all the other assumptions about exogenous signals noises and the dynamical system are the same as above for the constraint we consider a saturation non linearity in the dynamical system such that the second state cannot exceed 4 i e 0 1 xk 4 k assumed known constraint of saturation value of second state table 2 shows the comparison of average root mean square and absolute errors for the proposed estimation algorithms with kalman bucy h 2 smoothing algorithm the estimates for insensitive algorithms are based on theorems 3 3 and 3 4 here the improved performance of the insensitive algorithms is not only due to lower sensitivity to measurement noises but also due to the ability to incorporate constraints in the estimates figure 3 shows the comparison of actual value as well as estimates using different algorithms for one sample path 5 summary this paper presents optimal estimation algorithms for linear dynamical systems with insensitive loss functions such an optimization criteria is often used in support vector machines and provides greater robustness and lower sensitivity to measurement noises as small errors are ignored one of the cost functions is insensitive for which the penalty function switches from quadratic to linear for large errors and thus making the estimates less sensitive to outliers we also present algorithms to estimate states with the same objective functions while also incorporating additional constraints about the system results are also provided for the prediction problem where the goal is to estimate future states of the system though not recursive the algorithms are are easily implemented as they involve solving quadratic kalman bucy smoothing insensitive quadratic insensitive huber root mean square error x 1 13 34 12 08 11 58 root mean square error x 2 8 56 6 14 6 07 average absolute error x 1 10 06 9 7 9 1 average absolute error x 2 6 79 4 2 4 2 table 2 comparison of average root mean square and absolute errors with state constraint x 2 4 14 a x 1 k actual value and its estimates using different algo rithms b x 2 k actual value and its estimates using different algo rithms note that the proposed algorithms satisfy the constraint and provide noticeably more accurate estimate of x 2 k figure 3 comparison of actual value estimates using optimal kalman bucy smoother optimal insensitive quadratic and huber loss function estimators under the constraint that x 2 k 4 for all k the plot on the left is for the first state while the one on the right is for the second state estimates based on 20 measurements n 20 optimization problems with linear constraints an example illustrates the improved performance of the proposed algorithms compared to kalman bucy or optimalh 2 smoothing algorithm a appendix this first part of the appendix provides proof of lemmas described in the problem formulation section lemmas 2 1 and 2 3 the proofs of estimation problems with insensitive quadratic cost function theorems 3 1 3 3 and 3 5 are provided next as they are similar to each other proofs of algorithms with insensitive huber cost function are provided at the end of this section a 1 proof of lemma 2 1 substituting for v k yk cx k the lagrangian for the optimization problem 2 subject to constraints 3 a and 3 b is l 1 2 x 0 x 0 p x 0 x 0 n 1 k 0 w kqw k n k 1 yk cx k r yk cx k n k 0 k x k 1 ax k bw k where k rn for k 0 to n are the lagrange multipliers the necessary conditions for minimum which are also sufficient given convex cost function and linear constraints are l x k 0 for 1 k n c rcx k c ryk k 1 a k 0 k 1 a k c rcx k c ryk 50 a l x 0 0 px 0 px 0 a 0 0 x 0 x 0 p 1 a 0 l x n 1 0 n 0 50 b l w k 0 qw k b k 0 w k q 1 b k 50 c l k 0 x k 1 ax k bw k 50 d from equations 50 c and 50 d one obtains x k 1 ax k bq 1 b k optimal smoother described in 4 a follows from the above optimality conditions 15 a 2 proof of lemma 2 3 from lemma 2 2 the optimal x for problem 11 can be obtained from the following optimization problem min x 1 2 t 1 2 x qx 1 2 m i 1 ri 2 1 i m i 1 iti subject to constraints ax b 2 2 0 and 1 2 t 51 the above optimization problem can be written in terms of its lagrangian as max 1 2 1 2 min x 1 2 t 1 2 x qx 1 2 1 rh 1 t 1 2 ax b 2 2 ax b 1 t 1 2 2 t 1 2 2 with 1 0 2 0 1 0 2 0 0 due to inequality constraints with l defined as the lagrangian expression above karush kuhn tucker kkt conditions for optimality are l x 0 x q 1 a 2 1 52 a l 1 0 1 r 1 h 2 1 52 b l 2 0 1 2 1 2 0 52 c l t 0 1 2 52 d 1 2 1 2 0 for all i 0 52 e 1 2 ax b 0 2 2 ax b 0 1 t 1 2 0 2 t 1 2 2 0 52 f utilizing the above optimality conditions we next show two facts 1 i 2 i 2 i 1 i 1 i 2 i for all i i 1 i 2 i i for all i if 2 i 0 then i 0 and thus from 52 c one observes that 1 i 2 i 2 i 1 i if 2 i 0 then 1 i 0 since from 8 b 1 i 2 i if 2 i i ri from the above optimal expression for 1 above we conclude that 2 i 1 i 0 when 1 i 0 since rh is diagonal thus in this case also 1 i 2 i 2 i 1 i since 0 1 i 2 i 2 i 1 i 0 note that from kkt optimality condition if 1 i 6 0 then 2 i 0 and vice versa since i 0 and 2 i 0 because 1 i 2 ax b i 0 and 2 i 2 ax b i 0 thus 1 i 2 i 2 i 1 i for all i to see the second claim above let us consider two separate cases of ti 0 and ti 0 in the first case ti 0 implies 1 i 2 i from lemma 2 2 equation 8 b we know that 1 i 2 i when 1 i i ri from the optimal expression of 1 i above this implies 2 i 1 i i when ti 0 at least one of 1 i or 2 i has to be zero and thus 2 i 1 i 1 i 2 i i thus 2 i 1 i i in all cases since 2 i 1 i 1 i 2 i 0 2 i 1 i 2 i 1 i i from this and the first claim above one notes that i 1 i 2 i i utilizing these two facts the lagrangian for the optimization problem is l 1 2 2 1 aq 1 a 2 1 1 2 2 1 r 1 h 2 1 1 2 2 1 b 1 2 2 1 aq 1 a 2 1 1 2 2 1 r 1 h 2 1 1 2 2 1 b 1 2 aq 1 a r 1 h b where 2 1 the second equality above holds due to the fact that 1 ri 2 i 1 i 2 1 ri 2 i 1 i 2 as shown above i i i and with one obtains the results described in lemma 2 3 16 a 3 proof of theorem 3 1 the optimization problem 14 subject to constraints 15 a to 15 b can be described as follows in terms of its lagrangian where k rn k and k rm are the lagrange multipliers max k k k min x 0 x k w k k l with k 0 and k 0 where l 1 2 x 0 x 0 p x 0 x 0 n 1 k 0 w kqw k n k 1 yk cx k k r yk cx k k n k 0 k x k 1 ax k bw k n k 1 k k n k 1 k k 53 in the above k 0 and k 0 because of the constraint 15 b the necessary conditions for optimality are l x k 0 for 1 k n c rcx k c r yk k k 1 a k 0 k 1 a k c rcx k c r yk k 54 a l x 0 0 px 0 px 0 a 0 0 x 0 x 0 p 1 a 0 54 b l x n 1 0 n 0 54 c l w k 0 qw k b k 0 w k q 1 b k 54 d l k 0 r k r yk cx k k k 0 k yk cx k r 1 k k 54 e k 0 k k 0 k 0 k k 0 and l k 0 x k 1 ax k bw k 54 f define k k k 1 n 55 let k k 1 km k k 1 km and k i be the i th element of k from kkt optimality conditions as in the proof of lemma 2 3 we know that ki k i 0 and ki k i 0 i and k thus ki 6 0 only if k i 0 and ki 6 0 only if k i 0 since 0 it follows that if ki 6 0 then ki 0 and if ki 6 0 then ki 0 since ki ki 0 and at most only one of them can be non zero one observes ki ki ki ki for all i 1 m and k 1 n k k k where k k 1 km 56 from 54 a 54 c and 54 e and the fact that k k k one obtains the expression for optimal k described in 27 similarly one obtains optimal expression for obtaining x k described in 28 a from 54 d and 54 f one also notes that k 1 a k c k n 0 b 0 b n 1 f where f is defined in 25 a 57 a x k 1 ax k bq 1 b k x 0 x 0 p 1 a 0 cx 1 cx n fqinv b 0 b n 1 ca can x 0 p 1 a 0 57 b 17 from the above and noting from that 0 c a c a n 1 c from 57 a one observes that cx 1 cx n fqinvf ca can p 1 a c a n 1 c ca can x 0 58 n 1 k 0 kb q 1 b k fqinvf 59 incorporating the above together with 54 b to 54 e the definition of k k k and the fact that k k k the lagrangian in equation 53 becomes l 1 2 0 ap 1 a 0 n 1 k 0 kb q 1 b k n k 1 k k r 1 k k n k 1 k k n k 1 k k r 1 k k n k 1 k k yk cx k 1 2 0 ap 1 a 0 fqinvf rinv n k 1 k rinv cx 1 cx n y 1 yn 1 2 m n k 1 k y where m and y are as defined in 25 d and 25 a in obtaining the last expression above we utilized the fact that 0 c a c a n 1 c and the equation 58 since 0 and k 0 one notes that for any 1 2 m n k 1 k y max 1 2 m y subject to constraints where is defined in 25 c from above one notes that the dual of the optimization problem 14 subject to constraints 15 a and 15 b is as described by 26 a subject to constraint 26 b since optimal a 4 proof of theorem 3 3 the optimization problem 19 subject to constraints 15 a 15 b and 20 can be written in terms of its lagrangian as follows where k rn k and k rm and rl are the lagrange multipliers max k k k min x 0 x k w k l with k 0 k 0 and 0 where l 1 2 x 0 x 0 p x 0 x 0 n 1 k 0 w kqw k n k 1 yk cx k k r yk cx k k n k 0 k x k 1 ax k bw k n k 1 k k n k 1 k k a n k 1 ukx k n 1 k 0 vkw k 61 18 similar to the proof above the optimality conditions are l x k 0 for 1 k n c rcx k c r yk k k 1 a k u k 0 k 1 a k c rcx k c r yk k u k 62 a l x 0 0 px 0 px 0 a 0 0 x 0 x 0 p 1 a 0 62 b l x n 1 0 n 0 62 c l w k 0 qw k b k v k 0 w k q 1 b k q 1 v k 62 d l k 0 r k r yk cx k k k 0 k yk cx k r 1 k k 62 e k 0 k 0 0 k k 0 k k 0 a n k 1 ukx k n 1 k 0 vkw k 0 62 f l k 0 x k 1 ax k bw k 62 g from 62 a to 62 g and noting the definition of in 55 one observes that k 1 a k c k u k n 0 b 0 b n 1 f g where f and g are defined in 25 a and 32 63 n 1 k 0 w kqw k f g v qinv f g v where v is defined in 32 64 x k 1 ax k bq 1 b k bq 1 v k x 0 x 0 p 1 a 0 cx 1 cx n fqinv f g v ca can x 0 p 1 a 0 65 v 0 vn 1 w 0 w n 1 v qinv f g v 66 u 1 un x 1 x n g qinv f g v n i 1 uia i x 0 p 1 a 0 67 19 from 63 one also observes that 0 c a c a n 1 c n i 1 a i 1 u i this together with the above implies that 0 ap 1 a 0 hp 1 h where h is as defined in 32 68 a ca can p 1 a 0 n i 1 uia i p 1 a 0 hp 1 h 68 b cx 1 cx n v 0 vn 1 w 0 w n 1 u 1 un x 1 x n f g v qinv f g v hp 1 h n i 1 uia i x 0 ca can x 0 68 c using the above identities the expression of lagrangian 61 simplifies into the following form after some algebraic simplification l 1 2 0 ap 1 a 0 n 1 k 0 w kqw k rinv n k 1 k k y 1 yn cx 1 cx n rinv u 1 un x 1 x n v 0 vn 1 w 0 w n 1 a 1 2 hp 1 h f g v qinv f g v rinv n k 1 k k y a n i 1 uia ix 0 1 2 t n k 1 k k y a n i 1 uia ix 0 where t is defined in 33 69 as in proof of the theorem 3 1 from kkt optimality conditions one notes that since 0 if ki 0 then ki 0 and if ki 0 then ki 0 thus just as in equation 56 here also for the optimal solution it is true that k k k k k since 0 as in the previous case one observes that for any n k 1 k k n k 1 k max subject to constraints the optimization problem described in 35 a and 35 b to obtain optimal and follows from 69 and the above a 5 proof of theorem 3 5 the proof is similar to that of theorem 3 3 and so we will only highlight key differences the lagrangian for the optimization problem 22 subject to constraints 15 a 15 b and 23 can be described as follows where the main 20 differences compared to the lagrangian for theorem 3 2 are in upper limits of the summation in some of the terms max k k k min x 0 x k w k l with k 0 k 0 and 0 where l 1 2 x 0 x 0 p x 0 x 0 n j 1 k 0 w kqw k n k 1 yk cx k k r yk cx k k n j k 0 k x k 1 ax k bw k n k 1 k k n k 1 k k a n j k 1 ukx k n j 1 k 0 vkw k 70 as in the previous case k rn k and k rm and rl are the lagrange multipliers with k 0 k 0 and 0 due to the inequality constraints the necessary conditions for optimality are l x k 0 for 1 k n c rcx k c r yk k k 1 a k u k 0 k 1 a k c rcx k c r yk k u k for 1 k n 71 a l x k 0 for n 1 k n j k 1 a k u k for n 1 k n j 71 b l x 0 0 px 0 px 0 a 0 0 x 0 x 0 p 1 a 0 71 c l x n j 1 0 n j 0 71 d l w k 0 qw k b k v k 0 w k q 1 b k q 1 v k 71 e l k 0 r k r yk cx k k k 0 k yk cx k r 1 k k 71 f k 0 k 0 0 k k 0 k k 0 a n j k 1 ukx k n j 1 k 0 vkw k 0 71 g l k 0 x k 1 ax k bw k 71 h equations 45 a and 45 b for obtaining optimal k follow from 71 a 71 b 71 d and 71 f from 45 a and 45 b one also notes b 0 b n j 1 f g where f and g are defined in 41 a and 41 b 72 a n j 1 k 0 w kqw k f g v q inv f g v where v is defined in 41 b 72 b from equations 45 a and 45 b one observes that 0 c a c a n 1 c n j i 1 a i 1 u i as in the proof of the previous theorem one can show that 0 ap 1 a 0 h p 1 h where h is as defined in 41 b 21 cx 1 cx n v 0 vn j 1 w 0 w n j 1 u 1 un x 1 x n j f g v q inv f g v h p 1 h n j i 1 uia i x 0 ca can x 0 using the above the expression of lagrangian 70 simplifies into the following form after some algebraic simplification l 1 2 0 ap 1 a 0 n j 1 k 0 w kqw k rinv n k 1 k k y 1 yn cx 1 cx n rinv u 1 un j x 1 x n j v 0 vn j 1 w 0 w n j 1 a 1 2 h p 1 h f g v qinv f g v rinv n k 1 k k y a n j i 1 uia ix 0 1 2 t n k 1 k k y a n j i 1 uia ix 0 where t is defined in 42 as before kkt optimality condition implies that since 0 k k k k k rest of the proof follows along the same lines as the one in the previous section a 6 proof of theorem 3 2 utilizing lemma 2 2 we can write the huber loss expression in terms of the optimization problem with variables 1 k 2 k r m and sk 1 k 2 k for each k 1 n n k 1 m j 1 f ykj cx kj rj j j min 1 k 2 k n k 1 m j 1 1 2 rj 2 1 kj j 2 kj 1 kj 74 a min 1 k 2 ksk n k 1 m j 1 1 2 rj 2 1 kj jskj 74 b subject to the constraints ykj cx kj j 2 kj for all k 1 n and j 1 m ykj cx kj j 2 kj for all k 1 n and j 1 m 2 k 0 for all k 1 n skj 2 kj 1 kj for all k 1 n and j 1 m skj 1 kj 2 kj for all k 1 n and j 1 m 22 incorporating the above the lagrangian for the optimization problem 16 can be written as max k 1 k 2 k 1 k 2 k k min x 0 x k w k k 1 k 2 sk l subject to constraints 1 k 2 k 1 k 2 k k 0 where l 1 2 x 0 x 0 p x 0 x 0 1 2 n 1 k 0 w kqw k 1 2 m j 1 n k 1 rj 2 1 kj m j 1 n k 1 jskj n k 0 k x k 1 ax k bw k n k 1 1 k 2 k yk cx k 2 k 2 k yk cx k 1 k sk 1 k 2 k 2 k sk 1 k 2 k k 2 k 75 the constraints om 1 k 2 k 1 k 2 k k 0 are due to the corresponding inequality constraints the optimality conditions for the lagrangian above are l 1 kj 0 1 kj 1 rj 1 kj 2 kj or equivalently 1 k r 1 h 1 k 2 k 76 a l 2 k 0 1 k 2 k 1 k 2 k k 76 b l sk 0 2 k 1 k where is defined in 10 76 c l x k 0 for 1 k n k 1 a k c 1 k 2 k 76 d l x 0 0 x 0 x 0 p 1 a 0 76 e l x n 1 0 n 0 76 f l w k 0 w k q 1 b k 76 g l k 0 x k 1 ax k bw k 76 h 1 k 2 k 1 k 2 k k 0 due to the inequality constraints 76 i substituting the above l 1 2 0 ap 1 a 0 n 1 k 0 kb q 1 b k m j 1 n k 1 1 rj 1 kj 2 kj 2 n k 1 1 k 2 k m j 1 n k 1 1 rj 1 kj 2 kj 2 n k 1 1 k k yk cx k 77 as in the proof of lemma 2 3 one can show that a 1 k 2 k 1 k 2 k 1 k 2 k and b 1 k 2 k let and be defined as k 1 k 2 k 1 n 23 one can proceed as in the proof of 3 1 and the optimality conditions above to show that 0 c a c a n 1 c and l 1 2 0 ap 1 a 0 fqinvf m j 1 n k 1 1 rj 2 kj n k 1 k m j 1 n k 1 1 rj 2 kj cx 1 cx n y 1 yn 1 2 0 ap 1 a 0 fqinvf n k 1 kr 1 h k n k 1 k n k 1 kr 1 h k cx 1 cx n y 1 yn 1 2 mh n k 1 k y where mh and y are as defined in 25 e and 25 a rest of the proof follows as in the proof of theorem 3 1 except that there is an additional constraint that k a 7 proof of theorems 3 4 and 3 6 proof of theorems 3 4 and 3 6 are are analogous to proofs of theorems 3 3 and 3 5 where the huber cost function is incorporated as shown in 74 b and the proof of theorem 3 2 references 1 b d o anderson and j b moore optimal filtering upper saddle river prentice hall 1979 2 s c chan z zhang and k w tse a new robust kalman filter algorithm under outliers and system uncertainties ieee international symposium on circuits and systems proceedings volume 5 2005 3 w chu s s keerthi c j ong a general formulation for support vector machines proceedings of the 9 th international conference on neural information processing 2002 4 z m durovic and b d kovacevic robust estimation with unknown noise statistics ieee transactions on automatic control volume 44 issue 6 1999 5 a elsayed and m j grimble a new approach toh design of optimal digital linear filters ima j math contr informat volume 6 issue 8 1989 6 a garulli a vicino and g zappa conditional central algorithms for worst case set membership identification and filtering ieee transactions on automatic control volume 45 issue 1 2000 7 p j huber robust statistics new york john wiley 1981 8 a j krener kalman bucy and minimax filtering ieee transactions on automatic control volume 25 issue 2 1980 9 j s meditch stochastic optimal linear estimation and control mcgraw hill 1969 10 o l mangasarian and d r musicant robust linear and support vector regression ieee transactions on pattern analysis and machine intelligence volume 22 no 9 2000 11 m milanese and a vicino optimal estimation theory for dynamic systems with set membership uncertainty an overview automatica volume 26 issue 6 1991 12 k r muller a smola g ratsch b sch lkopf j kohlmorgen and v vapnik predicting time series with support vector machines international conference on artificial neural networks icann 1997 13 k m nagpal and p p khargonekar filtering and smoothing in anh setting ieee transactions on automatic control volume 36 issue 2 1991 14 j l rojo alvarez m martinez ramon m de prado cumplido a artes rodriguez and a r figueiras vidal support vector method for robust arma system identification ieee transactions on signal processing volume 52 no 1 2004 15 u shaked h control minimum error state estimation of linear stationary processes ieee transactions on automatic control vol 35 issue 5 1990 16 j a k suykens t van gestel j de brabanter b de moor and j vandewalle least squares support vector machines world scientific 2002 24 17 b o s teixeira j chandrasekar l a b t rres l a aguirre and dennis s bernstein state estimation for linear and non linear equality constrained systems international journal of control 82 5 2009 18 v vapnik s golowich and a smola support vector method for function approximation regression estimation and signal processing advances in neural information processing systems 9 pages 81 287 mit press 1997 19 v vapnik statistical learning theory john wiley sons new york 1998 20 h l weinert fixed interval smoothing for state space models springer science 2001 21 l xu x rong li z duan and j lan modeling and state estimation for dynamic systems with linear equality constraints ieee transactions on signal processing volume 61 2013 25 1 introduction 2 problem formulation 2 1 l 2 optimality of h 2 and h smoothers 2 2 insensitive quadratic and huber loss functions 2 3 estimation problem descriptions 3 main results 4 illustrative example 5 summary a appendix a 1 proof of lemma 2 1 a 2 proof of lemma 2 3 a 3 proof of theorem 3 1 a 4 proof of theorem 3 3 a 5 proof of theorem 3 5 a 6 proof of theorem 3 2 a 7 proof of theorems 3 4 and 3 6