manuscript no will be inserted by the editor secant penalized bfgs a noise robust quasi newton method via penalizing the secant condition brian irwin eldad haber received date accepted date abstract in this paper we introduce a new variant of the bfgs method designed to perform well when gradient measurements are corrupted by noise we show that by treating the secant condition with a penalty method approach motivated by regularized least squares estimation one can smoothly interpo late between updating the inverse hessian approximation with the original bfgs update formula and not updating the inverse hessian approximation furthermore we find the curvature condition is smoothly relaxed as the in terpolation moves towards not updating the inverse hessian approximation disappearing entirely when the inverse hessian approximation is not updated these developments allow us to develop a method we refer to as secant penal ized bfgs sp bfgs that allows one to relax the secant condition based on the amount of noise in the gradient measurements sp bfgs provides a means of incrementally updating the new inverse hessian approximation with a con trolled amount of bias towards the previous inverse hessian approximation which allows one to replace the overwriting nature of the original bfgs up date with an averaging nature that resists the destructive effects of noise and can cope with negative curvature measurements we discuss the theoretical properties of sp bfgs including convergence when minimizing strongly con vex functions in the presence of uniformly bounded noise finally we present extensive numerical experiments using over 30 problems from the cutest test problem set that demonstrate the superior performance of sp bfgs compared to bfgs in the presence of both noisy function and gradient evaluations keywords quasi newton methods secant condition penalty methods least squares estimation measurement error noise robust optimization corresponding author brian irwin eldad haber department of earth ocean and atmospheric sciences the university of british columbia vancouver bc canada e mail birwin haber eoas ubc ca ar x iv 2 01 0 01 27 5 v 2 m at h o c 1 1 ju l 20 21 https orcid org 0000 0002 6086 4359 2 brian irwin eldad haber 1 introduction over the past 50 years quasi newton methods have proved to be some of the most economical and effective methods for a variety of optimization problems originally conceived to provide some of the advantages of second order meth ods without the full cost of newton s method quasi newton methods which are also referred to as variable metric methods 24 are based on the obser vation that by differencing observed gradients one can calculate approximate curvature information this approximate curvature information can then be used to improve the speed of convergence especially in comparison to first or der methods such as gradient descent there are currently a variety of different quasi newton methods with the broyden fletcher goldfarb shanno bfgs method 6 13 15 39 almost certainly being the best known quasi newton method modern quasi newton methods were developed for problems involving the optimization of smooth functions without constraints the bfgs method is the best known quasi newton method because in practice it has demonstrated superior performance due to its very effective self correcting properties 33 accordingly bfgs has since been extended to handle box constraints 8 and shown to be effective even for some nonsmooth optimization problems 27 furthermore a limited memory version of bfgs known as l bfgs 29 has become a favourite algorithm for solving optimization problems with a very large number of variables as it avoids directly storing approximate inverse hes sian matrices however bfgs and its relatives were not designed to explicitly handle noisy optimization problems and noise can unacceptably degrade the performance of these methods the authors of 7 make the important observation that quasi newton up dating is inherently an overwriting process rather than an averaging process fundamentally differencing noisy gradients can produce harmful efffects be cause the resulting approximate curvature information may be inaccurate and this inaccurate curvature information may overwrite accurate curvature information newton s method can naturally be viewed as a local rescaling of coordinates so that the rescaled problem is better conditioned than the orig inal problem quasi newton methods attempt to perform a similar rescaling but instead of using the inverse hessian matrix to obtain curvature infor mation for the rescaling they use differences of gradients to obtain curvature information thus it should be unsurprising that inaccurate curvature infor mation obtained from differencing noisy gradients can be problematic because it means the resulting rescaling of the problem can be poor and the condi tioning of the rescaled problem could be even worse than the conditioning of the original problem with the above in mind several works have dealt with how to improve the performance of quasi newton methods in the presence of noise many recent works focus on the empirical risk minimization erm problem which is ubiq uitous in machine learning for example in 7 the authors propose a technique designed for the stochastic approximation sa regime that employs subsam secant penalized bfgs 3 pled hessian vector products to collect curvature information pointwise and at spaced intervals in contrast to the classical approach of computing the differ ence of gradients at each iteration this work is built upon in 31 where the authors present a stochastic l bfgs algorithm that draws upon the variance reduction approach of 23 in 40 the authors outline a stochastic damped limited memory bfgs sdlbfgs method that employs damping techniques used in sequential quadratic programming sqp a stochastic block bfgs method that updates the approximate inverse hessian matrix using a sketch of the hessian matrix is proposed in 18 further work on stochastic l bfgs algorithms including convergence results can be found in 11 30 38 42 despite the importance of the erm problem due to the current prevalence of machine learning there are still a variety of important noisy optimization problems that arise in other contexts in engineering design numerical sim ulations are often employed in place of conducting costly if even feasible physical experiments in this context one tries to find optimal design param eters using the numerical simulation instead of physical experiments some examples from aerospace engineering including interplanetary trajectory and wing design can be found in 5 12 25 examples from materials engineering include stable composite design 1 and ternary alloy composition 19 amongst others 32 while examples from electrical engineering include power system operation 43 hardware verification 14 and antenna design 26 noise is of ten an unavoidable property of such numerical simulations as the simulations can include stochastic internal components and floating point arithmetic vul nerable to roundoff error apart from the analysis of the bfgs method with bounded errors in 41 there is relatively little work on the behaviour of quasi newton methods in the presence of general bounded noise as optimizing noisy numerical simulations does not always fit the framework of the erm problem analyses of the behaviour of quasi newton methods in the presence of general bounded noise are of practical value when optimizing numerical simulations 1 1 contributions noise is inevitably introduced into machine learning problems due to the ap proximations required to handle large datasets and numerical simulations due to the effects of finite precision arithmetic and parts of the simulator contain ing inherently stochastic components in this paper we return to the funda mental theory underlying the design of quasi newton methods which allows us to design a new variant of the bfgs method that explicitly handles the corrupting effects of noise we do this as follows 1 in section 2 we review the setup and derivation of the original bfgs method 2 in section 3 motivated by regularized least squares estimation we treat the secant condition of bfgs with a penalty method this creates a new bfgs update formula that we refer to as secant penalized bfgs sp 4 brian irwin eldad haber bfgs which we show reduces to the original bfgs update formula in a limiting case as expected 3 in section 4 we present an algorithmic framework for practically imple menting sp bfgs updating we also discuss implementation details in cluding how to perform a line search and choose the penalty parameter in the presence of noise 4 in section 5 we discuss the theoretical properties of sp bfgs including how the penalty parameter influences the eigenvalues of the approximate inverse hessian this allows us to show that under appropriate conditions sp bfgs iterations are guaranteed to converge linearly to a neighborhood of the global minimizer when minimizing strongly convex functions in the presence of uniformly bounded noise 5 in section 6 we study the empirical performance of sp bfgs updating compared to bfgs updating by performing extensive numerical experi ments with both convex and nonconvex objective functions corrupted by function and gradient noise results from a diverse set of over 30 problems from the cutest test problem set demonstrate that intelligently imple mented sp bfgs updating frequently outperforms bfgs updating in the presence of noise 6 finally section 7 concludes the paper and outlines directions for further work 2 mathematical background in this section as preliminaries to the main results of this paper we review the setup and derivation of the original bfgs method 2 1 bfgs setup the bfgs method was originally designed to solve the following unconstrained optimization problem min x x 1 with x rn rn 7 r and being a smooth twice continuously differen tiable and nonnoisy function below we use the notational conventions of 33 including k xk we begin by using the taylor expansion of to build a local quadratic model mk of the objective function at the k th iterate xk of the optimization procedure xk p k tk p 1 2 ptbkp mk p 2 where bk is an n n symmetric positive definite matrix that approximates the hessian matrix i e bk 2 k by setting the gradient of mk to zero we see that the unique minimizer pk of this local quadratic model is pk b 1 k k 3 secant penalized bfgs 5 and thus it is natural to update the next iterate xk 1 as xk 1 xk kpk 4 where k is the step size along the direction pk which is often chosen using a line search to avoid computing bk from scratch at each iteration k we use the cur vature information from recent gradient evaluations to update bk and thus relatively economically form bk 1 a taylor expansion of reveals xk p k 2 kp 5 and so it is reasonable to require that the new approximate hessian bk 1 satisfies k 1 k kbk 1 pk 6 which rearranges to bk 1 kpk k 1 k 7 now define the two new quantities sk and yk as sk xk 1 xk kpk 8 a yk k 1 k 8 b thus we arrive at 9 which is known as the secant condition bk 1 sk yk 9 in words the secant condition dictates that the new approximate hessian bk 1 must map the measured displacement sk into the measured difference of gradients yk if we denote the approximate inverse hessian hk b 1 k 2 1 k then the secant condition can be equivalently expressed as 10 hk 1 yk sk 10 as hk 1 is not yet uniquely determined to obtain the bfgs update for mula we impose a minimum norm restriction specifically we choose hk 1 to be the solution of the following quadratic program over matrices min h 1 2 w 1 2 h hk w 1 2 2 f s t h ht hyk sk 11 where f denotes the frobenius norm and w 1 2 the principal square root see 22 or a similar reference of a symmetric positive definite weight matrix w satisfying wsk yk 12 as we will see choosing the weight matrix w to satisfy 12 ensures that the resulting optimization method is scale invariant the weight matrix w can be chosen to be any symmetric positive definite matrix satisfying 12 and the specific choice of w is not of great importance as w will not appear directly in the main results of this paper however as a concrete example from 33 one could assume w g k where g k is the average hessian defined by g k 1 0 2 xk t kpk dt 13 6 brian irwin eldad haber 2 2 solving for the bfgs update to solve the quadratic program given by 11 we setup a lagrangian l h q involving the constraints recalling that w 1 2 h hk w 1 2 2 f tr w h hk w h hk t 14 this gives the lagrangian defined by 15 below l 1 2 tr w h hk w h hk t tr hyk sk qt tr h ht 15 where q is a vector of lagrange multipliers associated with the secant condi tion and is a matrix of lagrange multipliers associated with the symmetry condition taking the derivative of the lagrangian l h q with respect to the matrix h yields l h q h w h hk w qytk t 16 and so we have the karush kuhn tucker kkt system defined by the three equations 17 a 17 b and 17 c below w h hk w qytk t 0 17 a hyk sk 0 17 b h ht 0 17 c for brevity we omit the details of the solution of the kkt system defined above because it is a limiting case of the system solved in theorem 1 for an alternative geometric solution technique we refer the interested reader to section 2 of 20 the minimizer h hk 1 is given by the well known bfgs update formula hk 1 i sky t k stk yk hk i yks t k stk yk sks t k stk yk 18 which if we define the curvature parameter k 1 st k yk can be equivalently written as hk 1 i kskytk hk i kykstk ksks t k 19 applying the sherman morrison woodbury formula see 21 to the bfgs update formula immediately above one can also write the bfgs update in terms of the approximate hessian bk h 1 k instead of the approximate inverse hessian again for brevity the details are omitted because they are a special case of theorem 2 shown later the result is bk 1 bk bksks t kbk stkbksk yky t k stk yk bk bksks t kbk stkbksk kyky t k 20 secant penalized bfgs 7 to ensure the updated approximate hessian bk 1 is positive definite we must enforce that stkbk 1 sk 0 21 substituting bk 1 sk yk from the secant condition the condition 21 be comes stk yk 0 22 which is known as the curvature condition as it is equivalent to 1 k 0 23 3 derivation of secant penalized bfgs in this section having reviewed the construction of the original bfgs method we now show how treating the secant condition with a penalty method ap proach motivated by regularized least squares estimation allows one to gener alize the original bfgs update 3 1 penalizing the secant condition by applying a penalty method see chapter 17 of 33 to the secant condition instead of directly enforcing the secant condition as a constraint we obtain the problem min h 1 2 w 1 2 h hk w 1 2 2 f k 2 w 1 2 hyk sk 2 2 s t h ht 24 where k 0 is a penalty parameter that determines how strongly to penalize violations of the secant condition as we will see one recovers the solution to the constrained problem 11 in the limit k so k can be intuitively thought of as the cost of violating the secant condition by treating the symmetry constraint with a matrix of lagrange multipliers again we obtain the following lagrangian l 1 2 tr w h hk w h hk t k 2 w 1 2 hyk sk 2 2 tr h ht 25 defining the residual associated with the secant condition as rk h hyk sk and u kwrk the first order optimality conditions of 25 can be written as the system w h hk w uytk t 0 26 a hyk sk w 1 u k 0 26 b h ht 0 26 c note that as expected in the limit k the system given by 26 a 26 b and 26 c reduces to the kkt system given by 17 a 17 b and 17 c we now find an explicit closed form solution to the problem given by 24 which is given in theorem 1 8 brian irwin eldad haber theorem 1 sp bfgs update the update formula given by the mini mizer h of the problem defined by 24 which can be obtained by solving the system given by 26 a 26 b and 26 c is the sp bfgs update hk 1 i kskytk hk i kykstk k k k k k ytk hkyk sks t k 27 where k 1 stk yk 1 k k 1 stk yk 2 k 28 proof see appendix a at this point a few comments are in order regarding the sp bfgs update given by 27 first observe that as k we have that k k and k k as a result when k one recovers the original bfgs update as expected second also observe that as k 0 we have that k 0 and k 0 as a result we see that in the case k 0 the sp bfgs update reduces to hk 1 hk this is again expected because as k 0 the cost of violating the secant condition goes to zero and the minimum norm symmetric update is simply hk 1 hk we now examine what the analog of the curvature condition 22 is for sp bfgs lemma 1 demonstrates that 29 is the sp bfgs analog of the bfgs curvature condition 22 lemma 1 positive definiteness of sp bfgs update if hk is pos itive definite then the hk 1 given by the sp bfgs update 27 is positive definite if and only if the sp bfgs curvature condition stk yk 1 k 29 is satisfied proof see appendix b the result in lemma 1 warrants some discussion first the limiting be haviour with respect to k is consistent with theorem 1 as k condition 29 reduces to the bfgs curvature condition 22 as k 0 condition 29 reduces to no condition at all as stk yk is always true this is consistent with the observation that when k 0 the minimum norm symmetric update is hk 1 hk and in this case hk 1 is guaranteed to be positive definite if hk is positive definite regardless of s t k yk from the proof of lemma 1 see 93 it is now clear that ytkhk 1 yk ky t k sk 1 ky t k sk ytk sk 1 1 ky t k sk ytkhkyk 30 and so ytkhk 1 yk is a convex combination of y t k sk and y t khkyk thus hk 1 interpolates between the current inverse hessian approximation hk and the secant penalized bfgs 9 original bfgs update and as k decreases the interpolation is increasingly bi ased towards the current approximation hk from a regularized least squares estimation perspective k plays the role of a regularization parameter that controls the amount of bias in the estimate of hk 1 note that this behaviour is somewhat similar to the behaviour of powell damping 36 although powell damping was introduced to handle approximating a potentially indefinite hes sian of the lagrangian in constrained optimization problems and not noise we finish introducing the sp bfgs update by applying the sherman morrison woodbury formula to 27 which allows us to write the update in terms of the approximate hessian bk instead of the approximate inverse hes sian hk the result is given in theorem 2 theorem 2 sp bfgs inverse update the sp bfgs update formula given by 27 can be written in terms of bk h 1 k as bk 1 bk k k k ytk b 1 k yk k k bksks t kbk 1 ks t k yk bksky t k yks t kbk k s t kbksk yky t k k k ytk b 1 k yk k k ks t k bksk 1 kytk sk 2 proof see appendix c note that the limiting behaviour of theorem 2 with respect to k is again consistent when k we obtain the original bfgs inverse update 20 and when k 0 we obtain bk 1 bk one complication with respect to the sp bfgs inverse update 98 is that bk 1 cannot in general be expressed solely in terms of bk due to the presence of y t k b 1 k yk i e y t khkyk in the denominator 4 algorithmic framework we now outline how to practically implement sp bfgs updating we consider the situation where one has access to noise corrupted versions of a smooth function and its gradient that can be decomposed as f x x x 31 g x x e x 32 in 31 and 32 is a smooth twice continuously differentiable function as in section 2 1 and x is a scalar representing noise in the function evaluations similarly is the gradient of the smooth function while e x is a vector representing noise in the gradient evaluations similar decompositions are used in 2 14 41 10 brian irwin eldad haber 4 1 minimization routine algorithm 1 outlines a general procedure for minimizing a noisy function with noisy function and gradient values f and g that can be decomposed as shown in 31 and 32 the inputs to the procedure in algorithm 1 are a means of evaluating the noisy objective function f x and gradient g x the starting point x 0 and an initial inverse hessian approximation h 0 as the best con vergence stopping test is problem dependent we note that standard gradient and function value based tests can be employed in conjuction with smoothing and noise estimation techniques e g see section 3 3 4 of 2 in the next sev eral subsections we discuss how to choose the penalty parameter k and step size k and appropriate courses of action for when the sp bfgs curvature condition 29 fails algorithm 1 sp bfgs minimization routine 1 procedure sp bfgs minimize f x g x x 0 h 0 2 k 0 3 hk h 0 4 xk x 0 5 while not converged stopped do 6 pk hkgk 7 choose step size k 8 xk 1 xk kpk 9 sk xk 1 xk 10 yk gk 1 gk 11 choose penalty parameter k 12 if st k yk 1 k then 13 k 1 st k yk 1 k k 1 st k yk 2 k hk 1 i kskytk hk i kykstk k k k k k ytk hkyk sks t k 14 else 15 trigger sp bfgs curvature condition failure recovery procedure 16 k k 1 4 2 choosing the penalty parameter k as the choice of k determines how strongly to bias the estimate of hk 1 to wards hk the choice of k is fundamentally connected to the amount of noise present in the measured gradients gk 1 and gk in brief if the amount of noise present in the measured gradients is large k should be small to avoid over fitting the noise and if the amount of noise present in the measured gradients secant penalized bfgs 11 is small k should be large to avoid underfitting curvature information to make this point more rigorous we introduce the following assumption assumption 1 uniform gradient noise bound there exists a nonneg ative constant g 0 such that g x x 2 e x 2 g x r n 33 as x is continuous for each k 0 we have lim k 0 xk kpk xk 2 0 34 however due to noise we cannot in general guarantee lim k 0 g xk kpk g xk 2 0 35 using the continuity of x assumption 1 and the triangle inequality one can conclude that 0 lim k 0 g xk kpk g xk 2 2 g 36 as a result it is now clear that in the presence of uniformly bounded gradient noise sending the step size k to zero and thus sk to zero only bounds the difference of measured gradients within a ball with radius dependent on the gradient noise bound g as gk 1 and gk can be decomposed into smooth and noise components so can stk yk giving stk yk s t k y smooth k s t k y noise k s t k k 1 k s t k ek 1 ek 37 in conjunction with the cauchy schwarz inequality assumption 1 implies that 2 g sk 2 s t k ek 1 ek 2 g sk 2 38 and so we have the lower and upper bounds 2 g sk 2 s t k y smooth k s t k yk s t k y smooth k 2 g sk 2 39 from 39 it is clear that the bound on the effect of the noise grows linearly with sk 2 however by using the average hessian g k from 13 and applying taylor s theorem to it is also clear that stk y smooth k s t k g ksk o sk 2 2 40 and so stk yk o sk 2 2 o sk 2 41 where the o sk 2 2 term is due to the true curvature of the smooth function and the o sk 2 term is due to noise thus we have now illustrated an 12 brian irwin eldad haber important general behaviour given assumption 1 as sk 2 dominates sk 2 2 as sk 2 0 the effects of noise can dominate the true curvature for small sk conversely as sk 2 2 dominates sk 2 as sk 2 the true curvature can dominate the effects of noise for large sk given the above analysis a simple strategy for choosing k is to make k grow linearly with sk 2 such as k ns sk 2 42 where ns 0 is a slope parameter as sk 2 0 hk 1 hk which is desirable because the effects of noise likely dominate as sk 2 0 increas ingly biasing the estimate of hk 1 towards hk reduces how much hk 1 can be corrupted by noise and relaxes the sp bfgs curvature condition 29 reducing the likelihood of needing to trigger a recovery procedure described in section 4 4 also as shown earlier because is continuous the true dif ference of gradients is guaranteed to go to zero as sk approaches zero as a result without noise present it is natural that hk 1 hk as sk 0 in the presence of noise we wish for this behaviour to be preserved informally one can intuitively think of wanting hk to behave as an approximate average in verse hessian and the averaging should remove the corrupting effects of noise leaving hk to behave as if no noise were present similarly as sk 2 k and one recovers the bfgs update in the limit which is desir able because the effects of noise are likely dominated by the true curvature as sk 2 the slope parameter ns dictates how sensitive k is to sk 2 and should be set proportional to the gradient noise level i e g intuitively if the gradient noise level is low k should grow quickly with sk 2 as the effect of noise diminishes quickly and vice versa it may also be desirable to modify 42 to k max ns sk 2 no 0 43 where no 0 is an intercept parameter the inclusion of no allows one to stop updating hk if sk 2 is sufficiently small for example it may be desirable to stop updating hk when one is very close to a stationary point as gradient measurements are likely heavily dominated by noise 4 3 choosing the step size k classically during bfgs updating k is chosen to satisfy the armijo wolfe conditions as function and gradient evaluations are not corrupted by noise in the classical bfgs setting we can write the armijo condition also known as the sufficient decrease condition as k 1 k c 1 k tk pk 44 secant penalized bfgs 13 and the wolfe condition also known as the curvature condition as tk 1 pk c 2 t k pk 45 where 0 c 1 c 2 1 with well known choices being c 1 10 4 and c 2 0 9 observe that by adding tk pk to both sides of 45 and multiplying by k 45 becomes ytk sk k 1 k t kpk c 2 1 tk kpk 46 if pk is a descent direction then tk pk 0 and combined with c 2 1 0 and k 0 one sees that 46 implies ytk sk c 2 1 t k sk 0 47 so 45 effectively enforces 22 when no gradient noise is present in the presence of noisy gradients we argue that in general it no longer makes sense to enforce the wolfe condition 45 in the presence of gradient noise 45 becomes k 1 ek 1 t pk c 2 k ek t pk 48 which can behave erratically once the noise vectors ek 1 and ek start to dom inate the gradient of for example the noise vectors ek 1 and ek can cause both sides of 48 to erratically change sign in which case whether or not the wolfe condition is satisfied can be governed by randomness more than anything else we argue that because the sp bfgs update allows one to relax the cur vature condition based on the value of k as shown in the sp bfgs curvature condition 29 it is appropriate to drop the wolfe condition entirely in the presence of gradient noise and instead employ only a version of the sufficient decrease condition when choosing k in the situation where gradient noise is present but function noise is not i e f x x in 31 one can use a backtracking line search based on the sufficient decrease condition which can guarantee convergence to a neighborhood of a stationary point of the situation where noise is present in both function and gradient evaluations is trickier similar to the approach presented in section 4 2 of 2 one option is to use a backtracking line search with a relaxed sufficient decrease condition of the form fk 1 fk c 1 kgtk pk 2 a 49 where a 0 is a noise tolerance parameter and pk hkgk in theorem 4 2 of 2 the authors show that under assumptions 1 and 2 using the iteration 4 and a backtracking line search governed by the relaxed armijo condition 49 with pk gk guarantees linear convergence to a neighborhood of the global minimizer for strongly convex functions assumption 2 uniform function noise bound there exists a nonneg ative constant f 0 such that f x x x f x rn 50 14 brian irwin eldad haber we agree with the authors of 2 that it is possible to prove an extension of theorem 4 2 of 2 to a quasi newton iteration with positive definite hk and briefly outline why in section 5 2 a quasi newton extension of theorem 4 2 of 2 is relevant to sp bfgs updating because as we will formally see in section 5 1 control of k makes it possible to uniformly bound the minimum and maximum eigenvalues of hk 1 4 4 failed sp bfgs curvature condition recovery procedure in the classical bfgs scenario where no gradient noise is present the curva ture condition 22 may fail if k is not chosen based on the armijo wolfe conditions and is not strongly convex one of the most common strategies to handle this scenario is to skip the bfgs update i e set hk 1 hk when this occurs which corresponds to an sp bfgs update with k 0 however this simple strategy has the downside of potentially producing poor inverse hessian approximations if updates are skipped too frequently conditionally skipping bfgs updates is an option in the presence of noisy gradients as well in addition to skipping bfgs updates when 22 fails as described above another course of action sometimes recommended in the pres ence of noise is to replace 22 with stk yk sk 2 2 51 where 0 is a small positive constant and skip the bfgs update if 51 is not satisfied this strategy may be somewhat effective if sk 2 is large but reduces back to the initial update skipping approach as sk 2 0 a similar strategy e g see section 3 3 3 of 2 is to replace 22 with stk yk sk 2 yk 2 52 and skip the bfgs update if 52 is not satisfied for some 0 1 notice that none of the aforementioned update skipping strategies allow for curvature information to be incorporated if the measured curvature stk yk is negative unlike in the classical bfgs scenario with sp bfgs updating curva ture information can be incorporated even if the measured curvature stk yk is negative by decreasing k towards 0 in addition to having the option of conditionally skipping updates i e setting k 0 one can also alternatively relax the sp bfgs curvature condition by decreasing k towards 0 if 29 fails since k is chosen after sk and yk are fixed in algorithm 1 one can solve for k values satisfying 29 when s t k yk 0 yielding k 1 c 3 s t k yk 53 for all c 3 1 assuming that sk 6 0 and yk 6 0 note that if sk 6 0 and yk 6 0 and 29 fails then the measured curvature stk yk must be negative the choice of c 3 determines how much to shrink k compared to the largest value of k secant penalized bfgs 15 that still satisfies 29 and thus guarantees the positive definiteness of hk 1 hence if the value of k produced by 42 or 43 is too large and 29 fails one can choose an acceptable value of k by using 53 and selecting a c 3 1 thus instead of skipping the update i e setting k 0 if 29 fails one can reduce k towards 0 which has the effect of reducing the magnitude of the update by increasing how much hk 1 is biased towards hk an approach based on reducing k towards 0 never entirely skips incorporating measured curvature information even if the measured curvature information is negative but instead weights how heavily the measured curvature information affects hk 1 5 convergence of sp bfgs in this section we discuss relevant theoretical and convergence properties of sp bfgs first it is important to note that for specific choices of the sequence of penalty parameters k known convergence results already exist specifically if k for all k then sp bfgs updating is equivalent to bfgs updating although there are not many works on the convergence properties of bfgs updating in the presence of uniformly bounded noise such as in assumptions 1 and 2 in 41 the authors provide convergence results for a bfgs variant that employs an armijo wolfe line search and lengthens the differencing interval in the presence of uniformly bounded function and gradient noise at the other extreme if k 0 for all k then one obtains a scaled gradient method for general h 0 0 and this becomes the gradient method when h 0 i convergence analyses of the gradient method in the presence of uniformly bounded function and gradient noise for both a fixed step size and backtracking line search are provided in section 4 of 2 given that perhaps the defining feature of sp bfgs updating is the ability to vary k at each iteration we focus our attention on how varying k can influence convergence behaviour in this section as a result most of the ensuing analysis centers around situations where the condition number of hk can be bounded we do not employ the approach of bounding the cosine of the angle between the descent direction pk and the negative gradient above zero and then showing that the condition number of hk is bounded which is similar to the approaches taken when no noise is present in 9 10 and when noise is present in 41 although it may be possible to apply the strategies employed in 9 10 41 to establish convergence results for sp bfgs such an analysis is complicated enough that it is beyond the scope of this initial paper 5 1 the influence of k on hk 1 we first examine how k determines how much the maximum and minimum eigenvalues max hk 1 and min hk 1 can change in what follows h denotes the set of eigenvalues 1 n of the matrix h rn n we provide 16 brian irwin eldad haber upper bounds on max bk 1 and max hk 1 via theorem 3 as hk b 1 k 1 min hk 1 max bk 1 and putting an upper bound on max bk 1 is equivalent to putting a lower bound on min hk 1 theorem 3 eigenvalue upper bounds when hk 1 is given by the sp bfgs update 27 the following upper bounds 54 and 55 hold max hk 1 tr hk 1 1 k yk 2 sk 2 2 tr hk k sk 2 2 54 max bk 1 tr bk 1 1 k yk 2 sk 2 tr bk k yk 2 2 55 proof see appendix d with theorem 3 in hand we now formally see that when stk yk 0 as k increases from 0 to an upper bound on max hk 1 interpolates between tr hk and and an upper bound on max bk 1 interpolates between tr bk and similarly when stk yk 0 as k increases from 0 towards 1 st k yk an upper bound on max hk 1 interpolates between between tr hk and and an upper bound on max bk 1 interpolates between tr bk and standard bfgs updating corresponds to setting k for all k and as this is the largest possible value of k one can no longer formally guarantee that max bk 1 and max hk 1 are bounded from above at each iteration because the measured curvature stk yk may become arbitrarily close to zero due to the effects of noise the key takeaway is that upper bounds on max hk 1 and max bk 1 can be tightened arbitrarily close to tr hk and tr bk by shrinking k towards zero as sk yk and hk are fixed before the value of k is chosen in algorithm 1 thus if one must enforce a bound of the form max hk 1 ch or a bound of the form max bk 1 cb for all k 0 where ch tr h 0 0 and cb tr b 0 0 are positive constants there exist nontrivial sequences of sufficiently small k with limk k 0 that ensure the bounds hold for all k to see this observe that the interval max h 0 ch can be partitioned into subintervals corresponding to each iteration and the sum of the subinter vals cannot exceed ch max h 0 which can be guaranteed by assigning a small enough value of k to each subinterval as this guarantees the maximum eigenvalue does not grow too much at each iteration k furthermore although there clearly exist sequences of k that ensure the bounds hold for all k that satisfy k 0 for all k k where k is a positive integer there also exist sequences of k that ensure the bounds hold for all k where k instead only approaches zero in the limit k 5 2 minimization of strongly convex functions having established that sp bfgs iterations can maintain bounds on the maximum and minimum eigenvalues of the approximate inverse hessians via secant penalized bfgs 17 sufficiently small choices of k we now consider minimizing strongly convex functions in the presence of bounded noise we introduce assumption 3 and the notation x to denote the argument of the unique minimum of and x to denote the minimum assumption 3 strong convexity of the function c 2 is twice continuously differentiable and there exist positive constants 0 m m such that mi 2 x mi x rn 56 we also state a general result in lemma 2 that establishes a region where hkgk may not provide a descent direction with respect to due to noise dominating gradient measurements outside of this region hkgk is guaranteed to provide a descent direction for lemma 2 region where gradient noise can dominate suppose assumptions 1 and 3 and the decomposition in 32 apply let h be a sym metric positive definite matrix bounded by i h i where 0 define the neighborhood n 1 as n 1 x x 1 2 m g 2 57 for all x n 1 x thg x 0 contrapositively for all x such that x thg x 0 x n 1 proof see appendix e applying lemma 2 in the context of sp bfgs updating makes several convergence properties clear first if one chooses k such that i hk i for all k i e the eigenvalues of the approximate inverse hessian are uniformly bounded from above and below for all k by lemma 2 it becomes clear that in the presence of gradient noise and absence of function noise i e f x x in 31 the iterates of sp bfgs with a backtracking line search based on 49 with a 0 in the worst case approach n 1 as k to see this observe that hkgk is guaranteed to provide a descent direction outside of n 1 and the sufficient decrease condition guarantees that k is not too large while backtracking guarantees that k is not too small for more background see chapter 3 of 33 second if both function and gradient noise are present and one again chooses k such that the bounds i hk i hold for all k under additional conditions a worst case analysis in theorem 4 shows that an approach using a sufficiently small fixed step size approaches n 1 at a linear rate as k for a general quasi newton iteration of the form xk 1 xk hkgk 58 with constant step size andhk 0 theorem 4 establishes linear convergence to the region where noise can dominate i e n 1 in lemma 2 18 brian irwin eldad haber theorem 4 linear convergence for sufficiently small fixed sup pose that assumptions 1 and 3 hold further suppose that hk is symmetric positive definite and bounded by i hk i where 0 let be such that the inequality tkhkgk t k gk 59 is true for all k let xk be the iterates generated by 58 where the constant step size satisfies 0 2 m 60 then for all k such that xk n 1 one has the q linear convergence result k 1 1 2 m g 2 1 m k 1 2 m g 2 61 similarly for any x 0 n 1 one has the r linear convergence result k 1 1 m k 0 1 2 m g 2 1 2 m g 2 62 proof see appendix f theorem 4 can be considered a quasi newton extension of theorem 4 1 from 2 which lays the foundation for theorem 4 2 from 2 to extend the convergence result of theorem 4 to the backtracking line search approach based on 49 see that 60 115 and assumption 2 combined imply f xk hkgk f xk 2 k 2 2 ek 2 2 2 f 63 and so if a f comparing 49 and 63 makes it clear that 49 will be satisfied for sufficiently small hence the backtracking line search always finds an k satisfying 49 for brevity we defer a full rigorous quasi newton extension of theorem 4 2 of 2 to future work and instead investigate the per formance of an approach based on 49 via numerical experiments in section 6 6 numerical experiments in this section we test instances of algorithm 1 on a diverse set of 33 test problems for unconstrained minimization the set of test problems includes convex and nonconvex functions and well known pathological functions such as the rosenbrock function 37 and its relatives described in section 6 1 the first test problem is similar to the one used in the numerical experiments section of 41 and involves an ill conditioned quadratic function the other 32 problems are selected problems from the cutest test problem set 16 and are used for tests in section 6 2 code for running these numerical ex periments was written in the julia programming language 4 and utilizes secant penalized bfgs 19 the nlpmodels jl 35 cutest jl 34 and distributions jl 3 28 packages in all the numerical experiments that follow noise x was added to function evaluations by uniformly sampling from the interval f f and noise e x was added to the gradient evaluations by uniformly sampling from the closed euclidean ball x 2 g 6 1 ill conditioned quadratic function with additive gradient noise only the first test problem is strongly convex and consists of the 4 dimensional quadratic function given by x 1 2 xttx 64 where the eigenvalues of t are t 10 2 1 102 104 consequently the strong convexity parameter is m 10 2 the lipschitz constant is m 104 and the condition number of the hessian t is 106 for this test problem no noise was added to the function evaluations i e f x x in 31 and g 1 as a result in this scenario n 1 from lemma 2 with 1 i e the smallest possible n 1 becomes n 1 1 1 x x 50 65 following the discussion in section 4 2 we set the penalty parameters via the formula k 1 g sk 2 10 10 which corresponds to a choice of ns 1 in 42 the 10 10 term was added as a small perturbation to provide numerical stability the step size k was chosen using a backtracking line search based on the sufficient decrease condition 49 with pk hkgk where gk is defined by 32 a 0 and c 1 10 4 at each iteration backtracking started from the initial step size 0 1 decreasing by a factor of 1 2 each time the sufficient decrease condition failed if the backtracking line search exceeded the maximum number of 75 backtracks we set k 0 however the maximum number of backtracks was never exceeded when performing experiments with this first test problem algorithm 1 was initialized using the matrix and starting point h 0 i x 0 105 1 1 1 1 t 66 given in 66 with x 0 2 109 figures 1 2 and 3 compare the perfor mance of 30 independent runs of sp bfgs vs bfgs over a fixed budget of 100 iterations the relevant curvature condition failed an average of 25 7 total iterations per bfgs run and 0 6 total iterations per sp bfgs run for the sake of comparability both sp bfgs and bfgs skipped the update if the relevant curvature condition failed observe that sp bfgs reduces the objec tive function value by several more orders of magnitude compared to bfgs on average and maintains significantly better inverse hessian approximations than bfgs in the presence of gradient noise 20 brian irwin eldad haber fig 1 base 10 logarithm of the optimality gap vs the iteration number k for 30 independent runs after 100 iterations sp bfgs has an average log 10 100 of 5 03 while bfgs has an average log 10 100 of 1 27 observe that both sp bfgs and bfgs appear to enter n 1 1 1 which corresponds to values less than log 10 50 1 7 on the y axis but sp bfgs makes more progress inside n 1 1 1 outside of n 1 1 1 the performance of sp bfgs and bfgs is almost indistinguishable fig 2 base 10 logarithm of the euclidean norm of the true gradient k vs the iteration number k for 30 independent runs note that the bfgs values appear to vary more wildly than the sp bfgs values fig 3 base 10 logarithm of the condition number of the true hessian 2 k scaled by the approximate inverse hessian hk at each iteration k for 30 independent runs as ideally one wants hk 2 k i which has a condition number of 1 the ideal value on these plots is log 10 1 0 observe how the bfgs approximation deteriorates massively inside n 1 1 1 and how sp bfgs avoids this massive deterioration from examining the bfgs hk the authors were able to determine that in the region of deterioration the values of the entries of hk are often smaller than 10 5 secant penalized bfgs 21 6 2 cutest test problems with various additive noise combinations the remaining 32 test problems were selected from the cutest problem set the successor of cuter 17 at the time of writing sif files and descrip tions of all 32 test problems can be found at https www cuter rl ac uk problems mastsif shtml as a brief summary some of the problems can be interpreted as least squares type problems e g argtrgls some of the problems are ill conditioned or singular type problems e g boxpower some of the problems are well known nonlinear optimization test problems e g rosenbr or extensions of them e g rosenbrtu srosenbr and some of the problems come from real applications e g coating heart 6 ls vibrbeam as shown in tables 2 and 3 the selected cutest test problems vary in size from 2 dimensional to 1000 dimensional using these 32 cutest test problems and a fixed budget of 2000 objective function evaluations not 2000 iterations per test we tested the performance of sp bfgs compared to bfgs with various combinations of function and gradient noise levels f and g for all the experiments in tables 1 2 and 3 as well as the additional experiments in appendix g both sp bfgs and bfgs skipped updating if the curvature condition failed in tables 1 2 and 4 the spbfgs penalty parameter was set as k 108 g sk 2 10 10 as the authors heuristically discovered setting ns 108 g works well in practice for a variety of problems with regards to the backtracking line search based on 49 we set 0 1 a f c 1 10 4 1 2 and the maximum number of backtracks as 45 we define opt log 10 best as a measure of the optimality gap and use best to denote the smallest value of the true function measured at any point during an algorithm run the true minimum values for each cutest problem were obtained from the sif file for each cutest problem the sample variance i e the variance with bessel s correction is denoted by s 2 table 1 compares the performance of sp bfgs vs bfgs on the rosen brock function i e rosenbr corrupted by different combinations of func tion and gradient noise of varying orders of magnitude observe that sp bfgs outperforms bfgs with respect to the mean and median optimality gap for every noise combination in table 1 sometimes by several orders of magni tude tables 2 and 3 compare the performance of sp bfgs vs bfgs on the 32 cutest test problems with both function and gradient noise present gra dient noise was generated using g 10 4 x 0 2 and function noise was generated using f 10 4 x 0 both to ensure that noise does not initially dominate function or gradient evaluations note that as the noise in these numerical experiments is additive the signal to noise ratio of gradient mea surements decreases as a stationary point is approached overall sp bfgs outperforms bfgs on approximately 70 of the cutest problems with both function and gradient noise present and performs at least as good as bfgs on approximately 90 of these problems referring to appendix g with only gradient noise present these percentages become 80 and 95 respectively https www cuter rl ac uk problems mastsif shtml https www cuter rl ac uk problems mastsif shtml 22 brian irwin eldad haber f g mean opt median opt min opt max opt s 2 opt mean i spbfgs with no function noise 0 10 4 1 4 e 01 1 4 e 01 1 8 e 01 1 2 e 01 1 4 e 00 114 0 10 2 1 3 e 01 1 3 e 01 1 5 e 01 8 3 e 00 2 9 e 00 104 0 100 2 1 e 00 1 8 e 00 5 7 e 00 9 2 e 01 9 4 e 01 153 0 102 3 5 e 02 2 9 e 01 1 9 e 00 7 9 e 01 3 9 e 01 90 bfgs with no function noise 0 10 4 1 1 e 01 1 0 e 01 1 4 e 01 8 8 e 00 1 8 e 00 263 0 10 2 6 6 e 00 6 6 e 00 9 6 e 00 4 3 e 00 1 6 e 00 281 0 100 1 5 e 00 1 2 e 00 3 3 e 00 5 4 e 01 6 3 e 01 279 0 102 1 1 e 01 4 3 e 01 2 4 e 00 6 5 e 01 4 7 e 01 373 spbfgs with low function noise level 10 4 10 4 1 4 e 01 1 4 e 01 1 5 e 01 1 3 e 01 1 9 e 01 1980 10 4 10 2 1 0 e 01 1 0 e 01 1 2 e 01 8 0 e 00 1 3 e 00 1964 10 4 100 2 1 e 00 2 0 e 00 3 6 e 00 1 6 e 00 2 0 e 01 1759 10 4 102 8 7 e 02 3 1 e 01 2 2 e 00 9 1 e 01 4 5 e 01 1720 bfgs with low function noise level 10 4 10 4 1 1 e 01 1 2 e 01 1 5 e 01 8 7 e 00 1 7 e 00 1980 10 4 10 2 6 6 e 00 6 5 e 00 8 8 e 00 4 7 e 00 1 2 e 00 1975 10 4 100 1 2 e 00 1 1 e 00 1 8 e 00 8 6 e 01 5 9 e 02 1936 10 4 102 9 5 e 02 5 1 e 01 3 1 e 00 9 2 e 01 8 5 e 01 1934 spbfgs with medium function noise level 10 2 10 4 1 4 e 01 1 4 e 01 1 5 e 01 1 3 e 01 3 4 e 01 1981 10 2 10 2 1 0 e 01 1 0 e 01 1 3 e 01 7 5 e 00 1 5 e 00 1977 10 2 100 3 4 e 00 3 0 e 00 7 5 e 00 2 0 e 00 1 7 e 00 1934 10 2 102 1 8 e 01 1 7 e 01 3 7 e 00 7 4 e 01 1 0 e 00 1890 bfgs with medium function noise level 10 2 10 4 1 1 e 01 1 1 e 01 1 4 e 01 8 5 e 00 1 4 e 00 1981 10 2 10 2 6 7 e 00 6 7 e 00 1 0 e 01 4 9 e 00 1 7 e 00 1979 10 2 100 1 8 e 00 1 5 e 00 3 8 e 00 9 1 e 01 6 3 e 01 1961 10 2 102 1 4 e 01 3 9 e 01 2 3 e 00 8 5 e 01 6 1 e 01 1953 spbfgs with high function noise level 100 10 4 1 4 e 01 1 4 e 01 1 5 e 01 1 3 e 01 2 2 e 01 1980 100 10 2 1 0 e 01 1 0 e 01 1 2 e 01 7 3 e 00 9 6 e 01 1978 100 100 3 1 e 00 2 8 e 00 5 1 e 00 1 7 e 00 8 9 e 01 1969 100 102 2 2 e 01 1 1 e 02 1 9 e 00 8 4 e 01 7 6 e 01 1943 bfgs with high function noise level 100 10 4 1 1 e 01 1 1 e 01 1 3 e 01 9 0 e 00 1 4 e 00 1980 100 10 2 6 7 e 00 6 4 e 00 9 1 e 00 5 0 e 00 1 5 e 00 1980 100 100 1 8 e 00 1 4 e 00 5 3 e 00 8 2 e 01 1 1 e 00 1973 100 102 2 9 e 02 3 7 e 01 2 1 e 00 8 9 e 01 7 9 e 01 1965 table 1 performance of sp bfgs vs bfgs on the rosenbrock function i e rosenbr corrupted by noise opt log 10 best measures the optimality gap where best denotes the smallest value of the true function measured at any point during an algorithm run the number of objective function evaluations is fixed at 2000 but the number of iterations i can vary statistics are calculated from a sample of 30 runs per algorithm secant penalized bfgs 23 sp bfgs with function and gradient noise problem dim mean opt median opt min opt max opt s 2 opt argtrgls 200 4 5 e 02 4 8 e 02 1 7 e 02 8 0 e 02 2 5 e 04 arwhead 500 2 5 e 00 2 5 e 00 2 6 e 00 2 5 e 00 2 6 e 04 beale 2 1 1 e 01 1 1 e 01 1 4 e 01 9 8 e 00 8 0 e 01 box 3 3 7 1 e 00 6 8 e 00 8 9 e 00 6 5 e 00 6 2 e 01 boxpower 100 3 8 e 00 3 8 e 00 4 2 e 00 3 5 e 00 5 0 e 02 brownbs 2 1 2 e 00 7 4 e 01 5 2 e 00 2 0 e 00 3 5 e 00 broydnbdls 50 6 2 e 00 6 2 e 00 6 4 e 00 6 0 e 00 6 9 e 03 chainwoo 100 1 7 e 00 1 8 e 00 7 7 e 03 2 1 e 00 1 6 e 01 chnrosnb 50 4 2 e 00 4 0 e 00 5 5 e 00 3 6 e 00 3 8 e 01 coating 134 2 7 e 02 1 2 e 02 1 3 e 01 9 6 e 02 3 5 e 03 coolhansls 9 1 2 e 00 1 1 e 00 1 6 e 00 8 7 e 01 1 7 e 02 cube 2 5 2 e 00 4 7 e 00 8 9 e 00 3 1 e 00 2 2 e 00 cycloocfls 20 8 4 e 00 8 5 e 00 9 1 e 00 6 9 e 00 3 0 e 01 extrosnb 10 5 2 e 00 5 2 e 00 5 2 e 00 5 1 e 00 1 3 e 03 fminsrf 2 64 8 7 e 00 8 7 e 00 8 7 e 00 8 6 e 00 2 6 e 04 genhumps 5 4 1 e 02 2 4 e 01 2 9 e 00 7 8 e 01 4 5 e 01 genrose 5 9 4 e 00 9 3 e 00 9 9 e 00 9 1 e 00 5 6 e 02 heart 6 ls 6 3 5 e 01 2 7 e 01 2 0 e 00 1 2 e 00 1 5 e 00 helix 3 6 1 e 00 6 0 e 00 7 4 e 00 4 5 e 00 5 0 e 01 mancino 30 2 1 e 00 2 1 e 00 2 5 e 00 1 9 e 00 1 2 e 02 methanb 8 ls 31 3 8 e 00 3 9 e 00 4 2 e 00 3 4 e 00 3 6 e 02 modbeale 200 1 1 e 00 1 0 e 00 4 7 e 01 1 8 e 00 1 8 e 01 nondia 10 4 2 e 03 4 3 e 03 4 4 e 03 3 2 e 03 9 1 e 08 powellsg 4 6 1 e 00 6 0 e 00 7 9 e 00 4 6 e 00 9 1 e 01 power 10 3 9 e 00 3 8 e 00 4 9 e 00 3 3 e 00 1 9 e 01 rosenbr 2 8 6 e 00 8 5 e 00 1 1 e 01 6 3 e 00 1 8 e 00 rosenbrtu 2 1 8 e 01 1 8 e 01 2 0 e 01 1 7 e 01 4 0 e 01 sbrybnd 500 3 9 e 00 3 9 e 00 3 9 e 00 3 9 e 00 9 2 e 06 sineval 2 1 4 e 01 1 4 e 01 1 5 e 01 1 3 e 01 3 7 e 01 snail 2 1 2 e 01 1 2 e 01 1 4 e 01 1 1 e 01 2 9 e 01 srosenbr 1000 5 0 e 01 5 0 e 01 2 9 e 01 6 8 e 01 8 6 e 03 vibrbeam 8 1 5 e 00 1 5 e 00 1 2 e 00 2 1 e 00 2 6 e 02 table 2 performance of sp bfgs on 32 selected cutest test problems with noise added to both function and gradient evaluations the number of objective function evaluations is fixed at 2000 opt log 10 best measures the optimality gap where best denotes the smallest value of the true function measured at any point during an algorithm run statistics are calculated from a sample of 30 runs per algorithm and the dim column gives the problem dimension the spbfgs penalty parameter was set as k 108 g sk 2 10 10 for each problem function noise was generated using f 10 4 x 0 and gradient noise was generated using g 10 4 x 0 2 where the starting point x 0 varies by cutest problem 24 brian irwin eldad haber bfgs with function and gradient noise problem dim mean opt median opt min opt max opt s 2 opt argtrgls 200 5 6 e 02 5 5 e 02 2 4 e 02 8 4 e 02 2 7 e 04 arwhead 500 2 5 e 00 2 5 e 00 2 6 e 00 2 5 e 00 4 1 e 04 beale 2 7 7 e 00 7 8 e 00 9 7 e 00 6 1 e 00 7 1 e 01 box 3 3 6 5 e 00 6 5 e 00 6 7 e 00 6 4 e 00 4 7 e 03 boxpower 100 3 7 e 00 3 7 e 00 4 2 e 00 3 4 e 00 3 4 e 02 brownbs 2 6 8 e 01 1 3 e 00 3 2 e 00 3 1 e 00 2 9 e 00 broydnbdls 50 6 0 e 00 6 0 e 00 6 3 e 00 5 7 e 00 2 6 e 02 chainwoo 100 1 7 e 00 1 7 e 00 1 2 e 00 2 1 e 00 5 9 e 02 chnrosnb 50 4 2 e 00 4 1 e 00 5 7 e 00 3 4 e 00 4 4 e 01 coating 134 3 7 e 02 5 7 e 02 1 6 e 01 8 0 e 02 4 1 e 03 coolhansls 9 1 0 e 00 1 0 e 00 2 0 e 00 4 5 e 01 7 2 e 02 cube 2 1 6 e 00 1 4 e 00 3 6 e 00 9 7 e 01 4 1 e 01 cycloocfls 20 7 2 e 00 7 2 e 00 9 1 e 00 5 8 e 00 8 7 e 01 extrosnb 10 5 2 e 00 5 2 e 00 5 2 e 00 5 1 e 00 1 8 e 03 fminsrf 2 64 8 6 e 00 8 7 e 00 8 8 e 00 8 2 e 00 2 8 e 02 genhumps 5 1 2 e 01 1 2 e 01 1 2 e 00 8 1 e 01 2 3 e 01 genrose 5 7 5 e 00 7 6 e 00 9 1 e 00 6 2 e 00 7 3 e 01 heart 6 ls 6 3 1 e 01 6 1 e 01 1 9 e 00 1 2 e 00 1 4 e 00 helix 3 4 5 e 00 4 7 e 00 7 0 e 00 2 7 e 00 1 1 e 00 mancino 30 1 6 e 00 1 6 e 00 1 8 e 00 1 3 e 00 1 3 e 02 methanb 8 ls 31 3 9 e 00 3 8 e 00 4 4 e 00 3 6 e 00 5 5 e 02 modbeale 200 1 1 e 00 1 1 e 00 2 9 e 01 1 8 e 00 1 6 e 01 nondia 10 3 7 e 03 3 8 e 03 4 4 e 03 2 6 e 03 3 1 e 07 powellsg 4 5 2 e 00 5 2 e 00 7 6 e 00 4 2 e 00 7 1 e 01 power 10 3 5 e 00 3 5 e 00 4 1 e 00 2 9 e 00 1 0 e 01 rosenbr 2 5 9 e 00 5 5 e 00 9 2 e 00 4 5 e 00 1 4 e 00 rosenbrtu 2 1 6 e 01 1 6 e 01 1 8 e 01 1 4 e 01 1 5 e 00 sbrybnd 500 3 9 e 00 3 9 e 00 3 9 e 00 3 9 e 00 2 7 e 05 sineval 2 1 1 e 01 1 1 e 01 1 3 e 01 8 9 e 00 1 3 e 00 snail 2 9 4 e 00 9 2 e 00 1 2 e 01 8 0 e 00 7 2 e 01 srosenbr 1000 5 4 e 01 5 4 e 01 3 6 e 01 7 8 e 01 6 9 e 03 vibrbeam 8 1 7 e 00 1 7 e 00 1 2 e 00 2 0 e 00 2 9 e 02 table 3 performance of bfgs on 32 selected cutest test problems with noise added to both function and gradient evaluations the number of objective function evaluations is fixed at 2000 opt log 10 best measures the optimality gap where best denotes the smallest value of the true function measured at any point during an algorithm run statistics are calculated from a sample of 30 runs per algorithm and the dim column gives the problem dimension for each problem function noise was generated using f 10 4 x 0 and gradient noise was generated using g 10 4 x 0 2 where the starting point x 0 varies by cutest problem secant penalized bfgs 25 7 final remarks in this paper we introduced sp bfgs a new variant of the bfgs method de signed to resist the corrupting effects of noise motivated by regularized least squares estimation we derived the sp bfgs update by applying a penalty method to the secant condition we argued that with an appropriate choice of penalty parameter sp bfgs updating is robust to the corrupting effects of noise that can destroy the performance of bfgs we empirically validated this claim by performing numerical experiments on a diverse set of over 30 test problems with both function and gradient noise of varying orders of mag nitude the results of these numerical experiments showed that sp bfgs can outperform bfgs approximately 70 or more of the time and performs at least as good as bfgs approximately 90 or more of the time furthermore a theoretical analysis confirmed that with appropriate choices of penalty pa rameter it is possible to guarantee that sp bfgs is not corrupted arbitrarily badly by noise unlike standard bfgs in the future we believe it is worth investigating the performance of sp bfgs in the presence of other types of noise including multiplicative stochastic noise and deterministic noise and also believe it is worthwhile to study the use of noise estimation techniques in conjunction with sp bfgs updating the authors are also working to publish a limited memory version of sp bfgs for high dimensional noisy problems acknowledgements eh and bi s work is supported by the natural sciences and en gineering research council of canada nserc and the university of british columbia ubc conflict of interest the authors declare that they have no conflict of interest references 1 aydin l aydin o artem h s mert a design of dimensionally stable com posites using efficient global optimization method proceedings of the institution of mechanical engineers part l journal of materials design and applications 233 2 156 168 2019 doi 10 1177 1464420716664921 url https doi org 10 1177 1464420716664921 2 berahas a s byrd r h nocedal j derivative free optimization of noisy functions via quasi newton methods siam journal on optimization 29 965 993 2019 3 besanc on m anthoff d arslan a byrne s lin d papamarkou t pearson j distributions jl definition and modeling of probability distributions in the juliastats ecosystem arxiv e prints arxiv 1907 08611 2019 4 bezanson j edelman a karpinski s shah v b julia a fresh approach to nu merical computing siam review 59 1 65 98 2017 doi 10 1137 141000671 url https doi org 10 1137 141000671 5 bons n p he x mader c a martins j r r a multimodality in aerodynamic wing design optimization aiaa journal 57 3 1004 1018 2019 doi 10 2514 1 j 057294 url https doi org 10 2514 1 j 057294 https doi org 10 1177 1464420716664921 https doi org 10 1177 1464420716664921 https doi org 10 1137 141000671 https doi org 10 2514 1 j 057294 26 brian irwin eldad haber 6 broyden c g the convergence of a class of double rank minimization algorithms 1 general considerations ima journal of applied mathematics 6 1 76 90 1970 doi 10 1093 imamat 6 1 76 url https doi org 10 1093 imamat 6 1 76 7 byrd r h hansen s l nocedal j singer y a stochastic quasi newton method for large scale optimization siam journal on optimization 26 2 1008 1031 2016 doi 10 1137 140954362 url https doi org 10 1137 140954362 8 byrd r h lu p nocedal j zhu c a limited memory algorithm for bound con strained optimization siam journal on scientific computing 16 5 1190 1208 1995 doi 10 1137 0916069 url https doi org 10 1137 0916069 9 byrd r h nocedal j a tool for the analysis of quasi newton methods with appli cation to unconstrained minimization siam journal on numerical analysis 26 3 727 739 1989 url http www jstor org stable 2157680 10 byrd r h nocedal j yuan y x global convergence of a class of quasi newton methods on convex problems siam journal on numerical analysis 24 5 1171 1190 1987 url http www jstor org stable 2157646 11 chang d sun s zhang c an accelerated linearly convergent stochastic l bfgs algorithm ieee transactions on neural networks and learning systems 30 11 3338 3346 2019 12 fasano g pinte r j d modeling and optimization in space engineering state of the art and new challenges springer international publishing 2019 13 fletcher r a new approach to variable metric algorithms the computer journal 13 3 317 322 1970 doi 10 1093 comjnl 13 3 317 url https doi org 10 1093 comjnl 13 3 317 14 gal r haber e irwin b saleh b ziv a how to catch a lion in the desert on the solution of the coverage directed generation cdg problem optimization and engineering 2020 doi 10 1007 s 11081 020 09507 w url https doi org 10 1007 2 fs 11081 020 09507 w 15 goldfarb d a family of variable metric methods derived by variational means math ematics of computation 24 109 23 26 1970 url http www jstor org stable 2004873 16 gould n i m orban d contributors the constrained and unconstrained testing environment with safe threads cutest for optimization software https github com ralna cutest 2019 17 gould n i m orban d toint p l cuter a constrained and unconstrained testing environment revisited https www cuter rl ac uk 2001 18 gower r goldfarb d richtarik p stochastic block bfgs squeezing more curva ture out of data in m f balcan k q weinberger eds proceedings of the 33 rd international conference on machine learning proceedings of machine learning re search vol 48 pp 1869 1878 pmlr new york new york usa 2016 url http proceedings mlr press v 48 gower 16 html 19 graf p a billups s mdtri robust and efficient global mixed integer search of spaces of multiple ternary alloys computational optimization and applications 68 3 671 687 2017 doi 10 1007 s 10589 017 9922 9 url https doi org 10 1007 s 10589 017 9922 9 20 gu ler o gu rtuna f shevchenko o duality in quasi newton methods and new variational characterizations of the dfp and bfgs updates optimization methods and software 24 1 45 62 2009 doi 10 1080 10556780802367205 url https doi org 10 1080 10556780802367205 21 hager w w updating the inverse of a matrix siam review 31 2 221 239 1989 url http www jstor org stable 2030425 22 horn r a johnson c r matrix analysis 2 nd edn cambridge university press new york 2013 23 johnson r zhang t accelerating stochastic gradient descent using predictive vari ance reduction in advances in neural information processing systems pp 315 323 2013 24 johnson s g quasi newton optimization origin of the bfgs up date 2019 url https ocw mit edu courses mathematics 18 335 j introduction to numerical methods spring 2019 week 11 mit 18 335 js 19 lec 30 pdf https doi org 10 1093 imamat 6 1 76 https doi org 10 1137 140954362 https doi org 10 1137 0916069 http www jstor org stable 2157680 http www jstor org stable 2157646 https doi org 10 1093 comjnl 13 3 317 https doi org 10 1093 comjnl 13 3 317 https doi org 10 1007 2 fs 11081 020 09507 w https doi org 10 1007 2 fs 11081 020 09507 w http www jstor org stable 2004873 http www jstor org stable 2004873 https github com ralna cutest https github com ralna cutest https www cuter rl ac uk http proceedings mlr press v 48 gower 16 html https doi org 10 1007 s 10589 017 9922 9 https doi org 10 1007 s 10589 017 9922 9 https doi org 10 1080 10556780802367205 https doi org 10 1080 10556780802367205 http www jstor org stable 2030425 https ocw mit edu courses mathematics 18 335 j introduction to numerical methods spring 2019 week 11 mit 18 335 js 19 lec 30 pdf https ocw mit edu courses mathematics 18 335 j introduction to numerical methods spring 2019 week 11 mit 18 335 js 19 lec 30 pdf https ocw mit edu courses mathematics 18 335 j introduction to numerical methods spring 2019 week 11 mit 18 335 js 19 lec 30 pdf secant penalized bfgs 27 25 keane a j nair p b computational approaches for aerospace design the pursuit of excellence john wiley sons ltd 2005 doi 10 1002 0470855487 url https onlinelibrary wiley com doi abs 10 1002 0470855487 26 koziel s ogurtsov s antenna design by simulation driven optimization springer international publishing 2014 27 lewis a s overton m l nonsmooth optimization via quasi newton methods math ematical programming 141 135 163 2013 28 lin d white j m byrne s bates d noack a pearson j arslan a squire k anthoff d papamarkou t besanc on m drugowitsch j schauer m other contributors juliastats distributions jl a julia package for probability distributions and associated functions https github com juliastats distributions jl 2019 doi 10 5281 zenodo 2647458 url https doi org 10 5281 zenodo 2647458 29 liu d c nocedal j on the limited memory bfgs method for large scale optimization mathematical programming 45 1 503 528 1989 doi 10 1007 bf 01589116 url https doi org 10 1007 bf 01589116 30 mokhtari a ribeiro a global convergence of online limited memory bfgs journal of machine learning research 16 1 3151 3181 2015 31 moritz p nishihara r jordan m a linearly convergent stochastic l bfgs algorithm in a gretton c c robert eds proceedings of the 19 th international conference on artificial intelligence and statistics proceedings of machine learning research vol 51 pp 249 258 pmlr cadiz spain 2016 url http proceedings mlr press v 51 moritz 16 html 32 mun oz rojas p a computational modeling optimization and manufacturing simu lation of advanced engineering materials springer 2016 33 nocedal j wright s numerical optimization springer new york 2006 34 orban d siqueira a s contributors cutest jl julia s cutest interface https github com juliasmoothoptimizers cutest jl 2020 doi 10 5281 zenodo 1188851 35 orban d siqueira a s contributors nlpmodels jl data structures for optimization models https github com juliasmoothoptimizers nlpmodels jl 2020 doi 10 5281 zenodo 2558627 36 powell m j d algorithms for nonlinear constraints that use lagrangian functions mathematical programming 14 1 224 248 1978 doi 10 1007 bf 01588967 url https doi org 10 1007 bf 01588967 37 rosenbrock h h an automatic method for finding the greatest or least value of a function the computer journal 3 3 175 184 1960 doi 10 1093 comjnl 3 3 175 url https doi org 10 1093 comjnl 3 3 175 38 schraudolph n n yu j gu nter s a stochastic quasi newton method for online convex optimization in m meila x shen eds proceedings of the eleventh inter national conference on artificial intelligence and statistics proceedings of machine learning research vol 2 pp 436 443 pmlr san juan puerto rico 2007 url http proceedings mlr press v 2 schraudolph 07 a html 39 shanno d f conditioning of quasi newton methods for function minimization mathe matics of computation 24 111 647 656 1970 url http www jstor org stable 2004840 40 wang x ma s goldfarb d liu w stochastic quasi newton methods for non convex stochastic optimization siam journal on optimization 27 2 927 956 2017 doi 10 1137 15 m 1053141 url https doi org 10 1137 15 m 1053141 41 xie y byrd r h nocedal j analysis of the bfgs method with errors siam journal on optimization 30 1 182 209 2020 doi 10 1137 19 m 1240794 url https doi org 10 1137 19 m 1240794 42 zhao r haskell w b tan v y f stochastic l bfgs improved convergence rates and practical acceleration strategies ieee transactions on signal processing 66 1155 1169 2018 43 zhu j optimization of power system operation john wiley sons ltd 2008 doi 10 1002 9780470466971 url https onlinelibrary wiley com doi abs 10 1002 9780470466971 https onlinelibrary wiley com doi abs 10 1002 0470855487 https onlinelibrary wiley com doi abs 10 1002 0470855487 https github com juliastats distributions jl https doi org 10 5281 zenodo 2647458 https doi org 10 1007 bf 01589116 http proceedings mlr press v 51 moritz 16 html http proceedings mlr press v 51 moritz 16 html https github com juliasmoothoptimizers cutest jl https github com juliasmoothoptimizers cutest jl https github com juliasmoothoptimizers nlpmodels jl https doi org 10 1007 bf 01588967 https doi org 10 1093 comjnl 3 3 175 http proceedings mlr press v 2 schraudolph 07 a html http www jstor org stable 2004840 http www jstor org stable 2004840 https doi org 10 1137 15 m 1053141 https doi org 10 1137 19 m 1240794 https doi org 10 1137 19 m 1240794 https onlinelibrary wiley com doi abs 10 1002 9780470466971 https onlinelibrary wiley com doi abs 10 1002 9780470466971 28 brian irwin eldad haber a proof of theorem 1 to produce the sp bfgs update we first rearrange 26 a revealing that h hk w 1 uytk t w 1 67 and so the symmetry requirement that h ht means transposing 67 gives uytk t uytk t t ykut t 68 which rearranges to t 1 2 yku t uytk 69 and so h hk 1 2 w 1 yku t uytk w 1 70 next we right multiply 70 by yk to get h hk yk 1 2 w 1 yku tw 1 yk u y t kw 1 yk 71 and use 26 b to get that sk w 1 u k hkyk 1 2 w 1 yku tw 1 yk u y t kw 1 yk 72 we now left multiply both sides by 2 w and rearrange giving 2 w sk hkyk ykutw 1 yk u ytkw 1 yk 2 k 73 this can be rearranged so that u is isolated giving u 2 w sk hkyk ykutw 1 yk yt k w 1 yk 2 k 2 w sk hkyk ykutw 1 yk yt k w 1 yk 2 k 74 to get rid of the ut on the right hand side we first left multiply both sides by ytkw 1 and then transpose to get utw 1 yk 2 sk hkyk t yk ytkw 1 yk u tw 1 yk ytkw 1 yk 2 k 75 where we have taken advantage of the fact that the transpose of a scalar returns the same scalar this now allows us to solve for utw 1 yk using some basic algebra and resulting in utw 1 yk sk hkyk t yk ytkw 1 yk 1 k 76 secant penalized bfgs 29 substituting 76 into 74 gives u yky t k sk hkyk ytkw 1 yk 2 k ytkw 1 yk 1 k 2 w sk hkyk ytkw 1 yk 2 k 77 now if we substitute the expression for u in 77 into 70 after some simpli fication we get h hk 1 yt k w 1 yk 2 k sk hkyk ytkw 1 w 1 yk sk hkyk t yt k sk hkyk yt k w 1 yk 1 k w 1 yky t kw 1 78 now we further simplify by applying that wsk yk and thus w 1 yk sk revealing h hk sk hkyk stk sk sk hkyk t yt k sk 2 k yt k sk hkyk yt k sk 2 k yt k sk 1 k sks t k 79 which after a bit of algebra reveals that the update formula solving the system defined by 26 a 26 b and 26 c can be expressed as h hk hkyks t k sky t kh t k ytk sk 2 k ytk sk 2 k ytkhkyk ytk sk 2 k ytk sk 1 k sks t k 80 we can make 80 look similar to the common form of the bfgs update given in 19 by defining the two quantities k and k as in 28 and observing that completing the square gives h i sky t k ytk sk 2 k hk i yks t k ytk sk 2 k ytk sk 2 k ytkhkyk ytk sk 2 k ytk sk 1 k ytkhkyk ytk sk 2 k 2 sks t k 81 which is equivalent to h i kskytk hk i kykstk k k k k k ytkhkyk sks t k 82 concluding the proof b proof of lemma 1 the hk 1 given by 27 has the general form hk 1 g thkg dsks t k 83 30 brian irwin eldad haber with the specific choices g i kykstk d k k k k k ytkhkyk 84 by definition hk 1 is positive definite if vthk 1 v 0 v rn 0 85 we first show that 29 is a sufficient condition for hk 1 to be positive definite given that hk is positive definite by applying 83 to 85 we see that vt gthkg dsks t k v 0 v rn 0 86 must be true for the choices of g and d in 84 if hk 1 is positive definite substituting 84 into 86 reveals that v k stk v yk t hk v k stk v yk k k k k k ytk hkyk stk v 2 0 87 must be true for all v rn 0 if hk 1 is positive definite both stk v 2 and vtgthkgv are always nonnegative to see that v tgthkgv 0 note that because hk is positive definite it has a principal square root h 1 2 k and so vtgthkgv v tgth 1 2 k h 1 2 k gv h 1 2 k gv 2 2 0 88 we now observe that if d 0 the right term d stk v 2 in 87 is zero if and only if stk v 0 however if s t k v 0 then the left term v tgthkgv in 87 is zero only when v 0 hence the condition d 0 guarantees that 87 is true for all v excluding the zero vector and thus that hk 1 is positive definite the condition d 0 expands to k k k k ytkhkyk 0 89 using the definitions of k and k in 28 it is clear that k k 0 as k can only take nonnegative values furthermore as hk is positive definite ytkhkyk 0 for all yk as it is possible for k k y t khkyk to be zero we requre k 0 the condition k 0 immediately gives 29 as k can only be positive if the denominator in its definition is positive finally as k can only take nonnegative values 29 also ensures that k is nonnegative and so when 29 is true k k k ytkhkyk 0 in summary we have shown that the condition 29 ensures that the left term in 89 is positive and the right term nonnegative so d 0 and thus hk 1 is positive definite we now show that 29 is a necessary condition for hk 1 to be positive definite given that hk is positive definite if hk 1 is positive definite then ytkhk 1 yk 0 90 secant penalized bfgs 31 assuming yk 6 0 substituting 26 b into 90 gives ytk sk w 1 u k 0 91 and using 76 shows that 91 is equivalent to ytk sk k hkyk sk k 0 92 now some algebra shows that ytk sk k hkyk sk k ytk sk 1 1 ky t k sk ytk hkyk y t k sk 1 1 1 ky t k sk ytk sk 1 1 ky t k sk ytk hkyk ky t k sk 1 ky t k sk ytk sk 1 1 ky t k sk ytk hkyk k y t k sk 2 yt k hkyk 1 ky t k sk 93 and we also know that because hk is positive definite y t khkyk 0 for all yk 6 0 by definition k 0 and by the definition of the square of a real number ytk sk 2 0 as a result ytk sk w 1 u k k y t k sk 2 ytkhkyk 1 ky t k sk 0 94 is guaranteed only if the denominator 1 ky t k sk is positive which occurs when stk yk 1 k 95 this establishes that 29 is a necessary condition for hk 1 to be positive definite given that hk is positive definite and concludes the proof c proof of theorem 2 the sherman morrison woodbury formula says a ucv 1 a 1 a 1 u c 1 v a 1 u 1 v a 1 96 now observe that the sp bfgs update 27 can be written in the factored form hk 1 hk k sk hkyk k 1 k ytkhkyk 1 1 0 stk ytkhk 97 32 brian irwin eldad haber applying the sherman morrison woodbury formula 96 to the factored sp bfgs update 97 with a hk u k sk hkyk c k 1 k ytkhkyk 1 1 0 v stk ytkhk yields h 1 k 1 h 1 k h 1 k k sk hkyk k 1 k ytkhkyk 1 1 0 1 stk ytkhk h 1 k k sk hkyk 1 stk ytkhk h 1 k inverting c here gives c 1 k 1 k ytkhkyk 1 1 0 1 0 1 1 k 1 k ytkhkyk and we also have v a 1 u stk ytkhk h 1 k k sk hkyk k stk ytkhk h 1 k sk yk ks t kh 1 k sk ks t k yk ky t k sk ky t khkyk which is just a 2 2 matrix with real entries now it becomes clear that c 1 v a 1 u k 1 k yt k hkyk 1 1 0 1 st k yt k hk h 1 k k sk hkyk ks t k h 1 k sk 1 kstk yk 1 kytk sk ky t k hkyk k 1 k yt k hkyk for notational compactness let d c 1 v a 1 u ks t kh 1 k sk 1 ks t k yk 1 kytk sk ky t khkyk k 1 k ytkhkyk so d 1 1 det d ky t khkyk k 1 k ytkhkyk 1 kstk yk 1 kytk sk ks t kh 1 k sk where the determinant of d is det d ky t k hkyk k 1 k ytk hkyk ks t kh 1 k sk 1 kytk sk 2 k k ytk hkyk k k ks t kh 1 k sk 1 kytk sk 2 and we have used the fact that ytk sk s t k yk as this is a scalar quantity next u det d d 1 v k sk hkyk kytk hkyk k 1 k ytk hkyk 1 kstk yk 1 kytk sk ks t kh 1 k sk stk ytk hk k sk hkyk kytk hkykstk k 1 k ytk hkyk stk 1 kstk yk ytk hk 1 kytk sk s t k ks t kh 1 k sky t k hk secant penalized bfgs 33 so u det d d 1 v fully expanded becomes k sk ky t khkyks t k k 1 k ytkhkyk s t k 1 ks t k yk y t khk hkyk 1 kytk sk s t k ks t kh 1 k sky t khk this looks rather ugly at the moment but we continue by breaking the problem down further noting that sk ky t khkyks t k k 1 k ytkhkyk stk 1 ks t k yk y t khk k k ytkhkyk k k sks t k 1 ks t k yk sky t khk and hkyk 1 kytk sk s t k ks t kh 1 k sky t khk 1 kytk sk hkyks t k khkyk s t kh 1 k sk y t khk the above intermediate results further simplify u det d d 1 v to k k k ytkhkyk k k sks t k 1 ks t k yk sky t khk hkyks t k khkyk s t kh 1 k sk y t khk left and right multiplying the line immediately above by a 1 h 1 k gives k k k ytkhkyk k k h 1 k sks t kh 1 k 1 ks t k yk h 1 k sky t k yks t kh 1 k kyk s t kh 1 k sk y t k and thus after dividing out det d and applying bk h 1 k we arrive at the following final formula bk 1 bk k k k ytk b 1 k yk k k bksks t kbk 1 ks t k yk bksky t k yks t kbk k s t kbksk yky t k k k ytk b 1 k yk k k ks t k bksk 1 kytk sk 2 98 for the sp bfgs inverse update which concludes the proof d proof of theorem 3 referring to theorem 2 taking the trace of both sides of 98 and applying the linearity and cyclic invariance properties of the trace yields tr bk 1 1 tr bk 2 bksk 2 2 2 3 y t k bksk 4 yk 2 2 99 where 1 1 2 kd d ks t kbksk e 2 100 3 ke d ks t kbksk e 2 4 k 2 stkbksk d ks t kbksk e 2 101 34 brian irwin eldad haber with d and e defined as d k k ytk b 1 k yk k k e 1 kstk yk 2 k k 102 we now observe that after applying some basic algebra and recalling that bk is positive definite one can deduce that for all k 0 the following inequalities hold k k 0 1 k k d 1 0 2 k k 1 103 by minimizing the absolute value of the common denominator in 2 3 and 4 using the inequalities above we can obtain the bounds 1 stkbksk 2 0 0 4 k k 104 0 3 2 k k 1 stkbksk 2 k k 2 k k 2 105 as a result tr bk 1 tr bk 2 3 ytk bksk 4 yk 2 2 106 tr bk k yk 2 max bk sk 2 k yk 2 2 107 and applying max bk tr bk establishes 55 similarly referring to 80 reveals the upper bound tr hk 1 tr hk 2 k ytkhksk k k k y t khkyk sk 2 2 108 to establish 54 we apply max hk tr hk and k k to the line above and then factor this completes the proof e proof of lemma 2 the angle condition x thg x 0 expands to x thg x x th x x the x 0 109 and by applying the cauchy schwarz inequality and assumption 1 we see that if x 22 x 2 g 110 then x thg x 0 contrapositively if x thg x 0 then x 2 g 111 as is m strongly convex due to assumption 3 we have x min v x t v m 2 v 22 x 1 2 m x 22 112 squaring 111 and then combining it with 112 gives n 1 completing the proof secant penalized bfgs 35 f proof of theorem 4 as c 2 by assumption 3 applying taylor s theorem and using 58 and strong convexity gives k 1 k tk xk 1 xk 1 2 xk 1 xk t 2 u xk 1 xk k tkhkgk 2 m 2 hkgk 2 2 where u is some convex combination of xk 1 and xk proceeding note that the smallest n 1 from lemma 2 occurs when and in this case tk gk 0 if xk n 1 hence for all possible choices of n 1 it is true that tk gk 0 if xk n 1 combining this with 59 gives tkhkgk t k gk 0 113 if xk n 1 with 113 in hand continuing to bound terms gives k 1 k tk k ek 2 2 m 2 k ek 2 2 k m 2 k 2 2 m tk ek 2 2 m 2 ek 2 2 k m 2 k 2 2 m k 2 ek 2 2 2 m 2 ek 2 2 k m 2 k 2 2 m 1 2 k 2 2 1 2 ek 2 2 2 2 m 2 ek 2 2 where the last inequality follows from expanding 0 1 2 k 2 1 2 ek 2 2 1 2 k 2 2 k 2 ek 2 1 2 ek 2 2 114 and using 60 simplifying the last inequality reveals that k 1 k 2 k 2 2 2 ek 2 2 115 since is m strongly convex by assumption 3 we can apply k 2 2 2 m k 116 as shown in the proof of lemma 2 see appendix e which combined with 115 and assumption 1 gives k 1 k m k 2 g 2 117 subtracting from both sides we get k 1 1 m k 2 g 2 118 36 brian irwin eldad haber which by subtracting 1 2 m g 2 from both sides and simplifying gives k 1 1 2 m g 2 1 m k 2 g 2 1 2 m g 2 1 m k m 1 1 2 m g 2 1 m k 1 2 m g 2 thus establishing the q linear result 61 we obtain 62 by recursively ap plying the worst case bound in 61 noting that in the worst case if x 0 n 1 then the sequence of iterates xk remains outside of n 1 only approaching n 1 in the limit k g extended numerical experiments tables 4 and 5 compare the performance of sp bfgs vs bfgs on the 32 cutest test problems with only gradient noise present i e f 0 gradient noise was generated using g 10 4 x 0 2 where the starting point x 0 varies by cutest problem to ensure that noise does not initially dominate gradient evaluations overall sp bfgs outperforms bfgs on approximately 80 of the cutest problems with only gradient noise present and performs at least as good as bfgs on approximately 95 of these problems secant penalized bfgs 37 sp bfgs with gradient noise only problem dim mean opt median opt min opt max opt s 2 opt argtrgls 200 9 6 e 02 9 6 e 02 1 0 e 01 8 5 e 02 1 9 e 05 arwhead 500 2 8 e 00 2 8 e 00 2 8 e 00 2 7 e 00 1 7 e 03 beale 2 1 4 e 01 1 4 e 01 1 6 e 01 7 0 e 00 4 1 e 00 box 3 3 6 7 e 00 6 5 e 00 1 1 e 01 6 3 e 00 6 3 e 01 boxpower 100 2 7 e 00 2 7 e 00 3 1 e 00 2 3 e 00 4 6 e 02 brownbs 2 4 5 e 00 5 9 e 00 8 0 e 00 1 1 e 00 8 4 e 00 broydnbdls 50 5 4 e 00 5 4 e 00 5 9 e 00 5 0 e 00 3 4 e 02 chainwoo 100 1 6 e 00 1 7 e 00 7 6 e 02 2 1 e 00 1 5 e 01 chnrosnb 50 3 2 e 00 3 0 e 00 4 9 e 00 2 6 e 00 4 5 e 01 coating 134 3 4 e 01 3 4 e 01 1 8 e 01 4 2 e 01 3 1 e 03 coolhansls 9 9 4 e 01 9 4 e 01 1 2 e 00 4 8 e 01 4 2 e 02 cube 2 2 7 e 00 2 5 e 00 5 8 e 00 1 7 e 00 7 5 e 01 cycloocfls 20 7 4 e 00 7 2 e 00 9 3 e 00 5 9 e 00 8 1 e 01 extrosnb 10 5 1 e 00 5 2 e 00 5 3 e 00 4 7 e 00 3 0 e 02 fminsrf 2 64 8 6 e 00 8 7 e 00 8 8 e 00 8 1 e 00 3 4 e 02 genhumps 5 2 7 e 00 2 6 e 00 5 2 e 00 1 0 e 00 1 1 e 00 genrose 5 1 2 e 01 1 2 e 01 1 4 e 01 8 9 e 00 2 0 e 00 heart 6 ls 6 1 0 e 00 1 2 e 00 1 8 e 00 1 2 e 00 5 0 e 01 helix 3 5 7 e 00 5 9 e 00 8 7 e 00 3 4 e 00 1 4 e 00 mancino 30 1 0 e 00 1 0 e 00 1 4 e 00 7 0 e 01 3 7 e 02 methanb 8 ls 31 3 6 e 00 3 6 e 00 4 0 e 00 3 3 e 00 3 1 e 02 modbeale 200 1 2 e 00 1 2 e 00 3 8 e 01 1 9 e 00 1 8 e 01 nondia 10 3 5 e 03 3 6 e 03 4 3 e 03 1 1 e 03 6 6 e 07 powellsg 4 5 7 e 00 5 3 e 00 9 3 e 00 4 0 e 00 1 6 e 00 power 10 3 5 e 00 3 5 e 00 4 4 e 00 2 8 e 00 1 3 e 01 rosenbr 2 1 1 e 01 1 2 e 01 1 4 e 01 5 1 e 00 4 4 e 00 rosenbrtu 2 1 9 e 01 1 9 e 01 2 2 e 01 1 7 e 01 1 1 e 00 sbrybnd 500 3 9 e 00 3 9 e 00 3 9 e 00 3 9 e 00 2 0 e 05 sineval 2 1 3 e 01 1 3 e 01 1 8 e 01 1 1 e 01 3 3 e 00 snail 2 1 5 e 01 1 6 e 01 1 8 e 01 1 2 e 01 1 6 e 00 srosenbr 1000 9 7 e 01 9 7 e 01 1 3 e 00 4 8 e 01 3 2 e 02 vibrbeam 8 1 6 e 00 1 6 e 00 1 2 e 00 2 8 e 00 9 1 e 02 table 4 performance of sp bfgs on 32 selected cutest test problems with noise added to gradient evaluations only i e f 0 the number of objective function evaluations is fixed at 2000 opt log 10 best measures the optimality gap where best denotes the smallest value of the true function measured at any point during an algorithm run statistics are calculated from a sample of 30 runs per algorithm and the dim column gives the problem dimension the spbfgs penalty parameter was set as k 108 g sk 2 10 10 for each problem gradient noise was generated using g 10 4 x 0 2 where the starting point x 0 varies by cutest problem 38 brian irwin eldad haber bfgs with gradient noise only problem dim mean opt median opt min opt max opt s 2 opt argtrgls 200 9 2 e 02 9 3 e 02 9 9 e 02 8 2 e 02 1 7 e 05 arwhead 500 2 5 e 00 2 5 e 00 2 6 e 00 2 5 e 00 7 6 e 04 beale 2 8 3 e 00 8 5 e 00 1 2 e 01 5 8 e 00 3 9 e 00 box 3 3 6 4 e 00 6 4 e 00 6 6 e 00 6 3 e 00 2 3 e 03 boxpower 100 2 8 e 00 2 8 e 00 3 3 e 00 2 4 e 00 6 1 e 02 brownbs 2 6 8 e 02 1 0 e 00 8 2 e 00 3 6 e 00 1 0 e 01 broydnbdls 50 5 1 e 00 5 1 e 00 5 3 e 00 4 9 e 00 1 5 e 02 chainwoo 100 1 7 e 00 1 8 e 00 1 1 e 00 2 2 e 00 5 8 e 02 chnrosnb 50 2 9 e 00 2 7 e 00 4 5 e 00 2 1 e 00 3 8 e 01 coating 134 3 6 e 01 3 7 e 01 2 1 e 01 4 2 e 01 2 6 e 03 coolhansls 9 5 6 e 01 6 4 e 01 1 3 e 00 1 9 e 01 1 9 e 01 cube 2 1 1 e 00 1 1 e 00 1 8 e 00 9 6 e 01 5 6 e 02 cycloocfls 20 6 5 e 00 6 5 e 00 8 3 e 00 5 1 e 00 5 4 e 01 extrosnb 10 5 1 e 00 5 1 e 00 5 3 e 00 4 9 e 00 8 1 e 03 fminsrf 2 64 8 2 e 00 8 2 e 00 8 7 e 00 7 3 e 00 1 4 e 01 genhumps 5 1 5 e 00 1 2 e 00 4 0 e 00 2 8 e 01 8 4 e 01 genrose 5 6 8 e 00 6 7 e 00 8 7 e 00 5 9 e 00 4 8 e 01 heart 6 ls 6 1 2 e 00 1 2 e 00 1 2 e 00 1 2 e 00 1 9 e 04 helix 3 4 8 e 00 4 6 e 00 8 1 e 00 2 6 e 00 2 1 e 00 mancino 30 8 3 e 01 8 8 e 01 1 2 e 00 3 3 e 01 5 3 e 02 methanb 8 ls 31 3 5 e 00 3 4 e 00 3 9 e 00 3 3 e 00 2 8 e 02 modbeale 200 1 0 e 00 1 1 e 00 6 2 e 01 2 1 e 00 3 6 e 01 nondia 10 1 2 e 03 1 3 e 03 4 4 e 03 1 3 e 02 2 2 e 05 powellsg 4 5 3 e 00 5 0 e 00 8 0 e 00 3 6 e 00 1 6 e 00 power 10 3 4 e 00 3 4 e 00 4 3 e 00 2 8 e 00 1 4 e 01 rosenbr 2 6 1 e 00 5 9 e 00 1 0 e 01 3 7 e 00 2 9 e 00 rosenbrtu 2 1 5 e 01 1 5 e 01 1 8 e 01 1 4 e 01 1 6 e 00 sbrybnd 500 3 9 e 00 3 9 e 00 3 9 e 00 3 9 e 00 2 0 e 05 sineval 2 1 2 e 01 1 3 e 01 1 7 e 01 8 5 e 00 4 0 e 00 snail 2 1 1 e 01 1 1 e 01 1 6 e 01 8 2 e 00 3 5 e 00 srosenbr 1000 9 1 e 01 8 8 e 01 1 3 e 00 5 1 e 01 3 1 e 02 vibrbeam 8 1 7 e 00 1 6 e 00 1 4 e 00 2 6 e 00 1 0 e 01 table 5 performance of bfgs on 32 selected cutest test problems with noise added to gradient evaluations only i e f 0 the number of objective function evaluations is fixed at 2000 opt log 10 best measures the optimality gap where best denotes the smallest value of the true function measured at any point during an algorithm run statistics are calculated from a sample of 30 runs per algorithm and the dim column gives the problem dimension for each problem gradient noise was generated using g 10 4 x 0 2 where the starting point x 0 varies by cutest problem 1 introduction 2 mathematical background 3 derivation of secant penalized bfgs 4 algorithmic framework 5 convergence of sp bfgs 6 numerical experiments 7 final remarks a proof of theorem 1 b proof of lemma 1 c proof of theorem 2 d proof of theorem 3 e proof of lemma 2 f proof of theorem 4 g extended numerical experiments