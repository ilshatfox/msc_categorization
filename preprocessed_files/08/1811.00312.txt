a local block coordinate descent algorithm for the convolutional sparse coding model ev zisselman jeremias sulam michael elad abstract the convolutional sparse coding csc model has recently gained considerable traction in the signal and image processing communities by providing a global yet tractable model that operates on the whole image the csc was shown to overcome several limitations of the patch based sparse model while achieving superior performance in various applications contemporary methods for pursuit and learning the csc dictionary often rely on the alternating direction method of multipliers admm in the fourier domain for the computational convenience of convolutions while ignoring the local characterizations of the image a recent work by papyan et al 1 suggested the sbdl algorithm for the csc while operating locally on image patches sbdl demonstrates better performance compared to the fourier based methods albeit still relying on the admm in this work we maintain the localized strategy of the sbdl while proposing a new and much simpler approach based on the block coordinate descent algorithm this method is termed local block coordinate descent lobcod further more we introduce a novel stochastic gradient descent version of lobcod for training the convolutional filters the stochastic lobcod leverages the benefits of online learning while being applicable to a single training image we demonstrate the advantages of the proposed algorithms for image inpainting and multi focus image fusion achieving state of the art results i introduction sparse representation has been shown to be a very power ful model for many real world signals leading to impressive results in various restoration tasks such as denoising 2 deblurring 3 inpainting 4 5 super resolution 3 6 and recognition 7 to name a few the core assumption of this model is that signals can be expressed as a linear combination of a few columns also called atoms taken from a matrix d rn m termed a dictionary concretely for a signal x rn the model assumption is that x d v where v is a noise vector with bounded energy v 2 this allows for a slight deviation from the model and or may account for noise in the signal the vector rm is the sparse representation of the signal obtained by solving the following optimization problem 8 9 arg min 0 s t x d 2 1 where 0 denotes the l 0 pseudo norm that counts the number of non zeros in the representation solving this op e zisselman is with the department of electrical engineering tech nion israel institute of technology j sulam is with the department of biomedical engineering johns hopkins university m elad is with the department of computer science technion israel institute of technology timization problem known as the pursuit stage is generally np hard but under certain conditions 8 the solution of problem 1 can be approximated using greedy algorithms such as orthogonal matching pursuit omp 10 or convex relaxation algorithms such as basis pursuit bp 11 over the years various methods have been proposed to adaptively learn the model parameters from real data such dictionary learning methods attempt to find d that best represents the set of signals at hand prime examples are k svd 9 mod 12 double sparsity 13 online dictionary learning 14 trainlets 15 and more when dealing with high dimensional signals learning the dictionary suffers from the curse of dimensionality and this process becomes computationally infeasible to cope with this problem many algorithms suggest training a local model on fully overlapping patches taken from the signal x and processing these patches independently this patch based dictionary learning technique has gained much popularity over the years due to its simplicity and high performance 2 3 4 6 yet patch based approaches are known to be sub optimal as they ignore the relations between neighboring patches 16 17 an alternative approach to meet this challenge is posed by the convolutional sparse coding csc model this model assumes that the signal can be represented as a superposition of a few local filters convolved with sparse feature maps the csc model handles the signal globally and yet pursuit and dictionary learning are feasible due to the specific structure of the dictionary involved this model has been shown to be useful in tackling some limitations of the patch based model and led to superior performance in several applications such as super resolution 18 inpainting 19 image separation 1 source separation 20 image fusion 21 and audio processing 22 albeit with room for improvement contemporary csc based algorithms often rely on the admm 23 formulation in order to extract the signal representation of the model and train its corresponding filters while the majority of works employ admm in the fourier domain 19 24 25 a recent approach proposed by papyan at el 1 coined slice based dictionary learning sbdl adopts a local point of view and trains the filters in terms of only local computations in the signal domain this local global approach and the decomposition that it induces follows a recent work 26 that presented a novel theoretical analysis of the global csc model providing guarantees which stem from localized sparsity measures the ar x iv 1 81 1 00 31 2 v 1 cs c v 1 n ov 2 01 8 sbdl algorithm demonstrates state of the art performance compared to the fourier based methods while still relying on the admm algorithm as such this approach requires the introduction of n auxiliary variables increasing the memory requirements it can only be deployed in a batch learning mode its convergence is questionable 1 and strongly depends on the admm parameter which is application dependent in this work we propose intuitive and easy to implement algorithms based on the block coordinate descent approach for solving the global pursuit and the csc filter learning problems all done with local computations in the original domain the proposed algorithms operate without auxiliary variables nor extra parameters for tuning in the pursuit stage we call this algorithm local block coordinate descent lobcod in addition we introduce a stochastic gradient descent variant of lobcod for training the convolutional filters this algorithm leverages the benefits of online learn ing while being applicable even to a single training image the lobcod algorithm and its stochastic version show faster convergence and achieve a better solution to the csc problem compared to the previous admm based methods global or local we should note that a very recent work by moreau at el 27 also proposes a coordinate descent based algorithm for the pursuit task in the csc model their algorithm like ours operates locally and without necessitating additional parameters however the algorithm in 27 is restricted in two important ways compared to ours i their method is specifically tailored to 1 d signals and does not harness the 2 d structure of the csc model for images ii their algorithm is limited to pursuit only with no treatment for the filter learning the rest of this paper is organized as follows section ii provides an overview of the csc model and discusses previous methods the proposed pursuit algorithm and its derivation are presented in section iii in section iv we discuss dictionary update methods and introduce the stochas tic lobcod algorithm we compare these methods with previously published approaches in section v section vi shows how our method can be employed to tackle the tasks of image inpainting and multi focus image fusion and later in section vii we demonstrate this empirically section viii concludes this work ii convolutional sparse coding the csc model assumes that a signal 2 x rn can be represented by the sum of m convolutions these are built by feature maps zi mi 1 each of length of the original signal n convolved with m small support filters di mi 1 of length n n in the dictionary learning problem one minimizes the following cost function over both the filters 1 while their pursuit method is provably converging this is no longer the case when the dictionary is updated within the admm as suggested in their work 2 the description given here focuses on 1 d signals for simplicity of the presentation all our treatment applies to 2 d or higher dimensions signals just as well i 2 1 ix i x 2 1 fig 1 an illustration of the csc model and its local components and the feature maps 3 min di zi 1 2 x m i 1 di zi 22 m i 1 zi 1 2 given the filters the above problem becomes the csc pursuit task of finding the representations zi mi 1 consider a global dictionary d to be the concatenation of m banded circulant matrices where each matrix represents a convolution with one filter di by permuting its columns the global dictionary d consists of all shifted versions of a local dictionary dl of size n m containing the filters di mi 1 as its columns and the global sparse vector is simply the interlaced concatenation of all the feature maps zi mi 1 such a structure is depicted in fig 1 using the above formulation the convolutional dictionary learning problem 2 can be rewritten as min d 1 2 x d 22 1 3 similar to our earlier comment when d is known we obtain the csc pursuit problem defined as min 1 2 x d 22 1 4 herein we review some of the definitions from 26 as they will serve us later for the description of our algorithms the global sparse vector can be broken into n non overlapping m dimensional local vectors i referred to as needles this way one can express the global vector x as x n i 1 p t i dl i where p t i r n n is the operator that positions dl i in the i th location and pads the rest of the entries with zeros on the other hand a patch pix pid taken from the signal x equals to i see fig 1 where rn 2 n 1 m is a stripe dictionary containing dl in its center and i is the stripe vector containing the local vector i in its center in other words a stripe i is the sparse vector that codes all the content in the patch pix whereas a needle i only codes part of the information within it the theoretical work in 26 suggested an analysis of the csc global model augmented by a localized sparsity 3 throughout the subsequent derivations we assume that the filters are normalized to a unit l 2 norm measure specifically this work showed that if all the stripes i are sparse the solution to the convolutional sparse pur suit problem is unique and can be recovered by a greedy algorithm such as the omp 10 or a convex relaxation algorithm such as the bp 11 they extended the analysis to a noisy regime showing that under similar sparsity assump tions the pursuit algorithms for the global formulation are also stable inspired by this analysis herein we maintain such a local global decomposition and propose a global algorithm that operates locally on image patches prior to describing our algorithm we turn to review a closely related work the sbdl algorithm 1 equipped with the above definitions and the separability of the l 1 norm we can express the global csc problem in terms of the local sparse vectors i and the local dictionary dl by min dl i 1 2 x n i 1 pti dl i 2 2 n i 1 i 1 5 to solve this problem the slice based dictionary learning sbdl algorithm 1 adopts a variable splitting approach by denoting dl i as the i th slice si and writing the global signal in terms of the slices x n i 1 p t i si the above can be written as the following constrained minimization problem min dl i si 1 2 x n i 1 pti si 2 2 n i 1 i 1 s t si dl ni 1 6 for solving 6 the sbdl employs the admm algorithm 23 which translates the constraints to penalties and mini mizes the following augmented lagrangian problem min dl i si ui 1 2 x n i 1 pti si 2 2 n i 1 i 1 2 si dl i ui 22 7 by optimizing with respect to every set of variables i ni 1 si n i 1 ui n i 1 sequentially here ui n i 1 denote the dual variables of the admm formulation the mini mization with respect to the needles i ni 1 is separable and boils down to a traditional patch based bp formulation in particular the work in 1 employed the batch lars 28 algorithm for this stage whereas the minimization with respect to the slices si ni 1 amounts to a simple least squares problem the sbdl also accommodates learning of the filters i e the local dictionary dl using any patch based dictionary learning algorithm such as the k svd 9 or the mod 12 note however that in adding this part within the admm all the convergence guarantees are lost in this work unlike the above variable splitting approach we leverage the block coordinate descent algorithm to update the local sparse vectors needles without defining any addi tional variables in this manner we avoid the need of tuning extra parameters conserve memory all while demonstrating superior performance we also propose a dictionary update scheme which can operate both in batch and online modes iii proposed method csc pursuit a local block coordinate descent in this section we focus on the pursuit of the representa tions leaving the study of updating the dictionary for section iv the convolutional sparse coding problem presented in the previous section is solved by minimizing the global objective of equation 4 in this paper we adopt a local strategy and split the global sparse vector into local vectors needles as described in equation 5 however rather than optimizing with respect to all the needles together we can treat each needle i as a block of coordinates taken from the global vector and optimize with respect to each such block separately and sequentially consequently the update rule of each needle can be written as min i 1 2 x n j 1 j 6 i ptj dl j p t i dl i 2 2 i 1 8 by defining ri x n j 1 j 6 i ptj dl j as the residual image without the contribution of the needle i we can rewrite equation 8 as min i 1 2 ri pti dl i 2 2 i 1 9 while the above minimization involves global variables such as the residual ri one can show see appendix a that this can be decomposed into an equivalent and local problem min i 1 2 piri dl i 22 i 1 10 this follows from the observation that the update rule of the needle i is effected only by pixels belonging to the corresponding patch piri the part that fully overlaps with the slice si dl i for more details we refer the reader to appendix a the main idea of the block coordinate descent algorithm is that every step minimizes the overall penalty w r t a certain block of coordinates while the other ones are set to their most updated values following this idea every local pursuit stage 10 proceeds by updating the global reconstructed signal x and the global residual r x x as a preprocessing stage to the consecutive stage that updates the next needle based on the most updated values of the previous needles this pursuit algorithm is summarized in algorithm 1 an important insight is that needles that have no foot print overlap in the image can be updated efficiently in parallel in the above algorithm without changing the al gorithm s outcome this enables employing efficient batch implementations of the lars algorithm alternatively the calculation can be distributed across multiple processors to gain a significant speedup in performance to formalize these observations we define the layer li as the set of algorithm 1 the lobcod basic pursuit algorithm input signal x dictionary dl initial needles 0 i n i 1 output needles i ni 1 initialization r x x 0 k 0 while not converged do k k 1 for i 1 n do computation of the local residual ri r p t i dl k 1 i local sparse pursuit ki arg min i 1 2 piri dl i 22 i 1 update of the reconstructed signal x x pti dl k i k 1 i update of the residual signal r x x end end needles that have no induced overlap in the image we sweep through these layers and update their respective needles in parallel followed by updating the global reconstructed signal x and the global residual r this way the number of the layers imposes the number of the inner iterations which will determine the complexity of our final algorithm as opposed to algorithm 1 that has n inner iterations since it iterates trough the needles here the number of the inner iterations depends only on the patch size for n n patches the number of layers is n this parallelized pursuit algorithm is presented in algorithm 2 note that this algorithm can clearly be extended to iterate over multiple signals but for the sake of brevity we assume that the data corresponds to an individual signal x b boundary conditions and initialization in the formulation of the csc model as shown in fig 1 we assumed that the dictionary is comprised of a set of banded circulant matrices which impose a circulant bound ary conditions on the signals in practice however signals and images do not exhibit circulant boundary behavior therefore our model incorporates a preemptive treatment of the boundaries we adopt a similar approach to 1 in which the signal boundaries are padded with n 1 elements prior to decomposing it with the model at the end of the process we discard the added padding by cropping the n 1 boundary elements from the reconstructed signal and from the resulting feature maps sparse representation another beneficial preprocess step is needles initialization a good initialization would equally spread the contribution of the needles towards signal reconstruction with that goal we set the initial value of each needle i to be the sparse algorithm 2 the stochastic lobcod pursuit and dic tionary learning algorithm input signal x initial dictionary dl initial needles 0 i n i 1 output needles i ni 1 the trained dictionary dl initialization r x x 0 k 0 while not converged do k k 1 for j 1 n do computation of the residual rj r i lj pti dl k 1 i sparse pursuit i lj in parallel ki arg min i 1 2 pirj dl i 22 i 1 computation of the reconstructed signal x x i lj pti dl k i k 1 i computation of the residual signal r x x computation of the gradient w r t dl dl i lj pir ki t dictionary update dl p 1 dl dl end end representation of 1 n pix i e its relative portion of the corre sponding patch this can be done by solving the following local pursuit for every needle 0 i arg min i 1 2 1 n pix dl i 22 i 1 11 as a preprocess stage of our algorithm iv csc dictionary learning when addressing the question of learning the csc filters the common strategy is to alternate between sparse coding and dictionary update steps for a fixed number of iterations the dictionary update step aims to find the minimum of the quadratic term of equation 5 subject to the constraint of normalized dictionary columns min dl 1 2 x n i 1 pti dl i 2 2 s t di 2 1 mi 1 12 one can do so in a batch manner which requires access to the entire data set at every iteration or in an online stochastic manner that enables access to only small part of the dataset at every update step this way it is also applicable for streaming data scenarios when the probability distribution of the data changes over time a batch update usually for offline applications when the whole data set is given the batch approach is generally simpler and thus we start with its description the typical approach is to alternate between sparse coding 4 and dictionary update 12 phases for the latter solving problem 12 requires finding the optimum dl that satisfies the normalization constraint one can find this optimal solution using projected steepest descent perform steepest descent with a small step size and project the solution to the constraint set after each iteration until convergence to that end the gradient of the quadratic term in equation 12 w r t dl is 4 dl n i 1 pi x x ti 13 the final update step for the local dictionary dl is obtained by advancing in the direction of this gradient 13 and nor malizing the columns of the resulting dl in each iteration until convergence this batch dictionary update rule follows the line of thought of the mod algorithm 12 and thus improves the solution in each step however it exhibits a very slow con vergence rate since each dictionary update can be performed only after finishing the entire sparse coding pursuit stage which is markedly inefficient as the pursuit is the most time consuming part of the algorithm this brings us to the stochastic lobcod alternative b local stochastic gradient descent approach the traditional stochastic gradient descent sgd ap proach restricts the computation of the gradient to a subset of the data and advances in the direction of this noisy gradient with every update step building upon this concept and the fact that equation 13 reveals a separable gradient w r t the patches and their corresponding needles we can update the dictionary in a stochastic manner rather than concluding the entire pursuit stage and then advancing in the direction of the global gradient we can take a small step size and update the dictionary after finding the sparse representation of only a small group of needles according to section iii every iteration updates a group of needles referred to as a layer li which in turn could now serve to update the dictionary this way our algorithm convergences faster and adopts the stochastic behavior of the sgd while still operating on a single image the filters should be normalized after every dictionary update by projecting them onto the l 2 unit ball here due to the choice of small step size we simply normalize the atoms after every dictionary update dl p 1 dl dl 4 the full derivation for the gradient can be found in appendix b where p 1 denotes the operator that projects the dictionary atoms onto the unit ball the final algorithm that incorporates the dictionary update is summarized in algorithm 2 note that although this dictionary update rule introduces an extra parameter the step size determining its value is rather intuitive and can be performed automatically by setting it to 1 2 of the norm of the gradient furthermore this update rule may also leverage any stochastic optimization algorithm such as momentum adagrad adadelta adam 29 etc with their authors recommended parameter values this choice of parameter setting is sufficient as will be demonstrated empirically in section vii in the rest of this work we will use this dictionary update rule as it shows superior results v relation to other methods in this section we evaluate the proposed approach and describe its advantages over the fourier and admm based methods 1 parallel computation our algorithm is trivial to parallelize efficiently across multiple processors by virtue of operating directly on the image patches one can split the computation between n n processors in correspondence with the number of the needles in every layer and perform the local sparse pursuit stage in parallel for all the needles in the same layer at the end of this stage and in preparation for computing the next layer every processor needs to pass its new local sparse result solely to its neighboring processors i e those which act on common patches this way each processor waits only for its neighbors to finish their calculations and is unhindered by processors that target farther regions of the image this path of computing maintains its efficacy even in case where the processors differ in their capacity or in case of large variation in the local complexity of the image the global aggregation is performed only once if needed at the end of the algorithm to produce the reconstructed image such a parallel approach cannot be directly applied with admm based algorithms that use auxiliary variables or with fourier based methods that lose the relation to the local patches of the image 2 online learning the proposed algorithm due to its local stochastic manner can work in a streaming mode where the probability distribution of the patches varies over time it is unclear how to adopt such an approach in the fourier based methods 19 24 considering the global nature of the fourier domain or in the sbdl algorithm 1 considering its use of auxiliary variables another aspect of this advantage is our ability to run in an online manner even for a single input image this stands in sharp contrast to other recent online methods 30 31 which allow for online training but only in the case of streaming images other approaches took a step further and proposed partitioning the image into smaller sub images 32 but this is still far from method time complexity 32 sparse qimnlog n t pursuit inmk sgd using sparse matrix 32 freq qimnlog n t pursuit imnlog n inm sgd in the fourier domain sbdl innm in k 3 mk 2 lars nm 2 gram ink n m nm 2 k svd ours innm in k 3 mk 2 lars nm 2 gram in n nk m stochastic lobcod table i complexity analysis of our method compared to the online algorithms presented in 32 and the sbdl algorithm 1 i number of signals n signal dimension m number of filters n patch size k maximum number of non zeros per needle q number of inner iterations for the pursuit algorithm the dominant terms are highlighted in red color our approach which can stochastically estimate the gradient for each needle 3 parameter free contrary to admm based ap proaches our algorithm is unhindered by cumbersome manual parameter tuning at the pursuit stage more over it benefits from an intuitively tuned parameter the step size in the dictionary learning stage as described as section iv 4 memory efficient our algorithm has better storage complexity compare to the admm based approaches 1 19 since the update of the sparse vector is per formed in place and does not require any auxiliary variables for example the sbdl 1 requires o n auxiliary variables for every patch in the image each of these variables is of patch size n n thus this methods requires o nn extra memory compare to our algorithm 5 adaptive local complexity as opposed to the fourier oriented algorithms our algorithm is attuned to the local properties of the signal as such it can be easily modified to allow an adaptive number of non zeros in different regions of the global signal at this point we turn to evaluate our proposed approach in terms of computational complexity and compare it to previ ous methods we assume that the number of the non zeros in every needle is limited to at most k non zeros and we denote by i the number of the training images we evaluate the complexity of every outer iteration single epoch of our algorithm and compare it to the complexity of executing an epoch in the alternative algorithms every inner iteration of our algorithm algorithm 2 operates on a layer of n n needles while the global iteration operates on the whole dataset the resulting computation cost of the residuals rj for all the layers is o in nk n which is comprised of o innk for computing all the n slices dl i of all the i images and o inn computations for subtracting them from the global residual given the residuals every iteration applies the lars algorithm for solving the local sparse pursuit for all the ni needles requiring o k 3 mk 2 nm per needle 33 and o in k 3 mk 2 nm nm 2 computations for all the n needles in all the i images the latter term nm 2 corresponds to the precomputation of the gram matrix of the dictionary dl which is usually negligible since it is computed once for all the needle next we evaluate the complexity of reconstructing the signal direct computation requires o in nk n k operations which is effectively o innk since this is the dominant term the computation of the global residual requires another o in operations these last two phases are negligible compared to the sparse pursuit stage and are therefore omitted from the final expression finally the computation of the gradient is o in n nk and the dictionary update stage is o inm updating the dictionary requires o mn computations and occurs in n times in every epoch we summarize the above analysis in table i and compare it to the complexity analysis of the sbdl algorithm 1 in addition table i presents the complexity of executing an epoch of the two sgd based online algorithms that were introduced in 32 where q corresponds to the number of inner iterations of the sparse pursuit stage preformed in the fourier domain the most demanding stage in both the sbdl algorithm and in our approach is the local sparse pursuit which is o in k 3 mk 2 nm assuming that the needles are very sparse k m which is often the case with real world signals the complexity of the local sparse pursuit stage is governed by o ninm in both algorithms this implies that the complexity of our algorithm is comparable to the complexity of the sbdl which is a batch algorithm on the other hand the complexity of the online algorithm in 32 is dominated by the computation of the fft in the pursuit stage which is o qimnlog n meaning that their algorithm scales as o nlog n with the global dimension of the signals while our algorithm grows linearly vi image processing via csc having established the foundations for our algorithms we now set to detail their extended variants for tackling two image processing tasks image inpainting and multi focus image fusion a image inpainting the task of image inpainting pertains to filling in missing pixels at known locations in the image assume we are given a corrupted image y ax where a rn n is a binary diagonal matrix that represents the degradation operator so that a i i 0 implies that the pixel xi is masked the goal of image inpainting is to reconstruct the original image x using the csc formulation this can be performed by first solving the following optimization problem min 1 2 y ad 22 1 14 and then taking the found representation and multiplying by d by applying the steps described in section iii we split the above global optimization problem into a series of more manageable problems each acting on a block of coordinates i e a needle this yields the following version of equation 10 min i 1 2 piri aidl i 22 i 1 15 here ai piapti is the operator that masks the cor responding i th patch and ri y a n j 1 j 6 i ptj dl j is the residual between the corrupted image and the de graded version of the reconstructed image where the residual ri does not account for the needle i as mentioned in section iii we parallelize the computations of the needles that comprised each layer note that in this application as opposed to the general case every needle i is multiplied by a different effective dictionary aidl consequently the parallel computation of each needle has to also be carried out with a different effective dictionary preventing the use of batch lars and other similar strategies yet these pursuits are still parallelizable in different cores or nodes the dictionary dl can be pretrained on an external un corrupted dataset or trained on the corrupted image directly using the following gradient dl i lj pia t y ax ti 16 where x n j 1 p t j dl j is the reconstructed image the derivation of the gradient above is identical to that described in section iv with the exception of incorporating the mask a b multi focus image fusion image fusion techniques aim to integrate complimentary information from multiple images captured with different focal settings into an all in focus image of higher quality many patch based sparse formulations were proposed to ad dress this task such as choose max omp 34 simultaneous omp 35 and coupled sparse representation 36 in this work we adopt a similar scheme to 21 which utilizes the csc model for tackling the task of image fusion with the distinction of solving a unified minimization problem assume we are given a set y k lk 1 of source images to fuse as well as a pretrained dictionary di mi 1 each image y k is decomposed into a base component y kb which is a smooth piece wise constant image and an edge component y ke that contains the high frequency elements y k y kb y k e 17 where the separation is performed by means of applying distinctive priors the base component is usually extracted by imposing a prior which penalizes the l 2 norm of its gradient modeling the edge component however is more involved and has been the subject matter of many image processing algorithms 34 35 36 37 38 39 in this work we employ the csc model to describe the edge components as it has shown promising results in 21 using the aforementioned priors the separation of the image to its components amounts to solving the following optimization problem min ke y k b 1 2 y k de ke y k b 2 2 k e 1 1 2 y kb 2 2 18 where ke is the sparse representation of y k e under the given convolutional dictionary de i e y ke de k e and y kb 2 2 is given by y kb 2 2 gx y k b 2 2 gy y k b 2 2 19 where gx 1 1 and gy 1 1 t are the horizontal and vertical gradient operators respectively by taking similar steps to those presented in section iii we can once more rewrite the above optimization problem as min k j y k b 1 2 y k n j 1 ptj dl k j y k b 2 2 n j 1 kj 1 1 2 y kb 2 2 20 where kj n j 1 are the needles which compose the sparse vector ke and dl is the local dictionary of de problem 20 can be solved by alternating between min imizing w r t y kb and y k e where the latter boils down to seeking for the sparse needles kj n j 1 to that end the update rule of the kj n j 1 is the set of local pursuit problems min k j 1 2 y k y kb n j 1 ptj dl k j 2 2 n j 1 kj 1 21 which can be solved using our proposed algorithm whereas the update rule of yb is the following least square minimiza tion problem min y k b 1 2 y k de ke y k b 2 2 1 2 y kb 2 2 22 for solving problem 22 we set its gradient w r t yb to zero to obtain the following update rule y kb i g t xgx g t ygy 1 y k de ke 23 where gx and gy are the matrix representations of the gradient operators once these problems have been solved for all the input images y k lk 1 we aim to merge each set of feature maps 5 zkl l k 1 in a way that best captures the focused objects in the resulting images for each image y k we generate an activity map based on the intensity of the l 1 norm of its feature maps more specifically we sum pixel wise the absolute valve of its m feature maps zkl m l 1 to form an activity map that matches the size of the image n a k i j m l 1 zkl i j 1 24 to make this method more robust and less susceptible to misregistration we convolve the above activity maps with a 5 this set of feature maps refers to the l th feature maps of the input images uniform kernel us of a small support s s to produce the final activity maps ak a k us 25 based on the observation that a significant value in the activity map ak indicates a sharp region in the image y k we then reconstruct the all in focus edge component by selectively assembling the most prominent regions from the feature maps based on their pixel wise values in the corresponding activity maps z f l i j z k l i j k arg max k ak i j 26 where zfl m l 1 are the feature maps of the fused image afterward we fuse the base components either by taking their average y f b 1 n l k 1 y kb 27 or by nominating regions of the base components according to the maximum value in the respective activity maps i e y f b i j y k b i j k arg max k ak i j 28 where y fb is the base component of the fused image here we opt for the latter since it produces better results finally the fusion result y f is obtained by gathering its components y f y f b m l 1 di z f l 29 vii experiments we turn to demonstrate the performance of the proposed algorithm throughout all our experiments we used a local dictionary composed of m 81 filters each of size 8 8 in addition we used the lars algorithm 28 for solving the local sparse pursuit stage a run time comparison to begin with and to provide a comparison to other state of the art methods we evaluate the performance of the proposed algorithm for solving equation 5 against other leading batch algorithms for csc the sbdl algorithm 1 the algorithm in 40 and the algorithm presented in 41 all ran with 20 on the fruit dataset 42 the dataset contains 10 images of size 100 100 pixels and all the images were mean subtracted by convolving them with an 8 8 uniform kernel as a preprocessing step for learning the dictionary we used the adam algorithm 29 in the initial 30 iterations with 0 02 and instate the adam parameters in accordance with the authors recommendation 1 0 9 2 0 999 and 108 subsequent iterations applied the momentum algorithm with 10 7 and 0 8 until convergence 6 fig 2 presents a comparison of the objective value as a function of time for each of the competing algorithms showing that our method achieves the 6 all our notations are in accordance with those presented in 29 0 20 40 60 80 100 time seconds 1 1 2 1 4 1 6 1 8 2 2 2 2 4 2 6 2 8 o b je ct iv e 107 lobcod sbdl 1 garcia et al 41 wohlberg 40 fig 2 run time comparison between our method and the batch methods the sbdl algorithm 1 the algorithm of wohlberg 40 and the algorithm by garcia et al 41 fastest convergence fig 4 shows the dictionaries obtained by our method and the batch methods in 1 and 41 note that the obtained dictionaries tend to look similar we also compared our method to the online stochastic gradient descent sgd based algorithms in 32 which operate in the spatial and in the fourier domains in this comparison we used 40 images randomly selected from the mirflickr 1 m dataset 43 for training the dictionaries as well as a test set of 5 different images from the same source the images were cropped to reduce their size from 512 512 to 256 256 pixels in both the training and testing sets to expedite the computation in addition we divided the images by 255 and mean subtracted them as was done in the fruit dataset in this experiment we used 0 1 and a learning rate of 0 1 with learning rate decay of 1 200 t every 5 epochs and momentum with 0 8 fig 3 presents the objective of the test set as a function of time showing that our algorithm converges faster fig 5 shows the dictionaries obtained by the three method illustrating similar quality b image inpainting we applied our algorithm to the task of image inpainting as described in section vi a and compared our results to 1 which was shown to provide the best performance in this task amongst all previous methods in section vi a we described two viable methods for training the dictionary the first approach is to utilize an external dataset while the second is to train directly on the corrupted source image itself for training the dictionary using an external dataset we used the fruit dataset 42 for both algorithms as shown in fig 4 all the corrupted test images were mean subtracted prior to applying both algorithms by computing the patch table ii inpainting comparison between the proposed stochastic lobcod and the sbdl 1 algorithms barbara boat house lena peppers c man couple finger hill man montage sbdl external 30 41 31 76 36 17 35 92 33 69 28 76 32 16 30 91 33 12 33 04 28 93 proposed external 30 93 31 82 36 58 36 15 33 54 28 88 32 46 31 75 33 25 33 18 29 18 image specific sbdl 31 98 32 04 36 19 36 01 34 03 28 85 32 18 30 96 33 21 32 99 28 95 image specific proposed 32 50 32 27 36 74 36 17 34 48 29 04 32 56 31 76 33 42 33 25 29 23 0 500 1000 1500 2000 2500 3000 3500 time seconds 350 355 360 365 370 375 380 385 390 395 400 o b je ct iv e o n t e st s e t lobcod liu et al sparse 32 liu et al freq 32 fig 3 run time comparison between our method and the online algorithms of liu at el 32 in the original and frequency domains a ours b sbdl 1 c 41 fig 4 comparison between the dictionaries obtained using the stochastic lobcod method vs the methods in 1 and 41 on the fruit dataset a ours b 32 sparse c 32 frequency fig 5 comparison between the dictionaries obtained using the stochastic lobcod method vs the online methods in 32 in the original and frequency domains all run on the mirflickr 1 m dataset average of each pixel using only the unmasked pixels and subtracting the resulting mean image from the original one in addition we tuned in equation 14 for every corrupted test image to account for their varying complexity the top two rows of table ii present the results using an external dataset in terms of peak signal to noise ratio psnr 7 on a set of 11 standard test images showing that our method leads to quantitatively better results next we train the dictionary of both algorithms on the corrupted image itself where in our method we use the update rule for the gradient as described in equation 16 the results are presented in the last two rows of table ii indicating that the proposed stochastic lobcod algorithm achieves quantitatively better results c multi focus image fusion we conclude by applying our lobcod algorithm to the task of multi focus image fusion as described in section vi b we evaluate our proposed method using synthetic data as 7 the psnr is computed as 20 log 255 n x x 2 where x and x are the original and the restored images note that in contrast to 1 here the images were only mean subtracted thus their original gray scale range is preserved a barbara in focus b barbara out of focus c 21 psnr 41 96 db d proposed psnr 42 27 db e butterfly in focus f butterfly out of focus g 21 psnr 35 30 db h proposed psnr 36 01 db fig 6 fusion performance comparison between our algorithm and the results of 21 on synthetic images the psnr values were computed between the reconstructed and the original images a near in focus b far in focus c 21 d proposed e near in focus f far in focus g 21 h proposed fig 7 fusion examples of real images taken with different focal settings the images were taken from the dataset in 38 well as data from a real dataset and compare our results to the one reported in 21 the dictionaries of both methods were pretrained on the fruit dataset 42 in addition we set the coefficients described in equation 20 to 1 and 5 for the sparse pursuit and the base image extraction stages respectively and alternate between the stages at each iteration in practice convergence was achieved within 2 4 iterations of alternating between these two stages for the synthetic experiment we extracted a portion of the standard image barbara and created two input images one with a blurred foreground and another with a blurred back ground image blurring was performed using a 9 9 gaussian blur kernel with 2 the size of the reconstruction kernel us presented in equation 25 was chosen to be 9 9 we repeated the same procedure on the image butterfly 8 using a 16 16 gaussian blur kernel with 4 and a reconstruction 8 the image was taken form the dataset in 44 a original b 21 c proposed d 21 e proposed fig 8 zoom in on the fusion results of the image butterfly figures d and e present the error images of b and c respectively the error images were computed between the fusion results and the original image a a bird foreground b bird background c 21 psnr 39 29 db d proposed psnr 39 81 db fig 9 fusion comparison between the proposed method and the method in 21 on the image bird kernel us of size 8 8 both sets of synthetic blurred images are presented in fig 6 alongside their reconstructed images the psnr values between the reconstructed images and the original ones are also detailed in fig 6 the resulting images demonstrate that our approach leads to visually and quantitatively better results fig 8 presents a zoom in view of our reconstructed image butterfly compared to the result of 21 and the original image showing that for images with prominent blur as in the case of the image butterfly our method achieves visually better results we adapt our approach for fusion of colored images we a 21 b proposed c 21 d proposed fig 10 zoom in on the fusion results of the image bird figure c and d present the l channel error compared to that of the original image blurred the image bird 9 by applying a 16 16 gaussian blur kernel with 4 on each channel of the rgb color space separately to create the foreground and the background blurred images we chose to blur the image in the rgb color space to emulate a blur of a camera afterwords both blurred colored images were treated by transforming them to the lab color space and building the activity maps ak based on their l channel with kernel us of size of 14 14 then we reconstructed each channel from the lab color space by selecting regions based on the maximum pixel wise value of the activity maps the psnr for the bird image was computed between the l channels of the original and the reconstructed images we present the results together with their psnr values in fig 9 which shows that our approach leads to visually and quantitatively better results lastly for the experiment with the real dataset we ran our proposed algorithm on two sets of images the clocks and the planes both taken from the dataset in 38 to fuse the clocks images we used a uniform kernel of size 9 9 whereas for the planes images we used a 16 16 uniform kernel fig 7 presents the resulting fused images showing comparable results for both algorithms on this dataset viii conclusions in this work we have introduced the local block coordinate descent lobcod algorithm for performing pursuit for the global csc model while operating locally on image patches we demonstrated its advantages over contending state of the art methods in terms of memory requirements efficient parallel computation and its exemption from meticulous manual tuning of parameters in addition we proposed a stochastic gradient descent version stochastic lobcod of this algorithm for training the convolutional filters we highlighted its unique qualities as an online algorithm that retains the ability to act on a single image finally we illustrated the advantages of the proposed algorithm on a set of applications and compared it with competing state of the art methods acknowledgment the research leading to these results has received funding in part from the european research council under eus 7 th 9 the image was taken form the dataset in https github com titu 1994 image super resolution tree master val images set 5 https github com titu 1994 image super resolution tree master val images set 5 https github com titu 1994 image super resolution tree master val images set 5 https github com titu 1994 image super resolution tree master val images set 5 framework program erc under grant 320649 and in part by israel science foundation isf grant no 335 18 references 1 v papyan y romano j sulam and m elad convolutional dictionary learning via local processing in iccv 2017 pp 5306 5314 2 m elad and m aharon image denoising via sparse and redundant representations over learned dictionaries ieee transactions on im age processing vol 15 no 12 pp 3736 3745 2006 3 w dong l zhang g shi and x wu image deblurring and super resolution by adaptive sparse domain selection and adaptive regularization ieee transactions on image processing vol 20 no 7 pp 1838 1857 2011 4 j mairal m elad and g sapiro sparse representation for color image restoration ieee transactions on image processing vol 17 no 1 pp 53 69 2008 5 m elad j l starck p querre and d l donoho simultaneous cartoon and texture image inpainting using morphological compo nent analysis mca applied and computational harmonic analysis vol 19 no 3 pp 340 358 2005 6 j yang j wright t s huang and y ma image super resolution via sparse representation ieee transactions on image processing vol 19 no 11 pp 2861 2873 2010 7 j wright a y yang a ganesh s s sastry and y ma robust face recognition via sparse representation ieee transactions on pattern analysis and machine intelligence vol 31 no 2 pp 210 227 2009 8 m elad sparse and redundant representations from theory to applications in signal and image processing springer 2010 9 m aharon m elad and a bruckstein k svd an algorithm for designing overcomplete dictionaries for sparse representation ieee transactions on signal processing vol 54 no 11 pp 4311 4322 2006 10 s chen s a billings and w luo orthogonal least squares methods and their application to non linear system identification international journal of control vol 50 no 5 pp 1873 1896 1989 11 s s chen d l donoho and m a saunders atomic decompo sition by basis pursuit siam review vol 43 no 1 pp 129 159 2001 12 k engan s o aase and j h husoy method of optimal directions for frame design in acoustics speech and signal processing 1999 proceedings 1999 ieee international conference on vol 5 ieee 1999 pp 2443 2446 13 r rubinstein m zibulevsky and m elad double sparsity learning sparse dictionaries for sparse signal approximation ieee transac tions on signal processing vol 58 no 3 pp 1553 1564 2010 14 j mairal f bach j ponce and g sapiro online dictionary learning for sparse coding in proceedings of the 26 th annual international conference on machine learning acm 2009 pp 689 696 15 j sulam b ophir m zibulevsky and m elad trainlets dictionary learning in high dimensions ieee transactions on signal processing vol 64 no 12 pp 3180 3193 2016 16 j sulam and m elad expected patch log likelihood with a sparse prior in international workshop on energy minimization methods in computer vision and pattern recognition springer 2015 pp 99 111 17 y romano and m elad patch disagreement as away to improve k svd denoising in acoustics speech and signal processing icassp 2015 ieee international conference on ieee 2015 pp 1280 1284 18 s gu w zuo q xie d meng x feng and l zhang con volutional sparse coding for image super resolution in proceedings of the ieee international conference on computer vision 2015 pp 1823 1831 19 f heide w heidrich and g wetzstein fast and flexible convo lutional sparse coding in proceedings of the ieee conference on computer vision and pattern recognition 2015 pp 5135 5143 20 h w liao and l su monaural source separation using ramanujan subspace dictionaries ieee signal processing letters vol 25 no 8 2018 21 y liu x chen r k ward and z j wang image fusion with convolutional sparse representation ieee signal processing letters vol 23 no 12 pp 1882 1886 2016 22 r grosse r raina h kwong and a y ng shift invariant sparse coding for audio classification in the twenty third conference on uncertainty in artificial intelligence uai 2007 pp 149 158 23 s boyd n parikh e chu b peleato j eckstein et al dis tributed optimization and statistical learning via the alternating di rection method of multipliers foundations and trends r in machine learning vol 3 no 1 pp 1 122 2011 24 b wohlberg efficient convolutional sparse coding in acoustics speech and signal processing icassp 2014 ieee international conference on ieee 2014 pp 7173 7177 25 h bristow a eriksson and s lucey fast convolutional sparse coding in proceedings of the ieee conference on computer vision and pattern recognition 2013 pp 391 398 26 v papyan j sulam and m elad working locally thinking glob ally theoretical guarantees for convolutional sparse coding ieee transactions on signal processing vol 65 no 21 pp 5687 5701 2017 27 t moreau l oudre and n vayatis dicod distributed con volutional coordinate descent for convolutional sparse coding in international conference on machine learning 2018 pp 3623 3631 28 b efron t ihastie i johnstone r tibshirani et al least angle regression the annals of statistics vol 32 no 2 pp 407 499 2004 29 s ruder an overview of gradient descent optimization algorithms arxiv preprint arxiv 1609 04747 2016 30 y wang q yao j t kwok and l m ni scalable online convolutional sparse coding ieee transactions on image processing 2018 31 j liu c garcia cardona b wohlberg and w yin online convo lutional dictionary learning in image processing icip 2017 ieee international conference on ieee 2017 pp 1707 1711 32 first and second order methods for online convolutional dic tionary learning siam journal on imaging sciences vol 11 no 2 pp 1589 1628 2018 33 j mairal f bach j ponce et al sparse modeling for image and vision processing foundations and trends r in computer graphics and vision vol 8 no 2 3 pp 85 283 2014 34 b yang and s li multifocus image fusion and restoration with sparse representation ieee transactions on instrumentation and measurement vol 59 no 4 pp 884 892 2010 35 pixel level image fusion with simultaneous orthogonal match ing pursuit information fusion vol 13 no 1 pp 10 19 2012 36 r gao and s a vorobyov multi focus image fusion via cou pled sparse representation and dictionary learning arxiv preprint arxiv 1705 10574 2017 37 w wang and f chang a multi focus image fusion method based on laplacian pyramid jcp vol 6 no 12 pp 2559 2566 2011 38 s savic multifocus image fusion based on empirical mode decom position in twentieth international electro technical and computer science conference 2011 39 h li b manjunath and s k mitra multisensor image fusion using the wavelet transform graphical models and image processing vol 57 no 3 pp 235 245 1995 40 b wohlberg boundary handling for convolutional sparse repre sentations in image processing icip 2016 ieee international conference on ieee 2016 pp 1833 1837 41 c garcia cardona and b wohlberg subproblem coupling in convo lutional dictionary learning in image processing icip 2017 ieee international conference on ieee 2017 pp 1697 1701 42 r fergus m d zeiler g w taylor and d krishnan deconvo lutional networks in 2010 ieee computer society conference on computer vision and pattern recognition cvpr vol 00 2010 pp 2528 2535 43 m j huiskes b thomee and m s lew new trends and ideas in visual concept detection the mir flickr retrieval evaluation initia tive in proceedings of the international conference on multimedia information retrieval acm 2010 pp 527 536 44 w dong l zhang g shi and x li nonlocally centralized sparse representation for image restoration ieee transactions on image processing vol 22 no 4 pp 1620 1630 2013 45 t p minka old and new matrix algebra useful for statistics see www stat cmu edu minka papers matrix html 2000 appendix a transitioning from high dimensional problem 9 to a low dimensional problem 10 denote pi as the patch that fully contains the slice si dl i and define a patch layer li as the set of non overlapping patches taken from the image that contains the patch pi we can write the identity matrix as a sum of non overlapping patch extraction matrices k li ptkpk i where pi is one of these matrices by using these definitions and writing the definition of ri the residual image without the contribution of the i th needle we can write the l 2 fidelity term of equation 9 as arg min i 1 2 k li ptkpkri p t i dl i 2 2 arg min i 1 2 k li ptkpk x n j 1 j 6 i ptj dl j p t i dl i 2 2 arg min i 1 2 k 6 i k li ptkpk x n j 1 j 6 i ptj dl j pti pi x n j 1 j 6 i ptj dl j dl i 2 2 since the patch extraction matrix pti is orthogonal to all the matrices ptk for k 6 i the above is equal to arg min i 1 2 k l i k 6 i ptkpk x n j 1 j 6 i ptj dl j 2 2 pti pi x n j 1 j 6 i ptj dl j dl i 2 2 note that the first term of the above objective does not depend on i and thus we can ignore this term in our minimization of the objective arg min i 1 2 pti pi x n j 1 j 6 i ptj dl j dl i 2 2 in addition the matrix pti translates a patch size vector to the i th position in the global vector padded with zeros hence we can ignore all the zero entires in the resulting vector and the problem becomes equivalent to solving the following reduced minimization problem arg min i 1 2 pi x n j 1 j 6 i ptj dl j dl i 2 2 arg min i 1 2 piri dl i 22 where ri x n j 1 j 6 i ptj dl j b the gradient calculation w r t the local dictionary of the minimization problem 12 we can rewrite dl as a column vector dl and write equation 12 as we use to denote the kronecker matrix product 45 arg min dl 1 2 x n i 1 pti i in n dl 2 2 and by defining ai i in n the above problem can be rewritten as arg min dl 1 2 x n i 1 pti aidl 2 2 now it easy to see that the gradient of the above problem w r t dl is given by dl n i 1 pti ai t x n i 1 pti aidl n i 1 ati pi x n i 1 pti aidl n i 1 ati pi x x note that pi x x is the i th patch of the residual image and by substituting back the definition of ai and using the same property as before see 45 property no 40 we can write the gradient as dl n i 1 i in n tpi x x n i 1 vec pi x x ti where vec denotes the vec operator that stacks the columns of the gradient matrix into a vector so by reshaping the above expression we get the final expression for the gradient dl n i 1 pi x x ti