improving rna secondary structure prediction via state inference with deep recurrent neural networks devin willmott david murrugarra qiang ye feb 2020 department of mathematics university of kentucky lexington ky 40506 0027 usa abstract the problem of determining which nucleotides of an rna sequence are paired or unpaired in the secondary structure of an rna which we call rna state inference can be studied by different machine learning techniques successful state inference of rna sequences can be used to generate auxiliary information for data directed rna secondary structure prediction typical tools for state inference such as hidden markov models exhibit poor performance in rna state inference owing in part to their inability to recognize nonlocal dependencies bidirectional long short term memory lstm neural networks have emerged as a powerful tool that can model global nonlinear sequence dependencies and have achieved state of the art performances on many different classification problems this paper presents a practical approach to rna secondary structure inference centered around a deep learning method for state inference state predictions from a deep bidirectional lstm are used to generate synthetic shape data that can be incorporated into rna sec ondary structure prediction via the nearest neighbor thermodynamic model nntm this method produces predicted secondary structures for a diverse test set of 16 s ribosomal rna that are on average 25 percentage points more accurate than undirected mfe structures accuracy is highly dependent on the success of our state inference method and investigating the global features of our state predictions reveals that accuracy of both our state inference and structure inference methods are highly dependent on the similarity of pairing patterns of the sequence to the training dataset availability of a large training dataset is critical to the success of this approach code available at https github com dwillmott rna state inf 1 introduction the secondary structure of an rna sequence plays an important role in determining its func tion 10 24 16 but directly observing rna secondary structure is costly and difficult 3 9 therefore researchers have used computational tools to predict the secondary structure of rnas one of the most popular methods is the nearest neighbor thermodynamic model nntm 35 alternatively comparative sequence analysis methods 13 use a set of homologous sequences to infer a secondary structure 2 this method remains the gold standard for secondary structure prediction 30 nntm is based on thermodynamic optimization to find the secondary structure with the min imum free energy mfe there are several implementations of nntm some of the popular ones include rnastructure 25 gtfold 31 unafold 23 and viennarna package 22 however nntm has been shown to be ill conditioned 17 18 26 that is for a given sequence signifi cantly different secondary structures might exhibit very similar energies additionally the range of accuracies of the predictions of nntm shows significant variance 31 more recently high throughput data that correlates with the state of a nucleotide being paired or unpaired has been developed this data called shape 37 for selective 2 hydroxyl acylation analyzed by primer extension has been incorporated as auxiliary information into the objective function of nntm with the goal of improving the accuracy of the predictions this type of prediction is referred to as shape directed rna secondary structure modeling 5 36 the addition of auxiliary information usually improves the accuracy of the predictions of nntm 5 but it has been shown that the improvements are correlated with the mfe accuracy 30 the latter 1 ar x iv 1 90 6 10 81 9 v 2 q bi o b m 2 3 f eb 2 02 0 https github com dwillmott rna state inf result has been obtained by statistical modeling of shape the model in 30 gives distributions for the values of shape if the state of the nucleotide as paired or unpaired or helix end is known thus the model in 30 can be used to generate shape data for an rna sequence in silico given the state of each of the sequence s nucleotides in this paper we present a method for improving the rna secondary structure prediction based on state inference results to do so we first study the problem of determining the state of each nucleotide of an rna sequence which we refer to as state inference state inference is a binary classification task on each nucleotide which we note is in contrast to full secondary structure inference which seeks to identify sets of base pairs we have developed trained and tested a deep recurrent neural network that performs this task given an rna sequence the machine outputs a probability that each nucleotide is paired we can threshold this probability to obtain binary predictions for the state of each nucleotide additionally we use the probabilities from the state inference method to generate synthetic shape then we use this shape data for directed predictions via nntm leading to significant improvements in secondary structure accuracy on sequences where the state inference performed well we note that our approach for generating shape is different from the statistical models in 30 which generate synthetic shape data by sampling from the distribution models we note that other deep learning methods for the problem of rna secondary structure inference have been explored 38 although we are primarily interested in using state inference to direct secondary structure predictions there exist other motivations for state inference for example such a method could be used to identify binding sites in rna rna interactions 32 6 figure 1 bidirectional rnn diagram of a bidirectional rnn with two hidden layers for state inference at three different timesteps here x denotes the input h 1 the first layer hidden variable h 2 the second layer hidden variable and y the output with superscripts representing timesteps and arrows showing the propagation of information through the network each hidden layer combines information from the previous layer earlier and later timesteps and an internal memory state to compute its output 2 2 methods 2 1 shape directed nntm under the nearest neighbor thermodynamic model nntm stacks and loops are each assigned a free energy based on base pair type watson crick or wobble with pairs contributing negative energy and loops contributing positive energy the energy of a secondary structure is the sum of the energies of these local features dynamic programming methods can be used to efficiently find the secondary structure with the smallest energy called the minimum free energy mfe structure which is usually taken as the predicted structure by the nntm method the shape data comes from high throughput chemical probing experiments and associate a reactivity value to each nucleotide of an rna sequence it has been found experimentally that high shape values are correlated with unpaired positions and small values with paired position shape has been incorporated into nntm by adding a pseudo free energy term to the model 5 this term is generated by the following relation gshape i 2 6 ln shape i 1 0 8 1 this energy is added to base pair stacks involving nucleotide i in effect the term gshape i penalizes base pairs involving nucleotides with high shape values and encourages base pairs among nucleotides with low shape values incorporating shape data consistently leads to significant increases in the accuracy of the mfe structure 5 36 30 2 2 directing nntm with state information we are interested in using deep learning tools in tandem with shape direction to improve sec ondary structure inference however deep learning methods require extraordinary large datasets the relative dearth of available experimental shape data prohibits us from directly learning syn thetic shape data with a neural network instead our proposed method for secondary structure inference uses a viable method for state inference with the ability to influence the nntm energy function on a per nucleotide basis via shape direction note that the same limitation applies to the secondary structure data for which a large dataset is available for 16 s sequences only but not for other rna types see section 4 4 for more details our method is a three step process 1 a machine learning method for predicting the state of each nucleotide in a sequence 2 a function converting these state predictions into artificial shape data 3 the shape directed nntm function that takes both the original rna sequence and the generated shape data and outputs a predicted secondary structure note that step 3 is a well established method and the novelty here is in deriving the artificial shape data in steps 1 and 2 for the task of state inference we trained a deep neural network using a set of known rna sequences and structures that generates a sequence of state predictions detailed in section 2 3 the output of this neural network is a sequence p of the same length as the original rna sequence where p i is the predicted probability that the nucleotide in position i is paired with these predictions in hand we convert each predicted probability p i to a shape value to be associated with nucleotide i to construct a function for this purpose we note that a shape value of 0 3603 will not contribute any positive or negative energy to the nntm energy function this can be seen by setting gshape i to 0 in equation 1 and solving for shape i 8 we would therefore like to assign predictions of 0 5 to a shape value of 0 3603 as these predictions give no information as to the state of the nucleotide with this in mind we use the following piecewise linear function to generate shape where a and b are constants to be specified f i 2 0 3603 b p i b if 0 p i 0 5 2 a 0 3603 p i 1 a if 0 5 p i 1 2 this function has range a b with f i a if p i 1 f i b if p i 0 and f i 0 3603 if p i 0 5 to determine values of a and b we considered experimentally collected shape 3 data from two e coli sequences one 16 s sequence and one 23 s sequence 30 together these two sequences contain a a total of 4187 nucleotides and represent a wide variety of structural motifs we took the mean shape value among both paired nucleotides and unpaired nucleotides these values are 0 214 and 0 6624 respectively all of our experimental results in section 3 will use a 0 214 and b 0 6624 in our shape generation function these choices are motivated by real shape values and thus are sensible estimations of the best values however they may not be the optimal values for our purposes in section 4 2 we explore how varying these values may affect the accuracy of our predicted secondary structures these experiments indicate that the a and b values used in our experimental results are nearly optimal with set values of a and b we can generate a sequence of artificial shape data we then use shape directed nntm as described in the previous section to obtain our secondary structure prediction 2 3 state inference with deep neural networks neural networks are tools from the realm of machine learning for solving classification and re gression problems in a neural network model parameters are trained using a dataset of known input output pairs we define a loss function based on the difference between machine predictions and target outputs retrieve gradient directions for parameters with respect to this loss using the backpropagation algorithm 27 and optimize parameters using iterative first order methods such as gradient descent recent advances in machine learning come primarily from deep neural networks 11 which are stacks of multiple neural networks the output of one neural network in the stack acts as the input for the next each of these constituent neural networks is referred to as a layer of the deep neural network these multiple layers allow the deep neural network to learn and represent complex nonlinear relationships among inputs for the task of state inference we use a deep recurrent neural network deep rnn rnns work specifically with sequential data by combining the learning methods of neural networks with the architecture of a discrete time dynamical system a single layer rnn has a state h i that is a function of the state at previous time step h i 1 and the input x i at time i the function is a composition of an elementwise nonlinear activation function with a linear map sequence elements are fed to the machine as inputs one at a time at time i the machine receives the nucleotide in position i given as a one hot encoding x i from this input and the previous state h i 1 machine parameters generate a state h i that encodes the pairing information up to step i from which another function produces the output y i the machine prediction of the probability that nucleotide i is paired we make a number of modifications to our deep rnn to increase state inference accuracy most notably we use a popular variant of rnn called the long short term memory lstm architecture 14 which incorporates a gating mechanism and a memory cell to increase accuracy beyond that of traditional rnns on a variety of sequential learning tasks specifically the lstm gating architecture allows a certain component of the state to directly pass into future steps maintaining the flow of state information or memory over the long term this significantly increases machine capability to model long term dependence and hence capture potential long range base pairs 21 we also make this network bidirectional 28 12 this is a minor modification that allows information to flow both forward and backward through the sequence a two layer bidirectional rnn is shown in figure 1 2 4 dataset implementation and metrics our experiments will focus on a test set of sixteen 16 s ribosomal rna sequences used in shape direction experiments in 30 sequences in this set have a wide range of nntm accuracies our deep neural network requires a large dataset of rna sequences with known states from which to learn for this task we used secondary structure data from the comparative rna web site run by the gutell lab at the university of texas 1 this site hosts a collection of known rna sequences and secondary structures obtained using comparative sequence analysis compiling all of the available 16 s rrna results in a set of 17032 sequences and a total of over 21 million nucleotides we refer to this as the crw dataset 4 to ensure that our model does not simply memorize large portions of sequences in the test set we compared each crw dataset sequence with each test set sequence and removed crw sequences with significant similarities prior to training in this filtering process if the two sequences have a common block of nucleotides of more than 10 of the length of the test sequence or if the two sequences can be aligned such that they have common nucleotides accounting for more than 80 of nucleotides of the shorter sequence we remove it from the training set see available code for additional details this process leaves us with 13118 sequences and a total of approximately 16 5 million nucleotides with a mean and median sequence length of 1264 and 1431 respectively we then split this set into two random halves to produce a training and validation set for the sake of comparison we also trained and tested a number of higher order hidden markov models hmm using the same training validation and test sets used by the neural network in a k order hmm hidden state transitions depend on the previous k states in the sequences rather than only the previous state this improves their representational capacity but at the cost of a model size that increases exponentially with k we trained our hmms with maximum likelihood estimation over the training set and performed inference with the viterbi algorithm 7 a back tracking algorithm that exploits the markovian nature of hmms to efficiently produce the likeliest state sequence under the model s transition probabilities hmms are fundamentally incapable of recognizing dependencies across many timesteps and we therefore expect the deep neural net work to outperform the hmm however they provide a baseline against which to measure neural network output we note that the hmm formalism has been used before for state inference of rna but for a different purpose 29 20 15 code of the hmm implementation is available at https github com dwillmott rna state inf we implemented a variety of deep recurrent networks in keras 4 a python deep learning api with theano 33 as a backend code is available at https github com dwillmott rna state inf we found a four layer network to be the optimal balance of representational capacity and training speed the largest layers are the middle two which are both bidirectional lstms the first and last layers are small one dimensional convolutional layers 19 11 these layers act as learnable pre and post processing convolutions they take in and process local information in small regions of the sequence and allow the two recurrent layers to focus on long range dependencies across many timesteps the output dimension of each of the machine s four layers at each timestep are 100 400 100 and 2 respectively giving a machine with a total of 595 552 trainable parameters which we trained using binary cross entropy loss and rmsprop 34 a gradient descent like training algorithm see available code material for hyperparameters and training modifications our code works quite efficiently for the kind of sequences tested in this paper on a 2080 geforce rtx 2080 ti gpu training takes about 4 hours and once trained state inference takes about 0 005 seconds for a 5 s sequence 150 nucleotides 0 05 s for a 16 s sequence 1500 nucleotides and 0 1 s for a 23 s sequence 3000 nucleotides which are roughly linear with the size of the sequence with the sequence of state prediction probabilities p we generated artificial shape data using equation 2 finally we used both the original sequence and the generated shape data as input for gtfold 31 an efficient nntm implementation to compute a shape directed mfe structure gtfold is used in this work for all nntm experiments and results when evaluating predicted secondary structures we compare its set of base pairs with those of the native structure a predicted base pair is counted as true positive tp if it exists in both the predicted and native structure a false positive fp if it appears in the predicted structure but not in the native structure and a false negative fn if it appears in the native structure and not in the predicted structure we report on three measures of performance ppv the proportion of true positives in the predicted structure tptp fp sensitivity the fraction of true positives in the native structure tptp fn and accuracy the arithmetic mean of ppv and sensitivity 1 2 tptp fp tp tp fn 10 in later sections we will also consider the performance of our deep learning methods for state inference unlike secondary structure inference which classifies base pairs state inference is a binary classification of each nucleotide when discussing the accuracy of state predictions we will define accuracy to be the proportion of true predictions among all predictions in the sequence this is distinct from notions of secondary structure inference accuracy and so the metrics on state inference and structure inference cannot be directly compared 5 https github com dwillmott rna state inf https github com dwillmott rna state inf table 1 structure inference results on the test set sequence name undirected directed mfe mfe predicted s k sd native e cuniculi 0 171 0 183 0 273 0 336 v necatrix 0 181 0 314 0 503 0 705 c elegans 0 203 0 248 0 308 0 519 e nidulans 0 272 0 325 0 601 0 832 n tabacum 0 323 0 692 0 593 0 859 cryptomonas sp 0 339 0 838 0 739 0 898 synechococcus sp 0 361 0 848 0 697 0 885 m musculus 0 375 0 397 0 509 0 782 m gallisepticum 0 385 0 849 0 721 0 889 e coli 0 411 0 852 0 744 0 880 b subtilis 0 512 0 848 0 753 0 881 d desulfuricans 0 533 0 875 0 724 0 898 c reinhardtii 0 537 0 845 0 702 0 868 t maritima 0 562 0 881 0 733 0 896 t tenax 0 619 0 766 0 754 0 861 h volcanii 0 752 0 864 0 809 0 907 mean 0 408 0 664 0 635 0 806 median 0 380 0 841 0 712 0 874 table of accuracy of mfe structures using nntm with a variety of shape directions first column undirected mfe second column predicted state directed mfe see section 3 2 third column mean performance of sampled shape directed nntm in 30 fourth column native state directed nntm see section 3 1 3 results 3 1 native state directed nntm before analyzing the results of the entire pipeline of our method we first examined our shape generation function in detail to do so we used the native state of each sequence in our test set to generate shape this was done by setting p i to 1 if the nucleotide in position i is paired and p i to 0 if it is unpaired we then use equation 2 to generate artificial shape this will result in a generated shape value of 0 6624 for all paired nucleotides and 0 214 for unpaired nucleotides which we then use to direct nntm we refer to the resulting predicted structures as native state directed mfe this experiment is similar to those run in 30 and uses the same set of data to choose appro priate shape values the difference is in the method of shape generation whereas that paper constructs shape distributions from the data and stochastically samples from these distributions we use the mean of paired and unpaired nucleotides shape values the results of this experiment reinforce many of the findings in 30 a comparison of accuracy of all three methods undirected mfe stochastically directed mfe from 30 and native state directed mfe is available in table 1 overall native state directed mfe structures are highly accurate with twelve of the sixteen test sequences enjoying accuracy above 80 both direction methods are an improvement on the accuracy of the undirected mfe structure for every test set sequence and native state directed accuracy represents a further improvement from the stochastic model in 30 in the case of native state direction accuracy improvements over undirected mfe range between 15 percentage points h volcanii and 57 percentage points cryptonomas sp consistent with observations in 30 greatest increases are concentrated in sequences with middling undirected mfe accuracy for sequences with undirected accuracy between 25 and 45 native state directed mfe accuracy is an improvement by more than 40 percentage points this experiment is equivalent to assuming that our deep learning state inference method has perfect performance and as such we can interpret the accuracy of native state directed mfe structures to be an upper bound on the performance of our method on average the high accuracy exhibited in this experiment gives strong evidence that there are large potential gains in mfe 6 accuracy to be made with our method however several sequences with low undirected mfe accuracy sequences like e cuniculi and c elegans are known to be particularly resistant to shape direction 30 and this is reflected in relatively poor native state directed mfe accuracy we thus cannot expect our method to exhibit large improvements over undirected mfe structures in these cases 3 2 predicted state directed nntm we now use the predictions from our deep neural network to generate shape data that will in turn direct nntm we refer to these predictions as predicted state directed mfe structures we emphasize that unlike the native state direction explored in the previous section this method does not assume prior knowledge of the state of the sequence and thus represents a practical method of secondary structure inference the results of applying our method to the sequences in the test set are available in table 1 which indicates that the extraordinary gains from native state directed nntm are not always preserved in practice predicted state directed structures fall into two clear categories five are quite inaccurate with accuracy below 40 while among remaining eleven structures are all near or above 70 and nine of these are above 80 even with the high variance of accuracies among these structures predicted state directed mfe structures are 25 percentage points more accurate than undirected mfe structures on average and every sequence in the test set experiences some increase in accuracy however these improvements vary greatly with several sequence staying within 5 of undirected mfe accuracy while for four other sequences we improve by more than 40 with the highest improvement cryptonomas sp at 50 to some extent poor accuracy is explained by our experiment with native state directed nntm indeed the five sequences with poor accuracy from our method are the five worst performing with native state directed using native states to direct nntm and only one of these exceeds 80 with native states at worst native state directed nntm gives only 34 accuracy for e cuniculi and 52 6 for c elegans and this ceiling is much lower than accuracy achieved in many of our other predicted structures thus the difficulties of our method with these sequences may be attributed to the rna sequences themselves that are particularly challenging for structure inference as discussed in the directability of nntm section in 30 p 2812 however in these cases and some others e nidulans m musculus predicted state directed mfe accuracy does not come close to native state directed mfe accuracy this is in contrast to our highest performing sequences d desulfuricans t maritima where predicted state directed mfe is within several percentage points of native state directed mfe accuracy 4 discussion 4 1 state inference accuracy the foundation of our method is our deep neural network for state inference this network provides probabilities that are converted into a pseudo free energy term in the nntm energy function to table 2 state inference accuracy of neural network vs hmm on validation and test sets validation set test set machine acc ppv sen acc ppv sen order 1 hmm 0 623 0 632 0 852 0 612 0 646 0 767 order 2 hmm 0 662 0 671 0 826 0 651 0 686 0 759 order 3 hmm 0 674 0 693 0 794 0 672 0 713 0 750 order 4 hmm 0 685 0 714 0 771 0 684 0 729 0 742 order 5 hmm 0 684 0 711 0 776 0 683 0 730 0 742 neural network 0 954 0 950 0 972 0 839 0 858 0 873 a comparison of accuracy ppv and sensitivity of output from our lstm based neural network for state inference compared with those of hmms of various orders the composition of validation and test sets is described in section 2 4 7 table 3 state inference results on the test set from lstm vs hmm accuracy ppv sensitivity sequence name lstm hmm lstm hmm lstm hmm e cuniculi 0 680 0 661 0 713 0 693 0 774 0 773 vairimorpha necatrix 0 661 0 600 0 721 0 689 0 683 0 576 c elegans 0 558 0 584 0 570 0 613 0 624 0 552 emericella nidulans 0 657 0 584 0 692 0 681 0 741 0 539 nicotiana tabacum 0 913 0 705 0 917 0 734 0 938 0 787 cryptomonas sp 0 926 0 676 0 935 0 730 0 941 0 728 synechococcus sp 0 938 0 700 0 943 0 740 0 953 0 769 m musculus 0 608 0 603 0 626 0 655 0 637 0 520 mycoplasma gallisepticum 0 919 0 639 0 933 0 713 0 932 0 668 e coli 0 924 0 699 0 937 0 742 0 938 0 774 bacillus subtilis 0 973 0 698 0 979 0 731 0 976 0 788 desulfovibrio desulfuricans 0 926 0 712 0 940 0 741 0 938 0 803 chlamydomonas reinhardtii 0 906 0 687 0 915 0 725 0 928 0 761 thermotoga maritima 0 931 0 752 0 944 0 760 0 943 0 864 thermoproteus tenax 0 818 0 782 0 845 0 785 0 866 0 894 h volcanii 0 782 0 739 0 809 0 769 0 841 0 820 average 0 820 0 676 0 839 0 719 0 853 0 726 total 0 839 0 684 0 858 0 729 0 873 0 742 table of accuracy ppv and sensitivity for our lstm based state inference model vs an order 4 hmm sequences are arranged in ascending order of mfe accuracy as an indication of the difficulty of secondary structure inference for each sequence average indicates the average metric for each sequence while total gives the total metrics for all nucleotides in the test set understand the sources of high and low performance of our structure inference method we can directly evaluate the output of our deep neural network for state inference using higher order hmms as a baseline to calculate the accuracy of the neural network s output we thresholded each prediction p i above and below 0 5 taking p i 0 5 to be a positive prediction and p i 0 5 to be a negative prediction the accuracy ppv and sensitivity of both neural network and hmm predictions are shown in table 2 though the table exhibits an upward trend in accuracy as the order of the hmm increases we found that accuracy plateaued and eventually decreased beyond order 5 as expected the lstm clearly outperforms hmms of all orders on the validation set more importantly this is the case for our test set as well where the lstm outperforms the best hmm in accuracy by nearly 13 the order 4 hmm exhibits the highest accuracy on the validation set we further compared state inference accuracy on each test set sequence using both the order 4 hmm and the neural network the accuracy ppv and sensitivity of these predictions are shown in table 3 the accu racy of neural network state predictions were on average 15 percentage points higher than that of the hmm and was higher for every sequence but one c elegans table 3 orders sequences in ascending order of undirected mfe accuracy however this ordering reveals no straightfor ward relationships among neural network state inference accuracy hmm state inference accuracy and mfe structure accuracy neural network accuracy varies much more among sequences the difference between the sequences with lowest and highest accuracy c elegans and b subtilis respectively is more than 40 percentage points sequences can be grouped according to accuracy poor below 70 for five sequences medium near 80 for two more and high above 90 for the remaining nine as expected state inference accuracy exhibits a strong effect on predicted state directed mfe accuracy state inference accuracy above 90 means that our predicted states are quite close to native states consequently predicted state direction and native state direction equivalent to 100 accuracy should produce similar predicted structures in these cases as evidenced by their difference of only a few percentage points in table 1 meanwhile the five sequences with poor state inference accuracy are exactly those where predicted state directed mfe accuracy is below 40 8 figure 2 test set kl divergence plot comparing each test set sequence s lstm neural network and hmm state inference accuracy vs its kullback leibler divergence from training set paired region distribution kl divergence was calculated as kl p q where p is the test sequence distribution and q is the training set distribution the effect of state inference accuracy is particularly evident when considering the improvement over undirected mfe accuracy for four of the five sequences with poor accuracy all but v necatrix predicted state directed mfe accuracy is within 6 percentage points of undirected mfe accuracy for v necatrix and both sequences with medium state inference accuracy predicted state direction improves structure accuracy by 10 15 percentage points the remaining nine sequences all have high state inference accuracy and their directed structures are 30 percentage points more accurate than undirected mfe we note an interesting relationship between native state directed mfe accuracy and our neural network s state inference accuracy the five sequences with state inference accuracy below 70 are the five worst performing sequence when predicting structure with native state directed nntm this suggests that there may be fundamental difficulties in understanding pairing structures of these sequences 4 2 paired regions global structure our metrics in table 3 give us an idea of the proportion of correct machine predictions on individual nucleotides states but they do not indicate whether predictions produce state sequences that preserve global properties such as patterns of paired and unpaired states in particular we want the number and sizes of paired and unpaired regions of the state sequence prediction to match those in the original a paired region in the state roughly corresponds to one half of a helix in the secondary structure so we theorize that recognizing this information is vital for producing state predictions that successfully aid structure inference we considered the distribution of sizes of paired regions in each test set state sequence and compared them to the distributions of neural network and hmm state predictions despite larger variance in state inference accuracy we found evidence that the neural network was on average much more capable than the hmm of capturing this global structure the median size of paired 9 figure 3 shape direction heat maps accuracy of native state directed mfe left plot and predicted state directed mfe right plot for various ranges a b of output from our shape generation function in each the lower right corner corresponds to a b 0 3603 which is equivalent to no shape direction region in neural network predicted state differed from the median in the native state by at most one for every test set sequence while the hmm s median paired region size was routinely several nucleotides larger the neural network also performs better in predicting the total number of paired regions in the state producing predictions that on average had 6 more paired regions than the native state while hmm predictions had an average of 57 fewer regions we note that this discrepancy is to be expected in the context of nonlocal interactions paired region size is exactly the sort of nonlocal feature that hmms cannot predict at a given time the hmm does not know how long it has been outputting positive predictions and is thus limited in its capacity to detect large paired regions considering the non locality of paired regions can help to explain the poor performance of the neural network on certain test set sequences high neural network accuracy is nearly always accompanied by a particular type of distribution of large paired regions one of length 17 one of length 13 and several more of length 12 and 11 in contrast this pattern does not hold for those with low or medium state inference accuracy of the remaining seven all have either paired regions of length larger than 20 or very few paired regions of length larger than 10 we can compare the distribution of the lengths of paired regions in each of our test sequences to the distribution in the training set we find that the training set overwhelmingly contains sequences with paired region distributions similar to the test set sequences on which the neural network performs well in particular we note that the training set has relatively few large paired regions in the entire training set there are 5 regions of length 18 2 regions of size 19 4 regions 10 of size 20 and none larger than 20 thus during training the machine is penalized for outputting more than 20 contiguous positive predictions consequently neural network predictions do not create sufficiently large regions for many test set sequences to quantify this difference we considered the kullback leibler kl divergence of the distri bution of the paired region lengths between the entire training set and the distribution for each test set sequence the kl divergence measures the similarity of each test set sequence s paired region distribution as compared with the distribution of the entire training set figure 2 plots state inference accuracy for each machine and test set sequence against its kl divergence two clusters of sequences emerge in this plot one with kl divergence near 0 01 and another with kl divergence near 0 5 all nine sequences with high state inference accuracy are in the former cluster while the seven low and medium accuracy sequences are in the latter the disparity in neural network accuracy and hmm accuracy on sequences with more similarity to the training set suggests that the increase in neural network performance comes from its ability to recognize global structure in these sequences on the other hand neural network accuracy is only a modest improvement from hmm accuracy in the low to medium accuracy cluster where global structure diverges significantly from that of the training set 4 3 modifying synthetic shape values our method uses shape directed nntm as a means of assigning pseudo free energies to indi vidual nucleotides all of our results in section 3 assign nucleotides a shape value in the range 0 214 0 6624 these endpoints are based on the mean shape value of paired and unpaired nu cleotides from 16 s and 23 s e coli sequences however our method converts to shape primarily as a means of assigning pseudo free energies to individual nucleotides with nntm and not as a genuine attempt to generate plausible shape data thus the endpoints used may not be optimal for our purposes of converting from state inference predictions to evaluate potential output ranges for our shape generation function we reproduced experi ments with native state directed nntm section 3 1 and predicted state directed nntm section 3 2 while varying the endpoints a and b of our shape generation function given in equation 2 as noted previously a shape value of 0 3603 contributes no energy to the model thus it is only sensible to choose paired shape values below 0 3603 and unpaired shape values above 0 3603 nntm software such as gtfold ignores negative shape values so paired nucleotides generated shape must lie between 0 and 0 3603 the results of this experiment are shown in figure 3 in native state directed nntm increasing negative state shape above 0 3603 and and de creasing positive state shape below 0 3603 consistently increased performance this is consistent with our expectations as in this case we are increasing the energy of all base pairs involving nucleotides that remain unpaired in the native structure experiments with very large unpaired shape values such as b 20 were similar to the largest values shown in figure 3 indicating that there is a ceiling of approximately 90 test set accuracy for any method centered around shape directed nntm such as ours the plot for predicted state directed nntm shows a different picture with increasing unpaired shape values eventually leading to decreasing structure inference accuracy that this pattern ap pears in the predicted state experiments but not native state experiments suggests that incorrectly assigning large shape values to even a small number of natively paired nucleotides can be signifi cantly harmful to nntm performance there is a large region of highest accuracy with a between 0 and 0 22 and b between 0 7 and 1 5 giving accuracies near 68 the values of a 0 214 and b 0 6624 used in our results are near the boundary of this region but we note that even optimal values of a and b give an accuracy of 69 only 2 5 percentage points above the experimentally motivated choices of a and b used in our results 4 4 other rna types the dependence on large amounts of data inhibited our ability to extend this work to inference on other types of rna such as 5 s and 23 s ribosomal rna as we were unable to amass enough secondary structure data in these other contexts to successfully train a neural network instead we explored applying our trained network to other rna sequences but found that a neural network trained on 16 s rna sequences produces much weaker results on 5 s and 23 s rrna sequences than on 16 s sequences see tables 4 5 we suspect this is primarily due to differences in sequence length tables 4 5 also compare neural network state predictions with those from an hmm and shows 11 that the hmm is more capable of generalizing to these rna families however we note that in all cases hmm performance is poor relative to neural network performance on 16 s sequences this is consistent with our hypothesis that our neural network is recognizing long range dependencies specific to the family of 16 s sequences that hmms are inherently unable to capture table 4 state inference results on 5 s sequences accuracy ppv sensitivity sequence name lstm hmm lstm hmm lstm hmm d 5 e s pombe 1 no 0 647 0 605 0 735 0 685 0 676 0 676 d 5 e p waltl no 0 500 0 717 0 635 0 756 0 446 0 797 d 5 e o sativa 1 no 0 487 0 756 0 589 0 808 0 581 0 797 d 5 e m glyptostroboides no 0 467 0 700 0 586 0 757 0 460 0 757 d 5 e m fossilis no 0 442 0 675 0 571 0 733 0 378 0 743 d 5 b m luteus 0 516 0 650 0 700 0 709 0 449 0 782 d 5 b m genitalium 0 466 0 636 0 622 0 754 0 319 0 597 d 5 e l edodes 0 583 0 658 0 682 0 709 0 608 0 757 average 0 514 0 675 0 640 0 739 0 490 0 738 table of accuracy ppv and sensitivity for our lstm based state inference model and an order 4 hmm for 5 s sequences average indicates the average metric for each sequence table 5 state inference results on 23 s sequences accuracy ppv sensitivity sequence name lstm hmm lstm hmm lstm hmm d 233 a h marismortui 1 0 640 0 682 0 647 0 709 0 840 0 771 d 233 a t celer 0 666 0 745 0 666 0 729 0 854 0 892 d 233 m s sinuspaulianus 0 541 0 632 0 407 0 478 0 678 0 543 pdb 00784 0 652 0 669 0 714 0 764 0 803 0 732 pdb 00616 0 602 0 655 0 595 0 655 0 829 0 767 pdb 00953 0 595 0 663 0 567 0 622 0 868 0 856 pdb 00503 0 612 0 658 0 610 0 668 0 830 0 760 pdb 00846 0 642 0 664 0 709 0 763 0 793 0 725 pdb 00776 0 552 0 602 0 487 0 524 0 860 0 770 average 0 611 0 663 0 600 0 657 0 817 0 757 table of accuracy ppv and sensitivity for our lstm based state inference model and an order 4 hmm for 23 s sequences average indicates the average metric for each sequence 5 conclusion we introduced a method for secondary structure inference by connecting a deep learning method for state inference with previous work that leverages shape directed nntm for structure infer ence we tested this method on a set of 16 s rrna sequences with a wide range of mfe accuracies and found large improvements over undirected mfe structures in most cases these gains were not uniform throughout the test set as several sequences with low undirected mfe accuracy ex perienced very little increase from predicted state direction however median increase in accuracy was more than 30 percentage points and in the best case our method improved undirected nntm accuracy by nearly 50 percentage points experiments using sequences native states to predict structure showed that a data directed nntm approach has the potential to improve mfe accuracy throughout the test set however it also uncovered significant limitations regardless of the accuracy of the state inference method used to supply predictions directed nntm will not be able to produce high accuracy mfe structures for some sequences these findings reinforce results from 30 regarding the varying directability of sequences in the test set 12 the performance of our state inference method was highly variable among test set sequences with several clusters of high and low accuracy we found that high performance of our state inference method and in turn the accuracy of secondary structures generated from these state predictions was strongly linked to the similarity between a sequence s paired region distribution and that of the training set this finding highlights the connection between performance of our method and available sec ondary structure data as with any application of machine learning the state inference method presented here is only as good as the dataset used to train the model the task of 16 s rrna state inference explored here is feasible primarily due to the particularly large number of known 16 s rrna secondary structures we are hopeful that future increases in available data are able to increase the efficacy of this method both on 16 s rrna sequences as well as on 5 s and 23 s rrna availability of data and materials the sequence and secondary structure data used to train the neural network and hmms is available from the comparative rna web crw site repository https doi org 10 1186 1471 2105 3 2 this data along with the native secondary structure data for the test set is available at http ms uky edu dwi 239 rnastateinf data zip acknowledgements we would like to thank john hirdt who had initially participated in this project for many valuable discussions and suggestions dm thanks christine heitsch for introducing him into research on shape directed rna structure prediction references 1 jamie j cannone sankar subramanian murray n schnare james r collett lisa m d souza yushi du brian feng nan lin lakshmi v madabusi kirsten m m ller et al the compara tive rna web crw site an online database of comparative sequence and structure information for ribosomal intron and other rnas bmc bioinformatics 3 1 2 2002 2 jamie j cannone sankar subramanian murray n schnare james r collett lisa m d souza yushi du brian feng nan lin lakshmi v madabusi kirsten m m ller nupur pande zhidi shang nan yu and robin r gutell the comparative rna web crw site an online database of comparative sequence and structure information for ribosomal intron and other rnas bmc bioinformatics 3 1 2 2002 3 jonathan l chen stanislav bellaousov and douglas h turner rna secondary structure determination by nmr methods mol biol 1490 177 86 2016 4 fran ois chollet et al keras 2015 5 katherine e deigan tian w li david h mathews and kevin m weeks accurate shape directed rna structure determination proc natl acad sci u s a 106 1 97 102 jan 2009 6 laura dichiacchio michael f sloma and david h mathews accessfold predicting rna rna interactions with consideration for competing self structure bioinformatics 32 7 1033 1039 2015 7 richard durbin sean r eddy anders krogh and graeme mitchison biological sequence analysis probabilistic models of proteins and nucleic acids cambridge university press 1998 8 sean r eddy computational analysis of conserved rna secondary structure in transcriptomes and genomes annu rev biophys 43 433 56 2014 9 boris f rtig christian richter jens w hnert and harald schwalbe nmr spectroscopy of rna chembiochem 4 10 936 962 2003 13 https doi org 10 1186 1471 2105 3 2 https doi org 10 1186 1471 2105 3 2 http ms uky edu dwi 239 rnastateinf data zip 10 paul p gardner and robert giegerich a comprehensive comparison of comparative rna structure prediction approaches bmc bioinformatics 5 140 sep 2004 11 ian goodfellow yoshua bengio and aaron courville deep learning mit press 2016 12 alex graves and j rgen schmidhuber framewise phoneme classification with bidirectional lstm and other neural network architectures neural networks 18 5 602 610 2005 13 robin r gutell jung c lee and jamie j cannone the accuracy of ribosomal rna comparative structure models curr opin struct biol 12 3 301 10 jun 2002 14 sepp hochreiter and j rgen schmidhuber long short term memory neural comput 9 8 1735 1780 november 1997 15 risa kawaguchi hisanori kiryu junichi iwakiri and jun sese reactidr evaluation of the sta tistical reproducibility of high throughput structural analyses towards a robust rna structure prediction bmc bioinformatics 20 suppl 3 130 mar 2019 16 wan jung c lai mohammad kayedkhordeh erica v cornell elie farah stanislav bel laousov robert rietmeijer enea salsi david h mathews and dmitri n ermolenko mrnas and lncrnas intrinsically form secondary structures with short end to end distances nat com mun 9 1 4328 10 2018 17 d m layton and r bundschuh a statistical analysis of rna folding algorithms through thermodynamic parameter perturbation nucleic acids res 33 2 519 24 2005 18 s y le j h chen and j v maizel jr prediction of alternative rna secondary structures based on fluctuating thermodynamic parameters nucleic acids res 21 9 2173 8 may 1993 19 yann lecun l on bottou yoshua bengio and patrick haffner gradient based learning applied to document recognition proceedings of the ieee 86 11 2278 2324 1998 20 mirko ledda and sharon aviran patterna transcriptome wide search for functional rna elements via structural data signatures genome biology 19 1 mar 2018 21 thomas j x li and christian m reidys the rainbow spectrum of rna secondary structures bull math biol 80 6 1514 1538 06 2018 22 ronny lorenz stephan h bernhart christian h ner zu siederdissen hakim tafer christoph flamm peter f stadler and ivo l hofacker viennarna package 2 0 algorithms mol biol 6 26 nov 2011 23 nicholas r markham and michael zuker unafold software for nucleic acid folding and hybridization methods mol biol 453 3 31 2008 24 david h mathews and douglas h turner prediction of rna secondary structure by free energy minimization curr opin struct biol 16 3 270 8 jun 2006 25 jessica s reuter and david h mathews rnastructure software for rna secondary structure prediction and analysis bmc bioinformatics 11 129 2010 26 emily rogers david murrugarra and christine heitsch conditioning and robustness of rna boltzmann sampling under thermodynamic parameter perturbations biophysical journal 113 2 321 329 2017 27 david e rumelhart geoffrey e hinton and ronald j williams learning representations by back propagating errors cognitive modeling 5 3 1 1988 28 mike schuster and kuldip k paliwal bidirectional recurrent neural networks ieee trans actions on signal processing 45 11 2673 2681 1997 29 alina selega christel sirocchi ira iosub sander granneman and guido sanguinetti ro bust statistical modeling improves sensitivity of high throughput rna structure probing exper iments nat methods 14 1 83 89 01 2017 14 30 zsuzsanna s k sd m shel swenson j rgen kjems and christine e heitsch evaluating the accuracy of shape directed rna secondary structure predictions nucleic acids res 41 5 2807 16 mar 2013 31 m shel swenson joshua anderson andrew ash prashant gaurav zsuzsanna sukosd david a bader stephen c harvey and christine e heitsch gtfold enabling parallel rna secondary structure prediction on multi core desktops bmc res notes 5 1 341 jul 2012 32 hakim tafer fabian amman florian eggenhofer peter f stadler and ivo l hofacker fast accessibility based prediction of rna rna interactions bioinformatics 27 14 1934 40 jul 2011 33 theano development team theano a python framework for fast computation of mathe matical expressions arxiv e prints abs 1605 02688 may 2016 34 tijmen tieleman and geoffrey hinton lecture 6 5 rmsprop divide the gradient by a running average of its recent magnitude coursera neural networks for machine learning 4 2 2012 35 douglas h turner and david h mathews nndb the nearest neighbor parameter database for predicting stability of nucleic acid secondary structure nucleic acids res 38 database issue d 280 2 jan 2010 36 stefan washietl ivo l hofacker peter f stadler and manolis kellis rna folding with soft constraints reconciliation of probing data and thermodynamic secondary structure prediction nucleic acids res 40 10 4261 72 may 2012 37 kevin a wilkinson robert j gorelick suzy m vasa nicolas guex alan rein david h mathews morgan c giddings and kevin m weeks high throughput shape analysis reveals structures in hiv 1 genomic rna strongly conserved across distinct biological states plos biol 6 4 e 96 apr 2008 38 devin willmott recurrent neural networks and their application to rna secondary struc ture inference phd thesis university of kentucky 8 2018 15 1 introduction 2 methods 2 1 shape directed nntm 2 2 directing nntm with state information 2 3 state inference with deep neural networks 2 4 dataset implementation and metrics 3 results 3 1 native state directed nntm 3 2 predicted state directed nntm 4 discussion 4 1 state inference accuracy 4 2 paired regions global structure 4 3 modifying synthetic shape values 4 4 other rna types 5 conclusion