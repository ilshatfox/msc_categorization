vol no month yyyy 1 secure and privacy preserving federated learning via co utility josep domingo ferrer fellow ieee alberto blanco justicia jesu s manjo n and david sa nchez abstract the decentralized nature of federated learning that often leverages the power of edge devices makes it vulnerable to attacks against privacy and security the privacy risk for a peer is that the model update she computes on her private data may when sent to the model manager leak information on those private data even more obvious are security attacks whereby one or several malicious peers return wrong model updates in order to disrupt the learning process and lead to a wrong model being learned in this paper we build a federated learning framework that offers privacy to the participating peers as well as security against byzantine and poisoning attacks our framework consists of several protocols that provide strong privacy to the participating peers via unlinkable anonymity and that are rationally sustainable based on the co utility property in other words no rational party is interested in deviating from the proposed protocols we leverage the notion of co utility to build a decentralized co utile reputation management system that provides incentives for parties to adhere to the protocols unlike privacy protection via differential privacy our approach preserves the values of model updates and hence the accuracy of plain federated learning unlike privacy protection via update aggregation our approach preserves the ability to detect bad model updates while substantially reducing the computational overhead compared to methods based on homomorphic encryption index terms federated learning model poisoning privacy security co utility peer to peer f 1 introduction f ederated learning 5 12 is a decentralized ma chine learning technique that allows training a model with the collaboration of multiple peer devices holding private local data sets that include class labels this approach favors privacy because the peers do not need to upload their private data to a centralized server it is also naturally scalable because the computational load is split among the peers which may be edge devices such as idle smartphones and thus widely available in federated learning a special peer which we will call the model manager sends an initial model to all peers each peer then computes a model update by correcting the model so that when input the records in the peer s private data set the model s output fits the corresponding class attribute labels then the peer returns the update to the model manager the model manager aggregates the updates and distributes a new model to the peers a new learning iteration can now start iterations carry on until the models learned in successive iterations converge unfortunately as we discuss in section 2 the decen tralized nature of federated learning makes it vulnera ble to attacks against privacy and security substantial literature has been devoted to the privacy risks for peers 10 to what extent the model update returned by a peer can leak her private data privacy protection techniques include secure aggregation of updates which the authors are with the unesco chair in data privacy cybercat center for cybersecurity research of catalonia department of computer engineering and mathematics universitat rovira i virgili av pa sos catalans 26 e 43007 tarragona catalonia e mail josep domingo alberto blanco jesus manjon david sanchez urv cat hides individual updates to the model manager and distortion of updates via differential privacy which may significantly hamper the model s accuracy even more obvious are security attacks whereby one or several malicious peers return wrong model updates in order to prevent the convergence of the model byzan tine attack or cause a wrong model to be learned poi soning attack protection from byzantine and poisoning attacks requires the model manager to analyze individ ual peers updates thereby making privacy enhancing techniques based on secure aggregation of updates in adequate contribution and plan of this paper in this paper we build a federated learning framework that offers both privacy to the participating peers and security against byzantine and poisoning attacks our framework consists of several protocols designed in such a way that no rational party is interested in acting mali ciously this makes our protocols robust against security attacks our protocols also provide strong privacy to the participating peers via unlinkable anonymity and without requiring the aggregation of model updates in this way peer updates reach the model manager individually while being at the same time perfectly accurate this provides an optimum balance between security privacy and learning accuracy to be rationally sustainable our protocols are based on the co utility property 7 we also use reputation as a utility to reward well behaved peers and punish potential attackers in order to properly integrate repu tations in the federated learning scenario our reputation management is decentralized and itself co utile ar x iv 2 10 8 01 91 3 v 1 cs c r 4 a ug 2 02 1 vol no month yyyy 2 we report empirical results that show the effectiveness of our protocols at mitigating security attacks and at motivating rational peers to refrain from deviating section 2 discusses privacy and security attacks against federated learning section 3 introduces a co utile protocol suite for privacy preserving and secure federated learning section 4 shows that the proposed protocol suite achieves co utility and hence is rationally sustainable privacy and security experimental results are presented in section 5 finally conclusions and future research lines are gathered in 6 2 attacks on federated learning pri vacy and security in this section we will discuss the main attacks on privacy and security that are applicable to federated learning for a recent and exhaustive survey see 10 2 1 privacy attacks privacy attacks to federated learning exploit the update sent by a peer to infer information on that peer s private data in federated learning the private data sets held by the various peers are unlikely to be identically distributed what is more federated learning is explicitly designed to improve the learned model by capturing the differences among the peers private data sets data inference attacks can be mounted that aim at inferring how each class is represented in a certain peer s private data set in 9 a powerful data inference attack against fed erated deep learning is presented that relies on gans generative adversarial networks this attack assumes an attacker that can see and use internal parameters of the learned model the attacker participates as an honest peer in the collaborative learning protocol but she tries to extract information about a class of data she does not own to that end the attacker builds a gan locally and crafts gradient updates before returning them in order to influence other participating peers to leak more information on their data if the attacker is the model manager rather than a peer she can do more the model manager can isolate the shared model trained by the victim peer the victim peer s update trained on the victim s data is used to train the model manager s gan that can eventually re create the victim s data as explained in 9 not even differential privacy used as proposed in 17 can protect against the proposed gan attack a common requirement of all data inference attacks in federated learning is that the attacker must be able to link the successive updates submitted by a certain peer our aim is to make sure that such a linkage is not possible by making peers updates unlinkably anonymous in the model manager s view 2 2 security attacks security attacks on federated learning aim at disrupting model convergence and thereby the learning process they can be subdivided into byzantine and poisoning attacks byzantine attacks consist of malicious peers who submit defective updates in order to prevent convergence of the global model 3 subtler than byzantine attacks are model poisoning attacks rather than preventing convergence the latter aim at causing federated learning to converge towards a false global model normally one that misclassifies a specific set of inputs in 2 it is shown that a single non colluding malicious peer is enough to mount a poisoning attack yet security attacks can also be mounted by collusions of peers or by a single peer masquerading as several peers sybil attack countermeasures against byzantine or poisoning at tacks require seeing the exact values of the individual updates in order to assess their goodness this is why some techniques that are good to protect the privacy of peers such as secure aggregation of peer updates via homomorphic encryption 5 may impair the model manager s ability to thwart security attacks our aim is to protect privacy in such a way that malicious updates can still be attributed 3 a co utile framework for privacy preserving and secure federated learn ing the foundations of our proposed protocol suite are i the notion of co utility applied to protocol design and ii the use of reputations computed themselves in a decentralized and co utile manner to motivate all ratio nal players to behave honestly we start by giving some background on co utility and decentralized reputation also for convenience table 1 summarizes the notation used in the rest of this paper a self enforcing protocol is co utile 7 if it results in mutually beneficial collaboration between the participat ing agents more specifically a protocol is co utile if and only if the three following conditions hold 1 is self enforcing 2 the utility derived by each agent participating in is strictly greater than the utility the agent would derive from not participating 3 there is no alternative protocol giving greater utilities to all agents and strictly greater utility to at least one agent the first condition ensures that if participants engage in the protocol they will not deviate the second condi tion is needed to guarantee that engaging in the protocol is attractive for everyone the third condition can be rephrased in game theoretic terms by saying that the protocol is a pareto optimal solution of the underlying game vol no month yyyy 3 table 1 notation in this paper notation concept m model manager am accountability manager p peer m number of ams per peer reputation reward punishment u federated learning update nu random nonce encrypted with update u gi peer pi s reputation pkm public key encryption under m s public key sp digital signature under p s private key h cryptographic one way hash function flexibility parameter note 1 p forwarding probability p 0 probability of discarding an update from a peer with zero reputation b size of a batch of non discarded updates c centroid of a batch of updates t reputation threshold s t if a peer s reputation is at least t her updates are never discarded we describe a framework based on co utility that ensures that peers can keep their private data sets con fidential and at the same time makes them rationally interested in returning honest updates to the model manager 3 1 players and security model the players in our framework and their security prop erties are as follows model manager the model manager m is a player who wants to train a machine learning model on the private data of the peers in a peer to peer p 2 p network her interest is to obtain a good quality model but she might be curious to learn as much as possible on the peers private data sets hence m can be viewed as rational but curious rational to adhere to her prescribed function but curious on the peers private data peers they are participants in the network who compute model updates based on their local private data sets peers want to preserve their private data confidential we assume that a majority of peers are rational but curious like m they are interested in obtaining a good quality model but they also want to influence the model based on their own respective data further they might be curious to learn as much as possible on the other peers private data sets on the other hand there may be a minority of mali cious peers that wish to impair the learning process because they do not have the same utility function and or do not respond to the same incentives as the rest of peers accountability managers accountability managers ams are randomly chosen peers that manage the reputations of other peers being peers them selves most accountability managers are rational but curious but a minority may be malicious 3 2 requirements the assumption that peers are rational rather than honest calls for incentives to make honest behavior attractive to them we will use reputation as an incentive to reward or punish peers in order for this to be effective the following requirements need to be fulfilled reward if a peer contributes a good update her reputation must increase punishment if a peer contributes a bad update her reputation must decrease unlinkable anonymity peers contributing good up dates must stay not only anonymous but their successive updates must be unlinkable reputation utility having high reputation must be attractive for peers specifically it must be easier for peers with higher reputation to contribute their updates while preserving their privacy thus repu tation translates to influence without privacy loss unlinkability is our approach to thwarting the privacy attacks sketched in section 2 1 while perfectly retaining the accuracy of the updates on the other hand reward punishment and reputation utility are our tools to pro tect against the security attacks described in section 2 2 this will become clear in this section and in section 4 below 3 3 co utile decentralized reputation whereas we assume that a majority of peers want to learn a good model we still need to incentivize rational peers to abstain from free riding if they find greater utility in deviating from the federated learning pro tocol they might seriously impair the overall quality of the learned model also we need a way to stig matize recognize malicious peers in order to mitigate their attacks to meet the above purposes we will use reputation management in this section we present a reputation management system that does not require direct interaction between peers and has the following interesting properties pseudonymity of peers decentral ization resistance to tampering with reputations proper management of new peers to discourage whitewashing bad reputations as new identities and creating fake peers in sybil attacks and low overhead our reputation protocol maintains a public reputation for each peer p that is the result of updating p s previous reputation according to the behavior of p reported by the model manager m next we explain how the above interesting properties are satisfied pseudonymity of peers only the pseudonym of peers is known rather than their real identity further more updates that are sent over the network cannot be linked to the peers that generated them decentralization the reputation of every peer p is redundantly managed by a number m of peers that vol no month yyyy 4 act as accountability managers for p typically m is an odd number at least 3 and the pseudony mous identities of p s accountability managers are pseudo randomly determined by hashing the peer s pseudonym p in this way p cannot choose her m accountability managers which makes the latter more likely to perform their duty honestly tamper resistance since m does not know the iden tity of peers nor is able to link the updates to peers m cannot leverage her position to promote or slander any particular peer m likes or dislikes as a consequence m s rational behavior is to exclusively base her reports on the quality of the received model updates regarding tampering by account ability managers it is thwarted by their redundancy see the previous item on decentralization proper management of new peers reputations take values in the range 0 1 new peers start with reputation 0 which makes whitewashing and also sybil attacks unattractive let us describe the dynamics of reputation call epoch the period between two successive changes of the global model by m during an epoch peers generate and send model updates based on their private data with the aim of influencing the next global model change depending on their actions peers can earn or lose reputation gener ating a good update increases the generator s reputation by a certain quantum 2 fixed by the model manager furthermore helping a good update reach the model manager in a way unlinkable to the generator brings a 2 reputation increase to one of the helping peers thus every good update results in a total reputation increase on the other hand generating a bad update decreases the generator s reputation by thus the overall reward for a good update equals the punishment for a bad update some peer reputations may become negative and some may become greater than 1 as an epoch progresses at the epoch s end reputations are re normalized into the range 0 1 as follows first accountability managers reset any negative reputation to 0 then if there are reputations above 1 all reputations are divided by the largest reputation to that end when a peer s reputa tion becomes larger than 1 the peer s accountability managers broadcast that reputation which allows all accountability managers to compute the maximum rep utation reached in that epoch and thereby normalize all reputations into the interval 0 1 normalization has the beneficial effect of deterring free riding even if a peer has attained high reputation she will lose it gradually if she stops participating in deed any peer s reputation will decrease due to normal ization unless she continues to generate good updates or helps routing them this addresses the second condition of the co utility definition the utility derived from par ticipating must be greater than the utility derived from not participating fulfillment of the other two conditions for co utility will be justified in section 4 1 below 3 4 downstream from update generator to model manager we call downstream operation the submission of model updates from the peers to the model manager m in order to preserve privacy and encourage security we propose protocol 1 in section 4 we will show that it is co utile the idea of protocol 1 is that a peer say p 1 does not directly send her update to m rather p 1 asks another peer say p 2 to do so p 2 randomly decides whether to submit p 1 s update to m or forward it to another peer say p 3 who stands the same choice as p 2 forwarding continues until a peer is found that submits the update to m protocol 1 update submission 1 let p 1 be a peer that generates an update u then p 1 encrypts u along with a random nonce nu under the model manager s public key to ob tain pkm u nu we assume the message u nu to have a certain format that allows distinguish ing it from gibberish at decryption in this way only m will be able to recover the update u the generator p 1 never submits her own up date to the manager m rather p 1 forwards sp 1 pkm u nu h h h u nu p 2 where h is a one way hash function and sp 1 is p 1 s signature to another peer p 2 select g 1 where function select is explained below 2 if p 1 s reputation g 1 is such that g 1 min g 2 t where g 2 is p 2 s reputation t is a parameter such that updates submitted by peers with reputation t or above are never discarded and is a flexibility parameter discussed in note 1 then p 2 discards the received update otherwise p 2 makes a random choice with probability 1 p she submits sp 2 pkm u nu h h h u nu m to m and with probability p she forwards sp 2 pkm u nu h h h u nu p 3 to another peer p 3 select g 2 3 if p 2 s reputation is below min g 3 t then p 3 discards the received update otherwise p 3 makes a random decision as to submit or forward if it is forward p 3 will use the select function and there may be more peers involved p 4 p 5 etc 4 eventually m receives an update spi pkm u nu h h h u nu m from a peer pi upon this m does a directly discard the update with probability p 0 1 min gi t 1 where p 0 is a parameter in dicating the probability of discarding an update submitted by a peer with 0 reputation and gi is pi s reputation b if the update has not been discarded decrypt pkm u nu obtain u check that the nonce nu vol no month yyyy 5 was not received before to make sure u is not a replay of a previously received update and check the hash h h h u nu c wait until a batch of b non discarded updates has been received in order to be able to decide whether u is good or bad see section 4 3 below on how to detect bad updates d change the model with the good updates in the batch and publish the updated model e publish the value 1 b f for every good non discarded update u publish h h h u nu g for every bad non discarded update u call punish pi where pi is the peer having submitted u and punish is protocol 2 in section 3 5 function select gi is used by a peer pi to select a forwardee there are several ways in which this can be accomplished however the rational choice is for pi to select a forwardee pj with a sufficient reputation so that m does not reject the update should pj submit it directly to m hence if pi s reputation is gi t pi can randomly pick any of the peers whose reputation is t or above because none of those peers risks update discarding however if gi t pi chooses the peer with the maximum reputation that does not exceed gi because no peer with reputation above that value will accept to forward pi s update note 1 on the flexibility parameter in protocol 1 a peer accepts to forward updates from peers that have at least her own reputation minus a flexibility amount using a small value 0 introduces some flexibility and helps new peers that start with 0 reputation to earn reputation as generators or first forwardees of good updates large values of are not acceptable from the rational point of view high reputation peers have little to gain by accepting updates from peers who are much below them in reputation because the latter are likelier to convey bad updates or to fail to reward the first forwardee in case of good updates note 2 on loops multiple paths and other misbehaviors nothing is gained by any peer if loops arise accidentally or intentionally in protocol 1 as it will be seen in below protocol 3 and note 4 only the first peer chosen by the update generator is rewarded hence forwarding twice or more times the same message brings no additional benefit on the other hand a generator p might send the same good update through several paths to increase the reputation of several first peers however by pro moting more peers than necessary p may experience a decrease of her own reputation because reputations are normalized when any peer reaches a reputation above 1 see section 3 3 finally update generators could systematically choose themselves as first forwardees of good updates to collect additional reward but if they do so they weaken their privacy note 3 key generation in protocol 1 peers sign the messages they send to that end each peer needs a public private key pair at least the two follow ing alternative key generation procedures are conceiv able i identity based signatures in which the peer s pseudonym is her public key and the peer s private key is generated by a trusted third party 16 ii blockchain style key generation 13 in which the peer generates her own key pair without the intervention of any trusted third party or certification authority and then obtains her pseudonym pi her address in the blockchain net work as a function of her public key 3 5 upstream from model manager to update gen erator by upstream operation we denote the punishment of bad updates and the reward of good updates let us start with protocol 2 that seeks to penalize the generator of a bad update by retracing the reverse path from m to the generator the peer pi who submits an update found to be bad by the manager can escape punishment if pi can show to her accountability managers that she received the bad update from a previous peer say pi 1 protocol 2 punish pi every accountability manager am of pi s does 1 ask pi whether pi can prove she did not generate u 2 if pi can show to am a message spi 1 pkm u nu h h h u nu pi then a do not punish pi the peer s reputation is left intact b call punish pi 1 otherwise punish pi by decreasing her reputation by the punishment protocol must be initiated by m because the model manager is the only party that can detect bad updates and that is interested in punishing them however the punishment is actually executed by the guilty peer s accountability managers hence m cannot track which peer is actually punished for that bad update which prevents m from identifying the generator of an update by falsely claiming that the update is bad unlike the punishment protocol the rewarding proto col is initiated by the peer who submitted a good update because that peer is the one interested in the reward as we will later justify the first peer and only the first peer who is asked by the generator to submit or forward a good update is also entitled to a reward we will call that peer the first forwardee protocol 3 reward u 1 when m publishes h h h u nu for a good update then the update generator say p 1 sends to the first forwardee say p 2 sp 1 h h u nu p 2 2 p 2 checks that the hash of h h u nu matches h h h u nu published by m if it is so p 2 vol no month yyyy 6 returns a receipt sp 2 h h u nu p 1 to the gen erator p 1 3 p 1 proves to her accountability managers that she is the generator by showing h u nu to them and proves that she has acknowledged her first forwardee by showing the receipt sp 2 h h u nu p 1 4 every accountability manager am of p 1 s checks p 2 s receipt and checks that the double hash of h u nu received from p 1 matches h h h u nu published by m if both checks are fine am increases p 1 s reputation by 2 5 p 2 sends sp 1 h h u nu p 2 to her accountability managers to claim her reward 6 every accountability manager am of p 2 s checks that the hash of h h u nu matches h h h u nu published by m if it is so am increases p 2 s reputation by 2 note 4 on rewarding the first forwardee only in proto col 3 only the first forwardee is rewarded rather than all forwardees the reason is that we want the total budget to reward a good update to be fixed and equal to the budget used to punish a bad update we also want the reward share for the generator of a good update to be fixed say 2 and independent of the number of hops the update travels before reaching m hence if we chose to reward all forwardees the fixed reward share 2 for forwardees ought to be distributed among them therefore every forwardee would be better off by submitting the update to m rather than forwarding it to another forwardee who would take part of the reward as a consequence there would be only one forwardee who would know that the previous peer is the generator of u this would break privacy rewarding only the first forwardee avoids this problem and is a sufficient incentive because any forwardee can hope to be the first due to the protocol design a forwardee does not know whether she receives an update from the generator or from another forwardee and thus has a reason to collaborate note 5 on peer dropout accidental due to power or network failure or intentional peer dropout does not affect the learning process on the one hand once an update has been generated forwarded the genera tor forwarder can disappear on the other hand the next forwardee is chosen among the peers who are online reputation management is also resistant to dropout of accountability managers because there are m of them for each peer m just needs to be increased if dropout is very likely punishment is not affected even though a peer drops out he will be punished with a reputation decrease all the same however rewarding may be prob lematic in the very specific case that either the update generator p 1 or the first forwardee p 2 drop out before rewarding is complete the one of the two that remains online may not receive her his reward 4 discussion in this section we first demonstrate that the framework formed by protocols 1 2 and 3 is co utile that is that those protocols will be adhered to by the players defined in section 3 1 then we will show that the protocols satisfy the requirements of section 3 2 and thereby preserve the confidentiality of the users private data and protect the learned model from byzantine and poisoning security attacks 4 1 co utility to argue co utility for protocols 1 2 and 3 we must show that following them is a better option for m and the peers than deviating 4 1 1 co utility for the model manager the model manager s goal is to train a model based on the peers private data sets for that reason m is interested in encouraging good updates and punishing bad updates on the other hand m s role is limited to step 4 of protocol 1 let us examine in detail the actions of m in that step and whether m could gain by deviating from them or skipping them 1 in step 4 a m directly discards an update with a probability that is inversely proportional to the reputation of the submitting peer discarding is only based on reputation without examining whether the update is an outlier m is interested to perform this step at least for two reasons first it reduces m s computational overhead and second it allows m to make reputation attractive for peers only high reputation peers those with reputation at least t are sure of getting their updates examined at the same time if m wants to keep the peer community alive m should allow a nonzero probability 1 p 0 of examining an update submitted by a new peer that has 0 reputation also setting up a threshold t above which updates are examined for sure is a way for m of not losing too many good updates 2 step 4 b consists of decrypting the update checking its freshness and checking that the hash is correct obviously m is interested in carrying out these steps without the updates m cannot train the model 3 step 4 c is about deciding whether an update is good or bad m clearly needs to make this decision in order to use good updates to improve the model and punish bad updates to discourage them 4 step 4 d is about changing the model using the good updates this is exactly m s main goal 5 step 4 e publishes that determines the amount whereby reputations must be increased decreased by the accountability managers m is interested in publishing to facilitate a correct reputation man agement that keeps peers incentivized in fact if the number b of updates per batch is fixed then is also vol no month yyyy 7 fixed and does not need to be published at each protocol execution 6 step 4 f publishes information that peers can use to claim rewards for good updates if m deviates and does not publish this information then peers cannot claim rewards this would discourage peers from submitting good updates and would be against m s interests 7 step 4 g launches the punishment procedure for each bad update if m did not perform this step bad updates would go unpunished which would fail to discourage them 4 1 2 co utility for the update generator in protocol 1 the update generator only works in step 1 let us analyze the actions in this step 1 update generation and encryption the generator say p 1 generates an update and encrypts it together with a random nonce so that only m can decrypt the update and check its freshness a the intrinsic motivation for p 1 to generate an update is to have an influence on the model being learned a rational peer wants to help obtain an accurate model that is socially beneficial in some sense whereas a malicious peer wants to poison the learned model b the motivation for p 1 to generate a good update u is to keep her reputation high a high reputation brings more influence on the model learning specifically a high g 1 allows p 1 to find p 2 such that g 1 g 2 which means that p 2 does not discard p 1 s update and with g 2 high enough for p 1 to be confident that p 2 can be entrusted with relaying u towards m with little or no probability of u being discarded by m without examination see description of the select function in sec tion 3 4 if u eventually reaches m this brings p 1 influence and further reputation increase which means more influence in the future c the motivation for p 1 to encrypt u under m s public key is to prevent anyone else from claiming the reward for that update should u be good the motivation for p 1 to sign the forwarded message is that the forwardee p 2 will not accept an un signed message because p 2 will need that signed message to escape punishment in case u is bad 2 update forwarding in terms of privacy it is bad for p 1 to submit her generated update directly to m as it could leak information on her private data set it is still bad if p 1 directly submits with probability 1 p and forwards with probability p like in the crowds system 15 if we used the crowds algorithm from the point of view of m the most likely submitter of an update would be the update generator u would be submitted by p 1 with probability 1 p whereas it would be submitted by the i th forwardee with probability 1 p pi 1 p hence p 1 is interested in looking for a forwardee p 2 who takes care of her update rather than submitting her update herself specifically p 1 wants a forwardee p 2 such that a p 2 will accept to forward p 1 s update b p 2 does not risk update discarding g 2 t or risks it with the smallest possible probability see the description of the select function in section 3 4 further if p 1 can choose among several possible p 2 with g 2 t p 1 s best option is to pick p 2 randomly for the sake of unlinkability of successive updates to each other here we see a second benefit of a high reputation for p 1 the higher g 1 the more peers with reputation at least t p 1 can choose from and the higher is unlinkability in protocol 2 the update generator p 1 has a role only if her update is bad the generator s role in this case is a passive and inescapable one when p 1 is asked by her accountability managers to show that p 1 received the bad update from someone else p 1 cannot show it and is punished in protocol 3 the generator p 1 of a good update is clearly interested in running step 1 of the protocol to claim a reward in step 1 p 1 is forced to give the first forwardee p 2 the necessary information h h u nu so that p 2 can claim his reward the reason is that without p 2 s receipt p 1 cannot claim her own reward at step 3 this latter step is also self enforcing if p 1 wants her reward p 1 could certainly decide to favor a false first for wardee p 2 of her choice rather than the real first for wardee p 2 this would still work well for p 1 because p 2 would return a signed receipt for the same reasons that p 2 would do it however if p 1 wants to favor p 2 it entails less risk of being discovered for p 1 to use p 2 as a real first forwardee thus there is no rational incentive to favor false first forwardees 4 1 3 co utility for the update forwardees in protocol 1 the forwardees p 2 p 3 work in steps 2 and 3 which are analogous to each other let us examine the actions expected from a forwardee 1 update acceptance or discarding the incentive for a forwardee pi to accept to deal with an update u is to be rewarded in case u is good and pi is the first forwardee note that pi does not know whether she is the first but hopes to be thus if pi receives the update from a previous peer pi 1 with high reputation pi s rational decision is to accept that update there are chances that u is good which will bring reward if pi turns out to be the first forwardee in contrast if u comes from a peer pi 1 with low reputation it is less likely that the update is good so pi s rational decision is to discard u to avoid working and spending bandwidth for nothing 2 update submission or forwarding it takes about the same effort for a forwardee pi to submit an update to m or to forward it to some other peer pi 1 vol no month yyyy 8 hence it is rational for pi to make the decision randomly according to the prescribed probabilities 1 p for submission and p for forwarding in case of forwarding pi s rational procedure is like the generator s look for a forwardee with reputation at least t if gi t or the maximum possible reputation that does not exceed gi otherwise as per the select function also no matter whether forwarding or submitting pi has to replace the pre vious signature of the update by her own signature neither the model manager nor any forwardee will accept from pi a message that is not signed by pi because they will need the signed message in case u turns out to be bad and punishment is launched in protocol 2 if pi did not generate a bad update u pi will rationally do her best to avoid punishment reputation decrease by showing a message signed by whoever sent u to her in protocol 3 p 2 s best option is to return the receipt at step 2 because p 1 could otherwise blacklist p 2 and never make p 2 a first forwardee in future epochs finally p 2 is obviously interested in claiming her reward in step 5 4 1 4 co utility for the accountability managers the accountability managers are a keystone in proto cols 1 2 and 3 in our security model section 3 1 a majority of them is assumed to be rational and to be interested in obtaining a well trained model hence a majority of the m accountability managers pseudoran domly assigned to each peer can be expected to behave honestly which in turn means that the reputation of every peer can be expected to be honestly managed in protocol 1 there is no direct intervention of account ability managers it suffices that they honestly maintain and supply the reputations gi of all involved peers pi as described in section 3 3 as to protocol 2 it is launched at the request of m in the last step of protocol 1 in protocol 2 the accountability managers have the lead role most of each peer s accountability managers can be assumed rational and therefore they can be assumed to discharge their role as described in the protocol finally in protocol 3 the accountability managers of the generator reward the latter in step 4 then in step 6 the first forwardee is rewarded by her accountability managers again since for each peer a majority of accountability managers can be assumed rational we can expect them to honestly perform those two steps as described in protocol 3 note 6 non collusion scenario in fact given that the accountability managers assigned to a peer are randomly chosen it is reasonable to assume that in general they do not know each other and hence they do not collude in the non collusion scenario not even a majority of honest accountability managers is needed if malicious accountability managers do not collude each of them is likely to report different reputation results hence as long as two of the peer s accountability managers act rationally and follow the protocol their correct result is likely to be the most frequent one and thus to prevail 4 2 privacy as mentioned in section 2 1 ensuring the unlinkability of updates goes a long way towards guaranteeing that the private data sets of peers stay confidential we can state the following proposition proposition 1 if the forwarding probability is p 0 andthere is no collusion between the model manager m and peers the private data set of each peer remains confidential versus the model manager and the other peers confidentiality is based on update encryption and unlinkability and unlinkability increases with p and the generator s reputation proof the privacy guarantee is based on unlinkabil ity and update encryption let us first consider linkability by m by the design of protocol 1 m knows that the submitter of an update u is never the update generator at best m knows that the probability that u was submitted by the i th forwardee is 1 p pi 1 and hence that the most likely submitter is the first forwardee however the larger p the greater the uncertainty about the number of hops before the update is submitted and hence the harder for m to link a received update to its generator the next forwardee is selected using the select function described in section 3 4 if ggen t then pgen chooses the first forwardee randomly among the set of peers with reputation at least t and this set depends on the current reputations and varies over time hence as long as there are several peers with reputation t or above the fact that two updates were submitted by the same peer does not tell m that both updates were generated by the same peer if ggen t then pgen chooses as a first for wardee the peer with the maximum reputation that does not exceed gi if reputations do not change between two successive updates pgen would choose the same first forwardee for both updates yet m cannot be sure that the submitter of both updates is really the first forwardee and hence m cannot be sure that both updates were generated by the same pgen hence in no case can two different updates by the same generator be unequivocally linked even if the probability of correctly linking them is lower when ggen t on the other hand neither the reward nor the punish protocols allow m to learn who generated a good or a bad update thus m can neither link the updates he receives nor unequivocally learn who generated a certain update u therefore m cannot obtain any information on the private data set of any specific peer p consider now linkability by a peer pi if pi is a forwardee for two different updates from pi 1 and p 0 pi does not know whether pi 1 vol no month yyyy 9 generated any of the updates or is merely forward ing them pi s uncertainty about pi 1 being the generator is shannon s entropy h p which grows with p for p 0 5 for p 0 5 what grows with p is pi s certainty that pi 1 is not the generator in summary pi can only guess right that pi 1 is the generator if p is very small in this case forwarding hops after the first mandatory hop from generator to first forwardee are very unlikely the only exception is when pi is the first forwardee for two good updates from the same generator because in this case he receives a message from the generator in step 1 of protocol 3 however in this case pi can only link the encrypted version of updates that is pkm u nu and pkm u nu but has no access to the clear updates u u hence pi gets no information on pi 1 s private data set if pi is an accountability manager of a generator pj pi can link all encrypted good updates originated by pj however since those updates are not in the clear pi gets no information on pj s private data set note that assuming there are no collusions is plausi ble because peers are pseudonymous normally people collude only with those they know a successful collusion must include one or more first forwardees who know the pseudonyms of the update generators and m who can decrypt the updates in this way m can attribute updates and perhaps link those corresponding to the same generator then m can infer whatever information on the generator s private data set is leaked by the generator s updates however to allow update linkage a collusion requires a malicious model manager and a significant propor tion of malicious peers whereas in our security model section 3 1 we assume m and a majority of peers to be rational but curious a collusion of m with a substantial number of peers is hard to keep in secret and if it becomes known that m is malicious peers will be unwilling to help m to train the global model therefore m s rational behavior is to abstain from collusion 4 3 security guaranteeing security means thwarting byzantine and poisoning attacks section 2 2 which consist of submit ting bad model updates we first recall the approaches that have been proposed in the federated learning lit erature for the model manager to defend against bad updates they fall into the following three broad classes see the surveys 10 4 for more details detection via model metrics an update is labeled as bad if incorporating it to the model degrades the model accuracy this approach requires a validation data set on which the model with the update and the model without the update can be compared also the computation needed to make a decision on each received update is significant detection via update statistics an update is labeled as bad if it is an outlier with respect to the other updates neutralization via aggregation updates are aggre gated using operators that are insensitive to out liers such as the median 18 the coordinate wise median 18 or krum aggregation 3 in this way updates too different from the rest have little or no influence on the learning process in our protocol we want to explicitly detect bad updates in order to avoid interaction with the malicious peers generating them hence we discard methods in the third class neutralization any detection method in the two other classes can be used with our approach including new methods that may appear in the future yet detection based on model metrics is quite costly and requires validation data for this reason in the experimental work we have instantiated our implementation with a method based on update statistics more specifically a distance based method in line with 2 3 given a batch of updates this method labels as bad an update u if u is much more distant than the rest of updates in the batch from the batch centroid c one possible way to quantify what much more distant means is to check whether the distance between u and c is greater than the third quartile or greater than a small multiple of the third quartile say 1 5 times of the set of distances between updates in the batch and c protocols 1 2 and 3 are designed to incentivize the submission of good updates thus we can state the following proposition proposition 2 provided that the model manager can detect bad updates the rational behavior for generators and forwardees in protocol 1 is to submit good updates proof see discussion on co utility for generators and forwardees in section 4 1 as to collusions of irrationally malicious peers they can only disrupt the learning process if they are suffi ciently large so that the majority of updates received by m are bad ones and coordinated in the same direction note that uncoordinated bad updates are likely to cancel each other to some extent such large collusions seem hard to mount for the reasons explained in the previous section 4 4 computation and communications overhead let us compare the computation and communications overhead of the proposed method against alternatives based on homomorphic encryption he which offer a comparable level of privacy but cannot detect bad updates as argued below he has been used in federated aggregation mecha nisms to prevent the model manager and the rest of peers in the network from having access to the individ ual updates of peers in he based mechanisms peers first encrypt their respective updates using an additive vol no month yyyy 10 he scheme e g paillier 14 several protocols have been proposed in the literature to aggregate he updates and decrypt the aggregated he update let us focus on a protocol that minimizes the number of required messages and the amount of computation which is the most challenging benchmark when comparing with our proposed method i assume a sequence of peers is defined such that the first peer sends her he update to the next peer who aggregates it with her own he update and so on ii after the last peer has aggregated her he update she sends the encrypted update aggregation to the manager who can decrypt it to obtain the cleartext update aggregation in this protocol each peer sends only one message per update just as in plain federated learning whatever the protocol used he based solutions offer privacy no one other than the peer sees the peer s clear text update but they do not allow the model manager to detect bad updates because the manager does not see the individual updates in this respect he based solutions are inferior to our proposed method which offers privacy without preventing bad update detection even so let us compare he based systems and our system in terms of computational overhead he based systems require the peers to encrypt using a public key he scheme each individual model parameter at each training epoch an update contains values for all parameters the authors of 19 report an encryption time of 3111 14 seconds for a model with 900 000 pa rameters 6 87 mb using 3072 bit paillier a key size of 3072 bits in factorization based public key cryptosys tems offers equivalent security to 128 bit symmetric key schemes 1 expensive modular operations with 3072 bit long moduli in the case of paillier for each model parameter are required to aggregate the update of each peer in contrast our approach requires each peer to com pute an encryption of her update using a regular non homomorphic public key cryptosystem three hashes and one digital signature with the usual digital enve lope approach regular public key encryption amounts to encrypting a symmetric e g aes session key under the manager s public key and then encrypting the bulk of the update parameters using the much faster symmetric cryptosystem under the session key the encryption time of aes on current smartphones using aes 128 gcm is around 0 29 seconds for a model of the same size as reported above 1 to be compared with the afore mentioned 3111 14 seconds of he finally the model manager just needs to decrypt the received updates and aggregate them in cleartext as in plain federated learning mechanisms this is much faster than homomorphic aggregation in ciphertext regarding the communication overhead we first refer to the message expansion incurred by he based mech 1 aes performance per cpu core https calomel org aesni ssl performance html anisms and our proposal as stated above he based mechanisms require peers to encrypt each model param eter using an additive he scheme model parameters are usually 32 bit floating point values that when encrypted using paillier with sufficiently strong keys become 3072 bit integers this implies an increase in the message size of two orders of magnitude the proposal in 19 substantially reduces the communication requirements but it is still one order of magnitude above plain fed erated learning with cleartext updates in our proposal and thanks to the digital envelope technique updates are encrypted using a symmetric encryption scheme which does not expand the plaintext models save for potential paddings which are negligible for messages of the size we are considering additionally our messages include the session key encrypted under the model manager s public key a triple hash of the model and a signature this additional information increases the size of the message by approximately 6 5 kb with standard key and hash sizes which if we consider the example given before amounts to a 0 09 increase in the total size of the messages finally in the he based protocol considered the num ber of messages exchanged among participants does not increase with respect to plain federated learning i e for each training epoch there is one broadcast of the global model from the model manager to the peers and one message from each peer containing her update in contrast our proposal includes a forwarding mechanism which implies that for a forwarding probability p every encrypted model hops across an expected number of forwardees equal to 1 p i 1 ipi 1 1 1 p for example if p 1 2 there are 2 additional hopping messages with respect to plain federated learning ad ditionally if each peer has m accountability managers 2 m 1 messages containing one hash of the update and one digital signature of a hash value are re quired by the reward protocol 2 m messages of which m are short polling messages and m contain the signed encrypted update are required in the punishment protocol when a peer wishes to avoid punishment all in all our approach requires more messages per epoch than plain and he based federated learning however whereas the message expansion in our ap proach is almost negligible as the bulk of encryption is symmetric key the he based approach increases message length by one or two orders of magnitude with respect to plain federated learning in particular if we take say m 3 and p 1 2 the overall communications overhead of our approach stays below that of he based federated learning in summary our method achieves much less compu tation overhead and less communication overhead than https calomel org aesni ssl performance html https calomel org aesni ssl performance html vol no month yyyy 11 he based methods add to this performance advantage the functionality advantage our method offers both privacy for peers and detection of bad updates for the manager whereas the latter feature is lacking in he based methods 5 experimental results in this section we report the results of the experiments we conducted to test how the reputations of peers evolve over time depending on whether they submit good or bad updates first let us explain the expected system behavior if our protocols are well designed a peer s reputation should highly correlate with the probability that she generates good updates furthermore the reputation of the peer who submits an update to the model manager m should also highly correlate with the probability that the peer who generated that update generates good updates since the submitting peer s reputation is used by m to decide on processing or discarding an update m will only process a fraction of the received updates this reduces m s overhead related to detection and punishment of bad updates now let us go to the actual empirical results we bounded the range of reputations between 0 and 1 then we built a peer to peer network with 100 peers whose initial reputations were set to 0 we let the network evolve for 500 iterations or global training epochs at each epoch the model manager received one update from each peer thus the batch size was b 100 and the reward punishment quantum was 1 b 0 01 we then experimented with two test scenarios depending on the proportion of honest peers scenario 1 every peer is assigned a random goodness probability g r 0 1 with probability g the peer generates a good update and with probability 1 g she generates a bad update reputation manage ment is used by peers to decide on accepting or rejecting a forwarded update and to choose for wardees that is a peer pj accepts a forwarded update only if the requesting peer s reputation is at least gj where we set 0 03 in turn a peer pi chooses a forwardee based on reputations as described when explaining the function select in section 3 4 additionally reputation management is also used by the model manager m to decide on processing or directly discarding an update submit ted by a peer pk that is m directly discards the update with probability p 0 1 min gk t 1 where we set p 0 0 5 and t 0 5 scenario 2 90 of peers always generate good up dates whereas the remaining 10 have probability 0 2 of generating good updates and probability 0 8 of generating bad updates hence we can say that 90 of peers have goodness probability g 1 and 10 of peers have goodness probability g 0 2 like in the previous scenario reputation manage ment is set up by taking 0 03 p 0 0 5 and t 0 5 5 1 test scenario 1 in large real federated learning networks with say sev eral thousands or hundreds of thousands of peers e g smartphones a small proportion of malicious peers even smaller than in scenario 2 is the most realistic assumption nevertheless let us study an extreme sce nario with even proportions of good and bad updates this will allow us to demonstrate that the goodness probability of a peer correlates with her reputation and with the reputations of the peers submitting her updates let us assign a random goodness probability in the interval 0 1 to each of the 100 peers thus on average we can expect peers to generate good updates only half of the time reputations are computed after each of the 500 global training epochs and are used to decide on the one hand on update acceptance and forwarding peers accept updates from and forward updates to other peers depending on the flexibility parameter 0 03 and on the other hand on update processing and discarding by the model manager it directly discards updates with probability p 0 1 min gi t 1 with p 0 0 5 and t 0 5 figure 1 displays the goodness probability versus the reputation of every peer after the 500 global training epochs the goodness probability is represented in the abscissae and the reputation in the ordinates it can be seen that both the goodness probabilities and their corresponding reputations spread over the entire 0 1 range furthermore the peers goodness probabilities and their reputations are highly correlated 0 977 figure 2 displays for every update during the 500 global training epochs 50 000 updates the goodness probability of the update generating peer versus the reputation of the submitting peer it can be seen that both values are also highly correlated 0 833 in fact this correlation is even higher for peers with reputation below t 0 5 for submitting peers with reputations t 0 5 or above the precise reputation of the submitter is not that relevant because the model manager will process all updates submitted by peers with reputation t or above 5 2 test scenario 2 the previous scenario is highly unlikely in the real world as said above in large real federated learning networks a small proportion of malicious peers is the most realistic assumption in scenario 2 a clear majority of 90 of peers are com pletely honest goodness probability g 1 whereas the remaining 10 have a goodness probability of only g 0 2 reputations are computed after each epoch and vol no month yyyy 12 fig 1 scenario 1 goodness probability vs reputation for each peer correlation 0 977 fig 2 scenario 1 generating peer s goodness prob ability vs submitting peer s reputation for all updates the grayscale indicates the number of peers in each 2 dimensional interval correlation 0 838 are used to decide on the one hand on update accep tance and forwarding and on the other hand on update processing and discarding by the model manager figure 3 displays the goodness probability against the reputation of every peer after 500 global training epochs malicious peers those with g 0 2 are correctly assigned low reputations because most of the updates they generate are bad and they are punished when their updates reach the model besides that it is hard for such peers to be selected as forwardees of good updates and thereby improve their reputation on the other side all honest users those with g 1 0 achieve high reputation values that correspond to their good behavior peers with a reputation t 0 5 or above are part of a community whose members improve the reputations of each other by forwarding or submitting their respective updates fig 3 scenario 2 goodness probability vs reputation for each peer correlation 0 998 the evolution of the reputations of good peers with g 1 0 and bad peers g 0 2 is shown in figure 4 the average reputations of both types of peers swiftly diverge from the very beginning figure 5 shows how reputations evolve when peers change their behavior that is their g in the figure peer 0 is a good peer with g 1 0 that suddenly changes his behavior by setting g 0 2 at epoch 100 from that epoch onwards peer 0 generates bad updates with probability 0 8 we can see that his reputation drops fast and stabilizes around the average reputation value of bad peers see figure 4 around epoch 260 this shows that our system reacts suitably when a peer s behavior worsens on the other hand peer 98 in the figure represents a malicious peer with g 0 2 that changes her behavior by setting g 1 0 at epoch 100 from that epoch onwards peer 98 only generates good updates in this case we see that her reputation gradually and slowly vol no month yyyy 13 fig 4 scenario 2 evolution of the average depicted as a line and the standard deviation depicted as a gray band of the reputations of good peers and bad peers as a function of the epoch increases up to roughly the average reputation of good peers see figure 5 around epoch 360 this shows that not only malicious peers but also newcomers who have zero initial reputation can effectively reach high reputa tions if they behave well however the amount of effort needed to rise from a low reputation clearly discourages malicious peers from performing whitewashing or sybil attacks fig 5 scenario 2 evolution of the reputation of nodes who change their behavior g at epoch 100 peer 0 changes from good to malicious whereas peer 98 changes from malicious to good figure 6 displays for every update during the 500 global training epochs 50 000 updates the goodness probability of the generator versus the reputation of the submitter both values are highly correlated 0 799 however the correlation is higher after the system stabi lizes 0 9854 from epoch 100 onwards and all good peers reach high reputations initially reputations have not yet adjusted and hence the updates generated by good peers can be submitted by peers with reputation only slightly above or even slightly below t finally observe in figure 7 the effectiveness of making reputation based decisions to filter out bad updates out of the 50 000 updates generated over the 500 epochs around 46 000 are good while around 4 000 are bad based on the submitting peer s reputation the model fig 6 scenario 2 generating peer s goodness prob ability vs submitting peer s reputation for all updates the grayscale indicates the number of peers in each 2 dimensional interval correlation 0 799 manager m discards 2 831 updates the figure shows that when the system stabilizes on average 80 of the updates discarded by m are bad this is the right pro portion because malicious peers do not always generate bad updates they generate bad updates with probability 1 g 0 8 fig 7 scenario 2 ratio of bad updates discarded by the model manager as a function of the training epoch note that reducing the proportion of bad updates processed by the model manager is also a good security defense indeed the fewer the bad updates processed by the model manager the more those bad updates are likely to stand out as outliers which will enable m to de tect and discard them additionally fewer bad updates processed by m also mean less detection overhead for m and especially less punishment and tracing overhead for peers both normal peers and accountability man vol no month yyyy 14 agers 6 conclusions and future work we have presented protocols to improve privacy and security in federated learning while perfectly preserving the model accuracy our protocols rely on the notion of co utility that is they are self enforcing if players are rational we use a decentralized reputation management scheme that is itself co utile to incentivize peers to adhere to the prescribed protocols in this way peers do not need to be honest but curious per se as long as they are rational they will behave honestly and even a minority of malicious peers that do not respond to the same incentives as the other peers can be tolerated confidentiality of the peers private data is guaranteed by the unlinkability of updates when a peer generates an update neither the model manager nor the other peers can identify the update generator this way to provide privacy is superior to the state of the art alternatives unlike privacy protection via differential pri vacy 9 our protection mechanism does not alter the value of updates and hence does not affect the accuracy of the learned model furthermore our privacy notion based on unlinkability is also strong unlike privacy protection based on update aggrega tion our solution is compatible with punishing the peers that generate bad updates also our solution entails less computational overhead than aggrega tion based on homomorphic encryption security i e protection against bad updates is pursued in our approach via reputation whereas state of the art security countermeasures do nothing to reduce the number of bad updates that are processed by the model manager we address this issue in a way to achieve two beneficial effects first to decrease the overhead for the model manager and the peers related to processing tracing and punishing bad updates and second to make the fewer bad updates processed by the model manager more identifiable as outliers the design of our protocols also renders whitewashing and sybil attacks ineffective an interesting avenue for future research is to harden the proposed protocols so that they can filter out a greater proportion of bad updates in situations where a substantial share of the peers are malicious a possible strategy is for the model manager to preventatively reject without further examination any update submitted by a peer whose reputation is less than the average reputation of peers who submitted updates detected as bad in the past note that the peer submitting the update is not the peer having generated it but as shown in the experimental section above the submitter s and the generator s reputations are correlated another interesting direction is to incorporate new methods to detect bad updates that are better suited for non independent and identically distributed non iid private data than distance based methods most current detection methods mentioned in section 4 3 are ill suited when the private data of the different peers follow very different distributions in fact the extremely non iid case is challenging for the very notion of federated learning even if all peers compute their updates honestly con verging to an accurate model is more difficult than in the iid case acknowledgments and disclaimer partial support to this work has been received from the european commission projects h 2020 871042 so bigdata and h 2020 101006879 mobidatalab the government of catalonia icrea acade mia prizes to j domingo ferrer and d sa nchez and grant 2017 sgr 705 and from the spanish government project rti 2018 095094 b c 21 consent and tin 2016 80250 r sec mcloud the authors are with the unesco chair in data privacy but the views in this paper are their own and are not necessarily shared by unesco references 1 e barker nist special publication 800 57 part 1 revision 5 recommendation for key management nist 2020 available at https doi org 10 6028 nist sp 800 57 pt 1 r 5 2 a n bhagoji s chakraborty p mittal and s calo ana lyzing federated learning through an adversarial lens arxiv 1811 12470 v 3 2019 3 p blanchard e m el mhambi r guerraoui and j stainer machine learning with adversaries byzantine tolerant gradient descent in advances in neural information processing systems 30 nips foundation 2017 4 a blanco justicia j domingo ferrer s mart nez d sa nchez a flanagan and k e tan achieving security and privacy in federated learning systems survey research challenges and future directions arxiv 2012 06810 2020 5 k bonawitz v ivanov b kreuter a marcedone h b mcmahan s patel d ramage a segal and k seth practical secure aggregation for privacy preserving machine learning in pro ceedings of the 2017 acm sigsac conference on computer and communications security ccs 17 pp 1175 1191 acm 2017 6 j domingo ferrer o farra s s mart nez d sa nchez and j soria comas self enforcing protocols via co utile reputation manage ment information sciences 367 368 2016 159 175 7 j domingo ferrer s mart nez d sa nchez and j soria comas co utility self enforcing protocols for the mutual benefit of par ticipants engineering applications of artificial intelligence 59 2017 148 158 8 m fredrikson s jha and t ristenpart model inversion attacks that exploit confidence information and basic countermeasures in proceedings of the 22 nd acm sigsac conference on com puter and communications security pp 1322 1333 acm 2015 9 b hitja g ateniese and f perez cruz deep models under the gan information leakage from collaborative deep learning in proceedings of the 2017 acm sigsac conference on computer and communications security ccs 17 pp 603 618 acm 2017 10 p kairouz h b mcmahan et al advances and open problems in federated learning arxiv 1912 04977 2019 11 s d kamvar m t schlosser h garcia molina the eigentrust algorithm for reputation management in p 2 p networks in 12 th international conference on world wide web pp 640 651 acm 2003 12 h b mcmahan e moore d ramage s hampson and b agu era arcas communication efficient learning of deep net works from decentralized data arxiv 1602 05629 2017 13 a narayanan j bonneau e felten a miller and s goldfeder bitcoin and cryptocurrency technologies a comprehensive in troduction princeton university press 2016 https doi org 10 6028 nist sp 800 57 pt 1 r 5 http arxiv org abs 2012 06810 http arxiv org abs 1912 04977 http arxiv org abs 1602 05629 vol no month yyyy 15 14 p paillier public key cryptosystems based on composite degree residuosity classes in advances in cryptology proceedings of eurocrypt 99 pp 223 238 springer 1999 15 m k reiter and a d rubin crowds anonymity for web trans actions acm transactions on information and system security 1 1998 66 92 16 a shamir identity based cryptosystems and signature schemes in advances in cryptology proceedings of crypto 84 pp 47 53 springer 1984 17 r shokri and v shmatikov privacy preserving deep learning in proceedings of the 22 nd acm sigsac conference on computer and communications security ccs 15 pp 1310 1321 acm 2015 18 d yin y chen k ramchandran and p bartlett byzantine robust distributed learning towards optimal statistical rates arxiv 1803 01498 2018 19 c zhang s li j xia w wang f yan and y liu batchcrypt ef ficient homomorphic encryption for cross silo federated learning in usenix annual technical conference 2020 usenix atc 20 pp 493 506 2020 josep domingo ferrer fellow ieee is a dis tinguished professor of computer science and an icrea acade mia researcher at universitat rovira i virgili tarragona catalonia where he holds the unesco chair in data privacy and leads cybercat he received the bsc msc and phd degrees in computer science from the autonomous university of barcelona in 1988 and 1991 respectively he also holds a bsc msc degree in mathematics his research in terests are in data privacy data security and cryptographic protocols more information on him can be found at http crises deim urv cat jdomingo alberto blanco justicia is a postdoctoral re searcher at universitat rovira i virgili he ob tained his msc in computer security in 2013 from universitat rovira i virgili and his phd in com puter engineering and mathematics of security from the same university in 2017 his research interests are in data privacy data security cryp tographic protocols ethically aligned design and machine learning explainability he has been in volved in several european and national spanish research projects as well as technology transfer contracts jesu s manjo n is a computer engineer with the unesco chair in data privacy and the crises research group at the department of computer engineering and mathematics of uni versitat rovira i virgili tarragona catalonia he received his bsc in computer engineering in 2004 and his msc in computer security in 2008 he has participated in several national and european funded research projects and he is a co author of several research publications on security and privacy david sa nchez is a serra hunter associate pro fessor and an icrea acade mia researcher at universitat rovira i virgili he received his phd in computer science from the technical univer sity of catalonia in 2008 he has participated in several national and european funded research projects and he has authored several papers and conference contributions his research in terests include data semantics ontologies data privacy and security http arxiv org abs 1803 01498 http crises deim urv cat jdomingo 1 introduction 2 attacks on federated learning privacy and security 2 1 privacy attacks 2 2 security attacks 3 a co utile framework for privacy preserving and secure federated learning 3 1 players and security model 3 2 requirements 3 3 co utile decentralized reputation 3 4 downstream from update generator to model manager 3 5 upstream from model manager to update generator 4 discussion 4 1 co utility 4 1 1 co utility for the model manager 4 1 2 co utility for the update generator 4 1 3 co utility for the update forwardees 4 1 4 co utility for the accountability managers 4 2 privacy 4 3 security 4 4 computation and communications overhead 5 experimental results 5 1 test scenario 1 5 2 test scenario 2 6 conclusions and future work references biographies josep domingo ferrer alberto blanco justicia jes s manj n david s nchez