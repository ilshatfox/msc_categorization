ar x iv 1 00 9 07 44 v 4 cs i t 1 1 f eb 2 01 1 new and improved johnson lindenstrauss embeddings via the restricted isometry property felix krahmer and rachel ward october 22 2018 abstract consider anm n matrix with the restricted isometry property of order k and level that is the norm of any k sparse vector in rn is preserved to within a multiplicative factor of 1 under application of we show that by randomizing the column signs of such a matrix the resulting map with high probability embeds any fixed set of p o ek points in rn into rm without distorting the norm of any point in the set by more than a factor of 1 4 consequently matrices with the restricted isometry property and with randomized column signs provide optimal johnson lindenstrauss embeddings up to logarithmic factors in n in particular our results improve the best known bounds on the necessary embedding dimension m for a wide class of structured random matrices for partial fourier and partial hadamard matrices we improve the recent bound m 4 log p log 4 n appearing in ailon and liberty 3 to m 2 log p log 4 n which is optimal up to the logarithmic factors in n our results also have a direct application in the area of compressed sensing for redundant dictionaries 1 introduction the johnson lindenstrauss jl lemma states that any set of p points in high dimensional euclidean space can be embedded into o 2 log p dimensions without distorting the distance between any two points by more than a factor between 1 and 1 in its original form the johnson lindenstrauss lemma reads as follows theorem 1 1 johnson lindenstrauss lemma 28 let 0 1 and let x 1 xp rn be arbitrary points let m o 2 log p be a natural number then there exists a lipschitz map f rn rm such that 1 xi xj 22 f xi f xj 22 1 xi xj 22 1 for all i j 1 2 p here 2 stands for the euclidean norm in rn or rm respectively as shown in 5 the bound for the size ofm is tight up to an o log 1 factor in the original paper of johnson and lindenstrauss it was shown that a random orthogonal projection suitably normalized provides such an embedding with high probability 28 later this property was also verified for gaussian random matrices among other random matrix constructions 21 15 as a consequence the jl lemma has become a valuable tool for dimensionality reduction in a myriad of applications ranging from computer science 26 numerical linear algebra 36 23 18 manifold learning 6 and compressed sensing 7 40 10 in most of these frameworks the map f under consideration is a linear map represented by an m n matrix in this case one can consider the set of differences e xi xj to prove the theorem one then needs to show that 1 y 22 y 22 1 y 22 for all y e 2 hausdorff center for mathematics universita t bonn bonn germany courant institute of mathematical science new york university new york ny usa 1 http arxiv org abs 1009 0744 v 4 when is a random matrix the proof that satisfies the jl lemma with high probability boils down to showing a concentration inequality of the type p 1 x 22 x 22 1 x 22 1 2 exp c 0 2 m 3 for an arbitrary fixed x rn where c 0 is an absolute constant in the optimal case and in addition possibly mildly dependent on n in almost optimal scenarios as for example in 3 indeed it directly follows by a union bound over e as in the proof of theorem 3 1 below that 2 holds with high probability in order to reduce storage space and implementation time of such embeddings the design of struc tured random jl embeddings has been an active area of research in recent years 4 37 3 29 see 4 or 29 for a good overview of these efforts of particular importance in this context is whether fast i e o n log n multiplication algorithms are available for the resulting matrices fast jl embeddings with optimal embedding dimension m o 2 log p were first constructed by ailon and chazelle 1 but their embeddings are fast only for p en 1 3 vectors this restriction on the number of vec tors was later weakened to p en 1 2 2 in 3 fast jl embeddings were constructed without any restrictions on the number of vectors but the authors only provide sub optimal embedding dimension m o 4 log p log 4 n in this paper we provide the first unrestricted fast jl construction with optimal embedding dimension up to logarithmic factors in n note that in the range p en 1 2 not covered by the constructions in 1 2 a logarithmic factor in n is bounded by log log p and thus plays a minor role the johnson lindenstrauss lemma in compressed sensing one of the more recent applications of the johnson lindenstrauss lemma is to the area of compressed sensing which is centered around the following phenomenon for many underdetermined systems of linear equations x y the solution of minimal 1 norm is also the sparsest solution to be precise a vector x rn is k sparse if j xj 0 k a by now classical sufficient condition on the matrix for guaranteeing equivalence between the minimal 1 norm solution and sparsest solution is the so called restricted isometry property rip 11 13 17 definition 1 2 a matrix rm n is said to have the restricted isometry property of order k and level 0 1 equivalently k rip if 1 x 22 x 22 1 x 22 for all k sparse x rn 4 the restricted isometry constant k is defined as the smallest value of for which 4 holds in particular if has 2 k 2 k rip with 2 k 2 3 7 4 4627 and if y x admits a k sparse solution x then x argmin z y z 1 19 gaussian and bernoulli random matrices have k rip with high probability if the embedding dimension m 2 k log n k 7 up to the constant lower bounds for gelfand widths of 1 balls 22 20 show that this dependence on n and in k is optimal the restricted isometry property also holds for a rich class of structured random matrices where usually the best known bounds for m have additional log factors in n all known deterministic constructions of rip matrices require that m k 2 or at least m k 2 for some small constant 0 9 the similarity between the expressions in 2 and 4 suggests a connection between the jl lemma and the restricted isometry property a first result in this direction was established in 7 wherein it was shown that random matrices satisfying a concentration inequality of type 3 and hence the jl lemma satisfy the rip of optimal order more precisely the authors prove the following theorem theorem 1 3 theorem 5 2 in 7 suppose that m n and 0 1 are given if the probability distribution generating the m n matrices satisfies the concentration inequality 3 with and absolute constant c 0 then there exist absolute constants c 1 c 2 such that with probability 1 2 e c 2 2 m the rip 4 holds for with the prescribed and any k c 1 2 m log n k in this sense the jl lemma implies the restricted isometry property 2 contribution of this work we prove a converse result to theorem 1 3 we show that rip matrices with randomized column signs provide johnson lindenstrauss embeddings that are optimal up to logarithmic factors in the ambient dimension in particular rip matrices of optimal order provide johnson lindenstrauss embeddings of optimal order as such up to a logarithmic factor in n see theorem 3 1 note that without randomization such a converse is impossible as vectors in the null space of the fixed parent matrix are always mapped to zero this observation has several consequences in the area of compressed sensing and also allows us to obtain improved jl embedding results for several matrix constructions with existing rip bounds 13 35 31 38 33 of particular interest is the random partial fourier or the random partial hadamard matrix which is formed by choosing a random subset of m rows from the n n discrete fourier or hadamard matrix respectively and with high probability has k rip if the embedding dimension m 2 k log 4 n for these matrices with randomized column signs the running time for matrix vector multiplication is o n log n as opposed to the running time of o nm for purely random ma trices for such constructions the previous best known embedding dimension to ensure that 2 holds with probability 1 given by ailon and liberty 3 is m 4 log p log 4 n we can improve their result to have optimal dependence on the distortion showing that m 2 log p log 4 n rows suffice for the embedding this paper is structured as follows section 2 introduces necessary notation in section 3 we state our main results and section 4 gives concrete examples of how these results improve on the best known jl bounds for several matrix constructions as well as applications of our findings in compressed sensing in section 5 we give the relevant concentration inequalities and explicit rip based matrix inequalities that are needed for the proofs which are then carried out in section 6 2 notation before continuing let us fix some notation to be used in the remainder for n n we denote n 1 n the p norm of a vector x x 1 xn rn is defined as x p n j 1 xj p 1 p 1 p and x maxj 1 n xj as usual for a matrix j rm n its operator norm is sup x 2 1 x 2 and its frobenius norm is defined by f m j 1 n 1 j 2 1 2 for two functions f g s r s an arbitrary set we write f g if there is a constant c 0 such that f x cg x for all x s we write f g if f g and g f let n and s n be given and set r n s for given x x 1 xn rn we say that x is in decreasing arrangement if one has xi xj for i j for vectors in decreasing arrangement we decompose x x 1 x j x r into blocks of size s k 2 i e x j rs the last block x r is potentially of smaller size we will also consider the coarse decomposition x x 1 x where x x 2 x r rn s denote by l the indices corresponding to the l th block for j n we write j l if the two indices are associated to the same block and we write j otherwise given a matrix rm n write j to denote the j th column j rm s to denote the matrix that is the restriction of to the s columns indexed by j again with the obvious modification for j r and to denote the restriction of to all but the first k columns finally for a vector x rn we denote by dx di j rn n the diagonal matrix satisfying dj j xj 3 the main results theorem 3 1 fix 0 and 0 1 and consider a finite set e rn of cardinality e p set k 40 log 4 p and suppose that rm n satisfies the restricted isometry property of order k and 3 level 4 let rn be a rademacher sequence i e uniformly distributed on 1 1 n then with probability exceeding 1 1 x 22 d x 22 1 x 22 5 uniformly for all x e along the way our method provides a direct converse to theorem 1 3 proposition 3 2 fix 0 1 and suppose that there is a constant c 3 such that for all pairs k m that are admissible in the sense that k c 3 2 m log n k m rm n has the restricted isometry property of order k and level 4 fix x rn and let rn be a rademacher sequence i e uniformly distributed on 1 1 n then there exists a constant c 4 such that for all m d satisfies the concentration inequality 3 for c 0 c 4 log 1 n k where k is any integer such that k m is admissible 4 concrete examples and applications using theorem 3 1 we can improve on the best johnson lindenstrauss bounds for several matrix constructions that are known to have the restricted isometry property 1 matrices arising from bounded orthonormal systems consider an orthonormal sys tem of real valued functions j j n on a measurable space s with respect to an orthogonalization measure d such systems are called bounded orthonormal systems if supj n supx s j x k for some constant k 1 we may associate to such a system the m n matrix with entries j 1 m j x where x m are drawn independently according to the orthogonalization measure d as shown in 13 35 31 matrices arising as such have k rip with high probability if m 2 k log 4 n by theorem 3 1 these embeddings with randomized column signs satisfy the jl lemma for m 2 log p log n which is optimal up to the log n factors 1 for measures with discrete support such constructions are equivalent to choosingm rows at random from an n n matrix with orthonormal rows and uniformly bounded entries examples include the random partial fourier matrix or random partial hadamard matrix formed from the discrete fourier matrix or discrete hadamard matrix respectively in the fourier case we distribute the resulting real and complex parts in different coordinates inducing an additional factor of 2 note that the structure of these matrices allows for fast matrix vector multiplication recently ailon and liberty 3 verified the jl lemma for such constructions with column signs randomized when m 4 log p log 4 n our result improves the factor of 4 in their result to the optimal dependence 2 we note that while their proof also uses the rip it also requires arguments from 35 that are specific to discrete bounded orthonormal systems examples of bounded orthonormal systems connected to continuous measures include the trigono metric polynomials and chebyshev polynomials which are orthogonal with respect to the uniform and chebyshev measures respectively the legendre system while not uniformly bounded can still be transformed via preconditioning to a bounded orthonormal system with respect to the chebyshev measure 33 note that all of these constructions have an associated fast transform 2 partial circulant matrices other classes of structured random matrices known to have the rip include partial circulant matrices 34 30 32 in one such set up the first row of the n n matrix is a gaussian or rademacher random vector and each subsequent row is created by rotating one element to the right relative to the preceding row vector again m rows of this matrix are sampled but in contrast to partial fourier or hadamard matrices the selection need not be random using that convolution corresponds to multiplication in the fourier domain these matrices have associated fast matrix vector multiplication routines in 32 such matrices were shown to have the rip with high probability for m max 1 k 3 2 log 3 2 n 2 k log 4 n 1 actually the bounds in 31 yield thatm 2 k log 3 k log 2 n is sufficient for to have k rip with high probability hence d is a jl embedding for m 2 log p log 3 log p log n however in order to work with simpler expressions we bound k n in the logarithmic factors 4 on the other hand such a matrix composed with a diagonal matrix of random signs was shown to be a jl embedding with high probability as long as m 2 log 2 p 39 through theorem 3 1 the same results also obtain if m max 1 log 3 2 4 p log 3 2 n 2 log 4 p log 4 n for large p this is an improvement compared to 39 3 deterministic constructions several deterministic constructions of rip matrices are known including a recent result in 9 that requires only m k 2 we refer the reader to the exposi tion in 9 for a good overview in this direction we highlight two such deterministic constructions here using finite fields devore 16 provides deterministic constructs of cyclic 0 1 valued matrices with k rip with m 2 k 2 log 2 n iwen 27 provides deterministic constructions of 0 1 valued ma trices whose number theoretic properties allow their products with discrete fourier transform dft matrices to be well approximated using a few highly sparse matrix multiplications both the binary valued matrices and their products with the dft yield k rip matrices with m 2 k 2 log 2 n by theorem 3 1 the class of matrices that results by randomizing the column signs of either of these deterministic constructions satisfies the jl lemma with m 2 log 2 p log 2 n note that the amount of randomness needed to construct such embeddings is still comparable to the first two examples requiring n random bits under the model assumption that the entries of each vector x e to be embedded has random signs however the required randomness in the matrix is removed completely in addition to their fast multiplication properties these examples have the advantage in that the construction of the matrix embedding only uses n m 2 n m and n independent random bits respectively compared to mn bits for matrices with independent entries we note that stronger embedding results are known with fewer bits if one imposes restrictions on the norm of the vectors x e to be embedded see 29 and 14 for each of the aforementioned examples we summarize the number of dimensionsm that are known to be sufficient k rip to hold we also list the previously best known bound for jl embedding dimension if there is one along with the jl bounds obtained from theorem 3 1 where theorem 3 1 yields a better bound than previously known at least for some range of parameters we highlight the result in bold face in each of the bounds we list only the dependence on k and n or k and n omitting absolute constants rip bounds previous jl bound jl bound from theorem 3 1 partial fourier 2 k log 4 n 4 log p log 4 n 2 log p log 4 n partial circulant max 1 k 3 2 log 3 2 n 2 log 2 p max 1 log 3 2 p log 3 2 n 2 k log 4 n 2 log p log 4 n deterministic 2 k 2 log 2 n 2 log 2 p log 2 n devore iwen subgaussian 2 k log n k 2 log p 2 log p log n 4 compressed sensing in redundant dictionaries as shown recently in 10 concentra tion inequalities of type 3 allow for the extension of the compressed sensing methodology to redundant dictionaries in particular tight frames as opposed to orthonormal bases only since signals with 5 sparse representations in redundant dictionaries comprise a much more realistic model of nature this extension of compressed sensing is fundamental our results show that basically all random matrix constructions arising in the standard theory of compressed sensing i e based on rip estimates also yield compressed sensing matrices for the redundant framework 5 compressed sensing with cross validation compressed sensing algorithms are designed to recover approximately sparse signals if this assumption is violated they may yield solutions far from the input signal in 40 a method of cross validation is introduced to detect such situations and to obtain tight bounds on the error incurred by compressed sensing reconstruction algorithms in general there a subset y 1 1 x of themmeasurements y x are held out from the reconstruction algorithm and only the remaining measurements y 2 2 x are used to produce a candidate approximation x to the unknown x if the hold out matrix 1 satisfies the johnson lindenstrauss lemma then the observable quantity 1 x x 2 can be used as a reliable proxy for the unknown error x x 2 our work shows that any rip matrix as in the standard compressed sensing framework can be used for cross validation up to a randomization of its column signs 6 optimal asymptotics in for rip to hold as mentioned above it can be shown using a gelfand width argument that m k log n k is the optimal asymptotics in n and k of the embedding dimension for a matrix with the restricted isometry property 4 our results combined with the known optimality of the asymptotics m 2 log p for the embedding dimension in the johnson lindenstrauss lemma 1 1 imply that up to a factor of log 1 m 2 is the optimal asymptotics in the restricted isometry constant for fixed n and k as 0 recall that this rate is realized by many of the above examples such as gaussian random matrices 5 proof ingredients the proof of theorem 3 1 relies on concentration inequalities for rademacher sequences and explicit rip based norm estimates the first concentration result is a classical inequality by hoeffding 25 proposition 5 1 hoeffding s inequality let x rn and let j nj 1 be a rademacher sequence then for any t 0 p j jxj t 2 exp t 2 2 x 22 6 the second concentration of measure result is a deviation bound for rademacher chaos there are many such bounds in the literature the following inequality dates back to 24 but appeared with explicit constants and with a much simplified proof as theorem 17 in 8 proposition 5 2 let x be the n n matrix with entries xi j and assume that xi i 0 for all i n let j n j 1 be a rademacher sequence then for any t 0 p i j i jxi j t 2 exp 1 64 min 96 65 t x t 2 x 2 f 7 we also need the following basic estimate for rip matrices see for instance proposition 2 5 in 31 proposition 5 3 suppose that rm n has the restricted isometry property of order 2 s and level then for any two disjoint subsets j l n of size j s l s j l the proof of our norm estimate for rip matrices uses proposition 5 3 and relies on the observation commonly used in the theory of compressed sensing see for example 12 that for z in decreasing arrangement and z 2 1 for j 2 one has z j 1 s z j 1 2 and thus z 1 s 6 proposition 5 4 let r n s let j 1 2 r 1 rm n have the 2 s restricted isometry property let x xj x 1 x 2 x r x 1 x rn be in decreasing arrangement with x 2 1 and consider the symmetric matrix c rn n cj xj j x j j s 0 else and for b 1 1 s the vector v rn v dx 1 dx 1 b the following bounds hold c s c f s and v 2 s proof c sup y 2 1 y cy sup y 2 1 r j l 2 j 6 l y j dx j j l dx l y l sup y 2 1 r j l 2 j 6 l y j 2 y l 2 dx j j l dx l sup y 2 1 r j l 2 j 6 l y j 2 y l 2 x j x l 8 sup y 2 1 r j l 2 y j 2 y l 2 1 s x j 1 2 1 s x l 1 2 sup y 2 1 s r j l 2 1 2 x j 1 22 1 2 y j 22 1 2 x l 1 22 1 2 y l 22 9 s to obtain 9 we use the inequality of arithmetic and geometric means to obtain 8 we use propo sition 5 3 similarly v 2 sup y 2 1 r l 2 y l d x l l 1 d b x 1 sup y 2 1 r l 2 y l 2 x l l 1 b x 1 2 sup y 2 1 r l 2 y l 2 1 s x l 1 2 l 1 b s sup y 2 1 r l 2 1 2 y l 22 1 2 x l 1 22 s 7 for the frobenius norm we estimate c 2 f n j l s 1 j xj j x 2 r l 2 n j s 1 j l x 2 j j l d 2 x l l j r l 2 n j s 1 j l x 2 j dx l l j 2 r l 2 n j s 1 j l x 2 j x l 2 l j 2 r l 2 2 s x l 1 22 n j 1 x 2 j 2 s 6 proof of the main results we begin by proving theorem 3 1 without loss of generality we assume that all x e are normalized so that x 2 1 furthermore assume that k 2 s is even we first consider a fixed x e eventually taking a union bound over all x we further assume that x is in decreasing arrangement to achieve this we reorder the entries of x and permute the columns of accordingly this has no impact on the following estimates as the restricted isometry property of the matrix is invariant under permutations of its columns we need to estimate d x 22 dx 22 r j 1 j dx j j 2 2 2 1 dx 1 1 dx r j l 2 j 6 l j dx j j l dx l l 10 we will bound the terms separately 1 as has the restricted isometry property of order k s and level it also has the rip of order s and level and each j is almost an isometry hence noting that dx j j 2 d j x j 2 x j 2 the first term can be estimated as follows 1 x 22 r j 1 j dx j j 2 2 1 x 22 thus using that 4 1 4 x 22 r j 1 j dx j j 2 2 1 4 x 22 2 to estimate the second term fix 1 b and consider the random variable x b dx 1 1 dx v 8 with v as in proposition 5 4 by hoeffding s inequality proposition 5 1 combined with propo sition 5 4 p x 2 exp s 2 2 2 2 11 taking a union bound one obtains p x e x exp log p log 2 2 s 2 2 2 in order for this probability to be less than 2 we need log 2 p s 2 2 2 2 log 2 that is 4 8 2 s log 4 p 12 3 we can rewrite the third term as r j l 2 j 6 l j dx j j l dx l l c n j s 1 j cj where c rn n is the matrix as in proposition 5 4 by proposition 5 4 we have c s and c f s hence by proposition 5 2 p n j s 1 j cj 2 exp 1 64 min s 2 2 2 96 s 65 13 using a union bound one obtains p x e n j s 1 j cjl 2 exp log p 1 64 smin 2 2 2 96 65 in order for this probability to be less than 2 we need log 2 p 1 64 smin 2 2 2 96 65 log 2 that is 4 min 2 s 4 log 4 p 96 65 s 16 log 4 p 14 by assumption 4 so conditions 12 and 14 are satisfied by setting 55 1 and s 20 log 4 p that is k 2 s 40 log 4 p then the second term is bounded by 2 in absolute value and the last term is bounded by 55 together with the deterministic rip based estimate for the first term this implies the theorem 9 proof of proposition 3 2 fix 0 and suppose that there is a constant c 3 such that for all pairs k m with k c 3 2 m log n k m rm n has the restricted isometry property of order k and level 4 now let k m be admissible an elementary monotonicity argument shows that there exists k k such that k m is admissible and k 1 2 c 3 2 m log n k fix x rn and let rn be a rademacher sequence then for any fixed vector x rn the estimates in equations 11 and 13 with parameters 55 and 1 imply the existence of a constant c 5 1 for which p d x 22 x 22 x 22 2 exp c 5 k 2 exp c 4 2 m log 1 n k 15 where c 4 c 5 c 3 32 remarks although we have stated the main result for the setting x rn and rm n all of the analysis holds also in the complex setting x cn and cm n as shown in 7 a random matrix whose entries follow a subgaussian distribution is known to have with high probability the restricted isometry property of best possible order that is one can choose m 2 k log n k when k 40 log 4 p is a jl embedding by theorem 3 1 and our resulting bound for m is optimal up to a single logarithmic factor in n this shows that theorem 3 1 must also be optimal up to a single logarithmic factor in n acknowledgments the authors would like to thank holger rauhut deanna needell jan vyb ral mark tygert mark iwen justin romberg mark davenport and arie israel for valuable discussions on this topic rachel ward gratefully acknowledges the partial support of national science foundation postdoctoral research fellowship felix krahmer gratefully acknowledges the partial support of the hausdorff center for mathematics finally both authors are grateful for the support of the institute of advanced study through the park city math institute where this project was initiated references 1 n ailon and b chazelle approximate nearest neighbors and the fast johnson lindenstrauss transform stoc proceedings of the thirty eighth annual acm symposium on theory of comput ing 2006 2 n ailon and e liberty fast dimension reduction using rademacher series on dual bch codes soda 08 proceedings of the nineteenth annual acm siam symposium on discrete algorithms pages 1 9 2008 3 n ailon and e liberty almost optimal unrestricted fast johnson lindenstrauss transform symposium on discrete algorithms soda to appear 2011 4 n ailon e liberty and a singer dense fast random projections and lean walsh transforms proceedings of the 12 th international workshop on randomization and computation random pages 512 522 2008 5 n alon problems and results in extremal combinatorics discrete math 273 31 53 2003 6 r baraniuk and m wakin random projections of smooth manifolds in foundations of com putational mathematics pages 941 944 2006 7 r g baraniuk m davenport r a devore and m wakin a simple proof of the restricted isometry property for random matrices constr approx 28 3 253 263 2008 8 s boucheron g lugosi and p massart concentration inequalities using the entropy method ann probab 31 3 1583 1614 2003 9 j bourgain s dilworth k ford s konyagin and d kutzarova explicit constructions of rip matrices and related problems preprint 2010 10 e cande s y eldar and d needell compressed sensing with coherent and redundant dictio naries appl comput harmon anal to appear 2011 10 11 e j cande s j t tao and j romberg robust uncertainty principles exact signal reconstruc tion from highly incomplete frequency information ieee trans inform theory 52 2 489 509 2006 12 e j cande s j romberg and t tao stable signal recovery from incomplete and inaccurate measurements comm pure appl math 59 8 1207 1223 2006 13 e j cande s and t tao near optimal signal recovery from random projections universal encoding strategies ieee trans inform theory 52 12 5406 5425 2006 14 a dasgupta r kumar and t sarlos a sparse johnson lindenstrauss transform stoc page 341350 2010 15 s dasgupta and a gupta an elementary proof of a theorem of johnson and lindenstrauss random structures and algorithms 22 60 65 2003 16 r devore deterministic constructions of compressed sensing matrices j complexity 23 918 925 2007 17 d l donoho for most large underdetermined systems of linear equations the minimal 1 solution is also the sparsest solution commun pure appl anal 59 6 797 829 2006 18 edo liberty franco woolfe per gunnar martinsson vladimir rokhlin and mark tygert ran domized algorithms for the low rank approximation of matrices proceedings of the national academy of sciences 104 51 20167 20172 2007 19 s foucart a note on guaranteed sparse recovery via 1 minimization appl comput harmon anal 29 1 97 103 2010 20 s foucart a pajor h rauhut and t ullrich the gelfand widths of lp balls for 0 p 1 j complexity 26 629 640 2010 21 p frankl and h maehara the johnson lindenstrauss lemma and the sphericity of some graphs journal of combinatorial theory b 44 355 362 1988 22 a garnaev and e gluskin the widths of euclidean balls doklady an sssr 277 1048 1052 1984 23 n halko p martinsson and j tropp finding structure with randomness stochastic algorithms for constructing approximate matrix decompositions siam rev survey and review section to appear 2011 24 d l hanson and f t wright a bound on tail probabilities for quadratic forms in independent random variables ann math statist 42 1079 1083 1971 25 w hoeffding probability inequalities for sums of bounded random variables j amer statist assoc 58 13 30 1963 26 p indyk algorithmic applications of low distortion embeddings proc 42 nd ieee symposium on foundations of computer science 2001 27 m iwen simple deterministically constructible rip matrices with sublinear fourier sampling requirements information sciences and systems 2009 ciss 2009 43 rd annual conference on pages 870 875 2009 28 w b johnson and j lindenstrauss extensions of lipschitz mappings into a hilbert space contemp math 26 189 206 1984 29 d kane and j nelson a derandomized sparse johnson lindenstrauss transform preprint 2010 30 h rauhut circulant and toeplitz matrices in compressed sensing in proc spars 09 saint malo france 2009 31 h rauhut compressive sensing and structured random matrices in m fornasier editor theo retical foundations and numerical methods for sparse recovery volume 9 of radon series comp appl math pages 1 92 degruyter 2010 32 h rauhut j romberg and j tropp restricted isometries for partial random circulant matrices preprint 2010 33 h rauhut and r ward sparse legendre expansions via 1 minimization preprint 2010 11 34 j romberg compressive sensing by random convolution siam journal on imaging sciences 2 4 1098 1128 2009 35 m rudelson and r vershynin on sparse reconstruction from fourier and gaussian measure ments comm pure appl math 61 1025 1045 2008 36 t sarlos improved approximation algorithms for large matrices via random projections pro ceedings of the 47 th ieee symposium on foundations of computer science focs 2006 37 thong do lu gan yi chen nam nguyen and trac tran fast and efficient dimensionality reduction using structurally random matrices proc of icassp 2009 38 j tropp j laska m duarte j romberg and r g baraniuk beyond nyquist efficient sampling of sparse bandlimited signals ieee trans inform theory 56 1 520 544 2010 39 j vyb ral a variant of the johnson lindenstrauss lemma for circulant matrices journal of functional analysis 260 4 1096 1105 2011 40 r ward compressed sensing with cross validation ieee trans inform theory 55 5773 5782 2009 12 1 introduction 2 notation 3 the main results 4 concrete examples and applications 5 proof ingredients 6 proof of the main results