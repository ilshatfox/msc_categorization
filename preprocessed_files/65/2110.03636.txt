a hybrid direct iterative method for solving kkt linear systems shaked regeva nai yuan chiangb eric darvea cosmin g petrab michael a saundersa kasia s wirydowiczc and slaven peles c astanford university 450 serra mall stanford california usa blawrence livermore national laboratory 7000 east ave livermore california usa cpacific northwest national laboratory 902 battelle blvd richland washington usa article history compiled october 8 2021 abstract we propose a solution strategy for linear systems arising in interior method op timization which is suitable for implementation on hardware accelerators such as graphical processing units gpus the current gold standard for solving these sys tems is the ldlt factorization however ldlt requires pivoting during factoriza tion which substantially increases communication cost and degrades performance on gpus our novel approach solves a large indefinite system by solving multiple smaller positive definite systems using an iterative solve for the schur complement and an inner direct solve via cholesky factorization within each iteration cholesky is stable without pivoting thereby reducing communication and allowing reuse of the symbolic factorization we demonstrate the practicality of our approach and show that on large systems it can efficiently utilize gpus and outperform ldlt factorization of the full system keywords optimization interior methods kkt systems sparse matrix factorization gpu 1 introduction interior methods for nonlinear optimization 8 18 49 52 are essential in many areas of science and engineering they are commonly used in model predictive control with applications to robotics 44 autonomous cars 53 aerial vehicles 22 combustion engines 23 and heating ventilation and air conditioning systems 26 to name a few interior methods are also used in public health policy strategy development 4 43 data fitting in physics 37 genome science 5 50 and many other areas most of the computational cost within interior methods is in solving linear systems of karush kuhn tucker kkt type 30 the linear systems are sparse symmetric indefinite and usually ill conditioned and difficult to solve furthermore implemen tations of interior methods for nonlinear optimization such as the filter line search approach in ipopt 52 and hiop 34 typically expect the linear solver to provide the matrix inertia number of positive negative and zero eigenvalues to determine if the system should be regularized otherwise interior methods perform curvature tests to ensure descent in a certain merit function 11 relatively few linear solvers contact shaked regev email sregev stanford edu ar x iv 2 11 0 03 63 6 v 1 m at h o c 7 o ct 2 02 1 are equipped to solve kkt systems and even fewer to run those computations on hardware accelerators such as graphic processing units gpus 46 at the time of writing six out of the ten most powerful computers in the world have more than 90 of their compute power in hardware accelerators 3 hardware accelerator technologies are becoming ubiquitous in off the shelf products as well in order to take advantage of these emerging technologies it is necessary to develop fine grain parallelization techniques tailored for high throughput devices such as gpus for such sparse problems pivoting becomes extremely expensive as data man agement takes a large fraction of the total time compared to computation 15 un fortunately ldlt factorization is unstable without pivoting this is why ldlt ap proaches typically used by interior methods for nonlinear problems on cpu based platforms 47 have not performed as well as on hardware accelerators 46 some iter ative methods such as minres 31 for general symmetric matrices can make efficient albeit memory bandwidth limited use of gpus because they only require matrix vector multiplications at each iteration which can be highly optimized but they have limited efficiency when the number of iterations becomes large another approach for better conditioned kkt systems is using a modified version of the preconditioned con jugate gradient pcg with implicit factorization preconditioning 13 in our case the ill conditioned nature of our linear problems means that iterative methods alone are not practical 31 36 we propose a hybrid direct iterative method for solving kkt systems that is suit able for execution on hardware accelerators the method only requires direct solves using a cholesky factorization as opposed to ldlt which means it avoids pivot ing we provide preliminary test results that show the practicality of our approach our test cases are generated by optimal power flow analysis 9 24 28 applied to realistic power grid models that resemble actual grids but do not contain any propri etary data 27 these systems are extracted from optimal power flow analysis using ipopt 52 with ma 57 as its linear solver solving such sequences of linear problems gives us an insight in how our linear solver behaves within an interior method using these test cases allowed us to assess the practicality of our hybrid approach without interfacing the linear solver with an optimization solver power grids are representative of very sparse and irregular systems commonly found in engineering disciplines the paper is organized as follows table 1 defines our notations section 2 describes the optimization problem being solved section 3 defines the linear systems that arise when an interior method is applied to the optimization problem in section 4 we derive a novel hybrid direct iterative algorithm to utilize the block structure of the linear systems and prove convergence properties for the algorithm numerical tests in section 5 show the accuracy of our algorithm on realistic systems using a range of algorithmic parameter values section 6 compares our c and cuda implementation to ma 57 14 section 7 explains our decision to use a direct solver in the inner loop of our algorithm in section 8 we summarize our main contributions and results appendix a provides supplemental figures for section 5 2 table 1 notation sp s d stands for symmetric positive semi definite variable properties functions meaning m symmetric matrix max m min m largest smallest most negative eigenvalues m spsd matrix min m the smallest nonzero eigenvalue m spd matrix m max m min m condition number j rectangular matrix null j nullspace x vector x 0 x diag x a diagonal matrix x xii xi ep vector a p vector of 1 s 2 nonlinear optimization problem we consider constrained nonlinear optimization problems of the form min x rnx f x 1 a s t c x 0 1 b d x 0 1 c x 0 1 d where x is an nx vector of optimization parameters f rnx r is a possibly nonconvex objective function c rnx rmc defines mc equality constraints and d rnx rmd defines md inequality constraints problems with more general inequalities can be treated in the same way functions f x c x and d x are assumed to be twice continuous differentiable interior methods enforce bound constraints 1 d by adding barrier functions to the objective 1 a min x rnx s rmd f x nx j 1 lnxj md i 1 ln si where the inequality constraints 1 c are treated as equality constraints d x s 0 with slack variables s 0 the barrier parameter 0 is reduced toward zero using a continuation method to obtain solution that is close to the solution of 1 to within a solver tolerance interior methods are most effective when exact first and second derivatives are available as we assume for f x c x and d x we define j x c x and jd x d x as the sparse jacobians for the constraints the solution of a barrier subproblem satisfies the nonlinear equations f x jt y jtd yd zx 0 2 a yd zs 0 2 b c x 0 2 c d x s 0 2 d xzx enx 0 2 e szs emd 0 2 f where x and s are primal variables y and yd are lagrange multipliers dual variables for constraints 2 c 2 d and zx and zs are lagrange multipliers for the bounds x 0 3 and s 0 the conditions x 0 s 0 zx 0 and zs 0 are maintained throughout and the matrices x diag x and s diag s are spd analogously to 52 at each continuation step in we solve nonlinear equations 2 using a variant of newton s method typically zx and zs are eliminated from the linearized version of 2 by substituting the linearized versions of 2 e and 2 f into the linearized versions of 2 a and 2 b respectively to obtain a smaller symmetric problem newton s method then calls the linear solver to solve a series of linearized systems kk xk rk k 1 2 of block 4 4 form kk h dx 0 j t jtd 0 ds 0 i j 0 0 0 jd i 0 0 xk x s y yd rk r x rs ry ryd 3 where index k denotes optimization solver iteration including continuation step in and newton iterations each kk is a kkt type matrix with saddle point structure vector xk is a search direction 1 for the primal and dual variables and rk is derived from the residual vector for 2 evaluated at the current value of the primal and dual variables with rk 0 as the method converges r x f x jt y jtd yd x 1 enx rs yd s 1 emd ry c x ryd s d x with zx diag zx and zs diag zs the sparse hessian h 2 f x mc i 1 yc i 2 ci x md i 1 yd i 2 di x and diagonal dx x 1 zx are nx nx matrices ds s 1 zs is a diagonal md md matrix j is a sparse mc nx matrix and jd is a sparse md nx matrix we define m mc md n nx md and n m n interior methods may take hundreds of iterations but typically not thousands before they converge to a solution all kk matrices have the same sparsity pattern and their nonzero entries change slowly with k an interior method can exploit this by reusing output from linear solver functions across multiple iterations k ordering and symbolic factorization are needed only once because the sparsity pattern is the same for all kk numerical factorizations can be reused over several adjacent newton s iterations e g when an inexact newton solver is used within the optimization algorithm operations such as triangular solves have to be executed at each iteration k the workflow of the optimization solver with calls to different linear solver functions is shown in fig 1 where kk xk rk denotes the linear system to be solved at each iteration the main optimization solver loop is the top feedback loop in fig 1 it is repeated until the solution is optimal or a limit on optimization iterations is reached at each iteration the residual vector rk is updated advanced implementations have 1 search directions are defined such that xk 1 xk x for some linesearch steplength 0 4 figure 1 optimization solver workflow showing invocation of key linear solver functions the top feedback loop represents the main optimization solver iteration loop the bottom feedback loop is the optimization solver control mechanism to regularize the underlying problem when necessary control features to ensure stability and convergence of the optimization solver the lower feedback loop in fig 1 shows linear system regularization by adding a diago nal perturbation to the kkt matrix one such perturbation removes singularity 52 sec 3 1 which happens if there are redundant constraints the linear solver could take advantage of algorithm control like this and request matrix perturbation when beneficial 3 solving kkt linear systems ldlt factorization via ma 57 14 has been used effectively for extremely sparse prob lems on traditional cpu based platforms but is not suitable for fine grain paral lelization required for gpu acceleration parallel and gpu accelerated direct solve implementations such as superlu 1 25 strumpack 39 45 and pastix 20 32 exist for general symmetric indefinite systems although the first two are designed for general systems but these software packages are designed to take advantage of dense blocks of the matrices in denser problems and do not perform well on our systems of interest which do not yield these dense blocks 19 46 the fundamental issue with using gpus for ldlt is that this factorization is not stable without pivoting 17 pivoting requires considerable data movement and as a result a substantial part of the run time is devoted to memory access and communica tion any gains from the hardware acceleration of floating point operations are usually outweighed by the overhead associated with permuting the system matrix during the pivoting this is especially burdensome because both rows and columns need to be permuted in order to preserve symmetry 15 while any of the two permutations can be performed efficiently on its own with an appropriate data structure for the system s sparse matrix i e row compressed sparse storage for row permutations and analo gously column compressed sparse storage for column permutations swapping both rows and columns simultaneously is necessarily costly here we propose a method that uses sparse cholesky factorization in particular a gpu implementation cholesky factorization is advantageous for a gpu implemen tation because it is stable without pivoting and can use gpus efficiently compared to ldlt 38 furthermore the ordering of the unknowns also known as the sym bolic factorization for the purpose of reducing fill in in the factors can be established without considering the numerical values and only once at the beginning of the op timization process and hence its considerable computational cost is amortized over the optimization iterations 5 to make the problem smaller we can eliminate s jd x ryd and yd ds s rs from 3 to obtain the 2 2 system 33 sec 3 1 h jt j 0 x y rx ry h h dx jtd dsjd 4 where rx r x j t d dsryd rs this reduction requires block wise gaussian elimina tion with block pivot ds i i which is ill conditioned when ds has large elements as it ultimately does thus system 4 is smaller but more ill conditioned after solving 4 we compute s and yd in turn to obtain the solution of 3 4 a block 2 2 system solution method let q be any spd matrix multiplying the second row of 4 by jtq and adding it to the first row gives a system of the form h j t j 0 x y r x ry h h j tqj 5 where r x rx j tqry the simplest choice is q i with 0 h h j tj 6 when h is spd its schur complement s jh 1 jt is well defined and 5 is equivalent to s y jh 1 r x ry 7 h x r x jt y 8 this is the approach of golub and greif 16 for saddle point systems which have the structure of the 2 2 system in 4 golub and greif found experimentally that h j 2 made h spd and better conditioned than smaller or larger values we show in theorems 2 and 4 that for large the condition number h increases as but s converges to 1 as corollary 1 shows there is a finite value of that minimizes h this value is probably close to h j 2 our contribution is to combine the system reduction in 33 from 3 to 4 with the method of 16 for changing 4 to 5 to solve an optimization problem consisting of a series of block 4 4 systems using a gpu implementation of sparse cholesky factorization applied to h a new method for regularizing is added and practical choices for parameters are given based on scaled systems also important convergence properties of the method are proven in theorems 1 4 if 5 or h are poorly conditioned the only viable option may be to ignore the block structure in 5 and solve 3 with an ldlt factorization such as ma 57 likely without help from gpus this is the fail safe option otherwise we require h to be spd for large enough and use its cholesky factorization to apply the conjugate gradient method cg 21 or minres 31 to 7 with or without a preconditioner if 6 the r part of qr factors of jt is not too dense we could use m jjt 1 jh jt jjt 1 as a multiplicative preconditioner this gives the exact solution if the rhs is orthog onal to null j or j is square which is not possible in our case in our experiments solutions with the preconditioner m lagged slightly behind those without it but both take o 1 iterations we proceed without the use of preconditioner 4 1 a hybrid solver with minimal regularization typically 4 starts with an spd h and full rank j as the optimization solver iter ations progress h may become indefinite and j s rank may shrink at least numeri cally this means the system becomes singular and must be regularized we seek a small regularization to avoid changing too much the solution of the equivalent system 5 an spd h guarantees that h is spd on null j a requirement at the solution of the optimization problem theorem 1 for large enough min and full row rank j h h j tj is spd iff h is positive definite on null j proof assume h is spd for any nonzero vector v 0 in null j we have v t 0 h v 0 vt 0 h v 0 0 this direction of the proof does not require j to have full row rank conversely assume h is positive definite on null j and j has full row rank for any nonzero vector v v 0 v 1 with v 0 in null j and v 1 orthogonal to null j vt h jtj v vt 0 h v 0 v t 1 h j tj v 1 we have vt 0 h v 0 0 by assumption further vt 1 h j tj v 1 min h min j tj vt 1 v 1 0 if min min h min jtj we work with 0 we use h spd as a proxy for h being spsd on null j keeping in mind that if it does not hold even for a very large in 6 h is singular and needs to be modified however cannot be made arbitrarily large without increasing h when j is rectangular as in our case there must be an ideal intermediate value of theorem 2 if j has full row rank with more columns than rows h is symmetric and positive definite on null j and h h j tj there exists max max min 0 such that for max h increases linearly with 7 proof max h max v 2 1 vth v max v 2 1 vt h v max v 2 1 vt jtj v max h max j tj max h min v 2 1 vt h v max v 2 1 vt jtj v min h max j tj hence min h max j tj max h max h max jtj meaning max h for large enough defined as max max min 0 similarly min h min v 2 1 vth v min v 2 1 vt h v min v 2 1 vt jtj v min h min h max v 2 1 vt h v min v 2 1 vt jtj v max h thus min h min h max h from theorem 1 min min h 0 so that h max h min h for max corollary 1 among s such that h is spd min h is minimized for some min max in practice the optimizer may provide systems where h is not spd on null j in this case we can regularize h by using h h 1 i instead unlike the introduction of 1 changes the solution of the system so it is essential to keep 1 as small as possible if h is not spd we set 1 min a parameter for some minimum value of regularization if h is still not spd we double 1 until it is this ensures we use the minimal value of regularization to within a factor of 2 needed to make h spd if 1 proves to be large which can happen before we are close to a solution it is essential for the optimizer to be informed to allow it to modify the next linear system when the optimizer nears a solution 1 will not need to be large in our tests 1 starts at 0 for the next matrix in the sequence but is set back to its previous value if the factorization fails we also set max the maximum allowed 1 before we resort to ldlt factorization of kk or return to the optimization solver if j has low row rank s in 7 is spsd in this case cg will succeed if 7 is consistent otherwise it will encounter a near zero quadratic form we then restart cg on the regularized system jh 1 j t 2 i y jw ry in this way 1 regularizes the 1 1 block and 2 regularizes the 2 2 block to ensure we can judge the size of parameters and min relative to system 5 we first scale 5 with a symmetrized version of ruiz scaling 41 algorithm 1 is a generalization of the uzawa iteration 48 for kkt systems with a 1 1 block that is not necessarily positive definite it gives a method for solving a sequence of systems 7 8 with q i used in the calculation of h the workflow is similar to fig 1 except only h is factorized on lines 16 19 h 1 is applied by direct triangular solves with the cholesky factors l lt of h each iteration of the cg solve on line 17 requires multiplication by jt applying h 1 multiplying by j and adding a scalar multiple of the original vector section 7 shows why complete cholesky factorization of h was chosen 8 algorithm 1 using cg on schur complement to solve the block system 5 by solving 7 8 h is a nonsingular perturbation of h h j tj typical parameters 104 min 2 10 9 max 10 6 1 for each matrix in the sequence such as in 5 do 2 1 0 3 h h used in the calculation of h 4 try llt chol h fail false if factorized true otherwise 5 while fail and 1 max 2 do 6 if 1 0 then 7 1 min 8 else 9 1 min 2 min 10 end if 11 h h 1 i 12 try llt chol h fail false if factorized true otherwise 13 end while 14 if fail false then 15 direct solve h w r x 16 if cg on jh 1 j t y jw ry produces a small quadratic form then 17 cg solve jh 1 j t 2 i y jw ry perturbed 7 18 end if 19 direct solve h x r x jt y perturbed 8 20 else 21 use ldlt to solve 3 or return problem to optimization solver 22 end if 23 end for the rest of the section discusses other important bounds that must obey however selecting an ideal in practice is difficult and requires problem heuristics like h j 2 in 17 or trial and error 4 2 guaranteed descent direction optimization solvers typically use an ldlt factorization to solve the n n system 3 at each step because with minimal extra computation it supplies the inertia of the matrix a typical optimization approach treats each of the four 2 2 block of 3 as one block accounting for the possible regularization applied to h we mean that the 1 1 block hk h dx 1 i ds is a hessian of dimension n and the 2 1 block jk j jd i represents the constraint jacobian and has dimensions m n the inertia being n m 0 implies that a hk is uniformly spd on null jk for all k meaning vthkv 0 for all vectors v satisfying jkv 0 iterations k and some positive constant b kkt matrix in 3 is nonsingular together these conditions are sufficient to ensure a descent direction and fast convergence in the optimization algorithm 30 52 we show that algorithm 1 ensures properties a and b without computing the inertia of the kkt matrix theorem 3 if algorithm 1 succeeds with 2 0 it provides a descent direction for 9 the interior method for large enough proof by construction h is uniformly spd because the cholesky factorization was successful for all iterations of the solver until this point and the kkt system in 5 is nonsingular because j has full row rank or 2 i was added to s to shift it from spsd to spd therefore algorithm 1 provides a descent direction for 5 even if regularization is added the rest of the proof assumes 2 0 and we return to the other case later since j has full row rank h dx 1 i is nonsingular by assumption all block rows in 3 have full rank internally and gaussian elimination cannot cause cancellation of an entire block row we conclude that 3 is nonsingular let hk hk kjtkjk for k used in algorithm 1 for any nonzero vector ut ut 1 u t 2 with u 1 and u 2 of dimensions nx and md uthk u u t 1 h dx kj tj u 1 u t 2 dsu 2 kw tw where w jdu 1 u 2 if w 0 then u 2 jdu 1 which means uthk u ut 1 h u 1 0 and the proof is complete with k otherwise w kwtw 0 so for large enough k u thk u 0 applying theorem 1 to 3 with hk and jk replacing h and j shows that hk is positive definite on null jk we note that 1 corresponds to the so called primal regularization of the filter line search algorithm 52 under this algorithm whenever 1 becomes too large one can invoke the feasibility restoration phase of the filter line search algorithm 52 as an alternative to performing the ldlt factorization on the cpu feasibility restoration restarts the optimization at a point with more favorable numerical properties we also note that when 1 is sufficiently large the curvature test used in 11 should be satisfied hence inertia free interior methods have the global convergence property without introduction of other regularization from the outer loop the 2 regularization is a numerical remedy for low rank j caused by redundant equality constraints this regularization is similar to the so called dual regularization used in ipopt 52 and specifically addresses the issue of rank deficiency however there is no direct analogue from a 2 regularization in 5 to ipopt s dual regularization in 3 and neither of the two heuristics guarantees a descent direction for 3 given the similarity between the two heuristics we believe that the 2 regularization can be effective within the filter line search algorithm when algorithm 1 is integrated with a nonlinear optimization solver such as 51 and 35 the while loop line 5 in algorithm 1 can be removed and the computation of 1 and 2 can be decided by the optimization algorithm the development of robust heuristics that allow algorithm 1 s 1 and 2 regularization within the filter line search algorithm will be subject of future work 4 3 convergence for large in section 4 1 we showed that h is spd for large enough and that should not be so large that the low rank term jtj makes h ill conditioned here we show that in order to decrease the number of cg iterations it is beneficial to increase beyond what is needed for h to be spd theorem 4 in exact arithmetic for 1 the eigenvalues of s s converge to 1 with an error term that decays as 1 10 proof by definition s j h j tj 1 jt j h jtj 1 jt 9 since h is nonsingular and j has full row rank by assumption the searle identity a bbt 1 bt a 1 b i bta 1 b 1 42 with a h and b jt gives s jh 1 jt i jh 1 jt 1 i c 1 c 1 jh 1 jt 1 for i c 1 i the identity i c 1 k 0 1 kck which applies when i c 1 i gives s k 0 1 kck i c o 1 2 10 a different proof of an equivalent result is given by benzi and liu 6 lemma 3 1 corollary 2 the eigenvalues of s are well clustered for 1 and the iterative solve in algorithm 1 line 16 converges quickly in the next section we show that our choices of 104 106 are large enough for the arguments to hold this explains the rapid convergence of cg we also show that our transformation from 3 to 5 and our scaling choice are stable on our test cases by measuring the error for the original system 3 5 practicality demonstration we demonstrate the practicality of algorithm 1 using five series of linear problems 27 generated by the ipopt solver performing optimal power flow analysis on the power grid models summarized in table 2 we compare our results with a direct solve via ma 57 s ldlt factorization 14 table 2 characteristics of the five tested optimization problems each generating sequences of linear systems kk xk rk 3 of dimension n numbers are rounded to 3 digits k and m signify 10 3 and 106 name n kk nnz kk south carolina grid 56 k 411 k illinois grid 4 64 k 21 6 k texas grid 55 7 k 268 k us western interconnection grid 238 k 1 11 m us eastern interconnection grid 1 64 m 7 67 m 11 figure 2 illinois grid a cg iterations on eq 7 with varying 103 gives good convergence the mean number of iterations for 104 is 9 4 b sorted eigenvalues of s jh 1 jt in 9 matrix 22 for 104 the eigenvalues are clustered close to 1 figure 3 illinois grid 3 with varying in 6 a backward error be and b relative residual rr a 104 gives results close to machine precision b 104 has rr 10 8 5 1 selection we use algorithm 1 with cg as the iterative method on line 16 a larger in h h jtj may improve cg convergence but makeh more ill conditioned and increase the error in the solution of 3 we therefore run some preliminary tests to find a suitable before selecting min and testing other problems and we require cg to solve accurately stopping tolerance 10 12 for the relative residual norm we start with one of the smaller problems the illinois grid to eliminate some values of and see if the condition number of kk for each matrix in the sequence is useful in determining the needed iterations or regularization we run these tests with min 10 10 figure 2 a shows that for the illinois grid sequence values of 104 give cg convergence in approximately 10 iterations for every matrix in the sequence for the last matrix we found that eigenvalues of s in 7 are very well clustered and the condition number of s is 1 04 as shown in fig 2 b this guarantees and explains the rapid convergence because for cg applied to a general spd system ax b ek a e 0 a 2 1 1 k 11 12 figure 4 south carolina grid 1 for 10 4 for other values of the graph was similar except for the first few and last few matrices 1 meaning the required regularization would make the solution too inaccurate the value of 0 is omitted on the log scale where ek x xk is the error in an approximate solution xk at iteration k 17 for the last few matrices 108 is the only value requiring 1 0 the final value of 1 was 16 min 1 6 10 9 no important information was gleaned from cond kk for all other values of 1 0 for the whole sequence for a system ax b and an approximate solution x x we define the backward error be as ax b 2 a 2 x 2 b 2 and the relative residual rr as ax b 2 b 2 as is common practice we use a to estimate a 2 which is too expensive to calculate directly a always provides an upper bound for a 2 but in practice is quite close to the actual value note that ma 57 always has a be of order machine precision figure 3 shows the a be and b rr for system 3 for varying results for the be and rr of system 4 are not qualitatively different and are given in appendix a one conclusion is that increasing to reduce cg iterations can be costly for the accuracy of the solution of the full system based on the results of this section in the range 102 106 gives reasonable cg iterations and final accuracy for other matrices we present a selected in this range that produced the best results 5 2 results for larger matrices solving larger and perhaps more poorly conditioned problems brings about new computational challenges and limits the amount of time any particular task can take we wish to set min small enough to avoid over regularizing the problem and large enough to eliminate wasteful iterations and numerical issues we want max small enough to recognize that we have over regularized and should try a different method but large enough to allow for reasonable regularization in our numerical tests we use min 10 10 and max large enough so that 1 can increase until h h 1 i is spd this informs the parameter selection for the next system figure 4 shows that the south carolina grid matrices as currently constructed cannot benefit from this method they need 1 1 to make h 1 i spd which on a scaled problem means as much weight is given to regularization as to the actual problem algorithm 1 succeeds on the other matrix sequences at least for certain s and needs no regularization 1 2 0 for the us western interconnection grid fig 5 a shows a cg convergence graph and b shows several types of error for the us eastern interconnection grid fig 6 a 13 figure 5 us western interconnection grid with 106 in 6 a cg iterations on eq 7 the mean number of iterations is 17 b be and rr for the sequence the be for 3 is less than 10 10 except for matrix 4 figure 6 us eastern interconnection grid with 106 in 6 a cg iterations on eq 7 the mean number of iterations is 13 1 b be and rr for 3 and 4 the be for 3 is less than 10 10 shows a cg convergence graph and b shows several types of error figures for the texas grid are given in appendix a as they do not provide more qualitative insight convergence occurs for all matrix sequences in less than 20 iterations on average the be for 3 is consistently less than 10 8 and with two exceptions in the us western interconnection grid is close to machine precision results for the us eastern interconnection grid show that the method does not deteriorate with problem size but rather there are some irregular matrices in the us western interconnection grid the results in this section suggest that min in the range 10 8 down to 10 10 is rea sonable for any 108 there is no clear choice for max but a plausible value would be max 2 10 min 1000 min this way we are guaranteed that the regularization doesn t take over the problem and the number of failed factorizations is limited to 10 which should be negligible in the total solution times for a series of 100 problems 5 3 reordering h the efficiency of sparse cholesky factorization ph p t llt depends greatly on the row column ordering defined by permutation p figure 7 compares the sparsity of l corresponding to h of matrix 22 in the illinois grid sequence obtained from two 14 figure 7 illinois grid matrix 22 a approximate minimum degree ordering of chol h is sparser than b nested dissection ordering of chol h both orderings are calculated in matlab table 3 accelerator devices and compilers used machine name host processor accelerator device host compiler device compiler newell ibm power 9 nvidia volta 100 gcc 7 4 0 cuda 10 2 deception amd epyc 7502 nvidia ampere 100 gcc 7 5 0 cuda 11 1 choices of p approximate minimum degree amd and nested dissection the data in this section are generated via matlab we see that amd produces a sparser l 17 413 nonzeros vs 20 064 reverse cuthill mckee and no ordering gave 46 527 and 759 805 nonzeros respec tively recall that the sparsity structure is identical for matrices from the same family and similar for other matrix families as expected amd was the sparsest ordering tested for other matrix families thus amd is our reordering of choice it is a one time cost performed during the optimization problem setup 6 comparison with ldlt we compare our method with a direct solve of 3 using ma 57 s ldlt factoriza tion 14 with default settings all testing in this section is done using a prototype c cuda code on a single gpu device reference ma 57 solutions were computed on a cpu further details on computational platforms used are given in table 3 the factorization density is l 2 nnz l nnz d n we define c analogously for the cholesky factors of h in 5 with nnz d 0 and dimension nx note that gives the average number of nonzeros in the factorization per row or column table 4 shows that the cholesky factor of h is usually less dense than the ldlt factor for 3 even though 3 is sparser than 5 table 5 shows the solve times on the newell computing cluster 29 the main trend is that as the problem size increases gpus using cusolver 2 increasingly outperform equivalent computations on a cpu supernodal cholesky via cholmod in suitesparse 10 does not perform well on gpus for these test cases but performs better than cusolver on cpus this matches 15 table 4 the dimensions number of nonzeros and factorization densities the number of nonzeros in the factors per row for solving 3 directly with ldlt n nnzl l respectively and for solving 6 with cholesky nx nnzc c respectively numbers are rounded to 3 digits k and m signify 10 3 and 106 for all cases c l and nx n 2 abbreviation n nnzl l nx nnzc c illinois 4 64 k 94 7 k 20 4 2 28 k 34 9 k 15 3 texas 55 7 k 2 95 m 52 9 25 9 k 645 k 24 9 western us 238 k 10 7 m 44 8 116 k 2 23 m 19 2 eastern us 1 64 m 85 4 m 52 1 794 k 17 7 m 22 3 table 5 average times in seconds for solving 3 directly on a cpu with ldlt via ma 57 14 or for solving one h linear system with supernodal cholesky via cholmod cm in suitesparse 10 or cholesky via cusolver 2 cs each on a cpu and on a gpu cholesky on a gpu is quicker than ldlt on a cpu by an increasingly large ratio cm gpu does not work for small problems all runs are on newell 29 name ma 57 cm cpu cm gpu cs cpu cs gpu illinois 7 35 10 3 1 74 10 3 2 25 10 3 5 80 10 3 texas 1 24 10 1 3 42 10 2 5 67 10 2 4 79 10 2 western us 4 30 10 1 1 02 10 1 1 89 10 1 1 59 10 1 eastern us 4 34 100 1 08 100 3 65 100 2 52 100 6 12 10 1 literature showing that multifrontal or supernodal approaches are not suitable for very sparse and irregular systems where the dense blocks become too small leading to an unfavorable ratio of communication versus computation 7 12 19 this issue is exacerbated when supernodal or multifrontal approaches are used for fine grain parallelization on gpus 46 our method becomes better when the ratio of ldlt to cholesky factorization time grows because factorization is the most costly part of linear solvers and our method has more but smaller and less costly system solves table 6 compares a direct solve of 3 using ma 57 s ldlt factorization 14 and the full cg solve and direct solves on 7 8 broken down into symbolic analysis of h factorization of h and cg on 7 on deception 29 when cholesky is used symbolic analysis is needed only for the first matrix in the sequence because pivoting is not a concern as problems grow larger the solve phase becomes a smaller part of the total run time also our method increasingly outperforms ma 57 the run time is reduced by a factor of more than 2 on one matrix from the us eastern interconnection grid and more than 3 on the whole series because the analysis cost can be amortized over the entire optimization problem this motivates using our method for even larger problems another advantage of our method is that it solves systems that are less than half the size of the original one though it does have to solve more of them notably the ldlt factorization may require pivoting during the factorization whereas cholesky does not with ma 57 all our test cases required substantial permutations even with a lax pivot tolerance of 0 01 and a tolerance as large as 0 5 may be required to keep the factorization stable this means that for our systems ldlt factorization requires considerable communication and presents a major barrier for gpu programming on the other hand the direct solve is generally more accurate by 2 3 orders of magnitude 16 table 6 average times in seconds for solving sequences of systems 3 directly on a cpu with ldlt via ma 57 14 or for solving sequences of systems 7 8 on a gpu the latter is split into analysis and factorization phases and multiple solves symbolic analysis is needed only once for the whole sequence fac torization happens once for each matrix the solve phase is the total time for lines 15 17 in algorithm 1 with a cg tolerance of 10 12 on line 16 the results show that our method without optimization of the code and kernels outperforms ldlt on the largest series us eastern interconnection grid by a factor of more than 2 on a single matrix and more than 3 on a whole series because the cost of symbolic analysis can be amortized over the series all runs are on deception 29 name ma 57 hybrid direct iterative solver analysis factorization total solves illinois 6 24 10 3 3 87 10 3 5 07 10 3 8 55 10 3 texas 1 00 10 1 2 58 10 2 3 54 10 2 1 02 10 1 western us 3 38 10 1 1 54 10 1 1 74 10 1 1 43 10 1 eastern us 3 48 100 5 81 10 1 6 94 10 1 3 25 10 1 table 7 densification of the problem for cases where the direct iterative method is viable numbers are rounded to 3 digits k and m signify 103 and 106 nnzop h nnz h 2 nnz j n is the number of multiplications when h is applied as an operator and nnzfac h 2 nnz l is the number of multiplications for solving systems with h the ratio nnzfac h nnzop h is only about 2 in all cases name nnzop h nnzfac h ratio illinois 20 5 k 34 9 k 1 70 texas 249 k 646 k 2 59 western us 1 05 m 2 23 m 2 11 eastern us 7 23 m 17 7 m 2 45 7 iterative vs direct solve with h in algorithm 1 we may ask if forming h h dx jtd dsjd j tqj 1 i and its cholesky factorization h ll t is worthwhile when it could be avoided by iterative solves with h systems 7 8 require two solves with h and an iterative solve with s jh 1 j t 2 i which includes inner solves with h until cg converges as we see in section 5 between 6 and 92 solves with h are needed in our cases and possibly more in other cases further the inner iterative solves with h would degrade the accuracy compared to inner direct solves or would require an excessive number of iterations therefore for iterative solves with h to be viable the direct solve would have to cause substantial densification of the problem i e the cholesky factor l would have to be very dense let nnzop h nnz h 2 nnz j nx be the number of multiplications when h is applied as an operator and nnzfac h 2 nnz l be the number of multiplications for solving systems with h these values generated in matlab and their ratio are given in table 7 the ratio is always small and does not grow with problem size meaning l remains very sparse and the factorization is efficient as the factorization dominates the total time of a direct solve with multiple right hand sides this suggests that performing multiple inner iterative solves is not worthwhile 8 summary following the approach of golub and greif 16 we have developed a novel direct iterative method for solving saddle point systems and shown that it scales better 17 with problem size than ldlt on systems arising from optimal power flow 27 the method is tailored for execution on hardware accelerators where pivoting is difficult to implement and degrades solver performance dramatically to solve kkt systems of the form 3 algorithm 1 presents a method with an outer iterative solve and inner direct solve the method assumes h h dx j t d dsjd j tj is spd or almost for some 0 and if necessary uses the minimal amount of regularization 1 0 to within a factor of 2 to ensure h h 1 i is spd we proved that as grows large the condition number of h grows linearly with and the eigenvalues of the iteration matrix s converge to 1 10 these results provide some heuristics for choosing and explain why cg on the schur complement system 7 converges rapidly a future direction of research is developing a better method to select on several sequences of systems arising from applying an interior method to opf problems the number of cg iterations for solving 7 was less than 20 iterations on average even though no preconditioning was used four of the five matrix series were solved with 1 2 0 and the be for the original system 3 was always less than 10 8 the efficiency gained by using a cholesky factorization instead of ldlt and avoiding pivoting is demonstrated in table 4 even though h in 5 is denser than kk in 3 its factors are sparser table 6 shows that our method when it succeeds has better scalability than ldlt and is able to utilize gpus this is the most substantial result of our paper for the fifth series smaller than 2 of the others 2 0 worked but 1 had to be of order 1 and no accurate solution could be obtained the development of robust heuristics to select 1 and 2 and to integrate with the filter line search algorithm will be subject of future work the fact that the cholesky factors are scarcely denser than the original matrix suggests that not much could be gained by using nullspace methods 40 for the four sequences we were able to solve as those require sparse lu or qr factorization of jt which is typically less efficient than sparse cholesky factorization of h for the fifth sequence and sequences similar to it an efficient nullspace method may be better than the current fail safe ldlt factorization of the 4 4 system 3 acknowledgements this research was supported by the exascale computing project ecp project num ber 17 sc 20 sc a collaborative effort of two doe organizations the office of sci ence and the national nuclear security administration responsible for the planning and preparation of a capable exascale ecosystem including software applications hardware advanced system engineering and early testbed platforms to support the nation s exascale computing imperative we thank research computing at pacific northwest national laboratory pnnl for computing support we are also grateful to christopher oehmen and lori ross o neil of pnnl for critical reading of the manuscript and for providing helpful feed back finally we would like to express our gratitude to stephen thomas of national renewable energy laboratory for initiating discussion that motivated section 7 references 1 https portal nersc gov project sparse superlu 2 https docs nvidia com cuda cusolver index html 18 https portal nersc gov project sparse superlu https docs nvidia com cuda cusolver index html 3 top 500 list june 2021 https www top 500 org lists top 500 2021 06 4 d acemoglu v chernozhukov i werning and md whinston optimal targeted lock downs in a multi group sir model working paper no 27102 national bureau of economic research 10 w 27102 2020 retrieved from http www nber org papers w 27102 doi 5 mirela andronescu anne condon holger h hoos david h mathews and kevin p murphy computational approaches for rna energy parameter estimation rna 16 12 2304 2318 2010 6 m benzi and j liu block preconditioning for saddle point systems with indefinite 1 1 block intern j of computer mathematics 84 8 1117 1129 2007 7 joshua dennis booth sivasankaran rajamanickam and heidi thornquist basker a threaded sparse lu factorization utilizing hierarchical parallelism and data layouts in 2016 ieee international parallel and distributed processing symposium workshops ipdpsw pages 673 682 ieee 2016 8 richard h byrd jorge nocedal and richard a waltz knitro an integrated package for nonlinear optimization pages 35 59 springer us boston ma 2006 9 sambuddha chakrabarti matt kraning eric chu ross baldick and stephen boyd security constrained optimal power flow via proximal message passing in 2014 clemson university power systems conference pages 1 8 ieee 2014 10 y chen t a davis w w hager and s rajamanickam algorithm 887 cholmod supernodal sparse cholesky factorization and update downdate acm trans math softw 35 3 22 1 22 14 2008 11 nai yuan chiang and victor m zavala an inertia free filter line search algorithm for large scale nonlinear programming comput optim appl 64 2 327 354 2016 12 t a davis and e palamadai natarajan algorithm 907 klu a direct sparse solver for circuit simulation problems acm trans math softw 37 3 1 17 2010 13 h dollar nicholas gould wil schilders and andrew wathen implicit factorization preconditioning and iterative solvers for regularized saddle point systems siam j matrix analysis applications 28 170 189 01 2006 14 i s duff ma 57 a code for the solution of sparse symmetric definite and indefinite systems acm trans math softw 30 2 118 144 2004 15 iain duff jonathan hogg and florent lopez a new sparse ldlt solver using a posteriori threshold pivoting siam j optim 42 2 c 23 c 42 2019 16 g h golub and c greif on solving block structured indefinite linear systems siam j sci comput 6 24 2076 2092 2003 17 g h golub and c f van loan matrix computations johns hopkins studies in the mathematical sciences the johns hopkins university press baltimore fourth edition 2013 18 jacek gondzio hopdm version 2 12 a fast lp solver based on a primal dual interior point method european journal of operational research 85 1 221 225 1995 19 kai he sheldon x d tan hai wang and guoyong shi gpu accelerated parallel sparse lu factorization method for fast circuit analysis ieee transactions on very large scale integration vlsi systems 24 3 1140 1150 2015 20 pascal he non pierre ramet and jean roman pastix a high performance parallel direct solver for sparse symmetric definite systems parallel computing 28 2 301 321 2002 21 m r hestenes and e l stiefel methods of conjugate gradients for solving linear systems j res nat bureau standards 49 409 436 1952 22 juan jerez sandro merkli samir bennani and hans strauch forces rtto a tool for on board real time autonomous trajectory planning in 10 th international esa confer ence on guidance navigation and control systems pages 1 22 esa salzburg austria 2017 23 martin keller severin geiger marco gu nther stefan pischinger dirk abel and thiva haran albin model predictive air path control for a two stage turbocharged spark ignition engine with low pressure exhaust gas recirculation international journal of engine re 19 https www top 500 org lists top 500 2021 06 http www nber org papers w 27102 doi search 21 10 1835 1845 2020 24 drosos kourounis alexander fuchs and olaf schenk toward the next generation of mul tiperiod optimal power flow solvers ieee transactions on power systems 33 4 4005 4014 2018 25 xiaoye s li an overview of superlu algorithms implementation and user interface acm trans math softw 31 3 302 325 september 2005 26 yudong ma jadranko matus ko and francesco borrelli stochastic model predictive control for building hvac systems complexity and conservatism ieee transactions on control systems technology 23 1 101 116 2014 27 j maack and s abhyankar acopf sparse linear solver test suite 2020 github com nrel opf matrices 28 daniel k molzahn florian do rfler henrik sandberg steven h low sambuddha chakrabarti ross baldick and javad lavaei a survey of distributed optimization and control algorithms for electric power systems ieee transactions on smart grid 8 6 2941 2962 2017 29 https www pnnl gov capabilities advanced computer science visualization data 30 j nocedal and s j wright numerical optimization springer series in operations research springer verlag new york second edition 2006 31 c c paige and m a saunders solution of sparse indefinite systems of linear equations siam j numer anal 12 617 629 1975 32 http pastix gforge inria fr files readme txt html 33 c petra b gavrea m anitescu and f potra a computational study of the use of an optimization based method for simulating large multibody systems optimization methods software 24 6 871 894 2009 34 cosmin g petra and nai yuan chiang hiop user guide technical report llnl sm 743591 lawrence livermore national laboratory https github com llnl hiop blob develop doc hiop usermanual pdf 35 cosmin g petra nai yuan chiang slaven peles asher mancinelli cameron rutherford jake k ryan and michel schanen hpc solver for nonlinear optimization problems 2017 https github com llnl hiop tree master 36 anna pyzara beata bylina and jaros law bylina the influence of a matrix condition number on iterative methods convergence in proceedings of the federated conference on computer science and information system pages 459 464 ieee 2011 37 p reinert hermann krebs and evgeny epelbaum semilocal momentum space regular ized chiral two nucleon potentials up to fifth order the european physical journal a 54 5 1 49 2018 38 steven c rennich darko stosic and timothy a davis accelerating sparse cholesky factorization on gpus parallel computing 59 140 150 2016 theory and practice of irregular applications 39 franc ois henry rouet xiaoye s li pieter ghysels and artem napov a distributed memory package for dense hierarchically semi separable matrix computations using ran domization acm trans math softw 42 4 2016 40 j rozloz n k saddle point problems and their iterative solution cham birkha user 2018 41 d ruiz a scaling algorithm to equilibrate both rows and columns norms in matrices technical report ral tr 2001 034 rutherford appleton laboratory 2001 42 s r searle matrix algebra useful for statistics john wiley and sons hoboken nj 1982 43 cristiana j silva and delfim fm torres optimal control for a tuberculosis model with reinfection and post exposure interventions mathematical biosciences 244 2 154 164 2013 44 jean pierre sleiman jan carius ruben grandia martin wermelinger and marco hut ter contact implicit trajectory optimization for dynamic object manipulation in 2019 ieee rsj international conference on intelligent robots and systems iros pages 20 github com nrel opf matrices github com nrel opf matrices https www pnnl gov capabilities advanced computer science visualization data http pastix gforge inria fr files readme txt html https github com llnl hiop blob develop doc hiop usermanual pdf https github com llnl hiop blob develop doc hiop usermanual pdf https github com llnl hiop tree master 6814 6821 ieee 2019 45 https portal nersc gov project sparse strumpack 46 k s wirydowicz e darve j maack s regev m a saunders s j thomas and s peles linear solvers for power grid optimization problems a review of gpu accelerated linear solvers parallel computing submitted 2020 47 byron tasseff carleton coffrin andreas wa chter and carl laird exploring benefits of linear solver parallelism on modern nonlinear optimization applications 2019 48 h uzawa iterative methods for concave programming johns hopkins studies in the mathematical sciences stanford university press stanford ca 1958 49 robert j vanderbei loqo an interior point code for quadratic programming opti mization methods software 11 1 451 484 1999 50 nelle varoquaux ferhat ay william stafford noble and jean philippe vert a statistical approach for inferring the 3 d structure of the genome bioinformatics 30 12 i 26 i 33 2014 51 a wa chter and l t biegler line search filter methods for nonlinear programming motivation and global convergence siam j optim 16 1 1 31 2005 52 a wa chter and l t biegler on the implementation of an interior point filter line search algorithm for large scale nonlinear programming math program 106 1 25 57 2006 53 allen wang ashkan jasour and brian c williams non gaussian chance constrained trajectory planning for autonomous vehicles under agent uncertainty ieee robotics and automation letters 5 4 6041 6048 2020 a additional opf matrix results figure 8 illinois grid 4 with varying in 6 a be 104 gives results close to machine precision b rr 104 has rr 10 8 figure 8 shows be and rr in 4 for varying for the illinois grid for 1 le 104 the results are accurate cg convergence for the texas grid with 104 is given in fig 9 a while b shows that the solution is very accurate and all errors are smaller than 10 8 21 https portal nersc gov project sparse strumpack figure 9 texas grid with 104 a cg iterations on eq 7 the mean number of iterations is 11 1 b be and rr for 3 and 4 the bes are roughly machine precision and the rrs are less than 10 8 22 1 introduction 2 nonlinear optimization problem 3 solving kkt linear systems 4 a block 22 system solution method 4 1 a hybrid solver with minimal regularization 4 2 guaranteed descent direction 4 3 convergence for large 5 practicality demonstration 5 1 selection 5 2 results for larger matrices 5 3 reordering h 6 comparison with ldlt 7 iterative vs direct solve with h in alg cgschur 8 summary a additional opf matrix results