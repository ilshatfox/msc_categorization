ar x iv 1 71 2 03 29 9 v 3 m at h s t 1 2 o ct 2 01 8 posterior distribution existence and error control in banach spaces in the bayesian approach to uq in inverse probelms j andre s christen marcos a capistra n mara luisa daza torres hugo flores argu edas j cricelio montesinos lo pez 10 oct 2018 abstract we generalize the results of capistra n christen and donnet 2016 on expected bayes factors bf to control the numerical error in the posterior distribution to an infinite dimensional setting when consid ering banach functional spaces and now in a prior setting the main result is a bound on the absolute global error to be tolerated by the forward map numerical solver to keep the bf of the numerical vs the theoretical model near to 1 now in this more general setting possibly including a truncated finite dimensional approximate prior measure in so doing we found a far more general setting to define and prove existence of the infinite dimensional posterior distribution than that depicted in for example stuart 2010 discretization consistency and rates of convergence are also investigated in this general setting for the bayesian inverse problem keywords inverse problems bayesian inference bayes factors nu merical analysis of ode s and pde s disintegration weak convergence total variation centro de investigacio n en matema ticas cimat jalisco s n valenciana guana juato gto 36023 mexico jac marcos mdazatorres jose montesinos moreles at cimat mx corresponding author 1 http arxiv org abs 1712 03299 v 3 2 1 introduction bayesian uq in a nutshell is bayesian inference on a possibly infinite di mensional parameter with data yi such that for example yi f i i n 0 2 the regressor f or forward map fm is commonly a complex non linear map arising from unknown parameters in a system of odes or pdes then to evaluate f we require to solve a system of o p des not only that but this commonly involves a numerical solution with some error f n which is the actual regressor we can work with in our computer a prior is stated for and a numerical posterior distribution is obtained n represents a discretization used to approximate the fm and as n increases the discretization becomes finer and the approximation becomes tighter in this paper we are concerned with the numerical error induced in this posterior in comparison to the theoretical posterior when considering the exact theoretical fm f and also on the error introduced in the numerical posterior when using a truncated finite dimensional prior k moreover we dicuss practical guidelines to choose the numerical discretization refinement n and the priori truncation k in order to have correct posterior numerical error control we consider a general not necessarily gaussian model for the data yis capistra n et al 2016 discuss the latter and this paper generalizes their results to functional spaces including a discretization truncation of the prior capistra n et al 2016 use a posterior bound once the data is seen and re quires the estimation of normalizations constants here we use a prior predictive bound that results in a global bound for the fm to control the numerical error a brief review of capistra n et al 2016 and its shortcomings is given below in section 1 2 undoubtedly the first step is to define the posterior distribution in a gen eral setting including infinite dimensional spaces in the context of bayesian inverse problems stuart 2010 did several advances and found regularity conditions for the posterior to exists in a fairly general setting see stuart 2010 and references therein the normalization constant in the posterior is proven to be finite and positive and thus the posterior is indeed a proba bility measure using boundedness assumptions on the likelihood and consid ering gaussian priors stuart 2010 assumption 2 7 i ii and theorem 4 1 3 recently hosseini and nigam 2017 generalized the latter now considering priors with exponentially decaying tails using the same regularity conditions however to our surprise in studying the mentioned results we found out that in other contexts defining the posterior distribution in general spaces is a very well known task a nice example is contained in the text book schervish 1997 a very powerful tool that can be used here is disintengration al though it is not essential the principal remark here is that the existence of the posterior distribution can be established in a far more general sense than what stuart 2010 establishes and these results are well known in the general bayesian literarure below we discuss the existence of the posterior distribution in this perspective 1 1 existence of the posterior distribution in infinite dimensional spaces it always puzzle us that in any other context of bayesain inference we need not worry for for example the prior tail behaviour stuart 2010 or in fact any other condition for the posterior to exists the usual practice is to define a parametric model for data y f y a prior for the parameter and without guilt and further protection we declare f y to be a joint distribution on y the usual argument being that it is indeed positive and f y dyd 1 but when does f y define a joint dis tribution when to start with the latter integrals exist and swap but in any case we depart from the construction of a joint probability measure for both y what we call modern bayesian statistics in its foundations requires ex actly that a joint probability measure p on the whole measurable space of uncertain events both observable y or not the existence of such measure p is proven by assuming a set of axioms on a preference rela tionship on events on based on a system of bets performed by an agent conditional on the chosen space and on the agent preferred system of bets p quantifies the agent s uncertainty on namely a system of bets comprising the axioms and this is the basis for the epistemic or conditional probabilistic or bayesian or which some also like to call lightly or pejora tive subjective christen 2006 approach to uncertainty quantification our preferred axiomatic development is that of degroot 1970 in the same axiomatic development if then an event d is observed 4 a new system of bets is precluded in which bets on events are only relevant in terms of the intersection of those events withd ie anything outsided ceases to be relevant the existence of a new measure pd on is guaranteed which coincides with the new updated system of bets after d has been observed and it turns out that pd a p a d p d a p a p d for all a that is given the set of axioms the updated measure pd is precisely the conditional probability conditional on d all inferences given that we observed d stem from the conditional probability p a d namely the posterior or a posteriori probability measure the way we perform any necessary calculations to obtain p a d exactly or approximately is up to us and certainly bayes theorem is used in most cases not always eg when calculating a predictive posterior only total probability is used note there fore that bayes theorem is not the fundamental issue in modern bayesian statistics nor its interpretations give meaning to modern bayesian uq however a problem arises when modeling data with continuos distribu tions since realized data d y y have p d 0 and the above simple calculation of pd a p a d cannot be used fortunately this is a classical problem in probability since conditioning on events of zero prob ability is a necessity well beyond bayesaian statistics kolmogorov studied the problem but the modern approach for very many technical reasons is called disintegration a very nice review may be found in chang and pollard 1997 specifically example 9 discusses the definition and existence of the posterior distribution leao jr fragoso and ruffino 2004 also present a nice review disintegration has the correct properties as a conditional distribution now generalized to events of probability zero in particular d becomes irrelevant the bottom line is the same as in stuart 2010 the posterior measure has as density the likelihood function w r t the prior measure how ever the posterior may be proven to exists in a very general setting without any regard to tail behaviour of the prior etc for completeness all these results are presented in detail in section 2 as it turns out a good enough regularity setting is this f y is con tinuos in and the joint measure space is polish leading to a radon joint measure p see lemmas 2 1 and 2 2 as far as section 2 is concerned we stress the fact that only the former we consider a relevant observation on 5 our part continuity of the likelihood the rest in that section is based on classical probability results and are well known in other areas of bayesian statistics 1 2 consistency convergence and eabf as mentioned above we are interested in establishing guidelines for choosing a discretization level n for the fm and a truncation for the prior k the problem is addressed in capistra n et al 2016 in the finite dimensional case and here we generalize their results for parameters in infinite dimensional banach spaces and a truncation in the prior distribution capistra n et al 2016 present an approach to address the above problem using bayes factors bf the odds in favor of the numerical model vs the theoretical model further details will be given in section 2 in an ode framework these odds are proved in capistra n et al 2016 to converge to 1 that is both models would be equal in the same order as the numerical solver used for high order solvers capistra n et al 2016 illustrates by reducing the step size in the numerical solver that there should exist a point at which the bf is basically 1 but for fixed discretization n step size greater than zero this is the main point made by capistra n et al 2016 it could be possible to calculate a threshold for the tolerance such that the numerical posterior is basically equal to the theoretical posterior so although we are using an approximate fm the resulting posterior is nearly error free capistra n et al 2016 illustrate with some examples that such optimal solver discretization leads to basically no differences in the numerical and the theoretical posterior since the bf is basically 1 potential saving cpu time by choosing a corser solver however capistra n et al 2016 still has a number of shortcomings first it depends crucially on estimating the normalizing constants from monte carlo samples of the unnormalized posterior for a range of discretizations n this is a very complex estimation problem subject of current research and is in fact very difficult to reliably estimate these normalizing constants in mid to high dimension problems second capistra n et al 2016 approach is as yet incomplete since one would need to decrease n systematically calculating the normalization constant of the corresponding numerical pos terior to eventually estimate the normalization constant of the theoretical posterior see figure 2 of capistra n et al 2016 which in turn will pin point a discretization at which both models are indistinguishable being this a 6 second complex estimation problem the main difficulty here is that one has already calculated the posterior for small step sizes and therefore it renders useless the selection of the optimal step size to improve on capistra n et al 2016 the idea of this paper is to consider the expected value of the bfs before data is observed we will try to bound this expected bf to find general guidelines to establish error bounds on the numerical solver depending on the specific problem at hand and the sample design used but not on particular data these guidelines will be solely regarding the forward map and although perhaps conservative represent useful bounds to be used in practice moreover as already mention we generalize capistra n et al 2016 to an infinite dimensional setting and also considering a truncation in the prior the basic idea then is to establish the relative merit of the numeric model vs the theoretical model using bayesian model selection we first prove that the approximations are consistent that is that the numerical posterior converges to the theoretical posterior this has been proved and discussed extensively using the hellinger distance eg stuart 2010 also rates of convergence have been discussed elsewhere bui thanh and ghattas 2014 stuart 2010 here in section 3 in the more general setting considered in this paper and for completeness we use weak convergence then to establish the consistency in the rate of convergence in section 3 2 we use the total variation norm having this we prove our main result for banach spaces for the expected absolute difference of the bf to 1 eabf considering any location scale family for the distribution of the data the main results of the paper are found in section 4 in section 5 we consider the prior truncation and in section 6 a series of examples for the moment we finish this introduction with a brief discussion on the use of weak convergence and the total variation tv norm 1 3 weak convergence and the total variation norm in probability theory the basic convergence criterion is weak convergence other convergence criteria in probability in tv in lp etc are commonly generalized from weak convergence billingsley 1968 probability measures k weakly converge to if the lebesgue integrals f x k dx converge to f x dx for all measurable non negative continuous bounded functions f we write k 7 in oder to have a clear concept of rates of convergence we require a metric to measure distance between the involved objects total variation tv is one of the most common for many reasons gibbs and su 2002 the tv distance between two measures 1 and 2 on the same measure space is defined as 1 2 tv sup a 1 a 2 a 1 2 max h 1 h x 1 dx h x 2 dx where h r measurable note that if 1 approximates the posterior distribution then 1 tv is the upper bound for the difference in any posterior probability we wish to calculate and or on the error in any bounded posterior expectation we need to calculate moreover note that utility func tions are bounded and with correct units belong to 0 1 degroot 1970 then 1 tv is the maximum error incurred in calculating expected utilities when using 1 instead of as far as bayesian theory is concerned tv is quite well suited for what is required indeed the hellinger distance could be used as well as has been the tradition in the bayesian uq context note however that tv is equivalent to hellinger gibbs and su 2002 convergence in tv implies convergence in hellinger and viceversa it bounds perhaps to facility in proofs and direct interpretation and that is why we choose tv 2 setting and existence lemmas let y y rm be the data at hand and p be a family of probability models for y we assume that the family of probability models for the observables y have a density f y w r t a finite measure namely a product of the lebesgue and counting measures in rn to accommodate possibly discrete and continuous observations that is p y a a p dy a f y dy for all measurable a for example y is a product space of subsets of r or z leading to discrete and or continuos data this is the usual setting in parametric inference in any case with the usual topological considerations we assume y is a polish space polish spaces include complete metric spaces that have a 8 countable dense subset y should be viewed as a polish space with the standard metric in r and the discrete metric in z and then results in a borel finite measure on y p is then a radon measure for all since any borel probability measure on a polish space is radon we use this last fact in the proof of lemma 2 2 below until now the parameter space is arbitrary we need to define a mea surable space to be able to define a probability measure on namely a prior distribution so far f y cannot be considered a conditional distribution but due to the next two lemmas we adopt the more common notation f y f y lemma 2 1 let g rm r be any measurable function if g y f y is a measurable function then g y q dy d g y p dy d g y f y dy d defines a joint probability measure q on the product space rm proof 2 1 since and are finite is finite then by tonelli s theorem 7 g y f y dy is measurable g y f y is non negative integrable and the above integrals swap moreover using g 1 we have q y f y dy d 1 see for example schervish 1997 p 16 lemma 2 2 bayes theorem if is a separable banach space and 7 f y is continuos for all y y then 1 the joint measure q exists as defined in lemma 2 1 2 the disintegration q of q exists p may be seen as such desintegration and therefore f y may be seen as the conditional density of y given 3 the y disintegration qy of q exists and is the general definition of the conditional measure q y y on given y y namely the posterior distribution 4 moreover for any measurable g we have g qy d g f y d f y d that is qy f y for all y y 9 proof 2 2 the measurability of f y was proven in gowrisankaran 1972 since g y f y is also measurable from lemma 2 1 1 above follows moreover any separable banach space is a polish space and the product space y is also polish therefore the joint probability measure q is a radon measure and the prior is also radon the rest follows from standard results in disintegration with radon probability measures see example 9 of chang and pollard 1997 this is also proven in for example schervish 1997 p 16 although not using the disintegration argument 2 1 remarks on lemmas 2 1 and 2 2 generality the combination of lemas 2 1 and 2 2 state the existence of the posterior measure which are based on standard results in prob ability and integration note that we do not require any restriction on the tail behavior on the likelihood nor on the prior this is a far more general result than stuart 2010 or hosseini and nigam 2017 exis tence of the posterior measure in the parametric setting is guaranteed with the continuity of the likelihood and regularity of the underlying space namely a polish space continuous likelihood note that for each f y is a measurable function with continuity on it follows that f y is measurable this is indeed a profound result in measure theory that puzzled topol ogists for many years eg sierpin ski 1920 the reference we use gowrisankaran 1972 made his prove for when is a suslin space which is a generalization of polish spaces counterexamples showing that a measurable function on each variable separately is not mea surable in the product space show that the continuity requirement on 7 f y may not be relaxed without further provisions that is the likelihood is required to be continuous cromwell s rule if an event has zero a priori probability then it will have zero posterior probability indeed since qy we will adopt the notation for the posterior measure qy to make the dependance explicit both on the data y and on the prior in this respect qy may be seen as an operator that transforms updates the prior measure into the posterior measure qy which represents the inference process of learning from the data y 10 likelihood principle as usual bayesian inference follows the like lihood principle since the posterior measure depends on the data only through the likelihood well posedness as studied by stuart 2010 or hosseini and nigam 2017 in which close enough data y and y will lead to similar posteriors is interesting but we believe is a wrong con cept two very different data sets should lead to the same inferences eg y and y having the same mean and even two alternative models should lead to the same conclusions when following the likelihood prin ciple eg binomial vs negative binomial sampling see for example berger and wolpert berger and wolpert prior predictive measure as usual from lemmas 2 1 and 2 2 we see that the normalization constant or partition function for the posterior z y f y d now viewed as a function of y is in fact the marginal density w r t of the joint measure q that is is a density for not yet observed data y namely the prior predictive measure defining the posterior through radon nikodym derivatives does not preclude directly the existence of such measure in the next section we discuss how to ensure that when substituting the likelihood with a numeric approximation fn y the corresponding posterior qny is close enough to the theoretical posterior qy also we will discuss the analogous when using an alternative prior k instead of and combining both leading to the approximate posteriors qy k and q n y k 3 the inverse problems setting and discretiza tion consistency we follow the general setting of scheichl stuart and teckentrup 2017 for the statistical inverse problem let and v be separable banach spaces let f v be the borel measurable forward map fm and h v a rm s the borel measurable observation operator the composition h f defines a borel measurable mapping from the parameter space to the data sample space in rm plus possibly additional parameters going beyond 11 gaussian noise assume that fo y is a density for data y w r t for all a the parametric family of sample models as in section 2 is defined with the family of densities f y fo y h f to fix ideas we elaborate the usual independent gaussian noise case fo y m j 1 1 yj j and x 1 2 e x 2 2 ie yi hj f j j n 0 1 if is also unknown we may take s 1 and include it as a parameter the same if we had and unknown variance covariance matrix etc we do not discuss this case further in the main part of the paper some notes are added in section 7 regarding the case when is unknown let f n be a discretized version of the forward map f for some dis cretization that depends on an integer refinement n for example a time step size fem discretization etc this is the actual numerical version of the forward map defined in our computers let fn y fo y h f n be the resulting discretized numerical likelihood moreover suppose there are approximate or alternative prior measures k also defined in in the rest of the paper we take the following assumption assumption 3 1 assume that for all y y the observation model fo y is uniformly lipschitz continuous for each and for y y a s fo y is bounded moreover the fm maps h f and h f n are continuous if h f and h f n are continuous then 7 f y and 7 fn y are continuous and all requirements are met for lemmas 2 1 and 2 2 and the posterior measures are well defined and exist as probability measures when using the theoretical likelihood and exact prior qy and also when using the numerical likelihood or and an alternative prior namely qny qy k and qny k also let z n y zk y and z n k y be the corresponding partition functions in each case in the usual setting of stuart 2010 scheichl et al 2017 and others it is also assumed that h f is continuous here we require nothing further 12 note that if we consider independent data with a location scale model as fo y m j 1 1 yj j 1 where x is uniformly lipschitz continuous and known the first part of assumption 3 1 is met and we only require to establish that h f and h f n are continuous indeed the former is true if x is gaussian assume a global error control of this numeric fm as h f h f n k 0 n p 2 for some functional note that this is a global bound valid for all and includes already the observational operator that is it is a global bound for all but is only a statement at the locations hjs where each yj is observed usually the error control global bounds are proven for the fm but these are easily inherited to the composition h f by ensuring for example that h is lipschitz continuous as we next explain from assumption 3 1 fo y is uniform lipschitz continuous for any given y then since fo y fo y l we have fn y f y fo y h f n fo y h f k 1 n p 3 which is also a global error bound now for the numeric likelihood where the constant k 1 lk 0 is independent of the next step is to prove the consistency of using the discretization and the prior truncation k the term will be clear in section 5 that is how q n y and qy k tend to the theoretical posterior measure qy we first prove the latter in weak convergence rates of convergence are proven in the then total variation norm in the following section as mentioned before we stress the fact that similar consistency results have proved before in this bayesian inverse setting in a more particular setting we present weak convergence and tv rates of convergence results since our setting is more general basically only requiring assumption 3 1 3 1 weak convergence the following theorem presents our discretization consistency results 13 theorem 3 2 discretization consistency with assumption 3 1 1 with the fm approximation result in 3 then qny k qy k and qny qy as n 2 if k then qny k qny and qy k qy as k proof 3 1 1 from 3 we have that fn y f y for all then by bounded convergence znk y fn y k d f y k d zk y since k is finite swartz 1994 chap 3 since z n k y 1 zk y 1 0 we also have fn y zn k y f y zk y for all now since qny k and qy k have the latter as densities w r t k this implies q n y k qy k by scheffe s lemma the prove for qny qy is analogous 2 note that fn y is bounded real non negative continuos function therefore znk y fn y k d fn y d zn y let g be any bounded real non negative continuos function then since znk y 1 zn y 1 and g fn y k d g fn y d then znk y 1 g fn y k d zn y 1 g fn y d which implies qny k qny the prove for qy k qy is analogous 3 2 total variation and rates of convergence as previously mentioned we use tv to establish rates of convergence in our discretizations theorem 3 3 assume 3 1 and the rate of convergence in 3 then qny k qy k tv k 1 zk y n p and qny qy tv k 1 z y n p for big enough n 14 proof 3 2 this is proven in lemma a 2 theorem 3 4 with assumption 3 1 if k tv 0 then qny k qny tv fn y n zn y k tv and qy k qy tv f y z y k tv for big enough k where n maximize fn y and f y proof 3 3 for h measurable with h 1 we have h fn y k d h fn y d h fn y k d h fn y k d let bk h fn y k d and b h fn y d the above implies bk b fn y n k tv znk y zn y fn y n k tv and bk znk y b zn y b zn y 1 zn y 1 zn y fn y n k tv 4 2 zn y fn y n k tv since b zn y 1 and we obtain the result the prove involving qy k and qy is analogous theorem 3 5 consistent rate of convergence with assumption 3 1 the rate of convergence in 3 and k tv 0 we have qny k qy tv 0 as k n and qny k qy tv k 1 zk y n p f y z y k tv 5 for big enough k and n note that zk y z y 0 and zk y z y 15 proof 3 4 note that qny k qy tv qny k qky qky qy tv qny k qky tv qky qy tv and from theorems 3 3 and 3 4 we obtain the result corollary 3 6 with assumption 3 1 the rate of convergence in 3 and k tv 0 we have qny k qy 3 3 remarks on theorems 3 3 3 4 and 3 5 the posterior operator is lipschitz continuos that is qy k qy tv f y z y k tv if the rate of convergence of the truncated prior k to the complete prior is k tv k q then since zk y 1 z y 1 0 qny k qy tv k 2 n p k 2 k q with n p o np that is the discretized version of the posterior converges in total variation to the theoretical posterior at the same rate as the fm and the prior truncation in many cases of pde discretization schemes the number of parameters or dimension of the prior k increases linearly quadratically etc with the discretization size n as it is the case in some inverse problems using the finite element method eg bui thanh ghattas martin and stadler 2013 petra martin stadler and o 2014 in principle this should not represent an additional problem and the consistency result in 5 still holds for big enough n as far as k tv 0 3 4 posterior estimates in modern bayesian theory all inference problems are viewed in a perspec tive of a decision under uncertainty ultimately needing to maximize posterior expected utility which is in fact the bayesian paradigm moreover all util ity functions are bounded and by convention normalized to 0 1 degroot 16 1970 if one wants to calculate the posterior expectation of an utility func tion or any other bounded functional h 0 1 note that h n k h qny k qy tv k 1 zk y n p f y z y k tv where h n k h qny k d and h h qy d that is control ling qny k qy tv will bound the error in any estimation required and the rates of convergence are transferred in passing note from the prove of theorem 3 3 that is lemma a 1 that k 1 zk y n p is the bound for zn k y zk y zk y zn k y zk y 1 traditionally we are used to working with the posterior mean and or variance in that case h is not bounded however if h is continuos and the h n k are uniformly integrable then h exists and h n k h this can be verified if sup n k h 1 qny k d 6 for some positive billingsley 1968 chap 2 for example if the tails of the finte dimensional posterior decay exponentially then s 2 n k h 2 qny k d needing only to verify that these s 2 n k are bounded 4 expected a priori bounds and bayes fac tors as in capistra n et al 2016 in order to find reasonable guidelines to choose a discretization level n and a suitable prior truncation k we compare the numeric posterior qny k with the theoretical posterior qy using bayesian model selection namely bayes factors bf assuming an equal prior prob ability for both models the bf is the posterior odds of one model against the other that is p 1 p where p zn k y zn k y z y the posterior probability of the numerical model that is the bf is the ratio of the normalization constants zn k y z y in terms of model equivalence an alternative expression conveying the same odds is 1 2 znk y z y 1 17 we now try to control the bayes factor between the discretized model and the theoretical model zn k y z y through the use of the absolute bf abf in order to do that independently of the specific data at hand we try to bound the expected abf the eabf 1 2 znk y z y dy 1 2 znk y z y 1 z y dy in terms of estimates on the error in the numeric forward map as in 2 the idea is to keep the eabf below a small threshold eg 1 20 so that the bf is close to 1 and the difference between the numeric and the theoretical model is not worth more than a bare mention jeffreys 1961 kass and raftery 1995 theorem 4 1 with assumption 3 1 the rate of convergence in 3 k tv 0 and y log fo y c 1 a s we have 1 2 znk y z y 1 z y dy 7 k 0 n p 2 m i 1 i y h f n fo y h f n dy k d k tv proof 4 1 as seen in the proof of theorem 3 4 we have zk y z y f y k d f y k d and therefore zk y z y dy f y k d dy f y dy k d 2 k tv therefore 1 2 znk y z y 1 z y dy 1 2 znk y zk y dy k tv to bound the last integral note that znk y zk y f y rn 1 k d rn fn y f y 18 for close enough to 1 the a likelihood ratio fo y fo y 1 is near to 1 and fo y fo y 1 1 log fo y fo y 1 y y 1 y 1 y with the first order taylor approximation of y around 1 we have rn 1 y 1 y y h f n h f n h f r ignoring the higher order terms in the residual and using the error bound in 2 we have 1 2 znk y zn y dy k 0 n p 2 fo y y h f n 1 k d dy since for any two vectors a b aibi c bi with ai c and we obtain the result we may attempt to calculate the remaining double integral by changing the order of integration letting m h f k d and m y 1 fo y dy m i 1 i y fo y dy 8 this in general is difficult to achieve however it is possible if it happens that m does not depend on in the usual case of independent gaussian errors with known variance 2 y 1 1 m i 1 yi i and m i 1 yi i n yi i dyi 2 m since x 1 2 e x 2 2 dx 2 this result may be generalized to any location scale family and we present it next theorem 4 2 with the setting of theorem 4 1 assuming independent data arising from a location scale family namely fo y m i 1 1 yi i 19 with a bounded c 1 symmetric lebesgue density in r with x 2 x dx 1 then 1 2 znk y z y 1 z y dy 0 k 0 n p m k tv 9 proof 4 2 from 8 note that i y fo y dy 1 v yi i 1 yi i dyi where x ev x the integral on the rhs is in fact equal to 2 1 0 v x x dx 2 1 0 since x v x x and we obtain the result since k 0 n p is the error in the fm with the observation operator in 2 measured in the same units as the yjs note from 9 that k 0 n p is the relative error in the numeric fm with respect to the standard error in the observations in order to keep the eabf below a threshold we require more precision in the fm if the sample size m increases and more less precision in the fm if the standard error decreases increases it makes much sense to measure k 0 n p with respect to and k 0 n p becomes units free if we let the eabf b and for example b 1 20 0 05 we expect nearly no difference in the numerical and the theoretical posterior if we set the error in the fm k k 0 n p then we require 0 k m k tv b that is we need the numerical error in the fm in 2 k m b k tv 0 10 we require k tv b but since this only involves the prior truncation we should be able to fix it from the onset for example k tv 1100 our suggested procedure is to run the solver including an after the fact error estimate or a posteriori error estimate we use after the fact given the conflict of terms with the bayesian jargon if the error in the fm does not comply with the bound in 10 then run the solver again with a finer discretization n in passing we assure 2 for all note that in odes the rk 45 method rungue kutta order 5 method of cash and karp 1990 for example produces after the fact error estimates more recently the discontinuos galerking method for pdes may include high 20 order solvers with after the fact error estimates di pietro and ern 2011 hesthaven and warburton 2007 in general error estimates for pdes are much harder to obtain and the usual strategy is to consider adjoint base methods 5 using a base for defining a prior directly on the banach space is difficult and we have little options as for example an infinite dimension gaussian distribution stuart 2010 a perhaps more pragmatic approach is to decide on a base for to represent its elements and then take the coefficients in the base representation as random as in scheichl et al 2017 accordingly let c d be the continuous functions on a compact domain d r with norm which can be l 2 for example this indeed constitutes a separable banach space let for any t 0 t i 1 i i t 11 where i are our chosen base i r and o is fixed we take the base functions normalized i 1 let be a discrete random variable and 1 2 be random variables in r then a probability measure on n r defines the distribution f of 1 2 and the prior distribution will be the push forward measure over the function 1 2 7 g 0 t i 1 i i t the marginal distribution fk of the first k terms which is its kth natu ral projection defines the push forward measure k from k t 0 t min k i 1 i i t which is our truncated approximate prior with lemma 2 1 of rosalsky and rosenblatt 1997 on convergence of random elements in banach spaces we have that if i 1 e i i i 1 e i 21 then there exists t such that 0 t k i 1 i i t t a s this implies k in probability and therefore k since fk f the fks are the finte dimensional marginals and g is continuous by the mapping theorem it also implies k billingsley 1968 with this we have qny k qny and qy k qy as in theorem 3 4 and note that so far the is need not be independent the only requirement here is i 1 e i 12 to control the rate of convergence we requiere convergence in total varia tion from the coupling characteristic of the total variation norm gibbs and su 2002 k tv p k 6 and therefore we have k tv p k 13 let h be the prior for then p k i k 1 h i a typical choice for h i would be a poisson distribution with parameter then i k 1 e i i for example if a priori the average number of terms in 11 is 10 then with k 20 i k 1 e i i 1 100 from 9 we see that the overall eabf bound in this case is 1 2 znk y z y 1 z y dy 0 k m i k 1 h i 14 with h f h f n k 5 1 the discretized numeric posterior to be able to work on our posterior distribution we need to truncate the prior of below some maximum k thus implicitly truncating the prior to k at the end we are left to deal with the varying dimensional posterior 22 with maximum dimension k 1 l l y m m j 1 yj hj f n 0 t l i 1 i i t 1 l k h l subject to h f h f n k m b i k 1 h i 0 eg b 1 20 at this point we have two options we may work with the full model with l k or take l also as a parameter to be inferred the latter has the great advantage in that the posterior will select the effective dimension palafox capistra n and christen 2014 of our model although is far more computational demanding than the former for the sheer complexity of the fms we leave l k fixed in examples 6 3 and 6 4 when l is also a parameter we may run an mcmc for each l k the posterior probability of each l can be obtained estimating the normaliza tion constant given l this is a difficult estimation processes de valpine 2008 palafox et al 2014 but in some cases of near gaussian posteriors the normalization constants are easier to obtain this approach is used in example 6 1 a different approach is to use a transdimensional mcmc as rjmcmc to include l in the mcmc process this we do in example 6 2 6 examples we first review some representative bayesian uq examples that recently appeared in the literature and briefly view them in the perspective of our results second in sections 6 1 6 2 6 3 and 6 4 we present workout examples considering bayesian uq problems for a 1 d wave equation deconvolution and 1 d and 2 d heat equations respectively example 1 in lassas and siltanen 2004 and kolehmainen lassas niinima ki and siltanen 2012 the parameter space is the space of continuous functions in the unit interval c 0 1 for piecewise linear continuos functions on 0 1 the total variation prior is proposed to be used for a discretization k k u ck exp k k 1 j 1 ukj ukj 1 23 where u t ukj 1 t tj 1 uk j uk j 1 tj tj 1 t tj 1 tj and tj jk 1 incon sistencies are found in the map and cm estimators the maximum of the posterior and the posterior mean when k 1 or k k 1 and k a clear problem with this approach is that we do not know which is the prior on c 0 1 what is the measurable space and how k converges to if at all converges how can we expect consistency without the latter defining a probability measure on c 0 1 is a complex and delicate endeav our billingsley 1968 chap 2 and is indeed a source of classic results in probability eg the weiner process is a measure on c 0 1 example 2 scheichl et al 2017 worked with a continuos fm with gaus sian errors derived from an elliptic pde with error bounds equivalent to 2 the posterior is needed to be defined in a functional space a separable banach space this is sufficient for assumption 3 1 to hold they use a base expansion as in 11 with independent and summable is the latter is suffi cient for weak convergence beyond their specific prior for the is therefore the results in section 3 apply example 3 christen capistra n and moreles 2016 considered a two di mensional inverse problem of the logistic ode the fm is indeed continuos seen from the analytic solution x t kx 0 x 0 k x 0 e rt they consider gaus sian errors and a rungue kutta method of order 5 with error bounds similar to 2 therefore lemmas 2 1 and 2 2 and consistency theorems 3 2 and 3 3 apply they used a rk 45 to solve the ode and obtain error estimates these were larger than the actual errors also available from comparison from the analytic solution the bound for the numeric solver in 10 is kept adap tively for eabf 1 20 no prior truncation is needed and also a fine grid solver was use the adaptive solver gave posterior distributions basically indistinguishable to those obtained by the fine solver with more than 90 cpu time save example 4 christen et al 2016 also considered a fm arising from the burgers pde in a two dimensional bayesian inverse problem with gaussian errors the fm is indeed continuos seen again from the analytic solution the authors used a second order accurate finite volume solver with error bounds as in 2 again lemmas 2 1 and 2 2 and consistency theorems 3 2 and 3 3 apply 24 more importantly they kept adaptively eabf 1 20 and compared with a finer solver obtaining a 60 save in cpu time the resulting posteriors where indistinguishable for all practical purposes example 5 in capistra n christen and velasco herna ndez 2012 and inverse problem in epidemics driven by a system of odes is analyzed with a generalized discrete distribution model for the data a combination of bino mial poisson and negative binomial distributions see capistra n and christen 2011 this discrete family can be seen to produce continuos likelihoods for independent data and since these are pmf s the likelihood is always below or equal to 1 using standard results on the continuity of solutions of ode over parameters the fm may be proved to be continuos example 6 bui thanh et al 2013 and petra et al 2014 work with an infinite dimensional bayesian inverse problem using a gaussian prior in a l 2 functional space with possibly correlated gaussian data the fm is assumed continuous and therefore the existence lemmas 2 1 and 2 2 apply the authors suggest using a langrange basis functions to represent the elements of as in 11 in a finite element discretization of the fm however the authors do not discuss the a priori convergence of i 1 e i i therefore the results of section 5 cannot be applied directly this is an example where the number of parameters represents the prior truncation k and this increases with the discretization size n we now present 4 workout examples in all cases we consider gaussian noise for the observations with known stadard error as in 1 and therefore the only relevant part to be taken care for in assumption 3 1 is that the theoretical and the numeric fm are continuos in order for the corresponding posteriors to be correctly defined as far as the derivation of the eabf bound is concerned we require that the numeric fm error bound in 2 exists for all 6 1 a 1 d wave equation example consider the homogeneous dirichlet conditions for the wave equation utt c 2 uxx x 0 l u 0 t 0 u l t 15 with initial conditions u x 0 x ut x 0 x 25 under a separation of variables technique a solution of the above problem can be found substituting u x t x x t t in the pde this problem becomes a pair of separate ordinary differential equations for x x and t t given by x 2 x 0 and t c 2 2 t 0 16 with x nan sin n x l and x n n c l bn sin n x l we obtain u x t n an cos n ct l bn sin n ct l sin n x l 17 to simplify the computations let us consider the case c 1 and x 0 that is bn 0 for all n therefore u x t nan cos n t sin n x where an 2 1 0 x sin n x dx and t 1 then u x 1 n 0 an 1 n sin n x 18 the inverse inference problem is as follows given measurements of u x 1 at z 0 z 1 zm 0 1 we need to infer the unknown function x namely consider the case yj u zj 1 j j 1 2 m 19 where j n 0 2 in this case we consider the fm and the observation functional as the identity hf n 0 an n x with n x 1 n sin n x no er ror is considered in the fm and only a truncation is considered in the series that is k n 0 an n x evidently the fm is continuos and regularity conditions are met for the infinite dimension posterior to exists regarding the bound in 10 only the k tv term is relevant since the error bound for the fm is zero that is to bound the eabf we only need to bound the a priori truncation error k tv which we do below in this case the marginal posterior distribution of is simple to calculate since normaliza tions constants are available analytically to obtaining the effective dimension of the problem since the fm is linear may therefore express 19 as a linear model in the usual way namely y x 20 26 where a 1 a 2 a and x a m matrix where each row of x is sin zj sin 2 zj 1 sin zj as in section 5 a priori h and a truncated prior is obtained by restricting k considering a priori n 0 2 a 0 1 given the posterior for is y n xt x a 0 1 a 0 0 x t y 2 xt x a 0 1 the normalization constant for these models are readily available to obtain the marginal posterior distribution for namely p i y h i i i k a i 0 1 2 xti xi ai 0 1 2 exp 1 2 2 ytxi xti xi a i 0 1 xti y 21 synthetic data was obtained with 0 025 m 15 with the true x 1 5 sin x 0 8 sin 2 x 0 7 sin 3 x 0 3 sin 4 x that is 4 the prior h for is a po 10 in figure 1 we present p i y truncated to 15 note that i k 1 h i 1 20 already to bound the eabf accord ingly additionally we produced p i y renormalizing it with 20 obtaining virtually the same results not shown in fact summing up the normalization constants in 21 provides zk y and summing up to 20 pro vides an estimate of z y from which we can produce an estimate of the abf 1 2 zk y zy 1 which results in 1 3 10 10 very well below 1 20 6 2 a deconvolution example we present a 1 d deconvolution example where an exact solution is available and simpson s rule is used to also have a numeric version of the fm here we illustrate both a numeric fm with a discretization and a truncation in the prior the bound in 10 is used to bound the eabf obtaining nearly identical results as using the exact fm in a trans dimensional mcmc to also obtain the marginal posterior for we consider the convolution of with the kernel c f 1 0 c y x y dy 22 27 2 4 6 8 10 12 14 16 0 0 0 2 0 4 0 6 0 8 1 0 pr ob ab ili ty figure 1 the marginal posterior pmf of the parameter dimension and the prior of green truncated to 15 the true dimension is 4 which corresponds to the map of this posterior which constitutes de fm assume c z 1 2 1 z and t 0 i 1 i cos 2 it i sin 2 it with the l 2 norm the base functions have constant norm independent of i equal to 1 2 we do not multiply by 2 with the change of variable u y x and identifying correctly the indicator function 22 may be calculated with min x 1 max x 0 1 2 z dz this integral may be calculated analytically for each base function cos 2 it or sin 2 it in the series definition of therefore for a truncated series given t 0 i 1 i cos 2 it i sin 2 it and f is available analytically to construct a numerically defined fm fn we use simpson s rule with a grid of size n to evaluate the integral min x 1 max x 0 1 2 z dz the deconvolution inverse problem arises for the case when there are observations available from the convolution ie is unknown and one wants to infer that is yi f ti i j n 0 1 0 t 1 t 2 tm 1 evenly spaced observation points in this case the observation functional h is the identity the error in the fm 28 is calculated directly with f ti fn ti since in this example the theoretical fm f is also available the parameters needed to be inferred are 0 1 1 2 2 a priori an independent truncated normal prior in a a with mean 0 tna s 1 2 s 1 2 as exp 1 2 x 2 s 2 i a a x is assigned to each i i such that 0 tna s 0 and i i tna si e i 1 i 1 2 evidently f and fn are continuos a global error bound as in 2 is indeed sought for all s since the support for the js and js is compact since si is convergent the sine cosine series converges and the prior distribution on the js and js induces a prior for as explained in section 5 for the prior for we take a poisson with mean 8 but shifted to 1 and renormalized to odd numbers so 1 3 only truncating this prior to k terms induces the truncated prior k as explained in section 5 we produce m 10 synthetic data points with 0 02 taking as the true the sine cosine series function with coefficients 0 0 9 1 1 0 4 2 2 0 3 3 3 0 2 i i 0 i 4 that is the true dimension is 7 the true sine cosine series function its convolution and the simulated data points may be seen in figure 2 for the prior we let a 1 0 3 and 1 0 10 log 0 1 so that 10 has 0 1 of the std dev of 1 the truncated normals are well contained in the a a interval the posterior is truncated at dimension k 12 so that the tail of the poisson prior is less than 0 01 leading to k 0 01 as explained in section 5 we designed a rjmcmc using the t walk an affine invariant mcmc within each dimension the transdimensional jump move is simple proposing a new p j 1 n 0 j 4 centered at cero with a smaller size than the previous and equivalently for the js we ran our rjmcmc with the approximate fm with errors complying with the bound in 10 k m b k tv 0 taking b 1 20 in this case 0 1 2 since we are considering gaussian errors we also ran our rjmcmc with the exact fm for comparisons the t walk mixes quite well in each dimension and with an integrated autocorrelation time of around 120 we took 1 000 000 iterations of the rjmcmc with a burn in of 1 000 leading to an effective sample size of roughly 8 000 this is good enough to create a histogram for parameters up to 4 and 4 dimension 9 see figure 2 b higher dimensions are seldom 29 0 0 0 2 0 4 0 6 0 8 1 0 0 5 0 0 0 5 1 0 1 5 2 0 1 3 5 7 9 11 13 15 17 19 21 23 0 00 0 05 0 10 0 15 0 20 0 25 0 30 pr ob ab ili ty a b figure 2 a true sine cosine series function with coeficients 0 0 9 1 1 0 4 2 2 0 3 3 3 0 2 i i 0 i 4 red its convolution black and simulated data points b prior green and posterior probability of each dimension using the approximate fm blue complying with the bound in 10 and using the exact fm magenta the true dimension 4 is marked with red visited and the corresponding effective sample for 5 and 5 and above is very small even for 1 000 000 iterations leading to high monte carlo errors the posterior probability for each dimension is shown in figure 2 b and the corresponding posterior marginals are shown in figure 3 in this since we use a mc approach no estimation of the abf is readily available our approximate fm leads to basically error free posteriors as seen in figures 2 b and 3 any extra precision put into the simpson s rule integrator will lead to useless extra cpu time with respect to the resulting numeric posterior for the sample size and noise level at hand for more realistic applications where f k is not available analytically error bounds on the integrator could be used moreover since the error bound is required at observations points tis only an irregular integration grid could be used by making it finer around the tis this could lead to further improvements in cpu time 30 0 5 0 6 0 7 0 8 0 9 1 0 0 0 2 4 6 8 10 12 14 16 18 1 0 0 5 0 0 0 5 1 0 1 0 0 0 5 1 0 1 5 2 0 2 5 1 0 0 8 0 6 0 4 0 2 0 0 0 2 0 4 0 6 0 8 1 0 0 0 5 1 0 1 5 2 0 1 0 0 8 0 6 0 4 0 2 0 0 0 2 0 4 0 6 0 8 2 0 0 0 5 1 0 1 5 2 0 2 5 3 0 1 0 0 8 0 6 0 4 0 2 0 0 0 2 0 4 0 6 2 0 0 0 5 1 0 1 5 2 0 2 5 3 0 3 5 4 0 0 8 0 6 0 4 0 2 0 0 0 2 0 4 3 0 1 2 3 4 5 6 0 8 0 6 0 4 0 2 0 0 0 2 0 4 3 0 1 2 3 4 5 0 4 0 2 0 0 0 2 0 4 0 6 0 8 4 0 0 0 5 1 0 1 5 2 0 2 5 3 0 3 5 4 0 0 6 0 4 0 2 0 0 0 2 0 4 0 6 4 0 1 2 3 4 5 6 figure 3 prior green and posterior marginals for 0 i i i 1 2 3 4 for the approximate fm blue complying with the bound in 10 and using the exact fm magenta the true value of the parameter is marked with a red tick 31 6 3 a 1 d heat equation inferring the thermal conduc tivity let us consider the thermal conductivity problem for the stationary heat equation in 1 d d dx a x du x dx f x x 0 1 23 subject to dirichlet boundary conditions u 0 u 1 0 with forcing term f x sin x and thermal conductivity a x 0 that varies with the space parameter x in this example the fm is not available analytically and a numeric fem fm is used we use an error estimation in the fm to bound the eabf in this case since the fem used is numerically demanding we keep the prior truncation fixed k the numerical solution of 23 is computed using the finite element method fem which allows us to calculate a local error estimation in the l 2 norm see babus ka and rheinboldt 1978 for more details given by uh u l 2 ii xi xi 1 uh u 2 dx 1 2 h 2 2 aimin r l 2 ii i 1 n where m is the number of elements uh the numerical solution with step size h ii xi 1 xi a i min min x ii a x and r x f x d dx a x duh x dx is the residual then the the error estimation k 0 is computed by k 0 max ii h 2 2 aimin r l 2 ii i 1 n 24 the inference problem is the estimation of the function a x exp b x given observations of uj u xj at a fixed locations xj j 1 m cer tainly the theoretical and the numeric fms are continuos we simulate a synthetic data set with the true thermal conductivity is a x k 0 r k 0 1 exp xa as and error model yj u xj j where j n 0 1 with the following parameters k 0 5 r 0 9 a 20 s 2 and 0 0005 to maintain a 0 01 signal to noise ratio the data are plotted in figure 4 b we consider m 30 observations at locations xj regularly spaced between 0 and 1 32 in order to define the parametric space the function b is represented as a third order b spline that passes through the set of points bi ki 0 where bi b xi therefore the parameter space is defined by bi ki 0 in this case the number of parameters is taken as fixed k 20 regarding the prior distribution for the parameters bi ki 0 we define their prior us ing gaussian markov random field gmrf zero mean and sparse precision matrix inverse covariance encoding statistical assumptions regarding the value of each element bi based on the values of its neighbors see details in bardsley and kaipio 2013 we restrict the support of 0 b x b that is bi 0 b where b log 10 then the parameter space is compact and there exist a global bound for 24 complying with 2 with the standard error and sample size used calculating the error bound for the forward map fm as stated in 10 we require k 0 2 1 10 6 to sample from the posterior distribution we also use the t walk christen and fox 2010 regarding the numerical solver we begin with a relatively large step size h 0 02 considering n 50 elements in the fem and start the mcmc at each iteration the fm is first computed along with its error estimation k 0 if the solution uh do not satisfy the estimated global bound ie k 0 2 1 10 6 we increase the number of elements by 50 h 1 m 50 until the bound is met for h 0 0066 n 150 elements in the fem the bound is achieved for all iterations for comparisons a smaller grid is considered with h 0 002 n 500 elements the results are shown in figure 4 we took 50 000 iterations of the twalk the mcmc mixes quite well with n 150 the sampling took 3 min and with n 500 16 min in a standard 2 6 ghz processor computer as seen in figure 4 the conductivity is recovered and taking n 500 elements in the fem results in basically the same posterior as for only n 150 which already comply with the eabf bound only resulting in unnecessary cpu effort 6 4 a 2 d heat equation inferring the initial condition we present a 2 d heat equation problem to determine the initial conditions from observations of transient temperature measurements taken within the 33 0 0 0 2 0 4 0 6 0 8 1 0 x 0 1 2 3 4 5 6 a x n 150 n 500 real 0 0 0 2 0 4 0 6 0 8 1 0 x 0 00 0 01 0 02 0 03 0 04 0 05 0 06 u x n 150 n 500 exact data a b figure 4 a the true conductivity a x black the posterior mean with n 150 elements red and n 500 elements green in the fem b the exact solve u x black the posterior mean with n 150 elements red and n 500 elements green shaded areas represent the uncertainty in the model fit as draws from the posterior distribution using 150 elements blue and 500 elements yellow note that if we use a smaller step size than that required by the bound in 10 results are basically same simply adding cpu time 34 0 0 0 2 0 4 0 6 0 8 1 0 x 0 0 0 2 0 4 0 6 0 8 1 0 y 0 1 2 3 4 5 6 0 0 0 2 0 4 0 6 0 8 1 0 x 0 0 0 2 0 4 0 6 0 8 1 0 y 0 1 2 3 4 5 6 a b c figure 5 heat equation in 2 d a exact solution at t t 1 b numerical solution using finite element method with fenics with mesh 40 40 with t 0 067 and c numerical solution with an additive noise gaussian with variance 0 3 and data point locations domain at a time t t 1 the heat transfer pde is given by u t u in d 0 1 0 1 25 u x y t 0 on d u x y 0 f x y 26 taking the forcing term f x y b sin x sin y c sin 2 x sin y as initial condition the pde has an analytical solution u x y t b exp 2 2 t sin x sin y c exp 5 2 t sin 2 x sin y in this example we consider a more complex 2 d pde inverse problem the fm is available analytically and a numeric fm is also used the numeric error is directly calculated in this case only two parameters are needed to be inferred a numerical solution of equation 25 b is also computed using the finite element method fem within fenics martin et al 2015 which allows us to calculate the error in the numerical solver using the exact solution the inferential problem is to estimate b c given measurements of u at time t 1 0 3 a priori we took independent truncated gamma distributions for b and c with parameters 2 0 7 and 2 0 4 respectively both restricted to 0 8 certainly the theoretical and the numeric fms are 35 2 4 2 6 2 8 3 0 3 2 3 4 b 0 0 0 5 1 0 1 5 2 0 2 5 3 0 d en si ty true fem exact 4 4 4 6 4 8 5 0 5 2 5 4 5 6 c 0 0 0 5 1 0 1 5 2 0 2 5 d en si ty true fem exact figure 6 comparison between numerical blue a theoretical magenta pos teriors for both parameters in the initial conditions of the 2 d heat equation continuos and since the support is compact we may conclude that the error bound in 2 exists for all we simulate a synthetic data set with the error model yi u xi yi t 1 i where i n 0 1 i 1 n 0 3 using a the signal to noise ratio of 5 with b 3 and c 5 the data are plotted in figure 5 b we consider n 25 observations xi yi i n regularly spaced on d since we have an analytic solution if we run the pde solver we may calculate the maximum absolute error k 0 exactly the error bound for the fm as stated in 10 is 0 0015 to sample from the posterior distribution we use the t walk christen and fox 2010 regarding the numerical solver we start with a large step size of x y 0 1 and t 0 268 and calculate k 0 if the solution does not comply with the bound that isk 0 0 0015 a new solution is attempted by reducing the step size in x y and t by half until the global absolute errors is within the bound k 0 6 0 0015 the resulting mesh is x y 0 025 and t 0 067 we compare the above fem numerical fm with the exact fm with 250 000 iterations of our mcmc the result are shown in figure 6 and in table 1 the differences observed in both results may be attributed to the monte carlo sampling 36 b c true 3 0 5 0 pm exact 2 9396 5 0966 pm fem 2 9377 5 0969 table 1 comparison of the posterior mean pm of parameters b and c using the exact fm and the fem approximate fm 7 discussion the generalization of the results of capistra n et al 2016 to a priori state ments banach parameter spaces and a truncation in the prior makes the error control strategy ie using bfs of the latter far more feasible general and applicable in passing we needed to define the posterior distribution in this general setting and prove its existence as presented in section 3 however this we did using standard results in probability and modern bayesian theory regarding the finite dimensional numeric posterior weak convergence is then not difficult to prove and also tv rates of convergence are proved to be maintained as seen in theorems 3 3 and 3 4 this relaying on lemmas a 1 and a 2 we have not discussed the scenario when error parameters are not known in this case we may consider that a priori and are independent and equivalent results should follow this was discussed in a previous version of this manuscript but not here christen et al 2016 we only need to prove that the likelihood including follows assumption 3 1 in particular that it is bounded a s we have not proved that stylized posterior estimates like the mean or variance exists for qy elsewhere these are proven to exists with addi tional requirements and for gaussian priors or with exponential tails using fernique s theorem hosseini and nigam 2017 stuart 2010 in our case an additional sufficient requirement is mentioned in 6 which only involves the finite dimensional measures qny k which can be examined in a case by case basis note however that as far as bayesian inference is concerned we need not to guarantee the existence of the posterior expected mean variance etc adding regularity conditions on the observational model and or on the 37 prior if for example a posterior distribution has no variance that is a very relevant and important information regarding the statistical inference prob lem at hand nonetheless all posterior probabilities and posterior expected utilities are proven to be consistent and well defined given weak convergence and tv convergence rates we presented 4 workout examples of increasing difficulty in all cases the numerical error in the posterior was controlled successfully leading to negible increase in precision if a more precise fm is considered this in turn may result in cpu time save as cheaper rougher solvers are used note that decreasing solver precision can only be done within limits that is within the stable regime of the solver used moreover in real case applications increas ing the mesh size or any mesh refinements come a great coding effort for example in a large scale 3 d geothermal inversion cui fox and o sullivan 2011 our approach only makes sense in the case where mesh refinements and reliable after the fact error estimates are readily available 8 acknowledgments we thank tan bui thanh ut austin form prompting us to work on this generalization and for several comments on a previous draft of the paper also to peter mu ller ut austin jose luis perez garmidia and fernanda me ndez cimat for invaluable comments during the many previous drafts of the paper this research is partially founded by conacyt cb 2016 01 284451 rdecomm and onrg grants references babus ka i and w c rheinboldt 1978 a posteriori error estimates for the finite element method international journal for numerical methods in engineering 12 10 1597 1615 bardsley j m and j kaipio 2013 gaussian markov random field priors for inverse problems inverse problems imaging 7 2 397 416 berger j o and r l wolpert billingsley p 1968 convergence of probability measures john wiley sons inc 38 bui thanh t and o ghattas 2014 an analysis of infinite dimensional bayesian inverse shape acoustic scattering and its numerical approxima tion siam asa journal on uncertainty quantification 2 1 203 222 bui thanh t o ghattas j martin and g stadler 2013 a computa tional framework for infinite dimensional bayesian inverse problems part i the linearized case with application to global seismic inversion siam journal on scientific computing 35 6 a 2494 a 2523 capistra n m and j christen 2011 march a generic multivariate dis tribution for counting data arxiv e prints capistra n m j christen and s donnet 2016 bayesian analysis of ode s solver optimal accuracy and bayes factors journal of uncertainty quantification 4 1 829 849 capistra n m j christen and j velasco herna ndez 2012 towards un certainty quantification and inference in the stochastic sir epidemic model mathematical biosciences 240 2 250 259 cash j r and a h karp 1990 sep a variable order runge kutta method for initial value problems with rapidly varying right hand sides acm trans math softw 16 3 201 222 chang j t and d pollard 1997 conditioning as disintegration statistica neerlandica 51 3 287 317 christen j m capistra n and m moreles 2016 july numerical poste rior distribution error control and expected bayes factors in the bayesian uncertainty quantification of inverse problems arxiv e prints christen j and c fox 2010 a general purpose sampling algorithm for continuous distributions the t walk bayesian analysis 5 2 263 282 christen j a 2006 09 stop using subjective to refer to bayesian analyses comment on articles by berger and by goldstein bayesian anal 1 3 421 422 cui t c fox and m j o sullivan 2011 bayesian calibration of a large scale geothermal reservoir model by a new adaptive delayed acceptance metropolis hastings algorithm water resources research 47 10 39 de valpine p 2008 improved estimation of normalizing constants from markov chain monte carlo output journal of computational and graphical statistics 17 2 333 351 degroot h 1970 optimal statistical decisions john wiley sons new york di pietro d a and a ern 2011 mathematical aspects of discontinuous galerkin methods volume 69 springer science business media gibbs a l and f e su 2002 on choosing and bounding probability metrics international statistical review 70 3 419 435 gowrisankaran k 1972 measurability of functions in product spaces proc amer math soc 31 485 488 hesthaven j s and t warburton 2007 nodal discontinuous galerkin methods algorithms analysis and applications springer science busi ness media hosseini b and n nigam 2017 well posed bayesian inverse problems priors with exponential tails siam asa journal on uncertainty quan tification 5 1 436 465 jeffreys h 1961 theory of probability third ed oxford england oxford kass r and a raftery 1995 jun 1995 bayes factors journal of the american statistical association 90 773 795 kolehmainen v m lassas k niinima ki and s siltanen 2012 sparsity promoting bayesian inversion inverse problems 28 2 025005 lassas m and s siltanen 2004 can one use total variation prior for edge preserving bayesian inversion inverse problems 20 5 1537 leao jr d m fragoso and p ruffino 2004 regular conditional probail ity desintegration of probability and radon spaces proyecciones antofa gasta 23 15 29 40 martin s a j blechta j hake a johansson b kehlet a logg c richardson j ring m e rognes and g n wells 2015 the fenics project version 1 5 archive of numerical software 3 100 9 23 palafox a m capistra n and j a christen 2014 september effective parameter dimension via bayesian model selection in effective parameter dimension via bayesian model selection in the inverse acoustic scattering problem mathematical problems in engineering 2014 427203 12 petra n j martin g stadler and g o 2014 a computational frame work for infinite dimensional bayesian inverse problems part ii stochastic newton mcmc with application to ice sheet flow inverse problems siam journal on scientific computing 36 4 a 1525 a 1555 rosalsky a and j rosenblatt 1997 12 on the rate of convergence of series of banach space valued random elements nonlinear analysis theory methods applications 30 4237 4248 scheichl r a m stuart and a l teckentrup 2017 quasi monte carlo and multilevel monte carlo methods for computing posterior expectations in elliptic inverse problems siam asa journal on uncertainty quantifi cation 5 1 493 518 schervish m j 1997 january theory of statistics springer series in statistics 1 st ed 1995 corr 2 nd printing ed springer sierpin ski w 1920 sur un proble me concernant les ensembles mesurables superficiellement fundamenta mathematicae 1 1 112 115 stuart a m 2010 inverse problems a bayesian perspective acta nu merica 19 451 559 swartz c 1994 measure integration and function spaces world scien tific publishing co pte ltd singapore a auxiliary lemmas lemma a 1 let bn b r be bounded integrable functions and let zn bn d and z b d and assume that bn b kn p 41 for all n n k 0 p 1 fixed then zn z and bn zn b z with convergence rates zn z kn p and bn zn b z b z k z n p k z n p for all and big enough n proof a 1 since bn l m then by dominated convergence zn z since m l since zn z 0 bn b already implies bn zn b z now for the rate of convergence we have zn z bn d b d bn b d kn p since d 1 and therefore zn z z k z n p note that the first order taylor series with residual of x 1 around x 0 is x 1 x 10 x 20 x x 0 x 31 x x 0 2 for x 1 between x and x 0 assuming x x 0 0 then x 1 x 10 x 10 x x 0 x 0 x 0 x 1 3 x x 0 x 0 2 let x x 0 x 0 then also x x 0 x 0 and 1 x 1 x 0 1 since x 3 is decreasing then 1 3 x 0 x 1 3 1 3 therefore x 0 x 1 3 x x 0 x 0 2 1 3 2 if the relative error of estimating x 0 with x is below 20 1 3 2 is already one order of magnitud smaller than then ignoring this last term x 1 x 10 x 10 x x 0 x 0 assume n is big enough such that the relative error zn z z k z n p is small enough and we have z 1 n z 1 z 1 zn z z k z n p therefore z 1 k z 2 n p z 1 n z 1 k z 2 n p 27 since b kn p bn b kn p and given that we may assume 0 z 1 1 k z n p then multiplying 27 with the fomer term we have b kn p z 1 z 2 kn p bn zn z 1 z 2 kn p b kn p 42 b z 1 b z 2 kn p kn pz 1 z 2 k 2 n 2 p bn zn b z 1 b z 2 kn p kn pz 1 z 2 k 2 n 2 p ignoring the two terms of 2 p order we obtain the result lemma a 2 with the setting of lemma a 1 let h measurable and h n h bn zn d and h h b z d exists then h n h e 1 h k z n p e 0 h k z n p where e 1 h h b z d and e 0 h h d moreover for all h non negative and bounded h bn zn d and h b z d implic itly define the probability measures pn and p then pn p tv k z n p proof a 2 we have h n h h bn zn b z d and using lemma a 1 we obtain the first result moreover if h 1 then h k z n p h 0 k z n p 2 k z n p and therefore 1 2 max h 1 h pn d h p d 1 z kn p and we obtain the second result 1 introduction 1 1 existence of the posterior distribution in infinite dimensional spaces 1 2 consistency convergence and eabf 1 3 weak convergence and the total variation norm 2 setting and existence lemmas 2 1 remarks on lemmas 2 1 and 2 2 3 the inverse problems setting and discretization consistency 3 1 weak convergence 3 2 total variation and rates of convergence 3 3 remarks on theorems 3 3 3 4 and 3 5 3 4 posterior estimates 4 expected a priori bounds and bayes factors 5 using a base for 5 1 the discretized numeric posterior 6 examples 6 1 a 1 d wave equation example 6 2 a deconvolution example 6 3 a 1 d heat equation inferring the thermal conductivity 6 4 a 2 d heat equation inferring the initial condition 7 discussion 8 acknowledgments a auxiliary lemmas