operator augmentation for noisy elliptic systems philip a etter and lexing ying abstract in the computational sciences one must often estimate model parameters from data subject to noise and uncertainty leading to inaccurate results in order to improve the accuracy of models with noisy parameters we consider the problem of reducing error in an elliptic linear system with the operator corrupted by noise we assume the noise preserves positive definiteness but otherwise we make no additional assumptions the structure of the noise under these assumptions we propose the operator augmentation framework a collection of easy to implement algorithms that augment a noisy inverse operator by subtracting an additional auxiliary term in a similar fashion to the james stein estimator this has the effect of drawing the noisy inverse operator closer to the ground truth and hence reduces error we develop bootstrap monte carlo algorithms to estimate the required augmentation magnitude for optimal error reduction in the noisy system to improve the tractability of these algorithms we propose several approximate polynomial expansions for the operator inverse and prove desirable convergence and monotonicity properties for these expansions we also prove theorems that quantify the error reduction obtained by operator augmentation in addition to theoretical results we provide a set of numerical experiments on four different graph and grid laplacian systems that all demonstrate effectiveness of our method key words operator augmentation random matrices monte carlo polynomial expansion elliptic sys tems ams subject classifications linear and multilinear algebra matrix theory statistics computer science 1 introduction there are a plethora of different situations in the natural mathematical and computer sciences that necessitate computing the solution to a linear system of equations given by 1 1 ax b where a rn n and x b rn for n n when both the matrix a and b are known there are many decades of research on how to solve the system 1 1 efficiently unfortunately for a variety of reasons it is often the case that the true matrix a is not known exactly and must be estimated from data see 10 7 in this situation there is a sampling error between the unobserved true matrix a and the matrix a one constructs from data the discrepancy between a and a is often referred to as model uncertainty as it stems from incomplete or inaccurate information about the underlying system this model uncertainty means that with naive application of the inverse of the observed matrix a one is not solving the desired system 1 1 but rather the system 1 2 a x b where x a 1 b rn is the solution we observe when we solving the observed system naively often we will write 1 3 a a z where one can think of the matrix z as constituting the noise or sampling error in our measure ments of the system 1 1 hence the sampling error z between a and a translates into sampling error between the true solution x and the naively estimated solution x the question of interest in this paper is whether using the information available to us we can find a better approximation x for the true solution x by modifying how we solve the sampled system 1 2 better here means in the sense of average error measured in the norm of some symmetric positive definite matrix b i e that we have 1 4 eb x eb x where the error functional eb is defined as 1 5 eb x e x x 2 b e x x tb x x institute for computational and mathematical engineering stanford university paetter stanford edu department of mathematics stanford university lexing stanford edu 1 ar x iv 2 01 0 09 65 6 v 4 m at h s t 4 j un 2 02 1 mailto paetter stanford edu mailto lexing stanford edu 2 p a etter l ying fig 1 1 an example of the overshooting effect if we take a single sample of the of the scalar random variable x 2 1 2 and invert it the pdf of the inverted 1 x has an expectation that is significantly larger x 2 than the inversion 1 e x this means that naively trying to estimate 1 e x with only a single sample will likely give a significant overestimate the same principle also applies when x is a random matrix the two norms of particular interest to us are the l 2 norm for obvious reasons i e b i as well as the energy norm i e b a as the latter is an important metric of error in many physical problems in this paper we present an affirmative answer to the above question of error reduction in the form of a novel method we call operator augmentation the idea of operator augmentation is to add an augmenting term to the sampled inverse operator a 1 yielding a family of operators 1 6 a 1 k parameterized by an augmentation factor r for a choice of augmentation matrix k rn n depending on the problem setting our new approximation for x is then given by 1 7 x a 1 k b through judicious selection of the augmentation matrix k we show that one can estimate a that will reduce the error eb by a factor that depends on the variance of the naive solution x as we will see the power of operator augmentation lies in the fact that the technique works under very minimal assumptions on the randomness structure of a in general the only assumption we need to guarantee error reduction is that a is an unbiased estimator of a and even this assumption can be relaxed the intuition for operator augmentation comes from jensen s inequality note that the matrix inversion operation is a convex function wrt the lo wner order on the set of positive definite matrices this means that naively inverting an unbiased estimator a for a will always overshoot the true goal a 1 one can think of operator augmentation as correcting for this overshooting effect we structure the remainder of the paper as follows in section 4 we present the basic operator augmentation formalism and examine a special case to motivate the full operator augmentation technique in section 5 we present the full version of operator augmentation in the setting where the vector b is now also drawn from a probability distribution with known auto correlation i e a prior we prove bounds that quantify how much error our technique can reduce in section 6 we consider operator augmentation in the aforementioned energy norm i e when b a and prove similar bounds in section 7 we show that the previous setting of the energy norm has special monotonicity properties that are immensely useful for efficiently computing a good choice of finally in section 10 we present numerical experiments to verify the theoretical results in this paper note that we consider only elliptic systems in this paper i e requiring that a is symmetric positive definite and a is symmetric positive definite almost surely however one could the oretically apply the techniques we present herein to asymmetric systems as well but we do not provide any theoretical guarantees in the asymmetric case operator augmentation for noisy elliptic systems 3 finally in order to help readers quickly implement our method without getting caught up in all of the surrounding mathematical details we provide the quick start subsection 7 2 to give readers an alternate entry point to the algorithm we present in this paper for the accompanying source code for this paper please see appendix b 2 related work the spirit of our approach derives from stein estimation 4 in their seminal paper james and stein consider the high dimensional problem of estimating a parameter corrupted by noise and show that the standard pitman estimator is always inadmissible because appropriately shrinking the pitman estimate towards the origin produces an estimator with strictly better risk at a fundamental level one can frame our work as taking this idea and applying it to the novel setting of matrices corrupted by noise the immediately pertinent area of the literature related to our work is usually classified under model uncertainty quantifying and representing model uncertainty is important in many different fields of computational science ranging from structural dynamics 10 to weather and climate prediction 7 however work relating to model or parameter uncertainty is usually domain specific and focuses more on establishing a model for uncertainty than it does on trying to reduce error in the resulting predictions in contrast our work focuses entirely on reducing error rather than quantifying it our work is also not restricted to a particular domain class of problems or randomness structure as long as those problems are linear we note that our setting shares some similarities with the problem of uncertainty quantifica tion uq however the problem we face here is different from the standard uncertainty quan tification setting in a subtle but very important way in uq one is usually given a distribution p and a map t and asked to estimate statistics about the pushforward distribution t p i e expectation standard error etc practitioners typically accomplish this task via monte carlo techniques 6 or some form of stochastic galerkin projection 14 or collocation method 13 in contrast in our setting we have a distribution p d and our goal is to accurately compute the image under t of a statistic of p rather than a statistic of the push forward of p under t here t is matrix inversion and the statistic we are interested in is the true matrix s d a indeed the issue that e a 1 and e a 1 are significantly different is at the core of our paper as we are in the unbiased case interested in e a 1 and not e a 1 what does however fall into the purview of uncertainty quantification is the question of evaluating how well one does of estimating e a 1 but this is ultimately an orthogonal discussion the central problem in this paper is also not dissimilar to the setting of matrix completion seen in 2 5 in matrix completion one usually seeks to recover a low rank ground truth matrix mij from observations that have been corrupted by additive noise e g n m z if p denotes the subset sampling operator on matrix space then one is trying to recover m from 2 1 p n p m p z however the operator augmentation and matrix completion settings are subtly different the matrix completion analogue of a is the actual linear operator p and not the matrix m morally one may think of the matrix competition problem as solving the under determined linear system 2 2 p m p n by assuming a low rank regularity on m the randomness in this problem lies completely in the right hand side n and not in the actual linear operator p we also draw attention to the related field of perturbation matrix analysis in this setting one is usually interested in proving results about how various properties of matrices change under a perturbation to the elements of the matrix a seminal example of work in this field is the davis kahan theorem 3 which quantifies the extent to which the invariant sub spaces of a matrix change under perturbations in a similar vein work in backward stability analysis revolves around understanding the behavior of the solution of a linear system under perturbations to the matrix however backward stability analysis typically adopts a worst case mentality in analysis in contrast we care about average case error and more importantly how one can reduce it 4 p a etter l ying we should briefly mention that the mathematical branch of random matrix theory rmt studies the spectral properties of random matrix ensembles 1 12 however rmt results usually apply only when the entries of the random matrices are independent and in the large matrix limit we find these assumptions to be too stringent for the problem at hand in addition to these tangentially related settings we also call attention to the similarity of some of our techniques to those in harmonic analysis it is well known that the method of summation of an infinite series can affect the conditions under which it convergence as well as the quality of the convergence for example the fourier series of a continuous function f on the unit interval 0 1 may not converge pointwise to f if summed naively but feje r s theorem see 11 states that ce saro and abel sums of the fourier series of an integrable function f converge uniformly to f at any point of continuity our work takes on a similar favor in that it revolves heavily around the convergence properties of partial sums of infinite series expansion of the matrix function f a a 1 these partial sums are critical to accelerating an otherwise expensive monte carlo computation hence we develop methods of partial summation that have desirable properties such as convergence and monotonicity in conclusion we do not believe that the setting we introduce in this paper where the operator is noisy has been studied in the proposed fashion before there is little precedent in the literature for the operator augmentation method we present herein 3 probabilistic model and problem formalism formally we model this process by assuming that there exists an underlying parameter space with sigma algebra where the parameters contain a description of the system that produces the matrices above e x may be measurements of a scattering background edge weights vertex positions etc the relationship between parameters and matrices is given by a borel measurable mapping m s rn where s rn denotes the cone of symmetric positive definite matrices in rn n for example may be a weighted graph and m s rn may denote a minor of its laplacian we suppose that there exist some unobserved true system parameters that produce the true matrix a m we also suppose that there exists a known family of distributions p over indexed by that describes the observed randomness in the system if were to be the true system parameters it is this the relationship between and the distribution p that we assume is known as part of the model but not the true system parameters themselves once this family has been specified the distribution of a is given by m p where m denotes the pushforward for shorthand we will denote d m p finally we formally state our problem as follows suppose that we observe exactly one sample p given b or a distribution from which b is sampled we wish to build an inverse operator a 1 such that x a 1 b produces a better estimate of x a 1 b than x a 1 b in our case we take 3 1 a 1 a 1 k for some r k rn n to be specified later for ease of understanding we present flow chart of the above probabilistic setting together with the operator augmentation framework in figure 3 1 4 warm up basic operator augmentation recall our assumptions that a s rn and that a is sampled from a distribution d such that a s rn almost surely for ease of explanation we will make a number of assumptions in this section that we will later discard for now suppose that 1 we know the right hand side b a priori 2 we only consider the l 2 error norm i e b i and simply write e for the l 2 error ei supposing the above operator augmentation functions by attempting to find a good choice of augmentation factor such that the error indeed the choice of we would like to make is the minimizer of the error 4 1 arg min r e x operator augmentation for noisy elliptic systems 5 distribution family true parameters true distribution observed parameters bootstrap distribution observed matrix bootstrap parameters bootstrap matrices augmentation matrix augmentation factor augmented inverse unobserved observed our algorithm known chosen fig 3 1 a flow chart of the probabilistic setting of operator augmentation as well as the algorithm itself operator augmentation aims to find a that gives an optimal reduction in error given an augmentation matrix k in particular note that under the choice of 1 parameter family in 1 7 the error functional becomes a quadratic in e x e x x 22 e a 1 k b a 1 b 22 e a 1 b a 1 b 22 2 e bt k t a 1 a 1 b 2 e k b 22 e x 2 e bt k t a 1 a 1 b 2 e k b 22 4 2 hence the optimal choice of is given by 4 3 e bt k t a 1 a 1 b e k b 22 and under this optimal choice of we see a reduction of error given by 4 4 e x e x e bt k t a 1 a 1 b 2 e k b 22 where note that e x is the error when solving the system naively using a unfortunately this choice of depends on knowledge of the true inverse matrix a 1 and hence cannot be imple mented exactly to remedy this we take the following approach instead of computing we 6 p a etter l ying instead try to find a with a less explicit dependence on a 1 together with a choice of k such that 4 5 0 note that as long as falls within the above range it will reduce the value of the error functional e since the functional is quadratic in in accordance with the intuition we presented in the introduction in practice one observes that the sampling error in the matrix a causes the x to have a tendency to overshoot the true solution x hence a reasonable thing one might try to correct for this is to uniformly scale the entries of the estimated solution x by some scalar value in the range 0 1 note that this would correspond to a choice of augmentation matrix and factor given by 4 6 k a 1 0 indeed under this choice of k the numerator of 4 3 becomes 4 7 e bt k t a 1 a 1 b bt e a t a e a t a 1 b this expansion above is strongly suggestive of the moment formula for covariance matrices given by 4 8 e y t y e y te y cov y 0 one might suspect that it would therefore be possible to lower bound the troublesome term in 4 7 by something that depends on the variance of the estimated solution a 1 b x and not directly on a unfortunately the asymmetry of 4 7 and the fact that e a 1 6 a 1 in general makes this difficult however there is fortunately something we can say about the quantity e a 1 under mild assumptions namely lemma 4 1 lo wner order inversion suppose that a s rn and a d such that a s rn almost surely moreover suppose that in expectation a spectrally dominates a i e 4 9 e a a then matrix inversion inverts the expected lo wner order i e 4 10 e a 1 a 1 the proof of this fact is given in the appendix and relies on the fact that the function uta 1 u for arbitrary u rn is convex in a when a s rn now morally we would like to use this fact that a 1 e a 1 and say that the right hand side of 4 7 is bounded below by bt cov a b 0 however this would be incorrect indeed the fact that the matrix e a t a 1 in 4 7 is not symmetric makes this line of inquiry difficult therefore instead of shrinking the entire estimated solution x a strategy we will return to later let us consider simply shrinking the vector x along the b component for reasons that will soon be apparent i e 4 11 x x bbt x for some 0 1 b 22 this move corresponds to choosing 4 12 k bbt a 1 0 1 b 22 operator augmentation for noisy elliptic systems 7 note that from 4 3 the optimal choice of is now given by 4 13 1 b 22 e bt a 1 b 2 e bt a 1 b bta 1 b e bt a 1 b 2 if we assume e a a or e a a from lemma 4 1 it immediately follows that 4 14 bta 1 b e bt a 1 b ergo we immediately have that 4 15 1 b 22 var bt a 1 b e bt a 1 b 2 1 b 22 var bt x e bt x 2 0 we define the above lower bound as note that we have 0 as well as 1 b 22 moreover this confirms our intuition about x overshooting x on average as it indicates that we can always achieve better error by shrinking the b component of x by a small factor that depends on the magnitude of the variance relative to the second moment but before discussing the computation of let us restate the above result in a theorem theorem 4 2 suppose that a s rn and a d such that a s rn almost surely moreover suppose that e a a then consider the operator augmentation algorithm with the choice k bbt a 1 then it is the case that 4 16 1 b 22 var bt a 1 b e bt a 1 b 2 0 and furthermore 4 17 min r e x e x e x 1 b 22 var bt a 1 b 2 e bt a 1 b 2 the final bound is obtained by plugging into e x and repeating the bounding step we used to obtain in practice we estimate or the argmin via bootstrap i e we use the distribution d to approximate d and draw samples from d to estimate the quantity var bt x e bt x 2 via monte carlo note that since we only need to estimate a single quantity this bootstrap does not suffer from the curse of dimensionality however even with this we note that the above algorithm has a number of potential issues most pressingly 1 we assumed that the right hand side b is known a priori this may not be the case in practice 2 the augmented inverse operator a 1 k is not symmetric while this is not necessarily a bad thing in some situations this means that our augmentation may ignore the physics or structure of the underlying system 3 this type of operator augmentation is expensive note that estimating requires com puting var bt x e bt x 2 via monte carlo each monte carlo sample requires solving a linear system we will first address item 1 and item 2 in the next section and return to address issue item 3 in section 7 5 semi bayesian operator augmentation to address item 1 above we move from the setting where the vector b is known to the setting where the vector b is drawn from a prior distribution p we call this setting semi bayesian because the vector b has a prior while the matrix a does not now our new notion of error averages over both the randomness in a as well as the randomness in b we will denote this semi bayesian error as 5 1 ebayesb x eped x x 2 b 8 p a etter l ying where once again b s rn is a symmetric positive definite matrix and b denotes the b norm in exact analogy to the computation we did in the previous section letting 5 2 x a 1 k b for some k to be determined we have that 5 3 ebayesb x e bayes b x 2 eped bt k tb a 1 a 1 b 2 eped k b 2 b note that for a nonrandom c ep btcb ep tr btcb ep tr cbbt tr cep bbt tr cr tr r 1 2 cr 1 2 5 4 where r ep bbt 0 denotes the auto correlation matrix of the vector b thus 5 3 becomes ebayesb x e bayes b x 2 e tr r 1 2 k tb a 1 a 1 r 1 2 2 e tr r 1 2 k tbk r 1 2 5 5 where for convenience we have dropped the d subscript on e in analogy to section 4 we again have that 5 6 arg min r ebayesb x e tr r 1 2 k tb a 1 a 1 r 1 2 e tr r 1 2 k tbk r 1 2 and 5 7 ebayesb x e bayes b x e tr r 1 2 k tb a 1 a 1 r 1 2 2 e tr r 1 2 k tbk r 1 2 we note that the quantities above are very similar to their respective quantities in section 4 to generalize the analysis of section 4 it therefore makes sense to take 5 8 k ra 1 b and also to make the assumption that b and r commute note that this choice of augmentation no longer depends on the vector b moreover we can obtain the analogue of theorem 4 2 for this new setting theorem 5 1 assume that a s rn a d such that a s rn almost surely and that ed a a select a prior p and sample b p denoting the auto correlation matrix of b as r ep bbt choose b such that r b 0 and consider the operator augmentation algorithm with the choice k ra 1 b denote w rb under these assumptions we have that 5 9 tr cov w 1 2 a 1 w 1 2 e r 1 2 w 1 2 a 1 w 1 2 b 1 2 2 f 0 and furthermore 5 10 min r ebayesb x e bayes b x e bayes b x tr cov w 1 2 a 1 w 1 2 2 e r 1 2 w 1 2 a 1 w 1 2 b 1 2 2 f where f denotes the frobenius norm operator augmentation for noisy elliptic systems 9 proof consider under this choice of k using the fact that r 1 2 b 1 2 0 a straightfor ward calculation yields e tr w 1 2 a 1 w 1 2 w 1 2 a 1 a 1 w 1 2 e tr b 1 2 w 1 2 a 1 w 1 2 r 1 2 r 1 2 w 1 2 a 1 w 1 2 b 1 2 5 11 since tr ctc c 2 f we see that the denominator of the above fraction can be written as e r 1 2 w 1 2 a 1 w 1 2 b 1 2 2 f as for the numerator it is equal to 5 12 e tr w 1 2 a 1 w 1 2 2 e tr w 1 2 a 1 w 1 2 w 1 2 a 1 w 1 2 which can be written as 5 13 e tr w 1 2 a 1 w 1 2 2 tr w 1 2 e a 1 w 1 2 w 1 2 a 1 w 1 2 for brevity of notation let us denote w 1 2 e a 1 w 1 2 as c and w 1 2 a 1 w 1 2 as d note that lemma 4 1 tells us that 5 14 d w 1 2 a 1 w 1 2 w 1 2 e a 1 w 1 2 c hence it follows by the invariance of the lo wner under under congruence that 5 15 c 1 2 dc 1 2 c 1 2 cc 1 2 c 2 implying 5 16 tr cd tr c 1 2 dc 1 2 tr c 2 plugging the above inequality into 5 13 yields tr e w 1 2 a 1 w 1 2 2 tr w 1 2 e a 1 w 1 2 2 tr cov w 1 2 a 1 w 1 2 5 17 this proves 5 9 equation 5 10 can be proved similarly by plugging into the objective function and reusing the lower bound on the numerator of note that an advantage of this new formulation over the basic version presented in section 4 is that one may take b r such that the augmented inverse operator a 1 ra 1 r is symmetric addressing item 2 from the end of section 4 item 1 from section 4 is also addressed because one no longer needs explicit knowledge of the right hand side b a priori 5 1 general semi bayesian augmentation algorithm to convert the above into a general algorithm we need to first do two things the first is to convert into a form that is more amenable to monte carlo evaluation obviously computing the trace of a dim dim matrix is too expensive in most settings therefore we evaluate traces by using the probabilistic form of the trace i e if w s rn then 5 18 tr w 1 2 bw 1 2 eq q tbq where q is sampled from any distribution with auto correlation matrix w with this we can rewrite the expression 5 11 as 5 19 e q n 0 w a d a 1 q 2 w e q n 0 w a d a 1 q a 1 q w e q n 0 bw a d a 1 q 2 rw 10 p a etter l ying algorithm 5 1 operator augmentation ag input a right hand side b an operator sample a d with corresponding parameters a choice of auto correlation matrix r default to r i a choice of norm b with r b 0 default to b i sample count m output an estimate x of a 1 b 1 let w rb 2 draw m i i d bootstrap samples a b 1 a b m d 3 draw m i i d bootstrap samples q 1 q m n 0 w 4 draw m i i d bootstrap samples u 1 u m n 0 bw 5 assign m i 1 a 1 b i q i 2 w m i 1 a 1 b i q i a 1 q i w m i 1 a 1 b i u i 2 rw 6 assign x a 1 ra 1 b b 7 return x where the normal distribution n 0 can always be substituted for any other distribution with auto correlation we note that the above quantity is impossible to compute outright because we do not know the ground truth a or the distribution d to work around this limitation we approximate by bootstrapping the above quantity with observed data a and replacing a with an observed a and the distribution d with d this nets us the approximation 5 20 a e q n 0 w a b d a 1 b q 2 w e q n 0 w a b d a 1 b q a 1 q w e q n 0 bw a b d a 1 b q 2 rw where a b denotes a bootstrapped sample from the distribution d since bootstrapping tends to work well when estimating scalar quantities we believe that this approximation step is justified now the above can be estimated with monte carlo 5 21 a m i 1 a 1 b i q i 2 w m i 1 a 1 b i q i a 1 q i w m i 1 a 1 b i u i 2 rw where 5 22 a b 1 a b m d q 1 q m n 0 w u 1 u m n 0 bw i i d i i d i i d this gives us our general purpose operator augmentation algorithm which we give in full detail in algorithm 5 1 6 augmentation in the energy norm in many settings the error in the l 2 norm used in the previous section is less important than the error in the energy norm i e the a norm thus this consideration would demand an analysis of the operator augmentation technique specifically for the energy we will do so in this section moreover the energy norm also has a number of properties that make it particularly attractive in particular under certain assumptions on the distribution of the randomness in a we will prove in the sequel that can be approximated operator augmentation for noisy elliptic systems 11 effectively by using a modified 2 k th order taylor expansion for a 1 this means that one can perform monte carlo computation of effectively without needing to invert a full linear system for each sample nonetheless the analysis for the energy norm relies on slightly different machinery than the proof we have given above in particular one notes that the taylor expansion of the energy norm error has very nice properties in the framework of the previous section suppose that we consider b as being drawn from a distribution p with auto correlation r and that we take the augmentation matrix k to be 6 1 k a 1 c for some arbitrary positive definite c s rn similar to the previous section we make the assumption that 6 2 c r 0 and we will write 6 3 l cr note that unlike in the previous section operator augmentation in this setting corresponds to an asymmetric system unless one chooses c i moreover c does not have the same interpretation as b in the previous section repeating the computation done in the previous two sections we have that ebayesa x e bayes a x 2 e tr l 1 2 a 1 a a 1 a 1 l 1 2 2 e tr l 1 2 a 1 aa 1 l 1 2 6 4 the optimal choice of is given by 6 5 e tr l 1 2 a 1 a a 1 a 1 l 1 2 e tr l 1 2 a 1 aa 1 l 1 2 the properties that makes setting amenable for computation are the taylor series of the numerator and denominator of the above expression to demonstrate let us perform an explicit computation e tr l 1 2 a 1 a a 1 a 1 l 1 2 e tr l 1 2 a 1 aa 1 l 1 2 e tr l 1 2 a 1 l 1 2 6 6 note that the term e tr l 1 2 a 1 aa 1 l 1 2 is also precisely the denominator of we can expand this term using the taylor expansion of a 1 about base point a 1 a 1 a 1 a 1 z a 1 a 1 z a 1 z a 1 a 1 z a 1 z a 1 z a 1 a 1 2 k 0 a 1 2 z a 1 2 k a 1 2 6 7 however note that for this infinite taylor series to converge one must restrict the domain of a just like in the single variable case the taylor series converges absolutely on the event a 2 a we prove this in a lemma 12 p a etter l ying lemma 6 1 let x y s rn and consider the infinite taylor series for x 1 about base point y i e 6 8 x 1 y 1 2 k 0 y 1 2 x y y 1 2 k y 1 2 this series converges absolutely in the l 2 operator norm to x 1 when x 2 y moreover the tail of the series vanishes exponentially in k in the l 2 operator norm a proof of this fact is relegated to the appendix this places a damper on our ability to use the taylor expansion of a 1 with impunity over all of s rn for simplicity however we will assume for now that the distribution d is supported on the event a 2 a it turns out as we will show in section 8 that one can remove this assumption by instead expanding about a variable base point a a for some large enough factor a r therefore when we have supp d a 2 a the taylor series converges everywhere hence e l 1 2 a 1 aa 1 l 1 2 e l 1 2 a 1 2 k 0 a 1 2 z a 1 2 k k 0 a 1 2 z a 1 2 k a 1 2 l 1 2 e l 1 2 a 1 2 k 0 k 1 a 1 2 z a 1 2 k a 1 2 l 1 2 6 9 where we have used the property that k z k k z k k k 1 z k as well as the fact that the tail of the infinite series above vanishes exponentially in the l 2 operator norm note the melding of the a term into the taylor expansions on the second line above this splitting of the a matrix is the fundamental special property of of the energy norm that prompts all of the following analysis thus it follows from 6 6 that the numerator of is given by e tr l 1 2 a 1 a a 1 a 1 l 1 2 e tr l 1 2 a 1 aa 1 l 1 2 e tr l 1 2 a 1 l 1 2 e tr l 1 2 a 1 2 k 0 k 1 a 1 2 z a 1 2 k a 1 2 l 1 2 e tr l 1 2 a 1 2 k 0 a 1 2 z a 1 2 k a 1 2 l 1 2 e tr l 1 2 a 1 2 k 0 k a 1 2 z a 1 2 k a 1 2 l 1 2 6 10 thus we obtain that 6 11 e k 0 k tr l 1 2 a 1 2 a 1 2 z a 1 2 k a 1 2 l 1 2 e k 0 k 1 tr l 1 2 a 1 2 a 1 2 z a 1 2 k a 1 2 l 1 2 moreover the form 6 11 suggests a possible way of avoiding the need to invert a linear system for every monte carlo sample involved in approximating instead of attempting to approximate the quantity 6 5 one can truncate the series in 6 11 with an appropriate windowing function operator augmentation for noisy elliptic systems 13 to obtain a series of truncated augmentation factors defined as 6 12 n e k 0 n k tr l 1 2 a 1 2 a 1 2 z a 1 2 k a 1 2 l 1 2 e k 0 n k tr l 1 2 a 1 2 a 1 2 z a 1 2 k a 1 2 l 1 2 where n k z 0 r and n k z 0 r are two appropriately defined collections of discrete windowing functions each with bounded support such that the collection has the property that n k k and n k k 1 as n it turns out as we will discuss in the next section that regardless of the randomness structure of the distribution d one can choose an appropriate series of windowing functions n k n k such that 6 13 0 1 2 n 1 which means that using any of the truncated augmentation factors n underestimates the value of and hence still decreases the value of the objective ebayesa x from its base value of e bayes a x i e 6 14 ebayesa x e bayes a x 1 e bayes a x 2 e bayes a x n e bayes a x before we continue note that one can rewrite the truncated augmentation factors n in a form more amenable for computation namely 6 15 n e k 0 n k q ta 1 z a 1 kq e k 0 n k q ta 1 z a 1 kq where q is sampled from a distribution with auto correlation matrix l perhaps n 0 l and is independent from a d 7 truncated augmentation factors for energy norm augmentation our analyses of the monotonicity of the n relies upon the following lemma lemma 7 1 let a 1 a 2 ak r 0 and b 1 b 2 bk r 0 be two sequences of nonneg ative real numbers with b 1 0 and consider the truncated sum ratios 7 1 n n k 1 ak n k 1 bk then if it is the case that 7 2 anbk bnak for all 0 k n e g the ratios ak bk are monotonically increasing then the sequence 1 2 k is monotonically increasing to construct the discrete windowing functions n k n k it is instructive to think of the generating polynomials corresponding to n k n k i e 7 3 n x k 0 n k xk n x k 0 n k x k define the following simplifying notation x a 1 2 z a 1 2 i a 1 2 a a 1 2 s a 1 2 l 1 2 7 4 14 p a etter l ying note quickly that the assumption that e a a implies e x 0 the assumption that 0 a 2 a gives us that 7 5 i x i furthermore with this notation defined we can rewrite 6 12 as 7 6 n tr ste n x s tr ste n x s note that we have used the fact that n x n x are polynomial generating functions of bounded degree to interchange summation and expectation our intent now is to find a sequence of polynomials n x n x with the properties 7 7 n x k 0 k xk n x k 0 k 1 xk for x 1 as n such that the sequence in 7 6 allows us to invoke lemma 7 1 we do this by constructing n x n x from smaller primitive polynomials j x j x such that n x n j 0 j x n x n j 0 j x 7 8 e j x 0 e j x 0 7 9 j x 2 j 1 2 j j x for j 1 7 10 0 x 0 7 11 with this we can expand 7 6 into the required form of lemma 7 1 7 12 n n j 0 tr ste j x s n j 0 tr ste j x s nj 0 aj n j 0 bj note that property 7 9 implies aj 0 and bj 0 and the property 7 10 implies for j 1 7 13 aj bj tr ste j x s tr ste j x s 2 j 12 j tr ste j x s tr ste j x s 2 j 1 2 j and for j 0 we have a 0 b 0 0 hence the ratio aj bj is monotonically increasing in j and hence satisfies the requirement 7 2 of lemma 7 1 therefore the existence of such primitive polynomials n x n x immediately implies that n as n 7 14 0 1 2 3 n 1 7 15 where 7 14 follows from 7 7 the fact that 1 follows from from n and the fact that aj bj and hence the numerator of n j 0 aj n j 0 bj is always bounded by the denominator implying n 1 for all n and the fact that n 0 for any n comes from non negativity of the numerator and denominator of n operator augmentation for noisy elliptic systems 15 to show that such primitive polynomials n x and n x actually exist we consider the following definition 0 x 0 0 x 1 7 16 1 x x 1 2 x 2 1 x 2 x x 2 7 17 j x 2 j 1 1 2 x 2 j 2 x 2 j 1 1 2 x 2 j for k 2 7 18 j x 2 j 1 2 x 2 j 2 x 2 j 1 1 2 x 2 j for k 2 7 19 to show this family of primitive polynomials satisfies the desired properties note that for j 2 7 20 j x 2 j 1 2 x 2 j 2 x 1 2 0 this implies j x 0 moreover we can only have j x 0 if all of the eigenvalues of x are either 0 or 1 note that a 1 eigenvalue in x is impossible by the assumptions that 0 a 2 a and x i a 1 2 a a 1 2 as this implies i x i therefore j x 0 is only possible if x 0 however this cannot be the case almost surely as x 0 implies a a therefore with probability greater than 0 we have that j x 0 implying 7 21 e j x 0 e j x 0 furthermore for j 1 we have 7 22 e 1 x e x 1 2 e x 2 0 where we have used the fact that e x 0 from the fact that e a a and the fact that x 2 0 with probability greater than 0 unless a a a s finally to show 7 7 we simply note that for k odd and n large enough it is the case that xk n x xk k 1 2 x k xk n x x k k 1 2 x k 1 7 23 since k 1 2 x is the only primitive polynomial with a xk term in n x and likewise for n x for k 2 even we have that xk n x xk k 2 x k 2 1 x k 1 2 k 1 2 k xk n x x k k 2 x k 2 1 x k 2 k 2 2 k 1 7 24 thus the polynomials n x and n x have all the desired properties we restate the results of the past two sections in a theorem theorem 7 2 assume that a s rn a d such that a s rn almost surely ed a a and supp d a 2 a select a prior p and sample b p denoting the auto correlation matrix of b as r ep bbt choose c s rn such that c r 0 and consider the operator augmentation algorithm with the choice k a 1 c sample q rn independently from a distribution with autocorrelation rc let n be the truncated approximations to the optimal augmentation factor i e 7 25 n e k w 0 n k q ta 1 z a 1 kq e k 0 n k q ta 1 z a 1 kq 16 p a etter l ying where n k and n k are given by 7 26 n k k k 2 n k 1 2 k 2 n 0 o w n k k 1 k 2 n k 2 k 2 n 0 o w under these assumptions we have that n as n 0 1 2 3 n 1 ebayesa x e bayes a x 1 e bayes a x 2 e bayes a x n e bayes a x 7 1 hard windowing the tradeoff for monotone convergence to the true augmentation factor is that the windowing functions n k and n k presented above which we will refer to as soft windowing functions may be too conservative at low orders when this is the case one may instead choose to use hard windowing functions that perform a hard truncation of the infinite taylor series that is one may choose to instead use 7 27 n k k k 2 n 0 o w n k k 1 k 2 n 0 o w under the conditions of theorem 7 2 this choice of windowing function will still guarantee the convergence n however we lose the monotonicity guarantees of the soft windowing functions unless one makes very stringent assumptions on the underlying distribution that being said in practice this technique can perform quite well as indicated in our numerical experiments in section 10 to distinguish between truncated energy norm augmentation with soft and hard windows we will use the abbreviations t eag s and t eag h for truncated energy norm augmentation with soft and hard windows respectively 7 2 quick start for help readers with implementation we provide explicit formulas for the augmentation factor for low truncation orders as well as a pseudo code implementation of the different variants of energy norm augmentation 7 2 1 explicit formulas for low orders first we provide formulas for low orders of the algorithm presented in the previous section in the subsequent formulas we let 7 28 z a a l cr q n 0 l a d q a 1 t eag s order 2 7 29 t eag s 1 e q t 1 2 a 1 z a 1 z a 1 a 1 z a 1 q e q t a 1 z a 1 z a 1 2 a 1 z a 1 a 1 q 2 t eag s order 2 mean zero error in many cases the error matrix z may be mean zero i e e z 0 when this happens the above expression has an even simpler form 7 30 t eag s 1 1 2 e q t a 1 z a 1 z a 1 q e q t a 1 z a 1 z a 1 a 1 q 3 t eag h order 2 7 31 t eag h 1 e q t 2 a 1 z a 1 z a 1 a 1 z a 1 q e q t 3 a 1 z a 1 z a 1 2 a 1 z a 1 a 1 q operator augmentation for noisy elliptic systems 17 4 t eag h order 2 mean zero error in many cases the error matrix z may be mean zero i e e z 0 when this happens the above expression has an even simpler form 7 32 t eag h 1 e q t 2 a 1 z a 1 z a 1 q e q t 3 a 1 z a 1 z a 1 a 1 q 7 3 algorithm we give the full meta algorithm for all favors of energy norm augmentation in algorithm 7 1 note that in algorithm 7 1 like in algorithm 5 1 we replace expectations with bootstrapped monte carlo estimators if one wants to use the simplified expressions provided above in subsection 7 2 1 one must similarly replace the expectations with sampled and boot strapped versions this process is fairly straightforward for example for t eag h order 2 mean zero error we get 7 33 t eag h 1 m i 0 q t i 2 a 1 a b i a a 1 a b i a a 1 q i m i 0 q t i 3 a 1 a b i a a 1 a b i a a 1 a 1 q i where a b i and q i are defined as in algorithm 7 1 8 shifted base point estimation obviously the issue with the above theorem is that the restriction that supp d a 2 a is quite restrictive from a problem standpoint there are many natural problems that do not fall into this setting recall that this assumption comes from the fact that the infinite taylor series for a 1 about base point a only converges when a 2 a we address this issue with a technique we call shifted base point estimation the key idea is to grow the region of convergence of the infinite taylor series by changing the base point of the taylor series expansion if we make the assumption that the distribution d is bounded then there must exist some 1 such that a a for every a in the support of d lemma 6 1 then tells us that we are justified in taking an infinite taylor expansion about base point a 8 1 a 1 a 1 2 k 0 1 k 1 a 1 2 z a 1 2 k a 1 2 where z a a in general the best values of are those that are as small as possible while maintaining that the support of d lies within a a as the accuracy of a truncated series becomes less farther away from the base point with the above one can repeat the calculations of section 6 practically verbatim to derive the infinite series expression for the optimal augmentation factor 8 2 e k 0 k 1 k tr l 1 2 a 1 2 a 1 2 z a 1 2 k a 1 2 l 1 2 e k 0 k 1 k tr l 1 2 a 1 2 a 1 2 z a 1 2 k a 1 2 l 1 2 from here we follow the same schema to define the truncation of the infinite series above 8 3 n e k 0 n k k tr l 1 2 a 1 2 a 1 2 z a 1 2 k a 1 2 l 1 2 e k 0 n k k tr l 1 2 a 1 2 a 1 2 z a 1 2 k a 1 2 l 1 2 and its more compact form 8 4 n e k 0 n k k q ta 1 z a 1 kq e k 0 n k k q ta 1 z a 1 kq 18 p a etter l ying algorithm 7 1 energy norm augmentation meta algorithm input a right hand side b an operator sample a d with corresponding parameters a choice of auto correlation matrix r default to r i a choice of a positive definite c default to c i with r c 0 truncation order n and sample count m output an estimate x of a 1 b 1 let l rc 2 factorize preprocess a to precompute a 1 if necessary 3 draw m i i d bootstrap samples a b 1 a b m d 4 draw m i i d bootstrap samples q 1 q m n 0 l 5 if using truncated energy norm augmentation t eag then 6 if using soft truncation t eag s then 7 let n k k k 2 n k 1 2 k 2 n 0 o w n k k 1 k 2 n k 2 k 2 n 0 o w 8 else if using hard truncation t eag h then 9 let n k k k 2 n 0 o w n k k 1 k 2 n 0 o w 10 end if 11 assign m i 0 k 0 n k q ti a 1 a b i a a 1 kq i m i 0 k 0 n k q t i a 1 a b i a a 1 kq i where 12 else if using untruncated energy norm augmentation eag then 13 assign m i 0 q t i a 1 b i a a 1 b i a 1 b i q i m i 0 q t i a 1 b i a a 1 b i q i 14 end if 15 clamp max 0 16 assign x a 1 a 1 c b 17 return x where n k and n k are new window functions that converge to k 1 and k 1 respectively to show that this has the same properties as the truncated augmentation factors in the previous section we simply repeat the proof from the previous section but with a few small changes first for notational simplicity define 8 5 x 1 a 1 2 z a 1 2 note that 8 6 x i 1 a 1 2 a a 1 2 operator augmentation for noisy elliptic systems 19 from the fact that 0 a a it follows that 8 7 0 x i now we repeat the previous section to obtain the polynomial expression for n 8 8 n tr ste n x s tr ste n x s where once again n x n j 0 j x n x n j 0 j x 8 9 for brevity of notation we define the quantity 8 10 1 1 now our monotonicity analysis in this section is based upon the observation that for x 0 it is the case that 8 11 xj x j x and therefore it is also the case that for x 0 8 12 xj xk k xj xk 1 xk 2 k 2 x k 1 x k j k 1 x whereas the analysis in the previous section built a monotonic sequence of polynomials that were positive everywhere the above formula allows us to build a monotonic sequence of polynomials that are positive in expectation but not necessarily positive everywhere to do this we first note that by our e a a assumption 8 13 e x e i 1 a 1 2 a a 1 2 i 1 a 1 2 aa 1 2 i hence the above polynomial inequalities imply that 8 14 e x j x k ki e k j k 1 x i 0 this allows us to use the matrix polynomials x j x k ki as building blocks for a series that converges monotonically to the desired the final observation that one needs to build the series is the fact that 8 15 k 0 k 1 1 with this established we finally define the primitive polynomials 8 16 k x kx k xk 1 2 xk 2 k 1 x k k 1 xk k j 0 jxk j by 8 14 we have that 8 17 e k x e k 1 x k k j 0 jx k j k j 0 e x k j x j j 0 20 p a etter l ying however if one examines the individual terms of the composite sum k 0 k x by powers of x one observes that for x 0 1 8 18 xj k 0 k x j k 1 k j 1 ergo for x 0 1 we have that 8 19 n x n k 0 k x k 0 k 1 xk and therefore if we also take 8 20 k x k 1 x k n x n k 0 k x n k 0 k 1 xk and note that 8 21 tr ste k 0 k 1 x k s tr ste k 0 k 1 x k s we can conclude that 8 22 n tr ste n x s tr ste n x s and from the fact that the numerator is a sum of positive terms and the fact that n x n x by construction it therefore follows that 8 23 0 n 1 0 1 to achieve a proof of monotonicity of the n we appeal to lemma 7 1 which necessitates that we verify the inequality akbk 1 e tr st k x s e tr st k 1 x s e tr st k 1 x s e tr st k x s ak 1 bk 8 24 operator augmentation for noisy elliptic systems 21 to do this let us subtract and expand the above terms akbk 1 ak 1 bk e tr st k x s e tr st k 1 x s e tr st k 1 x s e tr st k x s ke tr st x k 1 s k 1 e tr st x k s k j 0 je tr st x k j s k 1 e tr st x k s ke tr st x k 1 s k 1 j 0 je tr st x k j 1 s k k 1 j 0 je tr st x k s e tr st x k j 1 s k k 1 j 0 je tr st x k 1 s e tr st x k j s k 1 j 0 je tr st x k s e tr st x k j 1 s ke tr st x k 1 s k tr sts 8 25 we now appeal to the following lemma which allows us to compare terms across the two sums above lemma 8 1 let x be a random matrix such that x 0 a s for j k and r 0 and any matrix s we have that 8 26 e tr st x js e tr st x ks e tr st x j rs e tr st x k rs the proof of this fact is relegated to the appendix however applying this lemma to the above 8 25 gives us akbk 1 ak 1 bk k 1 j 0 je tr st x k s e tr st x k j 1 s k ke tr st x k 1 s tr sts k 1 j 0 e tr st x k s je tr st x k j 1 s k 1 j 0 e tr st x k 1 s k 1 tr sts 8 27 finally we note that 8 14 gives us je x k j 1 k 1 i and therefore 8 28 je tr st x k j 1 s k 1 tr sts moreover 8 14 also gives us that e x k e x k 1 and therefore 8 29 e tr st x k s e tr st x k 1 s noting the the above two inequalities are between positive numbers and then substituting the above two inequalities into 8 14 gives the desired result 8 30 akbk 1 ak 1 bk 0 22 p a etter l ying thus the truncated estimators n form a positive monotonic sequence that converges to to summarize we restate the results we have just proved into a theorem theorem 8 2 assume that a s rn a d such that a s rn almost surely ed a a and supp d a a select a prior p and sample b p denoting the auto correlation matrix of b as r ep bbt choose c s rn such that c r 0 and consider the operator augmentation algorithm with the choice k a 1 c sample q rn independently from a distribution with auto correlation rc let n be the truncated approximations to the optimal augmentation factor i e 8 31 n e k 0 n k k q ta 1 z a 1 kq e k 0 n k k q ta 1 z a 1 kq where n k and n k are given by 8 32 n k k 1 n j k j k k n 0 o w n k k 1 k n 0 o w where 1 1 under these assumptions we have that n as n 0 1 2 3 n 1 ebayesa x e bayes a x 1 e bayes a x 2 e bayes a x n e bayes a x 9 accelerating shifted base point estimation in practice while the formula 8 31 provides a positive monotonically increasing series of estimates n for the optimal which only use n powers of the matrix z a 1 note that the larger one takes the factor the poorer the accuracy of the truncated approximation near the matrix a where most of the probability distribution is concentrated therefore while we get a guarantee of an estimate that will decrease the value of the objective ebayesa the convergence to the optimal factor might be very slow as a result necessitating larger and larger powers of z a 1 thus in practice it is often a good idea to let the quantity be a function of the sample a such that a a a this means that instead of using the estimator n in 8 31 above we use 9 1 n e a 2 k 0 n a k a k q ta 1 z a a 1 kq e a 2 k 0 n a k a k q ta 1 z a a 1 kq where one choice of a is 9 2 a a 1 2 a a 1 2 2 i e the smallest value for which a a a and the windowing functions n k and n k are defined as in 8 32 in practice one may choose to approximate a instead of computing it exactly note in 9 1 the reintroduction of the a 2 terms in the numerator and denominator originally these terms passed out of the expectation and cancelled but now the explicit dependence on a prevents this cancellation from happening computing a 1 2 a a 1 2 2 can be done with power method in particular with probability 1 if v rn is sampled from a distribution continuous with respect to the lebesgue measure on its support we have that a lim k a 1 2 a a 1 2 kv 2 a 1 2 a a 1 2 k 1 v 2 lim k v ta 1 2 a 1 a ka 1 a a 1 ka 1 2 v v ta 1 2 a 1 a k 1 a 1 a a 1 k 1 a 1 2 v 9 3 operator augmentation for noisy elliptic systems 23 since a is non singular transforming the random variable v by a 1 2 transforms the corresponding distribution into a distribution continuous with respect to the lebesgue measure on its support therefore it is sufficient to compute approximate a lim k v t a 1 a ka 1 a a 1 kv v t a 1 a k 1 a 1 a a 1 k 1 v lim k a a 1 kv a 1 a a 1 k 1 v a 1 9 4 and as a result we do not actually need to know a 1 2 or a 1 2 to be able to compute the correct value of now to produce an algorithm we follow the template of subsection 5 1 we bootstrap a by replacing it with our sampled a and bootstrap the expectation by using the distribution d instead of the true distribution d this nets us the approximate estimator 9 5 n a e q n 0 l a b d k 0 n a b k a b k 2 q t a 1 z b a b a 1 kq e q n 0 l a b d k 0 n a b k a b k 2 q t a 1 z b a b a 1 kq where z b a b a a b a b the above quantity can be estimated by monte carlo by com puting 9 6 n a m i 0 k 0 n a b i k a b i k 2 q ti a 1 z b a b i a 1 kq i m i 0 k 0 n a b i k a b i k 2 q t i a 1 z b a b i a 1 kq i where 9 7 a b 1 a b m d q 1 q m n 0 l i i d i i d the full algorithm is presented in algorithm 9 1 as a final note we observe that n as n although we do not prove it in full detail here we note that this is not difficult to prove as it is the case that pointwise in a 9 8 a 2 k 0 n a k a k q ta 1 z a a 1 kq q t a 1 aa 1 a 1 q likewise the term inside the expectation in the denominator of 9 1 converges to q t a 1 aa 1 q therefore a proof amounts to an interchange of limits an operation which can be performed by appealing to monotone convergence theorem for the numerator two separate invocations for the q t a 1 aa 1 q and q t a 1 q terms unfortunately we do not believe that the monotonic guarantees of the previous two sections carry over when acceleration is applied while it is not difficult to prove that the terms underneath the expectations in 9 1 become more accurate point wise in a as we are shifting the base point of the taylor expansion closer to the point we are evaluating it may be possible to construct contrived examples where this produces less accurate estimates of the expectations however we strongly believe that in almost all practical use cases one should expect a significant improvement in accuracy in using this technique as the reduction in truncation error is extremely substantial 10 numerical experiments in this section we present numerical experiments to bench mark the above methods we compare a number of different variations of operator augmentation 1 naive naive solve of the system a x b by inverting the system directly without modifying the operator a 2 ag augmentation the method presented in section 5 and algorithm 5 1 where we take r b i and let the prior on b be the standard normal distribution 24 p a etter l ying algorithm 9 1 accel shifted truncated en norm augmentation ast eag input a right hand side b an operator sample a d with corresponding parameters a choice of auto correlation matrix r default to r i a positive definite c with r c 0 default to c i truncation order n and sample count m output an estimate x of a 1 b 1 let l rc 2 factorize preprocess a to precompute a 1 if necessary 3 draw m i i d bootstrap samples a b 1 a b m d 4 for each a b i perform power method to assign a b i lim k a b ia 1 kv a 1 a b ia 1 k 1 v a 1 5 draw m i i d bootstrap samples q 1 q m n 0 l 6 assign m i 0 k 0 n a b i k a b i k 2 q ti a 1 i a b i a b ia 1 kq i m i 0 k 0 n a b i k a b i k 2 q t i a 1 i a b i a b ia 1 kq i where n k k 1 n j k 1 1 j k k n 0 o w n k k 1 k n 0 o w 7 clamp max 0 8 assign x a 1 a 1 c b 9 return x 3 eag energy norm augmentation the method presented in section 6 without any truncation i e computing the augmentation factor in 6 5 directly using bootstrap and monte carlo where we take c i and let the prior on b be the standard normal distribution 4 t eag truncated energy norm augmentation the method presented in section 7 in the numerical results we test different orders of truncation the order here denotes the highest power of a bootstrapped matrix sample which appears in the computation for the approximate augmentation factor furthermore we will also test both soft t eag s and hard t eag h truncation windows as discussed in subsection 7 1 5 ast eag accel shifted truncated energy norm augmentation the method pre sented in section 9 and algorithm 9 1 the order of truncation denotes the highest power of a bootstrapped matrix sample which appears in the computation unlike with t eag we will only benchmark the windowing function presented in 8 32 like above we take we take c i and let the prior on b be the standard normal distribution in our numerical experiments we measure two metrics of error 1 r mse relative mean squared error this is a normalized version of the error function ebayes from 5 1 with b i r mse ebayes x e a 1 b 22 e a 1 k a 1 b 22 e a 1 b 22 e a 1 k a 1 2 f a 1 2 f 10 1 operator augmentation for noisy elliptic systems 25 therefore this quantity measures both the relative error of x from the true solution x in l 2 as well as the relative error from our augmented operator a 1 k to the true operator a 1 in the frobenius norm we evaluate this quantity with monte carlo and provide a 2 estimate of the error of the monte carlo procedure 2 rel emse relative energy norm mean squared error this is defined like the above except it is defined using the energy norm a 10 2 rel emse ebayesa x e a 1 b 2 a this quantity may be of more interest than rel mse in many problems as for many elliptic systems it more heavily penalizes high frequency noise 10 1 1 d and 2 d poisson equation on a noisy background our first benchmark will be the poisson equation given by a x u x b x on d u x 0 on d 10 3 where a x 0 is a function determined by the physical background of the system we discretize this equation using finite differences as follows let gd v e be a regular grid on the domain d with vertices v and edges e let e rv e be the arbitrarily oriented incidence operator of the grid i e 10 4 ev e 1 v is incident to e 0 otherwise ev e is positive for one of the v incident to e and negative for the other the discrete approximation for the differential operator in 10 3 is given by 10 5 l ewet where w re e is a diagonal matrix whose e e th entry is the function a evaluated at the midpoint of e we suppose that we only have noisy measurements of the physical background i e that the matrix w is subject to some randomness hence in practice we only have access to an approximate 10 6 l ew et where l is drawn from a distribution d where a xe e e i e the background a evaluated at all the edge midpoints xe note to use the operator augmentation method one must prescribe a class of distributions d that we may sample from given background samples in particular the noisy background model we use for this benchmark perturbs every observa tion with independent multiplicative noise 10 7 w e e e z e e where z e z i i d for some positive distribution z to be specified to enforce dirichlet boundary conditions we solve l int gd int gd uint gd l int gd gdu gd b u gd 0 10 8 where int gd v denotes the interior of the grid gd and gd v denotes the boundary and l a b for a v and b v denotes the a b minor of l hence this becomes 10 9 a x b 26 p a etter l ying method order window r mse 2 r emse 2 naive 12 0 0352 55 1 0 1 ag 0 59 0 0203 24 6 0 448 eag 4 32 0 124 20 0 362 t eag 2 soft 4 77 0 155 39 7 0 723 t eag 4 soft 1 26 0 044 21 5 0 39 t eag 6 soft 3 11 0 0946 20 1 0 364 t eag 2 hard 0 79 0 0319 22 9 0 42 t eag 4 hard 2 71 0 0855 20 3 0 367 ast eag 2 0 798 0 029 22 7 0 406 ast eag 4 2 88 0 0913 20 2 0 366 ast eag 6 4 01 0 117 20 0 354 table 10 1 comparison of augmentation methods for a 1 d poisson problem on 128 grid points where a x 1 and z e u 0 5 1 5 method order window r mse 2 r emse 2 naive 7 52 0 0247 58 0 145 ag 0 802 0 0317 32 5 0 803 eag 6 5 0 189 24 9 0 546 t eag 2 soft 3 28 0 161 46 6 2 75 t eag 4 soft 1 15 0 0384 29 4 0 721 t eag 6 soft 5 22 0 177 25 1 0 524 t eag 2 hard 1 07 0 0385 29 7 0 716 t eag 4 hard 6 27 0 183 25 0 541 ast eag 2 1 28 0 0435 29 0 801 ast eag 4 7 04 0 203 24 7 0 507 ast eag 6 22 9 0 633 31 8 0 586 table 10 2 comparison of augmentation methods for a 1 d poisson problem on 128 grid points where a x 1 and z e 1 0 45 where a l int gd int gd and b is the function b x sampled at the interior vertices of gd in table 10 1 table 10 2 and table 10 3 we see the results of operator augmentation applied to the above poisson equation problem as we can see all our methods produce a substantial improvement in both relative mse and relative energy norm mse with ag obtaining the largest reduction in l 2 error and eag obtaining the largest reduction in energy norm error as is to be expected moreover note that the truncated methods t eag and ast eag quickly approach the efficacy of eag as one increases the truncation order with an order of 6 usually being enough to obtain an error comparable to baseline eag which requires significantly more computation for large scale problems note also that the energy error of t eag is always monotonically decreasing which agrees with theorem 7 2 moreover note that the error of ast eag is not always monotonically decreasing the unfortunate reality is that while ast eag is guaranteed to converge to eag as the order becomes large this convergence may be uneven and is not guaranteed to be monotonic like t eag with a soft window we also note that the performance of our technique is comparable across different problems i e 1 d vs 2 d as well as across different models of randomness i e discrete vs gamma 10 2 graph laplacian systems with noisy edge weights one may extend the model in the above section to general graphs g v e however convention typically dictates that the laplacian should be positive definite instead of negative definite i e 10 10 l ew et operator augmentation for noisy elliptic systems 27 method order window r mse 2 r emse 2 naive 6 43 0 105 45 3 0 11 ag 0 234 0 00974 25 0 64 eag 4 31 0 772 20 0 456 t eag 2 soft 2 57 0 465 35 6 0 908 t eag 4 soft 0 876 0 133 21 8 0 504 t eag 6 soft 2 59 0 515 20 3 0 561 t eag 2 hard 0 422 0 039 23 1 0 464 t eag 4 hard 2 13 0 353 20 5 0 519 ast eag 2 0 845 0 117 22 0 503 ast eag 4 3 62 0 586 20 1 0 499 ast eag 6 5 61 0 887 20 2 0 471 table 10 3 comparison of augmentation methods for a 2 d poisson problem on 128 x 128 grid points where a x 1 and z e u 0 4 1 6 fig 10 1 a visualization of the fb pages food graph used in our numerical experiments we give our performance results on this graph in table 10 4 and table 10 6 in this model we suppose that we are given a weighted graph g however that the true edge weights of the graph denoted by we are unknown to us but we have access to a noisy observation w e of we like in the previous setting we suppose that the observations are independent therefore the diagonal weight matrix w again has entries given by 10 11 w e e e z e e where z e z i i d for some distribution z to be specified like in the previous example we solve a dirichlet problem arbitrarily selecting approximately six vertices as our boundary g whose values we set to zero thus a l int g int g with int g v g like before and we again solve 10 9 with operator augmentation we see the results of this computation in table 10 4 and table 10 5 the graphs shown are from the network repository 8 we note that the method performs quite similarly on this problem as it does on the grid laplacian case this shows that the performance of the method 28 p a etter l ying method order window r mse 2 r emse 2 naive 46 9 0 386 46 2 0 14 ag 17 5 1 27 19 1 0 464 eag 18 1 1 37 19 1 0 502 t eag 2 soft 34 9 2 79 34 6 0 999 t eag 4 soft 18 9 1 1 20 0 433 t eag 6 soft 16 9 1 03 18 6 0 407 t eag 2 hard 20 1 1 11 21 0 438 t eag 4 hard 17 8 1 2 19 0 452 ast eag 2 20 6 1 18 21 6 0 451 ast eag 4 18 1 13 19 3 0 431 ast eag 6 18 2 1 17 19 3 0 449 table 10 4 comparison of augmentation methods for a graph laplacian system this particular weighted graph is the fb pages food graph 9 visualized in figure 10 1 in this benchmark we have z e u 0 5 1 5 fig 10 2 a visualization of the fb pages company graph used in our numerical experiments we give our performance results on this graph in table 10 4 and table 10 6 operator augmentation for noisy elliptic systems 29 method order window r mse 2 r emse 2 naive 33 8 3 16 32 4 1 28 ag 13 3 4 92 16 8 2 22 eag 13 7 5 7 16 1 2 15 t eag 2 soft 32 5 17 9 27 8 7 67 t eag 4 soft 18 11 3 18 1 4 78 t eag 6 soft 22 3 12 20 1 4 76 t eag 2 hard 11 8 4 59 15 3 2 18 t eag 4 hard 13 1 4 49 14 9 1 7 ast eag 2 11 6 4 08 15 8 1 81 ast eag 4 19 8 5 88 22 2 13 ast eag 6 35 4 11 3 34 7 4 17 table 10 5 comparison of augmentation methods for a graph laplacian system this particular weighted graph is the fb pages company graph 9 visualized in figure 10 2 in this benchmark we have z e u 0 5 1 5 is consistent across different types of problems 10 3 heat steady state with sparsified graph laplacians in many areas of com puter science one can use graph sparsification techniques to reduce the complexity of a laplacian system solve if one is able to tolerate some degree of approximation these graph sparsification techniques work by randomly selecting some subset of the edges of the graph g to remove and then re weighting the remaining edges to obtain a sparsifed graph g we consider the problem of approximating the solution to a laplacian system on g using the laplacian of g in particular suppose we are interested in the steady state heat distribution given by 10 12 l i u b where 0 is the coefficient of heat decay and b is the vector describing heat introduced to the system per unit time however we only have access to the topology of the sparsified g and its laplacian l naively one could solve 10 13 l i u b of course this naive solution carries a certain amount of error note that we can apply operator augmentation to l i to obtain a more accurate solution in particular for this numerical experiment we use the sparsification model 10 14 w e e e z e e where z e p 1 ber p i i d for p 0 1 we see in table 10 6 that our methods allow for a substantial reduction in energy norm mean squared error like in the previous two scenarios however this scenario seems to be more difficult for the augmentation process particularly the l 2 reduction is not as high as with previous examples regardless the fact that operator augmentation functions under this regime of noise shows us that operator augmentation is a technique which can be broadly applied to various problems 11 conclusion in this paper we have presented a novel method for reducing error in elliptic systems corrupted by noise that requires only a single sample of a corrupted system we have introduced the ag and eag methods as well as the t eag and ast eag methods for efficiently approximating eag moreover we have proved multiple important theorems that underlie our methods this includes the error reduction bound in theorem 5 1 for the ag method as well as monotone convergence guarantees theorem 7 2 and theorem 8 2 that provide justification and intuition for the t eag and ast eag methods 30 p a etter l ying method order window r mse 2 r emse 2 naive 18 5 0 0843 26 2 0 125 ag 12 6 0 386 16 9 0 613 eag 13 8 0 258 16 9 0 38 t eag 2 soft 16 9 0 771 23 7 1 18 t eag 4 soft 12 7 0 448 17 5 0 737 t eag 6 soft 13 0 379 17 2 0 611 t eag 2 hard 14 1 0 528 20 1 0 802 t eag 4 hard 12 2 0 326 16 4 0 498 ast eag 2 12 6 0 404 17 0 627 ast eag 4 13 7 0 315 17 0 493 ast eag 6 15 3 0 318 17 8 0 367 table 10 6 comparison of augmentation methods for a sparsified graph laplacian system this particular weighted graph is the fb pages food graph 9 visualized in figure 10 1 in this benchmark we have z e 10 75 ber 0 75 with 1 method computation l 2 energy convergence monotone naive lowest ag high best good eag high good best t eag s low good better when a 2 a always t eag h low good better when a 2 a empirically ast eag moderate good better always no table 11 1 comparison of pros cons of different augmentation methods presented in this paper l 2 and energy denote reduction in l 2 and energy norm error respectively s and h denote hard and soft windows respectively convergence denotes whether or not the method converges to eag when the order is taken to be large monotone denotes whether or not the truncated augmentation factors n of the method are monotonic furthermore we have demonstrated in our numerical experiments that the operator augmen tation methods we presented are effective in many different scenarios and different noise models consistently providing a 2 reduction in energy mean squared error and often a significantly higher reduction in l 2 error we have also shown that t eag and ast eag converge relatively quickly to eag which makes these truncated method good alternatives when solving a large number of matrix systems is computationally intractable our numerical results also make clear the relative benefits and trade offs of the different augmentation methods these are seen in table 11 1 as per these trade offs we recommend using eag if computation is not an issue if computation is an issue we recommend using hard window or soft window t eag depending on the scenario and if this approximation seems not to be performing well or the noise distribution is heavy tailed then we recommend using ast eag while the operator augmentation framework offers a new approach to reducing error in noisy elliptic systems there are still a number of interesting avenues for further exploration the most obvious is of course the extension of the operator augmentation framework machinery to the case of asymmetric systems unfortunately while there is nothing preventing one from using the same approach for asymmetric systems the question of how one would analyze such an algorithm remains open the machinery developed within does not relatively apply since the move from symmetric to asymmetric systems breaks a number of core tools used throughout since many systems of interest are indeed asymmetric this is an important direction for future research in addition while we leave the optional choice of matrices b r c up to the reader it is yet unclear how one should approach making a choice for these optional parameters in general finally to judge the performance of the method in real world problems one could apply the techniques we ve developed within to an application area where elliptic systems are corrupted by randomness operator augmentation for noisy elliptic systems 31 possible aforementioned applications include structural dynamics 10 or whether modelling 7 among a plethora of others 12 acknowledgements the work of l y is partially supported by the national science foundation under award dms 1818449 and dms 2011699 appendix a proofs of miscellaneous lemmas lemma a 1 lo wner order inversion suppose that a s rn and a d such that a s rn almost surely moreover suppose that in expectation a spectrally dominates a i e 4 9 e a a then matrix inversion inverts the expected lo wner order i e 4 10 e a 1 a 1 proof consider the exact second order taylor expansion of the inverse functional on the space of positive definite matrices a 1 a 1 a 1 a 1 a a a 1 a 1 a a a 1 a a a 1 where a is a matrix between a and a note that the last term is positive semi definite because a is positive definite therefore a 2 a 1 a 1 a 1 a a a 1 taking expectations of both sides and using the fact that e a a 0 yields a 3 e a 1 a 1 a 1 e a a a 1 a 1 lemma a 2 let x y s rn and consider the infinite taylor series for x 1 about base point y i e 6 8 x 1 y 1 2 k 0 y 1 2 x y y 1 2 k y 1 2 this series converges absolutely in the l 2 operator norm to x 1 when x 2 y moreover the tail of the series vanishes exponentially in k in the l 2 operator norm proof rewrite the series as a 4 x 1 y 1 2 k 0 i y 1 2 xy 1 2 k y 1 2 the term in brackets is a neumann series which is known to converge absolutely in the l 2 norm with exponentially vanishing tail exactly when i i y 1 2 xy 1 2 i i e 0 x 2 y since x is positive definite and y is nonsingular it follows that this is the sum k 0 i y 1 2 xy 1 2 k converges to y 1 2 xy 1 2 1 under the assumptions and the statement follows from there lemma a 3 let a 1 a 2 ak r 0 and b 1 b 2 bk r 0 be two sequences of nonneg ative real numbers with b 1 0 and consider the truncated sum ratios 7 1 n n k 1 ak n k 1 bk then if it is the case that 7 2 anbk bnak for all 0 k n e g the ratios ak bk are monotonically increasing then the sequence 1 2 k is monotonically increasing 32 p a etter l ying proof consider the following series of equivalent inequalities n n 1 n k 1 ak n k 1 bk n 1 k 1 ak n 1 k 1 bk n k 1 ak n 1 k 1 bk n k 1 bk n 1 k 1 ak an n 1 k 1 ak n 1 k 1 bk bn n 1 k 1 bk n 1 k 1 ak n 1 k 1 anbk n 1 k 1 bnak a 5 the last inequality above is clearly true because the terms in the sum on the left dominate their corresponding terms on the right therefore the first inequality is also true lemma a 4 let x be a random matrix such that x 0 a s for j k and r 0 and any matrix s we have that 8 26 e tr st x js e tr st x ks e tr st x j rs e tr st x k rs proof we make a series of simplifications the first assumption is that x is a uniform random variable over a set of not necessarily distinct outcomes x 1 xn i e has distribution a 6 d 1 n n i 1 xi where xk is the delta distribution supported at xk since any continuous distribution can be approximated by a series of discrete distributions of this above form it suffices to prove the statement for discrete distributions of the form above under this assumption the inequality 8 26 becomes a 7 i l tr stx j is tr stxkl s i l tr stx j r i s tr stxk rl s therefore it suffices to consider individual pairs i l under the sum and show that for any a b 0 tr stajs tr stbks tr stbjs tr staks tr staj rs tr stbk rs tr stbj rs tr stak rs a 8 let i a and i b denote the eigenvalues of a b respectively note that a 9 tr stajs i si i a j for some si 0 therefore a 8 amounts to i l sis l i a j l b k i b j l a k i l sis l i a j r l b k r i b j r l a k r a 10 operator augmentation for noisy elliptic systems 33 since si and s i are non negative it therefore suffices to prove that for any non negative a b 0 a 11 ajbk bjak aj rbk r bj rak r to prove a 11 define the function a 12 ca b s a s bs bs as 2 asbs cosh log a b if we take s k j 2 the claim a 11 can be rephrased as a 13 ca b s k j 2 ca b s k j 2 r so it suffices to prove ca b s is monotonic in for 0 and this follows from the fact that cosh x is monotonically increasing for x 0 and monotonically decreasing for x 0 appendix b source code for reproducibility and reference purposes we provide two implementations of the algorithms presented in this paper one of them is written in python https github com uniqueuptopermutation operatoraugmentationpython and the other is written in c https github com uniqueuptopermutation operatoraugmentationcpp we used the c version to produce the results presented in this paper references 1 g w anderson a guionnet and o zeitouni an introduction to random matrices vol 118 cambridge university press 2010 2 e j candes and y plan matrix completion with noise proceedings of the ieee 98 2010 pp 925 936 3 c davis and w m kahan the rotation of eigenvectors by a perturbation iii siam journal on numerical analysis 7 1970 pp 1 46 4 w james and c stein estimation with quadratic loss in breakthroughs in statistics springer 1992 pp 443 460 5 r keshavan a montanari and s oh matrix completion from noisy entries in advances in neural information processing systems 2009 pp 952 960 6 y marzouk t moselhy m parno and a spantini an introduction to sampling via measure transport arxiv preprint arxiv 1602 05023 2016 7 t palmer g shutts r hagedorn f doblas reyes t jung and m leutbecher representing model uncertainty in weather and climate prediction annu rev earth planet sci 33 2005 pp 163 193 8 r a rossi and n k ahmed the network data repository with interactive graph analytics and visualization in aaai 2015 http networkrepository com 9 b rozemberczki r davies r sarkar and c sutton gemsec graph embedding with self clustering in proceedings of the 2019 ieee acm international conference on advances in social networks analysis and mining 2019 acm 2019 pp 65 72 10 c soize a comprehensive overview of a non parametric probabilistic approach of model uncertainties for predictive models in structural dynamics journal of sound and vibration 288 2005 pp 623 652 11 e m stein and r shakarchi fourier analysis an introduction vol 1 princeton university press 2011 12 t tao topics in random matrix theory vol 132 american mathematical soc 2012 13 d xiu and j s hesthaven high order collocation methods for differential equations with random inputs siam journal on scientific computing 27 2005 pp 1118 1139 14 d xiu and g e karniadakis the wiener askey polynomial chaos for stochastic differential equations siam journal on scientific computing 24 2002 pp 619 644 https github com uniqueuptopermutation operatoraugmentationpython https github com uniqueuptopermutation operatoraugmentationcpp http networkrepository com 1 introduction 2 related work 3 probabilistic model and problem formalism 4 warm up basic operator augmentation 5 semi bayesian operator augmentation 5 1 general semi bayesian augmentation algorithm 6 augmentation in the energy norm 7 truncated augmentation factors for energy norm augmentation 7 1 hard windowing 7 2 quick start 7 2 1 explicit formulas for low orders 7 3 algorithm 8 shifted base point estimation 9 accelerating shifted base point estimation 10 numerical experiments 10 1 1 d and 2 d poisson equation on a noisy background 10 2 graph laplacian systems with noisy edge weights 10 3 heat steady state with sparsified graph laplacians 11 conclusion 12 acknowledgements appendix a proofs of miscellaneous lemmas appendix b source code references