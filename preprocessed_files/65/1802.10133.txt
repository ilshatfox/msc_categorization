ar x iv 1 80 2 10 13 3 v 2 m at h n a 2 5 n ov 2 01 8 coarse graining langevin dynamics using reduced order techniques lina ma 1 xiantao li 2 and chun liu 3 1 department of mathematics trinity college hartford ct 06106 usa 2 department of mathematics the pennsylvania state university university park pa 16802 6400 usa 3 department of applied mathematics illinois institute of technology chicago il 60616 usa november 27 2018 abstract this paper considers the reduction of the langevin equation arising from bio molecular models to facilitate the construction and implementation of the reduced models the problem is formulated as a reduced order modeling problem the reduced models can then be directly obtained from a galerkin projection to appropriately defined krylov subspaces the equivalence to a moment matching proce dure previously implemented in 2 is proved a particular emphasis is placed on the reduction of the stochastic noise which is absent in many order reduction problems in particular for order less than six we can show the reduced model obtained from the subspace projection automatically satisfies the fluctuation dissipation theorem details for the implementations including a bi orthogonalization procedure and the minimization of the number of matrix multiplications will be discussed as well 1 introduction langevin dynamics models arise from a wide variety of problems especially where a mechanical system is subject to random forces that can be modeled by white noise e g as in 4 a practical issue arises when the dimension of system is large in which the computational cost can be overwhelming for ex ample in bio molecular models the degrees of freedom are associated with the position and momentum of the constituting atoms and the large dimensionality makes it difficult to probe large scale biological processes over an extended period of time in this case it is of great interest to develop reduced models which in bio molecular modeling is known as coarse graining 4 1 8 1 http arxiv org abs 1802 10133 v 2 there are multiple benefits from such an approach for example reduced models can capture di rectly the dynamics of certain quantities of interest secondly with the reduction of the dimension the computational cost can be reduced dramatically in addition the quantities of interest often correspond to slow variables by eliminating fast variables the time step can also be increased considerably this allows one to access longer time scales there has been tremendous recent progress in the development of coarse grained models 7 9 1 3 7 5 0 most effort however is thermodynamics based namely one aims to construct the free energy associated with the reduced variables which then yields the driving force for the reduced dynamics known as the potential of mean forces pmf 3 4 as pointed out in 3 4 the damping mechanics which also plays an important role in the reduced dynamics is not part of the construction in this work we are interested in an equation based derivation where the reduced model can be de rived directly from the langevin dynamics deriving reduced models from a stochastic dynamical system has been a subject of extensive studies the most well known of which is the homogenization approach 9 another important approach is to employ a coordinate transformation using normal forms to sepa rate out the degrees of freedom that are less relevant 3 more recently legoll and lelievre proposed to use conditional expectations to derived reduced models 5 overall these methods require either sig nificant scale separation assumption or simple functions forms in the stochastic differential equations which for bi molecular models does not apply for example the force field for biomolecular models typically involves complicated function forms meanwhile in the field of molecular modeling there are also many methods that were proposed to coarse grain a molecular dynamics model most of these methods are derived from a hamiltonian sys tem of odes 9 1 7 8 4 either motivated by or directly obtained from the mori zwanzig projection formalism 5 9 strictly speaking such a procedure will break down for stochastic models due to the absence of the semi group evolution operator for langevin dynamics one empirical coarse graining approach is the partition method 6 in which the variables are projected into appropriate subspaces however the approach proposed in 6 does not reduced the number of variables rather it is a nu merical integration algorithm the main reduction comes from filtering out high frequency modes in the numerical algorithm in our previous work 2 we have furthered this approach by eliminating the fast variables this gives rise to a generalized langevin equation gle for the reduced variables in principle the gle under proper assumptions is an exact model after this reduction of the spatial di mensions a temporal reduction was introduced to represent the memory term with a small number of auxiliary variable known as markovian embedding this procedure approximates the gles by using an extended system of stochastic differential equations sdes with white noise the main idea is using a rational approximation for the laplace transform and the coefficients are determined based on a her mite interpolation the important advantage is that the approximation can be written as an extended system of sde with no memory a well known issue in pad type of approximations is that when more conditions are incorporated the resulting models tend to be ill posed in particular the coefficient matrices are usually ill conditioned 2 making it impractical therefore an important focus of this paper is on re formulating the coarse graining procedure into a reduced order problem which has been widely studied 3 in particular we observe a feedback loop between the coarse grain variables and the additional degrees of freedom i e the fast variables more specifically the slow variables impose a mechanical force on the fast dynamics and in turn such influences will be propagated back as a force on the slow variables as a result the elim ination of fast variables can be viewed as an order reduction problem in that it is a large dimensional dynamical system with low dimensional input and low dimensional output we will show that with an appropriate reformulation of the fast dynamics the transfer function from the order reduction prob lem corresponds precisely to the memory kernel in the gle for such problems one robust numerical method is the krylov subspace projection which uses a galerkin projection onto krylov subspaces the subspaces can be orthogonalized using the lanczos algorithm 3 1 as a result instead of man nually constructing the auxiliary system on a case by case basis as in the moment matching approach 2 we can automate the procedure numerically more importantly the bi orthogonalization alleviate the problem of having ill conditioned matrices for the current problem the presence of the noise presents another critical issue namely the ran dom noise in the gle must satisfy the second fluctuation dissipation theorem fdt 2 a necessary condition for the solution of the gle to be stationary and to have the correct variance in the galerkin projection method both the noise and the kernel function are being approximated in general they do not satisfy the second fdt unless the subspaces are properly selected we will provide two conditions that ensure such consistency and we will show krylov subspaces that fullfill these conditions this paper is organized as follows section 2 describes the derivation the gle system the classical approach of approximating the laplace transform of the memory kernel function t with a rational function will be presented section 3 presents a formulation using the galerkin projection to general subspaces criteria will be provided in order to maintain the fdt in the reduced system in section 4 we introduce appropriate krylov subspaces to fulfill the criteria the resulting system will also be compared to a moment matching procedure and the equivalence is proved in this section section 5 addresses two important issues in the numerical implementation numerical examples are shown in section 6 2 mathematical derivation 2 1 the reduction of the full langevin dynamics model we start with the full langevin dynamics model with n atoms after proper mass scaling 4 the system can be expressed as follows x t v t v t f x v t f t 1 where x x 1 x 2 xn denotes the displacement of all the atoms f x is the force derived from an empirical potentials v x with f v denotes the damping coefficient for the friction term with 3 dimension r 3 n 3 n and f t is a stochastic force usually modeled by a gaussian white noise which satisfies the fluctuation dissipation theorem fdt f t f t 2 kb t t t 2 for example the random force can be written in the conventional form d f t dw t with w t being the standard brownian motion and 2 kb t here kb is the boltzmann constant and t is the temperature of the system this fdt is crucial to ensure that the system reaches the correct equilibrium state 2 implementing the full langevin dynamics model can be very expensive due to the large number of atoms involved in the entire system here we briefly go over a reduction procedure more details can be found in 2 the first step in the reduction procedure is to identify slow variables which at the same time are sufficient to describe the overall dynamics in principle these variables can be selected by transforming the system into normal forms 3 for bio molecules a more intuitive and more efficient approach is based on the residues the building blocks of proteins by choosing the center of mass of each amino acid mathematically this can be expressed as a small number of basis functions 2 which span a subspace denoted here by y with its orthogonal complement denoted by y y has dimension m and y has dimension 3 n m m 3 n we denote the basis vectors by i and i respectively as follows y span 1 2 m y span 1 2 3 n m taking these basis vectors as columns and forming matrices and one can decompose the solution x in the following form x t q t t 3 where q rm and r 3 n m are nodal values associated with the basis vectors similarly v t p t t meanwhile a linearization of the force f ax is considered e g by principal component analysis pca 5 x t x t kb t a 1 which ensures that the covariance of the displacement is correct now define the following projected matrices and vectors a 11 a a 12 a 11 12 f 1 f t a 21 a a 22 a 21 22 f 2 f t 4 by using this partition of variables the original langevin dynamics can be written in terms of the following first order stochastic differential equations sdes q t p t p t f q a 12 t 11 p t 12 t f 1 t 4 t t t a 21 q t a 22 t 21 p t 22 t f 2 t 5 the linearization of the high frequency modes has been based on numerous observations e g 5 es sentially we assume that the low frequency can be well captured by the basis functions in and the high frequency is nearly gaussian for example in the rotation translation block rtb approach each residue is allowed to move as a rigid body there is overwhelming evidence that the low frequency nor mal modes are well represented by the subspace spanned by such basis functions 2 here q p are the reduced coarse grained variables notice that the interactions involving the fast variables have been linearized by eliminating we have derived a low dimensional reduced model 2 q t p t p t feff q 11 p t t 0 t p d f t 6 the effective force for the reduced system is feff q f q a 12 a 1 22 a 21 q 7 compared to system 4 the force feff has an extra term a 12 a 1 22 a 21 q from the derivation t is the memory kernel function which is expressed in terms of a matrix exponential t a 12 12 e dt a 1 22 0 0 i a 21 21 8 where the matrix d r 6 n 2 m 6 n 2 m is defined as d 0 i a 22 22 9 it has also been shown in 2 f f 1 t a 12 12 0 e d t s 0 f 2 s d s a 12 12 e dt 0 a 1 22 a 21 q 0 0 10 this random force is a stationary gaussian random process with mean zero satisfying the second fluctuation dissipation theorem f t f t 2 kb t 11 t t kb t t t 11 5 equation 6 is known as the generalized langevin equation gle currently there are primarily three existing methods to solve the gle numerically the first approach is to directly approximate the mem ory term either by using quadrature formula or by approximating the kernel function with a sum of exponentials known as the prony sum the later approach replaces the memory integral by additional variables that can be updated using certain recurrence formulas or by solving an odes system 8 the random noise can be approximated by introducing noises in those odes however the approximation of the sum of exponentials requires the values of the kernel function 8 which is difficult to compute due to the large dimensionality of the matrix d in the matrix exponential the second approach is to eliminate the memory effect by approximating the kernel function with a delta function in time 6 0 this approximation can be quite effective when the memory effect is not strong but in general the ac curacy is quite limited the third approach is to approximate the memory effect by introducing auxiliary variables this has been motivated by the mori s continued fraction approach 5 and has been pursued by many groups 0 9 2 for example in 2 the first order approximation leads to an extended dynamics with auxiliary vari able z q t p t p t feff q 11 p t z t f 1 t z t b z t c p t t 12 the coefficients b and c can be found by using a moment matching procedure and we will elaborate on such procedures in section 4 1 at the same time methods have been established to sample the additive noise t to ensure the fdt 11 in theory it is possible to advance to high order approximations using the above methods e g a third order method 2 however in practice the matrices generated from the moment matching procedure tend to become ill conditioned as the order of approximation increases moreover the covariance of the noise and the covariance of the auxiliary variable z need to be constructed specifically for each order of approximation to ensure the fdt 11 which is nontrivial therefore it is important to develop an alternative method to improve the robustness and automate the procedure inspired by order reduction methods for large scale dynamical system we will formulate the current problem as an order reduction problem with stochastic noise the key is to identify the low dimension input and low dimension out put 3 model reduction for the stochastic model 3 1 a reformulation of the orthogonal dynamics we will first introduce vector and matrix notations to rewrite the system 5 in a more compact form let y represents the partitioned variables and u t q p represents the coarse grained variables 6 system 5 can be rewritten as the following sdes y t d y t r u t g t y 0 n 0 kb t q 13 with r 0 0 a 21 21 g 0 f 2 t 14 the matrix q determines the initial covariance of y given by q a 1 22 0 0 i 15 further we let be the variance of the gaussian noise g t it follows the lyapunov equation to ensure the stationarity of the solution kb t dq qd 16 it can be directly verified that 0 0 0 2 kb t 2 2 17 at the same time we define l a 12 12 r a 1 22 a 21 21 18 now the equation 4 can be written as q t p t p t f q 11 p t ly f 1 t 19 the corresponding memory kernel in 6 is given by t le dt r 20 it is at this point that we recognize the similarity to an order reduction problem the large dimensional dynamics 13 contains an input variable u t which is low dimensional moreover of direct importance to the coarse grained dynamics 19 is ly which again is low dimensional also observed however is that the dimensions of l and r are different fortunately we can reformulate the problem into the fol lowing equivalent dynamics 21 where the input and output dimensions are the same q t p t p t feff q 11 p t ly f 1 t y t d y t rp t g t y 0 n 0 kb t q 21 7 theorem 1 consider the following dynamics y 1 t d y 1 t rp t g t y 1 0 n 0 kb t q 22 with a substitution into the first two equations in 21 in which y is replaced by y 1 one obtains a gle that is equivalent to 6 proof using a variation of constant formula we find y 1 t e dt y 1 0 t 0 e d t rp d t 0 e d t g d next we define the out quantity w 1 t from 22 w 1 t ly 1 t 0 t p d le dt y 1 0 t 0 le d t g d t 0 t p d t 23 here is the sum of the last two terms for t t we have t t kb t le dt qe d t l t 0 t 0 le d t g g ed t l d d kb t le d t t ql 24 the second step can be carried out by using the it s isometry now we replace the term a 12 t 12 t by w 1 t in system 21 we have q t p t p t feff q 11 p t t 0 t p d t f 1 t 25 let f 1 t f 1 t t with the assumption that the initial data of y 1 is uncorrelated with the noise term we get f 1 t f 1 t 2 kb t 11 t t 2 kb t le d t t 0 21 kb t le d t t ql 2 kb t 11 t t kb t t t 26 the last step requires that ql 2 0 21 r 27 which can be easily verified now according to theory of gaussian processes 0 the processes f t and f 1 t are equivalent finally the memory terms in 25 with 6 are the same the proof of equivalence is thus completed 8 it is clear that the dynamics 22 is very similar to dynamics 13 with subtle modification p t in stead of u t is involved in the system more importantly in 22 the input and the output of the dynamics have the same dimension our following discussion will be based on y 1 and instead we will denote this term as y due to the equivalence we now have formulated the problem as a reduce order problem the dynamics of y involves a large dimensional dynamical system in which the variable p t is acting as a control variable meanwhile what is of interest to the coarse grained dynamics is the quantity ly as a result we have at hand a large dynamical system with low dimensional input and a low dimensional output 3 2 properties of general galerkin projections a remarkable success in order reduction problems is the galerkin projection method to appropriately defined subspaces 7 motivated by such success we first consider a general galerkin projection of the sdes 22 y t d y t rp t g t 28 more specifically we seek y t in the subspace xn span v 1 v 2 vn with each basis having m columns we denote the space of test functions by x n span w 1 wn now the projection can be stated as fol lows find y t xn such that for any t x n y t d y t rp t g t t 0 to put it in a matrix vector form let v v 1 v 2 vn and w w 1 w 2 wn and we choose the columns as the basis for the two subspaces the approximate solution is written as y t v z t 29 with z t being the nodal values then the galerkin projection yields m z t d z t w rp t w g t 30 where we have defined m w v d w dv 31 with the assumption that m is nonsingular we can write z t m 1 d z t m 1 w rp t f t 32 where f t m 1 w f t 33 9 and its covariance matrix is given by f t f t t t m 1 w w m 34 with this reduction we can now write down the reduced model involving the variables p q z q t p t p t feff q 11 p t lv z t f 1 t 35 a z t m 1 d z t m 1 w rp t f t 35 b in contrast to the conventional order reduction problems the current approach yields a noise term its presence brings up an important issue appropriate conditions are needed to ensure that the solution reaches correct equilibrium which will be addressed here due to ergodicity the solution of the original sde y t will evolve into a stationary process and we expect the approximate solution to become a stationary process as well assuming that the initial variance of z is kb t q that is z 0 z 0 kb t q 36 then the stationarity implies that q must satisfy the lyapunov equation 2 kb t m 1 d q q d m condition a this condition as one of the necessary conditions to ensure the second fdt will be referred to as con dition a meanwhile the projected dynamics 35 corresponds to an approximation of the gles 6 this can be verified by directly solving 28 and then substitute ly into the equation for p with direct calcula tions we find that the approximated kernel can be expressed as t t lv e m 1 d t m 1 w r 37 moreover the low dimensional output is approximated by w t w t ly t 0 t p d t 38 where t lv e m 1 d t z 0 t 0 lv e m 1 d t f d as a result we obtain an approximate gle model q t p t p t feff q 11 p t t 0 t p d t f 1 t 39 10 the term t introduces an added gaussian noise to the coarse grained dynamics together with the lyapunov equation condition a we can express its time correlation as follows t t kb t lv e m 1 d t t q v l kb t t t t lv e m 1 d t q v l 40 clearly in general the correlation of the noise t in 40 might not be consistent with the memory kernel t in 39 and 37 namely the second fdt a necessary condition for the reduced model to have the correct statistics may not be fulfilled the following theorem identifies the condition under which such consistency can be guaranteed theorem 2 the coarse grained dynamics 35 and 39 derived from the petrov galerkin projection will obey the second fdt 11 if the following condition is satisfied m q v l w ql proof recall that w t ly from 35 b needs to be injected into the dynamics of the reduced variables 35 a the resulting random noise is t f 1 t with time correlation t t 2 kb t 11 t t kb t lv e m 1 d t t q v l 2 kb t lv e m 1 d t t m 1 w 0 21 41 it is clear that if q v l 2 m 1 w 0 21 m 1 w r 42 this will result in the second fdt t t 2 kb t 11 t t kb t t t in light of equation 27 equation 42 is equivalent to m q v l w ql condition b this equation will be referred to as condition b conditions a and b constitute the basis for constructing consistent stochastic reduced models while condition a can be enforced by solving the lyapunov equation condition b may not be satisfied by an arbitrary galerkin projection therefore we need to choose appropriate subspaces for this to hold auto matically 11 4 the projection to krylov subspaces in this section we will construct krylov subspaces for the galerkin projection procedure which subse quently leads to approximations of the memory kernel function and random force we will also discuss several issues related to the practical implementations it turns out that the krylov subspace approach has a close connection to a two point pad approx imation previously studied in 9 2 6 to incorporate both long time and short time statistics we will review this approach briefly which will be referred to as moment matching and then make connections to the krylov subspace projection approach we will consider the case where the damping coefficient is constant i e i 4 1 the moment matching approach define the moments m 0 0 m 1 0 m 0 m 0 t d t 43 notice that the moment m corresponds to the correlation time with the moments the memory func tion at t 0 can be expanded as t m 0 m 1 t m 2 2 t 2 m t since the exact memory kernel is ledt r it is clear that the moments are given by m 0 lr m 1 ldr m ld r m ld 1 r meanwhile the laplace transform can be expanded near zero s m 0 s m 1 s 2 m s 1 44 which can be obtained by repeated integration by parts the moment matching procedure is essentially a rational approximation for the laplace transform of the memory kernel n s s n i s n 1 b 0 s n 2 b 1 bn 1 1 s n 1 c 0 s n 2 c 1 cn 1 such that n 0 0 m n 0 0 m for i 0 2 n 2 to solve for the coefficients bi one needs to solve a linear system m m 0 mn 2 m 0 m 1 mn 1 mn 2 mn 1 m 2 n 3 bn 1 bn 2 b 0 mn 1 mn m 2 n 2 45 12 we will use the second order approximation as an example n 2 in this case the approximation would proceed as follows 1 set the laplace transform of the approximated kernel to 2 s s 2 sb 0 b 1 1 sc 0 c 1 2 solve for the coefficients using the moments m m 0 m 0 m 1 b 1 b 0 m 1 m 2 c 0 m 0 c 1 b 1 m 46 3 the approximate kernel function in the real time domain can be expressed as 2 t 0 i e b t c where b 0 b 1 i b 0 c c 1 c 0 47 remark 1 once b and c are computed the variance of the random noise in the stochastic equation z b z c p t as well as the variance of the z 0 will be chosen based on these two matrices to satisfy the fdt such computation is quite involved in general fortunately as we will show the subspace projection approach simplifies this effort considerably remark 2 although one can increase the order of the approximation by simply introducing more mo ments there remains an important practical problem that is the condition number of the matrix in equation 45 increases rapidly as the order increases we hereby list the condition numbers in the fol lowing table 1 for a test problem table 1 condition numbers of the matrix in 45 in the moment matching procedure approximation order 2 3 4 5 6 7 matrix condition number 4 98 e 03 1 59 e 12 4 57 e 14 1 11 e 22 5 58 e 27 1 76 e 33 we now turn to the krylov subspace projections 4 2 first order subspace projection n 1 as the first approximation we choose the subspaces v r and w d l 48 we show that the resulting approximate kernel function is the same as that from the moment match ing approach 13 theorem 3 by taking v r and w d l in the galerkin projection the memory kernel 1 t in the projected dynamics 39 is equivalent to that from the first order moment matching method in particular two moments are matched exactly by the approximate kernel functions 0 m 0 and 0 1 t d t m 49 proof with direct computation we get from 37 that 1 0 lv m 1 w r 0 1 t d t lv d 1 w r by the particular choice of v and w 48 we have m w r therefore 1 0 lr m 0 0 1 t d t w r ld 1 r m theorem 4 by taking v r and w d l in the galerkin projection the projected dynamics 39 will automatically satisfy the second fdt 11 proof we need to show that condition b is satisfied by this choice of w and v in this case given 34 and condition a we have w w m m kb t d q m m q d notice that since w d l one has d lv in addition from equation 16 we have lqw w ql lv q m m q v l it is clear that on both sides it is a summation of a matrix and its transpose by moving terms we find lqw lv q m m q v l w ql 50 and condition b would hold if either side equals to zero we will examine the two terms on the right hand side since i we have 12 0 further notice that d 1 a 1 22 22 a 1 22 i 0 by direct calculations the second term on the right hand side can be simplified to w ql ld 1 ql a 12 a 1 22 22 a 1 22 a 21 m regarding the first term on the right hand side of 50 it can be directly verified that m ld 1 r m d lr m 0 14 which are both symmetric matrices further by using equation 34 we get that 2 kb t m 1 then the lyapunov equation condition a becomes kb t m 1 m 0 q q m 0 m 1 2 kb t m 1 from which we obtain the solution q m 1 0 therefore the first term on the right hand side of 50 becomes m q v l m m 1 0 r l m which would cancel the second term and complete the proof 4 3 second order subspace projection n 2 we now extend the subspace by choosing v r dr and w d l l 51 as a result the two matrices m and d in the galerkin formulation are given by m m m 0 m 0 m 1 d m 0 m 1 m 1 m 2 52 it s easy to check that m 1 d b as in equation 47 within this extended approximation the approxi mate memory function is given by 2 t m 0 m 1 e b t m 1 m m 0 m 0 m 1 e b t i 0 53 we first show that this approximation is equivalent to the moment matching procedure it is straight forward to verify that the approximate kernel denoted by 2 from the moment matching procedure should satisfy the following second order differential equation 2 t b 0 2 t b 1 2 t 2 0 m 0 2 0 m 1 54 we now show that the kernel function 2 t follows the same equation thanks to the uniqueness we can then conclude the equivalence the key observation is that m 0 m 1 b m 1 m 2 as a result it can be quickly verified that b 1 m 0 m 1 b 0 m 0 m 1 b b 1 m 0 m 1 b 0 m 1 m 2 m 1 m 2 b m 0 m 1 b 2 15 which combined with 53 would lead to 2 t b 0 2 t b 1 2 t 2 0 m 0 2 0 m 1 55 therefore we have this following theorem theorem 5 the reduced model 39 from the galerkin projection with the choice of v r dr and w d l l produces an approximate memory kernel function which is equivalent to that from the second order moment matching procedure furthermore we have theorem 6 the projected system 39 with the choice of v r dr and w d l l will automat ically satisfy the second fdt 11 proof we only need to justify condition b it is straightforward to show that w ql ld 1 l a 1 22 0 0 i a 21 0 w r m m 0 with the choice of v we have v l m 0 m 1 m 0 0 notice m 1 0 which can be verified by direct calculation then by some direct calculations with the representation of the covariance matrix we have m m w w 2 kb t m 0 0 0 kb t d q m m q d meanwhile we have m m m 0 m 0 0 which gives m 1 0 m 1 0 m 1 0 m 1 0 m m 1 0 2 kb t 0 0 0 m 1 0 m m 1 0 now we solve the lyapunov equation and we find that q m 1 0 0 0 m 1 2 with q available it can be verified that m q v l m 0 0 w ql which is our condition b thus it completes the proof 16 4 4 generalization to high order approximation n 2 inspired by the previous choices we consider v r dr d n 1 r w d l l d l d n 2 l 56 and apply galerkin projection to the two subspaces generated by the columns of these two matrices the corresponding matrices m d b and w r are given by respectively m m m 0 mn 2 m 0 m 1 mn 1 mn 2 m 2 n 3 d m 0 m 1 mn 1 m 1 m 2 mn mn 1 m 2 n 2 57 b m 1 d 0 0 0 bn 1 i 0 0 bn 2 0 i 0 0 0 i b 0 w r m m 0 mn 2 58 therefore the approximate kernel under the galerkin projection can be expressed as n t m 0 m 1 mn 1 e b t i 0 0 59 meanwhile the high order approximate memory kernel from the moment matching procedure sat isfies the nth order differential equation n n t b 0 n 1 n t b 1 n 2 n t bn 1 n t n 0 m 0 n 1 n 0 mn 1 we first show that these approximate kernel functions are the same theorem 7 the function n t in equation 59 is equivalent to the function n t generated from moment matching procedure as described in section 4 1 in particular it also satisfies the initial value problem n n t b 0 n 1 n t b 1 n 2 n t bn 1 n t n 0 m 0 n 1 n 0 mn 1 proof each mi is a m by m matrix and the dimension of n t is also m m for simpler notations we will denote mi mi 1 mi n 1 gi if we can show that g 0 b n b 0 g 0 b n 1 b 1 g 0 b n 2 bn 1 g 0 17 this will prove n t satisfies the same differential equation notice that the recursive relation gi b gi 1 for i 0 n 2 comes straightforward since m b d then it remains to check that gn 1 b b 0 gn 1 b 1 gn 2 bn 1 g 0 we will take a closer look at each block elements the first block on the left hand side is mn and on the right hand side we have n 1 i 0 mi bn 1 i they are equal due to the equation m b d in fact all other blocks except the last one can be shown from the same equation the last block automatically equal to each other since they have exactly the same representation for the initial conditions they can be easily verified using m b d what we will study next is whether this formulation also obeys the second fdt however we are not able to prove the general case due to the lengthy calculations involved we are able to prove the consistency for n 5 the following few results are useful for the verification numerical tests suggest that the consistency holds also for higher order cases lemma 1 the moments of the memory function are all symmetric matrices as a result m and d as defined in equation 31 are also symmetric matrices proof we only need to show all moments mi are symmetric recall that d 0 i a 22 22 0 i i 22 a 22 0 0 i and r a 1 22 a 21 21 a 1 22 0 0 i a 21 21 therefore for i 0 mi ld i r a 12 12 0 i i 22 a 22 0 0 i 0 i i 22 a 22 0 0 i a 1 22 0 0 i a 21 21 a 12 12 0 i i 22 a 22 0 0 i 0 i i 22 a 21 21 which is clearly symmetric at the same time it is straightforward to see that m 0 is symmetric by direct calculation finally m a 12 12 a 1 22 22 a 1 22 i 0 a 1 22 0 0 i a 21 21 a 12 12 a 1 22 22 a 1 22 a 1 22 a 1 22 0 a 21 21 is symmetric as well 18 lemma 2 assume that m is invertible condition b is equivalent to q m 0 m 1 mn 1 i 0 0 60 proof when i 12 0 we have the identity ql r it is also easy to see that due to symmetry from lemma 1 one has v l m 0 m 1 m n 1 m 0 m 1 mn 1 therefore condition b becomes m q m 0 m 1 mn 1 w r m m 0 mn 2 multiplying both sides by m 1 with the assumption that m is invertible we arrive at equation 60 lemma 3 let w w which has dimension nm nm if it is partitioned into a block matrix with each block having dimension m m then the block elements have the following recurrence relations 1 1 2 m i 2 0 2 i 0 61 i j 1 i 1 j 1 i j 1 2 kb t mi j 3 i j 2 62 as a result the elements of can be constructed column by column using the recurrence relation they can be expressed in terms of the moments mi s the next lemma shows that the moments also exhibit a recurrence relation which can be exploited to make the calculation a bit easier lemma 4 the moments mi ld i r can be written as a linear combination of matrices a 12 a k 22 a 21 mi i 2 1 k 0 ci k a 12 a k 22 a 21 the proof of these lemmas can be found in the appendix theorem 8 the reduced system 39 from the petrov galerkin projection obeys the second fdt for orders n 5 19 proof it now becomes clear that in order to check wether the second fdt holds for high order approxi mation system one only needs to show equation 60 on the other hand we know q is the solution to lyapunov equation condition a which uniquely determined therefore under the assumption that d is nonsingular q q d is also uniquely determined this also leads to the following equation based on the fact that m is symmetric kb t q m m q m m w w now the goal is to compute the exact form of q we will present the expression of q and w w for n 3 4 5 and readers can substitute those forms into the equation above to verify there are some identities needed in order to complete the verification which we will present in the appendix for n 3 kb t 2 m 0 2 m 0 0 0 0 2 m 0 0 2 m 2 q i 0 0 0 i 0 0 2 i i for n 4 kb t 2 m 0 2 m 0 2 2 m 0 0 0 0 0 2 m 0 0 2 m 2 2 m 3 2 2 m 0 0 2 m 3 2 2 m 3 q i 0 0 0 0 i 0 0 0 2 i i 0 0 2 2 i 2 i i for n 5 kb t 2 m 0 2 m 0 2 2 m 0 2 3 m 0 2 m 2 0 0 0 0 0 2 m 0 0 2 m 2 2 m 3 2 2 m 3 2 2 m 0 0 2 m 3 2 2 m 3 2 2 m 4 2 3 m 0 2 m 2 0 2 m 4 2 2 m 4 2 3 m 4 2 2 m 5 2 m 6 q i 0 0 0 0 0 i 0 0 0 0 2 i i 0 0 0 2 2 i 2 i i 0 0 2 3 i 4 2 i 4 i i 5 numerical implementation in this section we will describe the numerical implementation of the krylov subspace projection method in the previous section we have studied properties of the projected dynamics with particular choices of 20 v and w however as is well known a direct implementation using those matrices often leads to ill conditioned matrices this has clearly been shown in table 1 a much more robust approach is to obtain orthogonal basis by using appropriate orthogonalization algorithms let us first introduce the notations for these two krylov subspaces for an nth order approximation kn d r span r dr d n 1 r kn d d l span d l l d n 2 l 5 1 block lanczos algorithms blbio we will adopt the non symmetric block lanczos algorithms from 1 to generate orthogonal basis v v 1 vn and w w 1 wn for kn d r and kn d d l respectively the lanczos algorithm proceeds as follows choose v 1 r w 1 d l and let 1 w 1 v 1 and for k 1 2 compute a k w k dvk 63 k 1 k a k k k a k 64 k 1 1 k 1 k 1 k k 1 k 1 k 1 k if n 0 65 vtmp dvk vk k vk 1 k 1 wtmp d wk wk k wk 1 k 1 66 tmp w tmp vtmp 67 choose k k and k 1 s t k k 1 k tmp 68 several possible choices have been recommended in 1 for k k and k 1 we found that the qr factorization with column pivoting for vtmp and wtmp is quite robust namely vtmp p u r wtmp p u r then we choose vk 1 u wk 1 u k rp k r p by following this algorithm we obtain the orthogonality properties among the basis vectors of the krylov subspaces in particular the matrix m is diagonal and the matrix d is block tridiagonal as a result the sdes for the auxiliary variable z 32 involves sparse matrices 5 2 implementation without the implementation of the algorithm requires the matrices l v w and r all involving the matrix as part of the construction constructing is usually not feasible for large systems here we present an algorithm that does not involve let s first derive a few useful identities involving the matrix we start with a a 11 a 12 a 21 a 22 a 1 a 1 21 using a block inversion formula we get a 1 1 a 11 a 12 a 1 22 a 21 by left multiplying the equation by a 1 together with the identity i we find that a 1 a 1 1 a 1 22 a 21 69 next right multiplying the above equation by a 1 we arrive at a 1 a 1 1 a 1 a 1 a 1 22 a a 1 a 1 a 1 22 i a 1 70 now we define d 0 0 d 0 0 r 0 0 r l a 0 0 d 1 0 0 we start with the following observation lemma 5 the following relation holds between the two krylov subspaces kn d r and kn d r 0 0 kn d r kn d r similarly 0 0 kn d d l kn d l with these observations we show that theorem 9 the lanczos algorithm the galerkin projection and the sampling of the noise can be done without proof first it can be directly shown that d 0 a thanks again to the identity i 71 we can evaluate through the matrix therefore the calculation of d can be done without secondly to compute l we notice that the terms involving are a 1 22 22 and a 1 22 and these terms can be represented without from equation 70 and 71 the calculation of r is similar 22 thirdly we see that the solution of the projected dynamics 32 enters the coarse grained dynamics via a matrix multiplication by lv it is straightforward to write l as l a 0 0 which means that for the term lv we can actually compute a v where v is constructed using using the block lanczos from space kn d r now let v and w be the basis generated from the orthogonalization of the new krylov subspaces kn d r and kn d l respectively therefore the matrices m w v w v d w dv w d v and w r w r can all be generated without introducing finally it remains to show that the sampling of the noise does not have to involve which is clearly true since the noise is represented as m 1 w 0 f and w w it is a trivial but important point in practice that in the numerical implementation it is not necessary to store the full matrix for a given vector u the multiplication u can be done through u 5 3 a summary of the galerkin projection the galerkin projection method can be summarized as follows 1 choose appropriate basis matrix 2 pick the order of approximation n 1 use the block lanczos algorithm to determine the orthog onal basis v and w for the krylov subspaces kn d r and kn d d l respectively 3 solve the stochastic differential equations 35 where m d f are defined from equations 31 and 33 the initial variance of z t is determine from condition a clearly this procedure avoided manual constructions of the reduced model this choice of the krylov subspaces guarantees that the fdt is satisfied through condition b at least till the fifth order of ap proximation numerical tests indicate that this is true for higher order cases 6 numerical test we test our algorithm on the example considered in 2 we simulate the dynamics of the protein chigno lin pdb id 1 uao at temperature t 298 for 4 nano seconds the system is set up in solvation modeled by the generalized born gb model and simulations have been conducted in tinker 0 using force field charmm 22 for the surrounding bath we considered the case 91 ps 1 which corresponds to water solvant 0 and a low friction case 5 ps 1 in the latter case the kernel function exhibits nontrivial be havior 2 it tends to be more oscillatory compared to the former case by calculating the eigenvalues of 23 a we have identified the under damped regime to be 13 4 and the over damped regime to 997 7 data are collected to compute the pca matrix a kb t x x 1 the projection matrix are composed of rtb basis 2 since there are 10 residues in chignolin the dimension of the coarse grained variables is 60 the explicit forms of the basis functions in each each translational and rotational mode can be found in we first present the numerical result for 91 in figure 1 on the left panel we showed the com parison of approximating memory function from order two to order seven the right panel of the figure provides the comparison of time correlation of the momentum both exact plots are obtained by run ning the full model the order of approximations n is defined as the order of krylov subspaces which is equivalent to the order of the rational functions in the moment matching approach since the kernel function t is matrix valued we chose the sixth diagonal 6 6 t for the comparison this index cor responds to the third rotational component of the first residue we can observe the approximation is satisfactory for n 5 0 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 1 time ps 0 1000 2000 3000 4000 5000 6000 7000 m e m o ry f u n ct io n exact second order third order forth order fifth order sixth order seventh order 0 0 02 0 04 0 06 0 08 0 1 0 1000 2000 3000 4000 5000 6000 7000 0 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 1 time ps 50 0 50 100 150 200 250 exact second order third order forth order fifth order sixth order seventh order 0 0 02 0 04 0 06 0 08 0 1 50 0 50 100 150 200 250 figure 1 numerical result for 91 from second order approximation to seventh order approximation all compared to exact solution left the memory kernel function right velocity auto correlation both plots are for the third rotation component of the first residue in figure 2 we present a comparison for 5 the small damping constant leads to a underdamped system making the approximation difficult due to the rapid and non trivial oscillation however we can observe substantial improvement of the accuracy on the memory kernel the memory effect on auto correlation is evident compared to system with high damping constant though improvement is significant for the memory kernel the velocity time correlation exhibits noticeable error in figure 3 we provide a close up view over the time interval 0 0 2 ps and show results from secon order to seventh order approximations we observe increased accuracy as the order of the approximation is increased within this time period finally we present the relative l 2 error for both memory kernel and time correlation comparing the results of second order and seventh order for both 91 and 5 in figure 4 this relative l 2 error is 24 0 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 1 time ps 6000 4000 2000 0 2000 4000 6000 8000 m e m o ry f u n ct io n exact second order seventh order 0 0 02 0 04 0 06 0 08 0 1 5000 0 5000 0 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 1 time ps 100 50 0 50 100 150 200 250 exact second order seventh order 0 0 02 0 04 0 06 0 08 0 1 100 0 100 200 300 figure 2 numerical result for 5 figures show the comparison of the exact solution second order approximation and seventh order approximation left the memory kernel function right velocity auto correlation both plots are for the third rotation component of the first residue 0 0 02 0 04 0 06 0 08 0 1 0 12 0 14 0 16 0 18 0 2 time ps 6000 4000 2000 0 2000 4000 6000 8000 m e m o ry f u n ct io n exact second order third order forth order fifth order sixth order seventh order 0 0 02 0 04 0 06 0 08 0 1 0 12 0 14 0 16 0 18 0 2 time ps 100 50 0 50 100 150 200 250 exact second order third order forth order fifth order sixth order seventh order figure 3 numerical result for 5 comparison of the second order through seventh order approxi mations left the memory kernel function right velocity auto correlation both plots are for the third rotation component of the first residue 25 computed for the time period 0 1 we showed error for each coarse grained variables and improvement of accuracy is significant 0 10 20 30 40 50 60 coarse grained variable index 0 0 1 0 2 0 3 0 4 0 5 0 6 0 7 second order seventh order 0 10 20 30 40 50 60 coarse grained variable index 0 3 0 4 0 5 0 6 0 7 0 8 0 9 1 1 1 1 2 second order seventh order 0 10 20 30 40 50 60 coarse grained variable index 0 0 01 0 02 0 03 0 04 0 05 0 06 0 07 0 08 0 09 0 1 second order seventh order 0 10 20 30 40 50 60 coarse grained variable index 0 0 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 second order seventh order figure 4 comparison between second and seventh order projection using the relative l 2 error for each coarse grained variable top the memory kernel function bottom the time correlation left 91 right 5 in addition to the krylov subspaces that were presented in the previous section we also implemented inverse krylov subspaces and shifted inverse krylov subspaces in the galerkin projection these varia tions can often offer better approximations to the transfer function in order reduction problems which in our case corresponds to the memory function however through our numerical computations we found that none of these choices satisfies condition b this implies that the second fdt is not fulfilled and the reduced dynamics 35 does not produce stationary processes 2 8 in fact the variance of the solution will follow the dynamic lyapunov equation 8 eqn 3 103 and it will not converge to the steady state 26 7 conclusion we adopted reduced order modeling techniques to reduce the langevin dynamics model we consider reduced models obtained from petrov galerkin projections by selecting appropriate krylov spaces we show the mathematical equivalence of the proposed model to the reduced models derived from mo ment matching procedure another emphasis is placed on the statistical consistency i e the fluctuation dissipation theorem we are able to identify two conditions that ensure such consistency we also showed that the galerkin projections to the selected subspaces automatically satisfy the fdt at least for n 5 with the block lanczos algorithm the models derived this way are more robust one open issue is the case when the damping coefficient is not proportional to an identity matrix in this case both condition a and condition b are still sufficient to ensure the fdt but the krylov subspaces construction in section 4 may not satisfy condition b another open question is whether one can bypass the linear approximations used in 4 and 5 it seems that a different methodology is needed to derive the generalized langevin equation 6 these issues will be addressed in future works acknowledgement the research of li is supported by nsf grant dms 1522617 and dms 1619661 the research of liu is supported by nsf grant nsf grant dms 1759535 and dms 1759536 a recurrence formula for w w this is the proof of lemma 3 for the nth approximation w w ld 1 d l ld 1 l ld 1 d l ld 1 dn 2 l l d l l l l dn 2 l ldn 2 d t l ldn 2 dn 2 l the block element of on the i th row and j th column is given by i j ld i 2 d j 2 l by using equation 16 we arrive at i j kb t ld i 1 q d j 2 l ld i 2 q d j 1 l next we define matrix s a 1 22 0 0 i it can be easily seen that since i we have sd ds sl r q s 1 2 kb t 27 with these identities we are able to manipulate terms and get i j kb t ld i 1 s d j 2 l kb t ld i 2 s d j 1 l 1 ld i 1 d j 2 l 1 ld i 1 d j 2 l 2 kb t ld i j 3 r 1 i 1 j 1 i j 1 1 i 1 j 1 i j 1 2 kb t mi j 3 where we have used the notation m 1 m meanwhile the block elements of in the second column and the second row are all zeros since by direct calculation l 0 furthermore we have 11 2 kb t m for example when j 1 we have i 1 i 1 j 2 kb t mi 3 i 3 therefore we are able to write out entries of using mi s b representation of mi s this is the proof of lemma 4 this calculation is based on the formulas in 20 and 44 for the memory function t the laplace transform will be given by l 1 i d r l i i a 22 1 i 1 r l i 2 1 1 a 22 1 r 72 here we have used a block inversion formula and the fact that the second block in both l and r are zero at this point we can invoke the neumann series of the first diagonal block and we have a 12 a 1 22 a 21 3 a 12 1 2 2 a 21 5 a 12 1 2 2 2 a 22 a 21 73 therefore the patterns in the representation of mi s can be observed as an example the first few moments are listed below m a 12 a 1 22 2 a 21 m 0 a 12 a 1 22 a 21 m 1 0 m 2 a 12 a 21 m 3 a 12 a 21 m 4 a 12 a 22 a 21 2 a 12 a 21 m 5 2 a 12 a 22 a 21 3 a 12 a 21 m 6 a 12 a 22 a 21 3 2 a 12 a 22 a 21 4 a 12 a 21 m 7 3 a 12 a 2 22 a 21 4 3 a 12 a 22 a 21 5 a 12 a 21 in addition here are a few identities that is used to prove second fdt for order n 3 4 5 m 2 m 3 0 2 m 3 2 m 4 m 5 0 3 m 4 3 2 m 5 3 m 6 m 7 0 references 1 anton arnold matthias ehrhardt ivan sofronov et al discrete transparent boundary conditions for the schr dinger equation fast calculation approximation and stability communications in mathematical sciences 1 2003 no 3 501 556 28 2 andrew d baczewski and stephen d bond numerical integration of the extended variable generalized langevin equation with a positive prony representable memory kernel the journal of chemical physics 139 2013 no 4 044107 3 zhaojun bai krylov subspace techniques for reduced order modeling of large scale dynamical systems applied numerical mathematics 43 2002 no 1 2 9 44 4 zhaojun bai patrick m dewilde and roland w freund reduced order modeling handbook of numerical analysis 13 2005 825 895 5 n bleistein and r a handelsman asymptotic expansions of integrals dover 1986 6 minxin chen xiantao li and chun liu computation of the memory functions in the generalized langevin models for collective dynamics of macromolecules j chem phys 141 2014 064112 7 a j chorin and p stinis problem reduction renormalization and memory comm appl math comp sc 1 2005 1 27 8 stefano curtarolo and gerbrand ceder dynamics of an inhomogeneously coarse grained multiscale system phys rev lett 88 june 2002 no 25 9 eric darve jose solomon and amirali kia computing generalized langevin equations and generalized fokker planck equa tions proc natl acad sci 106 2009 no 27 10884 10889 10 j l doob the elementary gaussian processes ann math stat 15 1944 229 282 11 pep espanol statistical mechanics of coarse graining novel methods in soft matter simulations 2004 pp 69 115 12 o marques f tama f x gadea and y sanejouand building block approach for determining low frequency normal modes of macromolecules proteins 41 2000 1 7 13 peter feldmann and roland w freund reduced order modeling of large linear subcircuits via a block lanczos algorithm pro ceedings of the 32 nd annual acm ieee design automation conference 1995 pp 474 479 14 john fricks lingxing yao timothy c elston and m gregory forest time domain methods for diffusive transport in soft matter siam journal on applied mathematics 69 2009 no 5 1277 1308 15 jennifer a hayward and jeremy c smith temperature dependence of protein dynamics computer simulation analysis of neu tron scattering properties biophysical journal 82 2002 no 3 1216 1225 16 carmen hij n mar serrano and pep espa ol markovian approximation in a coarse grained description of atomic systems j chem phys 125 2006 204101 17 s izvekov and g a voth modeling real dynamics in the coarse grained representation of condensed phase systems j chem phys 125 2006 151101 151104 18 shidong jiang and l greengard fast evaluation of nonreflecting boundary conditions for the schr dinger equation in one di mension computers mathematics with applications 47 march 2004 no 6 955 966 19 david kauzlaric pep espa ol andreas greiner and sauro succi three routes to the friction matrix and their application to the coarse graining of atomic lattices macromoal theor simul 20 2011 no 7 526 540 20 markovian dissipative coarse grained molecular dynamics for a simple 2 d graphene model the journal of chemical physics 137 2012 no 23 234103 21 david kauzlaric julia t meier pep espa ol sauro succi andreas greiner and jan g korvink bottom up coarse graining of a simple graphene model the blob picture j chem phys 134 2011 no 6 064106 064106 22 r kubo the fluctuation dissipation theorem rep prog phys 29 1 1966 255 284 23 oliver f lange and helmut grubm ller collective langevin dynamics of conformational motions in proteins j chem phys 124 2006 214903 24 a r leach molecular modelling principles and applications prentice hall 2001 25 fr d ric legoll and tony lelievre effective dynamics using conditional expectations nonlinearity 23 2010 no 9 2131 29 26 huan lei nathan baker and xiantao li the generalized langevin equation and the parameterization from data proc natl acad sci in press 2016 27 x li a coarse grained molecular dynamics model for crystalline solids int j numer meth engng 83 2010 986 997 28 x li and w e boundary conditions for molecular dynamics simulations at finite temperature treatment of the heat bath phys rev b 76 2007 104107 29 xiantao li coarse graining molecular dynamics models using an extended galerkin projection method international journal for numerical methods in engineering 99 2014 no 3 157 182 30 zhen li xin bian xiantao li and george em karniadakis incorporation of memory effects in coarse grained modeling via the mori zwanzig formalism the journal of chemical physics 143 2015 no 24 243128 31 damian loher reliable nonsymmetric block lanczos algorithms ph d thesis 2006 32 lina ma xiantao li and chun liu the derivation and approximation of coarse grained dynamics from langevin dynamics the journal of chemical physics 145 2016 no 20 204117 33 siewert j marrink h jelger risselada serge yefimov d peter tieleman and alex h de vries the martini force field coarse grained model for biomolecular simulations the journal of physical chemistry b 111 2007 no 27 7812 7824 34 luca monticelli senthil k kandasamy xavier periole ronald g larson d peter tieleman and siewert jan marrink the mar tini coarse grained force field extension to proteins journal of chemical theory and computation 4 2008 no 5 819 834 35 h mori a continued fraction representation of the time correlation functions prog theor phys 34 september 1965 399 416 36 w g noid perspective coarse grained models for biomolecular systems j chem phys 139 2013 no 9 090901 37 baldomero oliva xavier daura enrique querol francesc x avil s and o tapia a generalized langevin dynamics approach to model solvent dynamics effects on proteins via a solvent accessible surface the carboxypeptidase a inhibitor protein as a model theor chem acc 105 2000 no 2 101 109 38 grigorios a pavliotis stochastic processes and applications diffusion processes the fokker planck and langevin equations vol 60 springer 2014 39 grigoris pavliotis and andrew stuart multiscale methods averaging and homogenization springer science business media 2008 40 jay w ponder tinker software tools for molecular design washington university school of medicine saint louis mo 3 2004 41 sereina riniker jane r allison and wilfred f van gunsteren on developing coarse grained models for biomolecular simula tion a review phys chem ch ph 14 2012 no 36 12423 42 hannes risken fokker planck equation springer 1984 43 anthony j roberts normal form transforms separate slow and fast modes in stochastic dynamical systems physica a statistical mechanics and its applications 387 2008 no 1 12 38 44 t schlick molecular modeling and simulation an interdisciplinary guide springer verlag 2002 45 maria stepanova dynamics of essential collective motions in proteins theory phys rev e 76 2007 051918 46 christopher r sweet paula petrone vijay s pande and jes s a izaguirre normal mode partitioning of langevin dynamics for biomolecules the journal of chemical physics 128 2008 no 14 145101 47 christian de villemagne and robert e skelton model reductions using a projection formulation international journal of con trol 46 1987 no 6 2141 2169 48 gregory a voth coarse graining of condensed phase and biomolecular systems crc press 2008 49 r zwanzig nonlinear generalized langevin equations j stat phys 9 1973 215 220 30 1 introduction 2 mathematical derivation 2 1 the reduction of the full langevin dynamics model 3 model reduction for the stochastic model 3 1 a reformulation of the orthogonal dynamics 3 2 properties of general galerkin projections 4 the projection to krylov subspaces 4 1 the moment matching approach 4 2 first order subspace projection n 1 4 3 second order subspace projection n 2 4 4 generalization to high order approximation n 2 5 numerical implementation 5 1 block lanczos algorithms blbio 5 2 implementation without 5 3 a summary of the galerkin projection 6 numerical test 7 conclusion a recurrence formula for ww b representation of mis