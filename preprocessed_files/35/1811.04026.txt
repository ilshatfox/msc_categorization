adversarial uncertainty quantification in physics informed neural networks yibo yang paris perdikaris department of mechanical engineering and applied mechanics university of pennsylvania philadelphia pa 19104 usa abstract we present a deep learning framework for quantifying and propagating uncer tainty in systems governed by non linear differential equations using physics informed neural networks specifically we employ latent variable models to construct probabilistic representations for the system states and put forth an adversarial inference procedure for training them on data while constraining their predictions to satisfy given physical laws expressed by partial differ ential equations such physics informed constraints provide a regularization mechanism for effectively training deep generative models as surrogates of physical systems in which the cost of data acquisition is high and training data sets are typically small this provides a flexible framework for charac terizing uncertainty in the outputs of physical systems due to randomness in their inputs or noise in their observations that entirely bypasses the need for repeatedly sampling expensive experiments or numerical simulators we demonstrate the effectiveness of our approach through a series of examples involving uncertainty propagation in non linear conservation laws and the discovery of constitutive laws for flow through porous media directly from noisy data keywords variational inference generative adversarial networks probabilistic deep learning probabilistic scientific computing data driven modeling 1 introduction recent advances in machine learning and data analytics have yielded transformative results across diverse scientific disciplines including image preprint submitted to journal of computational physics november 12 2018 ar x iv 1 81 1 04 02 6 v 1 st at m l 9 n ov 2 01 8 recognition 1 natural language processing 2 cognitive science 3 and ge nomics 4 in all aforementioned areas the volume of data has increased substantially compared to even a decade ago but analyzing big data is ex pensive and time consuming data driven methods which have been enabled by the availability of sensors data storage and computational resources are taking center stage across many disciplines of science we now have highly scalable solutions for problems in object detection and recognition machine translation text to speech conversion recommender systems and informa tion retrieval 2 all of these solutions attain state of the art performance when trained with large amounts of data however more often than not in laboratory experiments and large scale simulations aiming to elucidate and predict complex phenomena a large number of quality and error free data is prohibitively costly to obtain un der this setting purely data driven approaches for machine learning present difficulties when the data is scarce relative to the complexity of the system the vast majority of state of the art machine learning techniques e g deep neural nets convolutional networks recurrent networks etc 5 are lack ing robustness and fail to provide any guarantees of convergence or quantify the error uncertainty associated with their predictions hence the ability to learn in a robust and sample efficient manner is a necessity in these data limited domains even less well understood is how one can constrain such algorithms to leverage domain specific knowledge and return predictions that satisfy certain physical principles e g conservation of mass momentum etc these shortcomings often generate skepticism and disbelief among ap plied mathematicians and engineers regarding the solid grounding of purely data driven machine learning approaches in recent work raissi et al 6 7 8 9 10 set foot exactly at this relatively unexplored interface between applied mathematics and contemporary machine learning by revisiting the idea of penalizing the loss function of deep neural networks using differential equation constraints as first put forth by psichogios and ungar 11 and la garis et al 12 this line of work has empirically demonstrated how such physics informed constraints regularize learning in small data regimes can lead to the discovery of governing equations and reduced order models as well as enable the prediction of complex dynamics from incomplete models and incomplete data despite a series of impressive results in canonical prob lems raissi et al 6 7 have also pointed out cases in which the training phase of these algorithms faces severe difficulties for reasons that are cur 2 rently poorly understood in lack of supporting theory on convergence and a posteriori error estimation this naturally poses the need for for scalable algorithms for uncertainty quantification a literature review of the current state of the art in uncertainty quan tification reveals a subtle dichotomy between different communities on one hand researchers in applied mathematics and scientific computing predom inately rely on mathematical models that are rigorously derived from first physical principles at the dawn of exascale computing such models have en abled the accurate simulation of increasingly more complex phenomena see for e g 13 14 they have also enabled in silico systematic studies in which the behavior of a system can be probed in a controlled fashion for different conditions parameter settings external inputs etc 15 the latter aims to both elucidate the key mechanisms that govern the behavior of a system but also characterize the robustness of the resulting predictions with respect to epistemic and aleatory uncertainty 16 however despite the fact that much progress has been made over the last two decades the most popular methods for scientific computing under uncertainty such as polynomial chaos expan sions 17 18 19 sparse grid quadratures 20 21 multi level multi fidelity monte carlo sampling 22 23 proper orthogonal decomposition 24 25 and gaussian process regression models 26 27 28 all face severe limita tions in view of the non gaussian likelihoods and high dimensional posterior distributions commonly encountered in realistic applications on the other hand the recent explosive growth of machine learning re search has put forth new effective ways of learning and manipulating complex high dimensional probability distributions inference tools like variational auto encoders 29 and generative adversarial networks 30 formulated on top of flexible building blocks such as feed forward convolutional recurrent neural networks 5 have introduced highly scalable solutions albeit for prob lems where not much prior information is assumed but instead large amounts of data can be obtained at relatively low cost e g image recognition 1 nat ural language processing 2 in this work we aim to leverage recent develop ments in machine learning to put forth a scalable framework for uncertainty propagation in physical systems for which the cost of data acquisition is high and training data sets are typically small but strong prior information exists by means of known governing laws expressed by partial differential equations specifically we construct a class of probabilistic physics informed neural networks that enables us to obtain a posterior characterization of the uncertainty associated with their predicted outputs moreover we will de 3 velop a flexible variational inference framework that will allow us to train such models directly from noisy input output data and predict outcomes of non linear dynamical systems that are partially observed with quantified uncertainty this setting necessitates a departure from the classical deter ministic realm of modeling and scientific computation and consequently our main building blocks can no longer be crisp deterministic numbers and governing laws but instead we must operate with probabilistic models this paper is structured as follows in section 2 1 we provide a brief overview of physics informed neural networks in sync with the recent de velopments in 6 7 9 10 in sections 2 2 and 2 3 we provide an outline of the proposed probabilistic formulation and the proposed variational inference framework finally in section 3 we will demonstrate the effectiveness of our approach through a series of examples involving uncertainty propagation in non linear conservation laws and the discovery of constitutive laws for flow through porous media directly from noisy data 2 methods 2 1 physics informed neural networks the recent works of raissi et al 6 7 9 10 have demonstrated how classical conservation laws and numerical discretization schemes can be used as structured prior information that can enhance the robustness and effi ciency of modern machine learning algorithms introducing a new class of data driven solvers as well as a physics informed machine learning approach to model discovery to this end the authors have considered construct ing deep neural networks that return predictions which are constrained by parametrized partial differential equations pde of the form ut nxu 0 1 where u x t is represented by a deep neural network parametrized by a set of parameters i e u x t f x t x is a vector of space coordinates t is the time coordinate and nx is a nonlinear differential operator as neural net works are differentiable representations this construction defines a so called physics informed neural network that corresponds to the pde residual i e r x t t f x t nxf x t this new network has the same parame ters as the network representing u x t albeit different activation functions due to the action of the differential operator 6 11 12 from an imple mentation perspective this network can be readily obtained by leveraging 4 recent progress in automatic differentiation 31 32 the resulting training procedure allows us to recover the shared network parameters using a few scattered observations of u x t namely xi ti ui i 1 nu along with a larger number of collocation points xi ti ri 0 i 1 nr that aim to penalize the pde residual at a finite set of nr collocation nodes this simple yet remarkably effective regularization procedure allows us to introduce the pde residual as a soft penalty constraint penalty in the like lihood function of the model 6 7 and the resulting optimization problem can be effectively solved using standard stochastic gradient descent without necessitating any elaborate constrained optimization techniques simply by minimizing the composite loss function l 1 nu nu i 1 f xi ti ui 2 1 nr nr i 1 r xi ti ri 2 2 where the required gradients l can be readily obtained using automatic differentiation 31 finally as the resulting predictions are encouraged to inherit any physical properties imposed by the pde constraint e g conser vation invariance symmetries etc this approach showcases how one can approximately encode physical and domain specific constraints in modern machine learning algorithms and introduce a new form of regularization for learning from small data sets 2 2 probabilistic physics informed neural networks here we put forth a probabilistic formulation for propagating uncertainty through physics informed neural networks using latent variable models of the form p u x t z z p z s t ut nxu 0 3 this setting encapsulates a wide range of deterministic and stochastic problems where u x t is a potentially multi variate field and z is a col lection of random latent variables the ability to learn such a model from data is the cornerstone of probabilistic scientific computing and uncertainty quantification in physical systems knowledge of the conditional probability p u x t z subject to domain knowledge constraints introduces a regulariza tion mechanism that limits the space of admissible solutions to a manageable size e g in fluid mechanics problems by discarding any non realistic flow 5 solutions that violate the conservation of mass principle thus enables train ing of probabilistic deep learning algorithms in small data regimes more over by providing a complete characterization of uncertainty it enhances the robustness of our predictions and provides a posteriori error estimates for assessing model inadequacy the latter can also enable downstream tasks such as the formulation of adaptive data acquisition policies for active learning or bayesian optimization 33 with domain knowledge constraints finally thanks to the structure encoded by the pde itself the resulting la tent variables z can potentially lead to the extraction of physically relevant and interpretable low dimensional feature representations which can subse quently introduce new techniques for nonlinear model order reduction and coarse graining of complex systems 2 3 adversarial inference for joint distribution matching following the recent findings of 34 we argue that matching the joint distribution of the generated data p x t u with the joint distribution of the observed data q x t u by minimizing the reverse kullback leibler di vergence kl p x t u q x t u is a promising approach to train the gen erative model presented in equation 3 this also implies that the respective marginal and conditional distributions are also encouraged to match the use of the reverse kullback leibler divergence in contrast to the maximum likelihood setup is motivated by examining the following decomposition kl p x t u q x t u h p x t u ep x t u log q x t u 4 where h p x t u denotes the entropy of the generative model the second term can be further decomposed as ep x t u log q x t u sp sq log q x t u p x t u dxdtdu 5 sp soq log q x t u p x t u dxdtdu where sp and sq denote the support of the distributions p x t u and q x t u respectively while soq denotes the complement of sq notice that by minimizing the kullback leibler divergence in equation 4 we introduce 6 a mechanism that is trying to balance the effect of two competing objec tives specifically maximization of the entropy term h p x t u encour ages p x t u to spread over its support set as wide while the second in tegral term in equation 5 introduces a strong negative penalty when the support of p x t u and q x t u do not overlap hence the support of p x t u is encouraged to spread only up to the point that sp sqo implying that sp sqo when sp sqo the pathological issue of mode collapse commonly encountered in the training of generative adversarial networks 30 is manifested 35 this issue is present if one seeks to directly minimize the reverse kullback leibler objective in equation 4 as this pro vides no control on the relative importance of the two terms as discussed in 34 we may rather minimize h p x t u ep x t u log q x t u with 1 to allow for control of how much emphasis is placed on mitigating mode collapse it is then clear that the entropic regularization introduced by h p x t u provides an effective mechanism for controlling and miti gating the effect of mode collapse and therefore potentially enhancing the robustness adversarial inference procedures for learning p x t u minimization of equation 4 with respect to the generative model param eters presents two fundamental difficulties first the evaluation of both distributions p x t u and q x t u typically involves intractable integrals in high dimensions and we may only have samples drawn from the two distri butions not their explicit analytical forms second the differential entropy term h p x t u is intractable as p x t u is not known a priori in the next sections we revisit the unsupervised formulation put forth in 34 and derive a tractable inference procedure for learning p x t u from scattered observation pairs of u x t namely xi ti ui i 1 nu 2 3 1 density ratio estimation by probabilistic classification by definition the computation of the kullback leibler divergence in equation 4 involves computing an expectation over a log density ratio i e kl p x t u q x t u ep x t u log p x t u q x t u in general given samples from two distributions we can approximate their density ratio by constructing a binary classifier that distinguishes between samples from the two distributions to this end we assume that n data points are drawn from p x t u and are assigned a label y 1 similarly we assume that n samples are drawn from q x t u and assigned label y 7 1 consequently we can write these probabilities in a conditional form namely p x t u x t u y 1 q x t u x t u y 1 where x t u y 1 and x t u y 1 are the class probabilities pre dicted by a binary classifier t x t u using bayes rule it is then straight forward to show that the density ratio of p x t u and q x t u can be computed as p x t u q x t u x t u y 1 x t u y 1 y 1 x t u x t u y 1 y 1 x t u x t u y 1 y 1 x t u y 1 x t u y 1 x t u 1 y 1 x t u t x t u 1 t x t u 6 this simple procedure suggests that we can harness the power of deep neu ral network classifiers to obtain accurate estimates of the reverse kullback leibler divergence in equation 4 directly from data and without the need to assume any specific parametrization for the generative model distribution p x t u 2 3 2 entropic regularization bound here we follow the derivation of li et al 34 to construct a computable lower bound for the entropy h p x t u to this end we start by consid ering random variables x t u z under the joint distribution p x t u z p u x t z p z p u x t z p x t z where p u x t z u f x t z and is the dirac delta function the mutual information between x t u and z satisfies the information theoretic identity i x t u z h x t u h x t u z h z h z x t u 8 where h x t u h z are the marginal entropies and h x t u z h z x t u are the conditional entropies 36 since in our setup x and t are determin istic variables independent of z and samples of p u x t z are generated by a deterministic function f x t z it follows that h x t u z 0 we therefore have h x t u h z h z x t u 7 where h z log p z p z dz does not depend on the generative model parameters now consider a general variational distribution q z x t u parametrized by a set of parameters then h z x t u ep x t u z log p z x t u ep x t u z log q z x t u ep x t u kl p z x t u q z x t u ep x t u z log q z x t u 8 viewing z as a set of latent variables then q z x t u is a variational approximation to the true intractable posterior over the latent variables p z x t u therefore if q z x t u is introduced as an auxiliary in ference model associated with the generative model p x t u for which u f x t z and z p z then we can use equations 7 and 8 to bound the entropy term in equation 4 as h p x t u h p z ep x t u z log q z x t u 9 note that the inference model q z x t u plays the role of a variational approximation to the true posterior over the latent variables and appears naturally using information theoretic arguments in the derivation of the lower bound 2 3 3 adversarial training objective by leveraging the density ratio estimation procedure described in section 2 3 1 and the entropy bound derived in section 2 3 2 we can derive the fol lowing loss functions for minimizing the reverse kullback leibler divergence with entropy regularization 9 ld eq x t p z log t x t f x t z eq x t u log 1 t x t u 10 lg eq x t p z t x t f x t z 1 log q z x t f x t z 11 where x 1 1 e x is the logistic sigmoid function notice how the binary cross entropy objective of equation 10 aims to progressively improve the ability of the classifier t x t u to discriminate between fake samples x t f x t z obtained from the generative model p x t u and true samples x t u originating from the observed data distribution q x t u simultaneously the objective of equation 11 aims at improving the abil ity of the generator f x t u to generate increasingly more realistic sam ples that can fool the discriminator t x t u moreover the encoder q z x t f x t z not only serves as an entropic regularization term than allows us to stabilize model training and mitigate the pathology of mode col lapse but also provides a variational approximation to true posterior over the latent variables the way it naturally appears in the objective of equation 11 also encourages the cycle consistency of the latent variables z a process that is known to result in disentangled and interpretable low dimensional representations of the observed data 37 which could be subsequently used as good features for nonlinear model order reduction in theory the optimal set of parameters correspond to the nash equilibrium of the two player game defined by the loss functions in equations 10 11 for which one can show that the exact model distribution and the exact posterior over the latent variables can be recovered 30 38 in practice although there is no guarantee that this optimal solution can be attained the generative model can be trained by alternating between optimizing the two objectives in equations 10 11 using stochastic gradient descent as max ld 12 min lg 13 2 3 4 adversarial training with physics informed constrains in order to learn the physics informed probabilistic model of equation 3 from data we can extend the adversarial inference framework presented 10 above by appropriately penalizing the loss function of the generator see equation 11 the available data correspond to scattered observation pairs xi ti ui i 1 nu originating from known initial or boundary con ditions or any other potentially noisy measurements of u x t in analogy to the deterministic setting put for in 6 and summarized in section 2 1 by defining r x t z t f x t z nxf x t z we essentially introduce a new conditional probability model p r x t z that shares the same param eters as p u x t z albeit the underlying neural network that serves as its approximation has different activation functions however since we would like to encourage every sample u f x t z produced by the generator to satisfy the pde constraint we can simply treat the residual as a determin istic variable i e r x t z r x t and enforce the constraint at a finite set of collocation points nr by simply minimizing the mean square loss lpde 1 nr nr i 1 r xi ti ri 2 14 then the resulting adversarial game for training the physics informed model of equation 3 takes the form max ld min lg lpde 15 where positive values of can be selected to place more emphasis on penal izing the pde residual for 0 the residual loss lpde acts as a reg ularization term that approximately enforces the given physical constraint and therefore encourages the generator p u x t z to produce samples that satisfy the underlying partial differential equation also note that this struc tured approach also encourages the encoder q z x t f x t z to learn a set of spatio temporal latent variables z that are relevant to the underly ing physics possibly opening a new directions for probabilistic model order reduction of complex systems 2 3 5 predictive distribution once the model is trained we can construct a probabilistic ensemble for the solution u u x t z by sampling latent variables from the prior p z and passing them through the generator to yield samples u f x t z that are distributed according to the predictive model distribution p u x t z 11 note that although the explicit form of this distribution is not known we can efficiently compute any of its moments via monte carlo sampling the cost of this prediction step is negligible compared to the cost of training the model as it only involves a single forward pass through the generator function f x t z typically we compute the mean and variance of the predictive distribution at a new test point x t as u x t ep u x t z 1 ns ns i 1 f x t zi 16 2 u x t varp u x t z 1 ns ns i 1 f x t zi u x t 2 17 where zi p z i 1 ns and ns corresponds to the total number of monte carlo samples 2 3 6 advantages and caveats of adversarial learning since their recent introduction 30 39 40 41 adversarial learning tech niques have provided great flexibility for performing probabilistic computa tions with arbitrarily complex implicit distributions essentially they have lifted the over simplified approximations typically used in variational infer ence gaussian approximations exponential families etc 42 yielding very general and flexible schemes for statistical inference however this flexibility comes at a price as such methods in practice require very careful tuning in order to achieve stable and accurate performance to this end recall the training objective defined in equation 15 that introduces an adversar ial game between the generator and discriminator networks 30 in practice this mini max optimization problem is solved by alternating stochastic gradi ent updates between the two competing objectives and it is highly sensitive on the capacity of the neural networks modeling the generator and discrim inator as well as the relative frequency with which the parameters of each network are updated within each iteration of stochastic gradient descent to this end we provide a series of empirical observations and lessons we learned throughout this study that can enhance the robustness and stability of this training procedure changing the relative number of stochastic gradient updates for the generator kg and the discriminator kd is equivalent to changing their 12 neural network architecture for example we can reduce the capacity of discriminator by either performing more stochastic gradient updates for the generator or remove one layer in the neural network architecture of the discriminator given enough collocation points nr for penalizing the pde residual we can obtain robust uncertainty estimates together with precise pre dictions simply by tuning the capacity of discriminator and generator networks typically by fixing the generator we expect the discriminator to have some capacity so that the model training dynamics remain stable but we do not want the discriminator to be very powerful as in that case there will by very little information from the discriminator that can help the generator to improve towards producing more realistic samples this a common characteristic of adversarial inference procedures 30 for cases with a small number of training data we should reduce the capacity of the discriminator this can be achieved by either changing the relative frequency of stochastic gradient updates for the generator and discriminator or by reducing the capacity of the discriminator neural network architecture 3 results in all examples we have trained the models for 30 000 stochastic gradient descent steps using the adam optimizer 43 with a learning rate of 10 4 while fixing a one to five ratio for the discriminator versus generator updates moreover we have fixed the entropic regularization and the residual penalty parameters to 1 5 and 1 0 respectively the proposed algorithms were implemented in tensorflow v 1 10 32 and computations were performed in single precision arithmetic on a single nvidia tesla p 100 gpu card all data and code accompanying this manuscript will be made available at https github com predictiveintelligencelab uqpinns 3 1 a pedagogical example let us illustrate the basic capabilities of the proposed methods through a simple example corresponding to the following nonlinear second order or 13 https github com predictiveintelligencelab uqpinns dinary differential equation uxx u 2 ux f x x 1 1 f x 2 sin x cos x sin 2 x 18 subject to random boundary conditions u 1 u 1 n 0 2 ni for this simple example the deterministic solution corresponding to 2 n 0 can be readily obtained as u x sin x given nu observations of u x corre sponding to different realizations of the random boundary conditions our goal is to obtain a probabilistic representation of the solution p u x z by train ing a physics informed generative model of the form u f x z z p z that is constrained by equation 18 to this end we introduce three deter ministic mappings parametrized by deep neural networks namely f x z q x u and t x u corresponding to the generator encoder and discrim inator functions introduced in section 2 3 by construction we also obtain a physics informed neural network r x corresponding to the determinis tic residual of equation 18 that will be used to approximately enforce the differential equation constraint at a set of nr 100 randomly distributed collocation points x 1 1 all neural networks were chosen to have two hidden layers with 50 neurons in each layer and a hyperbolic tangent acti vation function moreover the dimensionality of the latent variables was set to one i e z z and we have assumed an isotropic standard normal prior namely p z n 0 1 as the training data for u x reflects the uncertainty in the boundary conditions the role of the latent variables z is to enable the propagation of this uncertainty into the predicted solution obtained through the generative model p u x z here we have considered two cases corresponding to deterministic and random boundary conditions namely i 2 n 0 i e noise free data and ii 2 n 0 05 i e 5 gaussian uncorrelated noise in all cases the training data consists of nu 20 realizations for each boundary point u 1 u 1 and a total of nr 100 collocation points for enforcing the residual of equation 18 our probabilistic predictions for this example are summarized in figures 1 and 2 specifically figure 1 a shows the generative model predictive mean and two standard deviations plotted against the exact solution of this problem note that this case corresponds to deterministic training data for the boundary conditions hence the exact solution is deterministic and the prediction error here is measured as el 2 1 36 10 3 in the relative l 2 norm 14 a b figure 1 a pedagogical example a mean and two standard deviations of p u x z against the exact solution for deterministic boundary data b mean and two standard deviations of p u x z against the reference monte carlo solution for random boundary data corresponding to 5 gaussian uncorrelated noise a b figure 2 a pedagogical example predicted marginal densities against the reference monte carlo solution a p u x 0 5 z b p u x 0 5 z 15 el 2 n i 1 u x i u x i 2 n i 1 u x i 2 19 where n 200 denotes the total number of equidistant test points x in the interval 1 1 moreover the variance shown in the inset of figure 1 a serves as an a posteriori error estimate that quantifies the uncertainty asso ciated with the generative model predictions figure 1 b shows the result ing prediction and uncertainty estimates corresponding to random boundary conditions compared against a reference mean solution obtained numerically using a spectral method with 2 000 monte carlo samples in this case the predictive uncertainty of the generative model reflects the aggregate total uncertainty due to both randomness in the boundary conditions and the in herent epistemic uncertainty in the neural network approximation as the generative model can return a complete statistical characterization of the solution by means of its conditional probability density p u x z in figure 2 we provide a visual comparison of the one dimensional marginals between our predictions and the reference monte carlo solution corresponding to the spatial locations x 0 5 and x 0 5 albeit simple this example aims to demonstrate the basic capabilities of the proposed methodology in propagating uncertainty through non linear partial differential equations in contrast to previous approaches to infer ring solutions of partial differential equations from data 44 45 46 47 the proposed methodology does not rely on gaussian assumptions and it can directly tackle nonlinear problems without any need for linearization 3 2 burgers equation in this example we aim to provide a comprehensive systematic study to quantify the robustness of the proposed methods with respect to different parameter choices we will do so through the lens of a more challenging canonical problem involving the non linear time dependent burgers equation in one spatial dimension ut uux uxx 0 x 1 1 t 0 1 u 0 x sin x u t 1 u t 1 0 20 16 where the viscosity parameter is chosen as 0 01 in order to generate a strongly nonlinear response that leads to the development of shock discon tinuities in finite time this is one of the few nonlinear partial differential equations that admits an exact solution through the cole hopf transforma tion 48 a solution that will be subsequently used to test the validity of our predictions here we represent the unknown solution u x t using a physics informed generative model of the form u f x t z and we will introduce paramet ric functions corresponding to a generator f x t z an encoder q x t u and a discriminator t x t u all constructed using deep feed forward neural networks the baseline architectures for the generator and the encoder have 4 hidden layers with 50 neurons per layers while the discriminator network has 3 hidden layers and 50 neurons per layer the activation function in all cases is chosen to be a hyperbolic tangent non linearity the prior over the latent variables p z is chosen again to be a one dimensional isotropic gaussian distribution i e z z z n 0 1 first we consider a baseline scenario in which we train our probabilis tic model using a data set comprising of nu 150 noisy free input output pairs for u x t 50 points for the initial condition see figure 3 a and 50 points for each of the domain boundaries plus an additional nr 10 000 collocation points for enforcing the residual of the burgers equation using the loss of equation 14 all data points were randomly selected within the bounds given in equation 20 the result of this experiment is summarized in 4 where we report the predicted mean solution as well as the uncertainty as sociated with this prediction as quantified by two standard deviations of the generative model p u x t z as the training data for this case is noise free the solution to this problem is deterministic and the resulting uncertainty captured in p u x t z can be viewed as an a posteriori error estimate of the neural network approximation error due to the finite number of training data which is measured as el 2 4 1 10 2 in the relative l 2 norm as discussed in 49 a higher approximation accuracy can be achieved by training the gen erative model using a quasi newton optimizer e g l bfgs 50 however here we chose to use stochastic gradient descent using adam updates 43 in order to highlight the ability of the proposed method to return uncertainty estimates when the model predictions are not perfectly accurate second we repeat the same test for a more complicated scenario in which the initial condition has been corrupted by non additive non gaussian noise as shown in figure 3 b where the noise variance is larger around x 0 17 b a figure 3 burgers equation a exact initial condition and noise free training data 50 points b training data corresponding to a single realization of the non additive noise corruption process 100 points generated by equation 21 therefore amplifying the effect of uncertainty on the shock formation here the neural network architecture as well as the number and location of training points have been kept fixed as described above but the initial condition is now corrupted as u x 0 sin x 2 exp 3 x n 0 0 12 21 the results of this experiment are summarized in figure 5 we observe that the resulting generative model p u x t z can effectively capture the uncer tainty in the resulting spatio temporal solution due to the propagation of the input noise process through the complex non linear dynamics of the burg ers equation as expected the uncertainty concentrates around the shock although we only plot the first two moments of the solution we must empha size that the generative model p u x t z provides a complete probabilistic characterization of its non gaussian statistics in order to further investigate the performance of the proposed method ology for different parameter settings we have performed a series of compre hensive systematic studies that aim to quantify the sensitivity of the resulting predictions on i the neural network initialization ii the total number of training and collocation points iii the neural network architecture and iv the adversarial training procedure the results of these systematic studies are provided in appendix a 18 0 0 0 2 0 4 0 6 0 8 1 0 t 1 0 0 5 0 0 0 5 1 0 x u t x data 150 points 1 00 0 75 0 50 0 25 0 00 0 25 0 50 0 75 1 0 1 x 1 0 1 u t x t 0 25 1 0 1 x 1 0 1 u t x t 0 50 exact prediction two std band 1 0 1 x 1 0 1 u t x t 0 75 0 0 0 2 0 4 0 6 0 8 1 0 t 1 0 0 5 0 0 0 5 1 0 x variance of u t x 0 1 0 2 0 3 0 4 figure 4 burgers equation with noise free data top mean of p u x t z along with the location of the noisy training data xi ti ui i 1 nu middle prediction and predictive uncertainty at t 0 25 t 0 5 and t 0 75 bottom variance of p u x t z 19 0 0 0 2 0 4 0 6 0 8 1 0 t 1 0 0 5 0 0 0 5 1 0 x u t x data 200 points 0 75 0 50 0 25 0 00 0 25 0 50 0 75 1 0 1 x 1 0 1 u t x t 0 25 1 0 1 x 1 0 1 u t x t 0 50 exact prediction two std band 1 0 1 x 1 0 1 u t x t 0 75 0 0 0 2 0 4 0 6 0 8 1 0 t 1 0 0 5 0 0 0 5 1 0 x variance of u t x 0 2 0 4 0 6 0 8 figure 5 burgers equation with noisy data top mean of p u x t z along with the location of the training data xi ti ui i 1 nu middle prediction and predictive uncertainty at t 0 25 t 0 5 and t 0 75 bottom variance of p u x t z 20 3 3 discovery of constitutive laws for flow through porous media in our final example we aim to demonstrate the ability of the proposed methods to discover unknown constitutive relationships directly from data with quantified uncertainty to this end we revisit the darcy flow exam ple put forth in 51 corresponding to a two dimensional nonlinear diffusion equation with an unknown state dependent diffusion coefficient x k u xu x 0 x x 1 x 2 0 l 1 0 l 2 u x u 0 x 1 l 1 k u u x x 1 q x 1 0 u x x 2 0 x 2 0 l 2 22 where q 8 25 10 5 m s and u 0 10 m are known boundary conditions in order to benchmark and validate our model predictions we consider a realistic data set generated using the subsurface transport over multiple phases stomp code 52 with the van genuchten model 53 for k u which reads as k s u kss 1 2 1 1 s 1 m m 2 s u 1 ug u 1 1 m m 23 with the following parameter values ks 8 25 10 4 m s ug 0 m 0 469 0 1 l 1 10 m and l 2 10 m our goal is twofold we aim to construct a physics informed probabilistic model for p u x z and simultaneously learn the unknown state dependent diffusion coefficient k u directly from data on u x i e we assume no measurements of k u to this end in addition to the three determinis tic mappings f x z q x u and t x u corresponding to the genera tor encoder and discriminator described in section 2 3 here we also intro duce another neural network f u for approximating k u the parameters of f u are essentially inherited by the physics informed residual network r x x f f x z xf x z that aims to enforce the residual of equation 22 at the nr collocation points for any set of latent variables z all neural networks are chosen to have 2 hidden layers with 50 neurons per each and a hyperbolic tangent activation function while the probabilistic model for p u x z assumes a two dimensional latent space with an isotropic gaus sian prior i e z n 0 i 21 by construction our probabilistic model for p u x z can return predic tions of the unknown solution u x with quantified uncertainty we can then use this model to propagate uncertainty in our predictions of k u via monte carlo sampling specifically once the model is trained end to end we can easily generate samples of u x from p u x z and propagate them through f u to obtain a samples for k u essentially this results in an implicit generative model p k x z which can fully characterize uncertainty in our predictions of the unknown state dependent diffusion coefficient here we also have considered two cases corresponding to noise free train ing data and noisy data corrupted by 5 gaussian uncorrelated noise in the noise free case we used nu 600 scattered measurements of the unknown solution u x 200 inside the domain and 100 on each one of the four boundaries and total number of nr 10 000 randomly selected colloca tion points inside the domain for penalizing the residual of equation 22 for the noisy case we chose nu 1 400 scattered measurements of the unknown solution u x 1 000 inside the domain and 100 on each one of the four boundaries while still keeping nr 10 000 collocation points figure 6 summarizes the results for both cases by showing the predictive mean and two standard deviations of the corresponding generative model p k x z against the reference deterministic solution obtained from the subsurface transport over multiple phases stomp code 52 with the van genuchten model 53 see equation 23 evidently the generative model is able to re cover a sensible prediction for the unknown state dependent diffusion coef ficient with quantitative uncertainty even when the training data on u x is heavily corrupted by noise moreover notice that k u implicitly depends on the spatial coordinates x x 1 x 2 in figure 7 we present the resulting prediction for k u x 1 x 2 corresponding to the noise free case against the reference solution as well as their point wise absolute error again we must emphasize that these predictions are obtained without ever observing any data on k u while they are accompanied by quantita tive estimates that jointly characterize the uncertainty due to noise in the training data for u x and the underlying approximation error of the neural networks this theme of consistently inferring correlated continuous quanti ties of interest from a small set of measurements by leveraging the underlying laws of physics is a great example of the exciting capabilities that physics informed machine learning has to offer 22 a b figure 6 prediction with quantified uncertainty of unknown state dependent diffusion coefficient k u compared against the reference solution obtained from the subsurface transport over multiple phases stomp code 52 with the van genuchten model 53 see equation 23 a noise free training data for u x b noisy training data for u x with noise level of 5 b a c figure 7 prediction of unknown state dependent diffusion coefficient k u x 1 x 2 a reference solution obtained from the subsurface transport over multiple phases stomp code 52 with the van genuchten model 53 see equation 23 b predic tive mean of the generative model p k x z trained on noise free data for u x c absolute point wise prediction error 23 4 conclusions we presented a class of probabilistic physics informed neural networks that are capable of approximating arbitrary conditional probability densities while being constrained to generate samples that approximately satisfy given partial differential equations moreover we have derived a flexible regularized adversarial inference framework that enables the end to end training of such models directly from noisy and incomplete measurements uncertainty in the system inputs and or outputs is captured through a set of latent variables that are relevant to the underlying physics and could possibly open new directions for probabilistic model order reduction of complex systems these developments allow us to perform probabilistic computations for uncertain systems train deep generative models in small data regimes handle complex noise processes and seamlessly carry out uncertainty propagation studies for physical systems without the need for repeated evaluation of experiments and numerical simulations although the proposed adversarial inference framework provides great flexibility for performing probabilistic computations and approximating ar bitrarily complex and high dimensional probability distributions it relies on carefully tuning the interplay between the generator and discriminator net works this is a known limitation of adversarial algorithms and although several works have led to improvements 54 55 it still largely remains an open research problem an alternative path for enhancing the robustness of the inference procedure while not compromising its ability to handle complex probability distributions comes through the use of invertible transformations and flow based generative models 56 57 future work will examine the ap plications of such models in the context of physics informed neural networks with the goal of robustifying the proposed methods and scaling them to more realistic systems acknowledgements this work received support from the us department of energy under the advanced scientific computing research program grant de sc 0019116 and the defense advanced research projects agency under the physics of artificial intelligence program we would also like to thank dr alexandre tartakovsky from the pacific nortwest national laboratory for providing the darcy flow data set 24 http arxiv org abs de sc 0019116 relative l 2 error 4 1 e 02 7 9 e 02 4 4 e 02 4 0 e 02 3 8 e 02 3 2 e 02 5 7 e 02 4 7 e 02 6 5 e 02 4 0 e 02 3 5 e 02 3 5 e 02 6 4 e 02 4 0 e 02 4 9 e 02 table a 1 relative l 2 prediction error for different neural network initializations using a randomized seed appendix a sensitivity studies here we provide results on a series of comprehensive systematic studies that aim to quantify the sensitivity of the resulting predictions on i the neural network initialization ii the total number of training and collocation points iii the neural network architecture and iv the adversarial training procedure in all cases we have used the non linear burgers defined in section 3 2 as a prototype problem appendix a 1 sensitivity with respect to the neural network initialization in order to quantify the sensitivity of the proposed methods with respect to the initialization of the neural networks we have considered a noise free data set comprising of nu 150 and nr 10000 training and collocation points respectively and fixed the architecture for generator neural networks to include 4 hidden layers with 50 neurons each and discriminator neural networks to include 3 hidden layers with 50 neurons each and a hyperbolic tangent activation function then we have trained an ensemble of 15 cases all starting from a normal xavier initialization 58 for all network weights with a randomized seed and a zero initialization for all bias parameters in table a 1 we report the relative error between the predicted mean solution and the known exact solution for this problem for all 15 randomized trials using at set of 25600 randomly selected test points evidently our results are robust with respect to the the neural network initialization as in all cases the stochastic gradient descent training procedure converged roughly to the same solution we can summarize this result by reporting the mean and the standard deviation of the relative l 2 error as l 2 l l l l 4 7 10 2 1 3 10 2 4 7 10 2 1 3 10 2 25 nu nr 10 100 250 500 1000 5000 10000 60 9 3 e 01 5 6 e 01 4 8 e 01 5 0 e 02 1 9 e 01 5 0 e 02 5 1 e 02 90 5 8 e 01 5 3 e 01 3 5 e 01 1 5 e 01 4 9 e 02 1 0 e 01 5 8 e 02 150 6 7 e 01 1 4 e 01 3 0 e 01 3 6 e 02 4 9 e 02 1 2 e 01 4 7 e 02 table a 2 relative l 2 prediction error for different number of training and collocation points nu and nr respectively appendix a 2 sensitivity with respect to the total number of training and collocation points in this study our goal is to quantify the sensitivity of our predictions with respect to the total number of training and collocation points nu and nr respectively as before we have considered noise free data sets and fixed the architecture for generator neural networks to include 4 hidden layers with 50 neurons each and discriminator neural networks to include 3 hidden layers with 50 neurons each a hyperbolic tangent activation function and a nor mal xavier initialization 58 for all network weights and zero initialization for all network biases the results of this study are summarized in table a 2 indicating that as the number of collocation points are increased a more ac curate prediction is obtained this observation is in agreement with the orig inal results of raissi et al 6 7 for deterministic physics informed neural networks indicating the role of the residual loss as an effective regularization mechanism for training deep generative models in small data regimes appendix a 3 sensitivity with respect to the neural network architecture in this study we aim to quantify the sensitivity of our predictions with respect to the architecture of the neural networks that parametrize the gen erator the discriminator and the encoder here we have fixed the number of noise free training data to nu 150 and nr 10000 and we kept the number of layers for discriminator to always be one less than the number of layers for generator e g if the number of layers for generator is two then the number of layers for discriminator is one etc in all cases we have used a hyperbolic tangent non linearity and a normal xavier initialization 58 in table a 3 we report the relative l 2 prediction error for different feed forward architectures for the generator discriminator and encoder i e different number of layers and number of nodes in each layer the general trend suggests that as the neural network capacity is increased we obtain more 26 ng nn 20 50 100 2 4 2 e 01 3 8 e 01 5 7 e 01 3 6 5 e 02 3 5 e 02 2 1 e 02 4 9 3 e 02 4 7 e 02 5 4 e 02 table a 3 relative l 2 prediction error for different feed forward architectures for the generator encoder and the discriminator the total number of layers of the latter was always chosen to be one less than the number of layers for generator kg kd 1 2 5 1 3 5 e 01 5 0 e 01 1 5 e 00 2 4 3 e 02 3 2 e 01 5 4 e 01 5 4 7 e 02 2 3 e 01 7 0 e 01 table a 4 relative l 2 error with different number of training for generator and discrim inator in each epoch accurate predictions indicating that our physics informed constraint on the pde residual can effectively regularize the training process and safe guard against over fitting we note number of neurons in each layer as nn and number of layers for generator encoder as ng appendix a 4 sensitivity with respect to the adversarial training procedure finally we test the sensitivity with respect to the adversarial training process to this end we have fixed the number of noise free training data to nu 150 and nr 10000 and the neural network architecture to be the same as appendix a 2 and we vary the total number of training steps for the generator kg and the discriminator kd within each stochastic gradient descent iteration the results of this study are presented in table a 4 where we report the relative l 2 prediction error these results reveal the high sensitivity of the training dynamics on the interplay between the generator and discriminator networks and pinpoint on the well known peculiarity of adversarial inference procedures which require a careful tuning of kg and kd for achieving stable performance in practice 27 references 1 a krizhevsky i sutskever g e hinton imagenet classification with deep convolutional neural networks in advances in neural information processing systems pp 1097 1105 2 y lecun y bengio g hinton deep learning nature 521 2015 436 444 3 b m lake r salakhutdinov j b tenenbaum human level concept learning through probabilistic program induction science 350 2015 1332 1338 4 b alipanahi a delong m t weirauch b j frey predicting the se quence specificities of dna and rna binding proteins by deep learning nature biotechnology 33 2015 831 838 5 i goodfellow y bengio a courville y bengio deep learning vol ume 1 mit press cambridge 2016 6 m raissi p perdikaris g e karniadakis physics informed deep learning part i data driven solutions of nonlinear partial differential equations arxiv preprint arxiv 1711 10561 2017 7 m raissi p perdikaris g e karniadakis physics informed deep learning part ii data driven discovery of nonlinear partial differential equations arxiv preprint arxiv 1711 10566 2017 8 m raissi p perdikaris g e karniadakis multistep neural networks for data driven discovery of nonlinear dynamical systems arxiv preprint arxiv 1801 01236 2018 9 m raissi g e karniadakis hidden physics models machine learning of nonlinear partial differential equations journal of computational physics 357 2018 125 141 10 m raissi deep hidden physics models deep learning of nonlinear partial differential equations arxiv preprint arxiv 1801 06637 2018 11 d c psichogios l h ungar a hybrid neural network first principles approach to process modeling aiche journal 38 1992 1499 1511 28 http arxiv org abs 1711 10561 http arxiv org abs 1711 10566 http arxiv org abs 1801 01236 http arxiv org abs 1801 06637 12 i e lagaris a likas d i fotiadis artificial neural networks for solving ordinary and partial differential equations ieee transactions on neural networks 9 1998 987 1000 13 p perdikaris l grinberg g e karniadakis multiscale modeling and simulation of brain blood flow phys fluids 28 2016 021304 14 d rossinelli y h tang k lykov d alexeev m bernaschi p had jidoukas m bisson w joubert c conti g karniadakis et al the in silico lab on a chip petascale and high throughput simulations of microfluidics at cell resolution in proceedings of the international conference for high performance computing networking storage and analysis acm p 2 15 j s ukys u rasthofer f wermelinger p hadjidoukas p koumout sakos optimal fidelity multi level monte carlo for quantification of un certainty in simulations of cloud cavitation collapse arxiv preprint arxiv 1705 04374 2017 16 j t oden r moser o ghattas computer predictions with quantified uncertainty part ii siam news 43 2010 1 4 17 r ghanem p d spanos polynomial chaos in stochastic finite ele ments journal of applied mechanics 57 1990 197 202 18 d xiu g e karniadakis the wiener askey polynomial chaos for stochastic differential equations siam journal on scientific computing 24 2002 619 644 19 h n najm uncertainty quantification and polynomial chaos tech niques in computational fluid dynamics annual review of fluid mechan ics 41 2009 35 52 20 t gerstner m griebel numerical integration using sparse grids nu merical algorithms 18 1998 209 21 m eldred j burkardt comparison of non intrusive polynomial chaos and stochastic collocation methods for uncertainty quantification in 47 th aiaa aerospace sciences meeting including the new horizons forum and aerospace exposition p 976 29 http arxiv org abs 1705 04374 22 a barth c schwab n zollinger multi level monte carlo finite ele ment method for elliptic pdes with stochastic coefficients numerische mathematik 119 2011 123 161 23 b peherstorfer k willcox m gunzburger optimal model manage ment for multifidelity monte carlo estimation siam journal on scien tific computing 38 2016 a 3163 a 3194 24 g berkooz p holmes j l lumley the proper orthogonal decomposi tion in the analysis of turbulent flows annual review of fluid mechanics 25 1993 539 575 25 o le ma tre o m knio spectral methods for uncertainty quantifica tion with applications to computational fluid dynamics springer sci ence business media 2010 26 i bilionis n zabaras multi output local gaussian process regression applications to uncertainty quantification journal of computational physics 231 2012 5718 5746 27 i bilionis n zabaras b a konomi g lin multi output separable gaussian process towards an efficient fully bayesian paradigm for uncertainty quantification journal of computational physics 241 2013 212 239 28 p perdikaris d venturi g e karniadakis multifidelity information fusion algorithms for high dimensional systems and massive data sets siam j sci comput 38 2016 b 521 b 538 29 d p kingma m welling auto encoding variational bayes arxiv preprint arxiv 1312 6114 2013 30 i goodfellow j pouget abadie m mirza b xu d warde farley s ozair a courville y bengio generative adversarial nets in advances in neural information processing systems pp 2672 2680 31 a g baydin b a pearlmutter a a radul j m siskind au tomatic differentiation in machine learning a survey arxiv preprint arxiv 1502 05767 2015 30 http arxiv org abs 1312 6114 http arxiv org abs 1502 05767 32 m abadi p barham j chen z chen a davis j dean m devin s ghemawat g irving m isard et al tensorflow a system for large scale machine learning in osdi volume 16 pp 265 283 33 b shahriari k swersky z wang r p adams n de freitas taking the human out of the loop a review of bayesian optimization pro ceedings of the ieee 104 2016 148 175 34 c li j li g wang l carin learning to sample with adversarially learned likelihood ratio 2018 35 t salimans i goodfellow w zaremba v cheung a radford x chen improved techniques for training gans in advances in neural information processing systems pp 2234 2242 36 h akaike information theory and an extension of the maximum like lihood principle in selected papers of hirotugu akaike springer 1998 pp 199 213 37 j friedman t hastie r tibshirani the elements of statistical learn ing volume 1 springer series in statistics new york ny usa 2001 38 y pu l chen s dai w wang c li l carin symmetric variational autoencoder and connections to adversarial learning arxiv preprint arxiv 1709 01846 2017 39 a makhzani j shlens n jaitly i goodfellow b frey adversarial autoencoders arxiv preprint arxiv 1511 05644 2015 40 v dumoulin i belghazi b poole o mastropietro a lamb m ar jovsky a courville adversarially learned inference arxiv preprint arxiv 1606 00704 2016 41 l mescheder s nowozin a geiger adversarial variational bayes unifying variational autoencoders and generative adversarial networks arxiv preprint arxiv 1701 04722 2017 42 d m blei a kucukelbir j d mcauliffe variational inference a review for statisticians journal of the american statistical association 112 2017 859 877 31 http arxiv org abs 1709 01846 http arxiv org abs 1511 05644 http arxiv org abs 1606 00704 http arxiv org abs 1701 04722 43 d p kingma j ba adam a method for stochastic optimization arxiv preprint arxiv 1412 6980 2014 44 j cockayne c oates t sullivan m girolami probabilistic meshless methods for partial differential equations and bayesian inverse problems arxiv preprint arxiv 1605 07811 2016 45 m raissi p perdikaris g e karniadakis inferring solutions of dif ferential equations using noisy multi fidelity data journal of computa tional physics 335 2017 736 746 46 m raissi p perdikaris g e karniadakis numerical gaussian pro cesses for time dependent and nonlinear partial differential equations siam journal on scientific computing 40 2018 a 172 a 198 47 j cockayne c oates t sullivan m girolami bayesian probabilistic numerical methods arxiv preprint arxiv 1702 03673 2017 48 e hopf the partial differential equation ut uux xx communica tions on pure and applied mathematics 3 1950 201 230 49 m raissi p perdikaris g e karniadakis physics informed deep learning part i data driven solutions of nonlinear partial differential equations arxiv preprint arxiv 1711 10561 2017 50 d c liu j nocedal on the limited memory bfgs method for large scale optimization mathematical programming 45 1989 503 528 51 a m tartakovsky c o marrero d tartakovsky d barajas solano learning parameters and constitutive relationships with physics in formed deep neural networks arxiv preprint arxiv 1808 03398 2018 52 m white m oostrom r lenhard modeling fluid flow and transport in variably saturated porous media with the stomp simulator 1 non volatile three phase model description advances in water resources 18 1995 353 364 53 m t van genuchten a closed form equation for predicting the hy draulic conductivity of unsaturated soils 1 soil science society of amer ica journal 44 1980 892 898 32 http arxiv org abs 1412 6980 http arxiv org abs 1605 07811 http arxiv org abs 1702 03673 http arxiv org abs 1711 10561 http arxiv org abs 1808 03398 54 i tolstikhin o bousquet s gelly b schoelkopf wasserstein auto encoders arxiv preprint arxiv 1711 01558 2017 55 m arjovsky s chintala l bottou wasserstein gan arxiv preprint arxiv 1701 07875 2017 56 d j rezende s mohamed variational inference with normalizing flows arxiv preprint arxiv 1505 05770 2015 57 d p kingma p dhariwal glow generative flow with invertible 1 x 1 convolutions arxiv preprint arxiv 1807 03039 2018 58 x glorot y bengio understanding the difficulty of training deep feed forward neural networks in proceedings of the thirteenth international conference on artificial intelligence and statistics pp 249 256 33 http arxiv org abs 1711 01558 http arxiv org abs 1701 07875 http arxiv org abs 1505 05770 http arxiv org abs 1807 03039 1 introduction 2 methods 2 1 physics informed neural networks 2 2 probabilistic physics informed neural networks 2 3 adversarial inference for joint distribution matching 2 3 1 density ratio estimation by probabilistic classification 2 3 2 entropic regularization bound 2 3 3 adversarial training objective 2 3 4 adversarial training with physics informed constrains 2 3 5 predictive distribution 2 3 6 advantages and caveats of adversarial learning 3 results 3 1 a pedagogical example 3 2 burgers equation 3 3 discovery of constitutive laws for flow through porous media 4 conclusions appendix a sensitivity studies appendix a 1 sensitivity with respect to the neural network initialization appendix a 2 sensitivity with respect to the total number of training and collocation points appendix a 3 sensitivity with respect to the neural network architecture appendix a 4 sensitivity with respect to the adversarial training procedure