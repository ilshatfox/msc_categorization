ar x iv 1 71 0 01 13 3 v 1 cs m s 2 8 s ep 2 01 7 hpc optimal parallel communication algorithm for the simulation of fractional order systems cosmin bonchi eva kaslik and florin ro u west university of timi oara and the eaustria research institute bd v p rvan 4 cam 045 b timi oara ro 300223 romania september 12 2018 abstract a parallel numerical simulation algorithm is presented for fractional order systems involving caputo type derivatives based on the adams bashforth moulton abm predictor corrector scheme the parallel algorithm is implemented using several different approaches a pure mpi version a combination of mpi with openmp optimization and a memory saving speedup approach all tests run on a bluegene p cluster and comparative improvement results for the running time are provided as an applied experiment the solutions of a fractional order version of a system describing a forced series lcr circuit are numerically computed depicting cascades of period doubling bifurca tions which lead to the onset of chaotic behavior keywords fractional order system parallel numerical algorithm hpc processing 1 introduction compared to their integer order counterparts over the past decades fractional order dynamical systems have proved to provide more accurate and realistic results in the modeling of real world processes arising from diverse applied fields 5 although many qualitative properties of fractional order systems can be studied by analytical tools such as local stability of equilibrium states theoretical characterization of chaos in fractional order dynamical systems is yet to be investigated in order to assess chaotic behavior of fractional order dynamical systems accurate estimation of the solutions over large time intervals is of utmost importance however an essential observation is that the employed discretization should use a small step size with the aim 1 http arxiv org abs 1710 01133 v 1 of providing an accurate estimation to the solution of the fractional order system under investigation several numerical methods are used for fractional order systems such as generalizations of predictor corrector methods 7 11 15 p fractional linear multi step methods 14 18 or the adomian decomposition method 6 12 20 these numerical schemes have a major drawback due to the non locality of the fractional differential operators which reflects the hered itary nature of the problem in order to obtain a reliable estimation of the solution at every iteration step all previous iterations have to be taken into account therefore this implies extreme computational costs whenever the solution is computed over a large time interval with a small step size while the numerical computation of a solution of an ordinary fractional differen tial equation on a fixed interval 0 t by one of the standard algorithms described above has an arithmetic complexity of o h 2 where h denotes the step size in the case of ordinary differential equations of first order the arithmetic complexity is only o h 1 2 several approaches have been used to deal with these difficulties such as the short memory principle 8 9 or the nested mesh scheme 13 however a loss of accuracy is inevitable for both methods mainly due to the fact that parts of the integration interval are simply ignored nevertheless these difficulties may be overcome using parallel comput ing algorithms implemented in a conventional way or using available high performance computing systems 1 4 in this paper we will present an efficient parallel algorithm implemented using message passing interface mpi and running on a high performance computing system bluegene p cluster that has 1024 processors and 4 tb of ram memory the numerical method considered here for implement ing the fractional order system is the adams bashforth moulton predictor corrector scheme 11 the main challenge for implementing this method is to parallelize the computation of the solution because the computation of an iteration step requires to take into account all previous iterations 2 preliminaries consider an ordinary fractional differential equation of the form d y t f t y t t 0 t y k 0 yk 0 k 0 1 1 where 0 and denotes the ceiling function that rounds up to the nearest integer the fractional derivative of caputo type is defined as d y t 1 t 0 y t t 1 d 2 the numerical method used in this paper to solve 1 is a fractional ver sion of the adams bashforth moulton predictor corrector scheme 11 the domain 0 t is discretized into n intervals with a step size h t n and the grid points tn nh for n 0 n we will also denote yn y tn and fn f tn yn with y 0 y 0 0 as the initial condition the first step of the scheme is the predictor which will give a first approximation yp n 1 of our solution yp n 1 1 k 0 tk n 1 k y k 0 h n k 0 bn kfk where bn n 1 n 1 2 the final approximation of the solution called the corrector is given by yn 1 1 k 0 tk n 1 k y k 0 h cnf 0 n k 1 an kfk f tn 1 y p n 1 2 where the weights an and cn are defined as an n 2 1 2 n 1 1 n 1 2 and cn n 1 n n 1 2 this numerical scheme can be generalized in a straight forward way when one has to deal with a system of fractional order differential equations the main computational difficulty of this scheme arises from the fact that at each step we require the complete history of the variable i e when computing yn 1 we need to know all previous values yk that are used to compute fk for k n this makes numerical methods addressed at solving fractional differential equations or systems notoriously hard to parallelize 3 parallel numerical algorithm the parallel implementation of adams bashforth moulton algorithm as a method for solving a fractional order dynamical system was first presented by diethelm 10 the solution presented there is not suitable to be running on a hpc cluster because there is an unbalanced workload and waiting idle times for the processes that can cause the performance to be very low due to the hpc parallel implementation rules 19 also the amount of messages passed between processes does not respect the hpc parallel implementation idea 19 and strongly influence the overall performance 3 1 the classical parallel approach in the previous works 1 3 we explored how numerical implementations of the adams bashforth moulton method for fractional order systems can be 3 accelerated by using parallel computing techniques we investigated the fea sibility of parallel computing algorithms and their efficiency in reducing the computational costs over a large time interval the results in 1 concerning the parallel implementation for the adams bashforth moulton method on hpc and cuda show that some execution times are quite high and thus they limit the time frame needed for more accurate simulations figure 1 one master process in 1 the hpc implementation was made using a classical approach a process is the master and all others are slaves to help compute the values for the predictor and corrector the classical execution flow is presented in figure 1 and shows that the master process is working either by computing or communicating while the slave processes have idle times on a hpc architecture these idle times could causes huge delays in communication and drastically increase the overall simulation time due to this low performance the research of optimizing the hpc solution was further pursued generating the results presented in 3 3 2 parallel implementation using pure mpi in order to improve the overall simulation time the workload has been im proved in 3 first the idle times of the processes have been removed secondly we decreased the number of messages which implied saving times in the communication part the parallel implementation method presented in algorithm 1 reflects the core of our implementation the computation for the partial sum is done by each process and the final sum for the predictor and corrector is reduced to all processes by mpi allreduce sp sc so instead of having the classical architecture where one process is the master and all the other processes are slaves just to compute the partial sum in this approach all processes p compute the iteration function values yn and all act as master processes 4 figure 2 no master process algorithm 1 parallel algorithm for the adams bashforth moulton scheme t end of the time interval n global number of points p number of processes p current process np n p y 0 initial condition for n 1 n sp 0 sc 0 nmin np p nmax np p 1 compute local sum for predictor and corrector for k nmin nmax sp sp bn kfk sc sc cn kfk compute the global sum and sent to all processes mpi allreduce sp sc compute the predictor at time tn yp n y 0 an sp compute the corrector at time tn yn y 0 cn y p n sc in our approach designed in figure 2 we have been able to avoid idle times for the processes obtaining a more balanced work load another advantage is that the overall communication between processes was reduced by removing the messages between the slave and master processes there is only one message being exchanged when the global sum is reduced to all processes and because the workload inside the process is balanced the synchronized exchange time is very short a similar method solution was 5 also presented and tested on a pc using a cpu core by 21 with very interesting results from the parallel computational point of view an in depth analysis of the architecture of the available bluegene p clus ter used for simulations reveals the advantages provided by the hardware capabilities which further reduce the communication between the processes our actual bluegene p cluster consists of 1024 nodes one node having 4 g bytes of ram and a quad core processor in order to efficiently use the resources the processes are launched in virtual node vn mode accordingly to 19 in vn mode each process is executed by only one core from an available node hence on the bluegene p cluster four processes are executed on each physical node in this way a maximal number of 4048 parallel processes can be executed on our tests we run 1024 processes in vn mode and thus we actually use 256 physical nodes 3 3 optimal communication time using mpi and openmp aiming for a more efficient use of the bluegene resources we combine the mpi with openmp capabilities in our new implementation this leads to a full employment of the 256 physical nodes as follows in order to run one process in one physical node we change the previous virtual node mode by symmetrical multiprocessing mode thus a process can use all the cores from the processor and the computation can be paral lelized by multi threading therefore by using 256 processes we obtain 1024 computational threads but the mpi messages are exchanged only between 256 instead of 1024 processes using openmp the improvement of the previous algorithm 1 is reflected in algorithm 2 at line 8 which computes the partial sum by using all local cores on our bluegene p cluster the processor in one node is quad core thus we can use four parallel threads to compute the partial sum therefore only 256 processes are used instead of 1024 as in the pure mpi implemen tation the communication between processes being much faster while the computational time is the same 3 4 memory saving improves computing time all previous tests were done using long double data types because the simu lations need high precision and they are very costly in cpu operations and memory usage the long double data is classically represented in the memory on 10 bytes with the precision of 21 decimal points however a closer look at the memory usage in the current hardware by changing data type to double has a major impact on computation time due to data alignment having the 6 algorithm 2 parallel algorithm using openmp t end of the time interval n global number of points p number of processes p current process np n p y 0 initial condition for n 1 n sp 0 sc 0 nmin np p nmax np p 1 compute local sum for predictor and corrector pragma omp parallel for reduction sp sc for k nmin nmax sp sp bn kfk sc sc cn kfk compute the global sum and sent to all processes mpi allreduce sp sc compute the predictor at time tn yp n y 0 an sp compute the corrector at time tn yn y 0 cn y p n sc data aligned by 32 bytes the data access is faster and improves the overall performance by a factor of at least 10 compared to the mpi and openmp implementations the drawback is a decrease in precision for 3 millions steps the preci sion is still conserved by 10 6 so it can be used for an overall view of the evolution of the numerical solution and then for a more precise simulation long double version can be used even with this drawback the performance in computation time is a good compromise as it can be seen in the simulation results 4 simulation results we implemented and tested the presented approach using the hpc cluster of the west university of timi oara romania namely a bluegene p cluster that consists of a fully loaded single bluegene p rack that has 1024 quad core cpus and 4 tb of ram memory and can offer a performance up to 11 7 tflops in table 1 we present the simulation run time results in seconds using different number of time steps number of global points the total running 7 table 1 simulation results in seconds for different numbers of time steps steps hpc classic pure mpi mpi and openmp memory saving 1000000 4621 25 549 64 506 58 69 06 1500000 9162 33 1158 13 1059 35 115 67 2000000 14931 16 2009 87 1810 72 169 23 2500000 22697 66 3066 92 2762 92 234 56 3000000 31659 66 4381 42 3912 29 304 17 times of the hpc classic approach have been obtained by the algorithm presented in 1 with the hpc pure mpi implementation by the algorithm presented in 3 the running time decreased by a factor of 8 with respect to classical approach 1 although in these two approaches the pure computa tion time is similar we emphasize that the overall running time is massively improved due to the synchronized communication moreover using the mpi combined with openmp approach we can see an improvement of around 10 at the overall running time compared to the pure mpi version this is due to the fact that using the same resources instead of having 1024 processes running we only have 256 processes that have to communicate between each other we emphasize that the compu tation time is the same and the improvement is due to the communication time the graphical representation of the running times figure 3 clearly presents the speed up using our different approaches of parallelizing the algorithm studying the available hardware architecture we were able to improve the communication time confirmed by the 10 efficiency obtained in practice paying attention to data usage the computation time is massively im proved having the simulations running very fast the execution time see figure 4 is improved by a factor of at least 10 compared to the execution running time in the mpi and openmp implementations the non classical parallel approaches are fast enough to enable us to run simulations with even more than 5 million steps these improvements allow us to compute the numerical solution of a fractional order system over a large number of steps providing a better understanding about the system s behavior from the dynamic point of view the next section includes more details about the numerical analysis of a test system 8 figure 3 running time comparison for all approaches figure 4 running time comparison for optimized approaches 9 5 numerical experiment our test case is the fractional order version of the normalized system de scribing a forced series lcr circuit 17 d 1 x t y g x d 2 y t y x f sin t 3 where 1 2 0 1 f 0 and the function g is piecewise linear and is defined as g x bx a b if x 1 ax if x 1 bx a b if x 1 the parameter values considered for the numerical simulations are 1 015 0 55 a 1 02 and b 0 58 in the absence of the forcing term i e f 0 system 3 is autonomous and has three equilibrium states e 0 0 0 and e a b 1 b b a 1 b however when f 0 the system 3 is non autonomous and a series of period doubling bifurcations leading to onset of chaotic behavior has been reported 17 when f is increased from 0 to 0 2 considering the fractional orders 1 2 0 9 using the hpc implementation of the parallel algorithm described in section 3 we are able to depict the dynamic behavior of system 3 with an improved precision compared to 17 using a small step size and computing the numerical solution over a large time interval figure 5 shows the attractors of 3 for different values of the parameter f for f 0 085 the existence of two quasi periodic attractors is observed and the period doubling cascade actually involves both attractors eventually leading to the appearance of two chaotic attractors e g for f 0 117 when the value of f is increased these chaotic attractors collide and a double scroll attractor takes their place e g for f 0 125 as we further increase f a sequence of period doubling bifurcations and reversed period doubling bifurcations is observed involving the single attractor of the system 6 conclusion and future work by taking a closer look at the hardware architecture we obtained an improve ment on the running time by decreasing the communication time between the process a similar algorithm was implemented to run on pc using mpi or and openmp with similar results 21 22 additionally by running our mpi implementation combined with openmp on the available bluegene p hardware we gain 10 running time performance moreover in order to 10 figure 5 rich dynamic behavior in system 3 for 1 2 0 9 and different values of the parameter f improve the computation running time some hardware features and capa bilities were exploited leading to a 10 fold reduction of the overall simulation time with the expense of loosing data precision the algorithm that implements the adams bashforth moulton method is valid for solving any kind of fractional order system with fractional deriva tives of caputo type hence having the algorithm run with parameters and functions as input and execute the simulation as a black box is one of the future research objectives as another direction for future research other numerical methods pos sibly using nested meshes for solving fractional order systems of ordinary differential equations or partial differential equations 16 will be explored as well as their parallel implementation algorithms 11 acknowledgements this work was supported by a grant of the romanian national authority for scientific research and innovation cncs uefiscdi project no pn ii ru te 2014 4 0270 references 1 baban a bonchi c fikl a ro u f parallel simulations for fractional order systems in synasc 2016 pp 141 144 2016 2 baleanu d diethelm k scalas e trujillo j j fractional calculus models and numerical methods vol 5 world scientific 2016 3 bonchi c kaslik e ro u f improved parallel simulations for fractional order systems using hpc in cmmse 2017 2017 4 cafagna d grassi g bifurcation and chaos in the fractional order chen system via a time domain approach international journal of bi furcation and chaos 18 7 1845 1863 2008 5 cottone g paola m d santoro r a novel exact representation of stationary colored gaussian processes fractional differential approach journal of physics a mathematical and theoretical 43 8 085 002 2010 url http stacks iop org 1751 8121 43 i 8 a 085002 6 daftardar gejji v jafari h adomian decomposition a tool for solv ing a system of fractional differential equations journal of mathemat ical analysis and applications 301 2 508 518 2005 7 daftardar gejji v sukale y bhalekar s a new predictor corrector method for fractional differential equations applied mathematics and computation 244 158 182 2014 8 deng w short memory principle and a predictor corrector approach for fractional differential equations journal of computational and ap plied mathematics 206 1 174 188 2007 9 deng w li c numerical schemes for fractional ordinary differential equations in numerical modelling intech 2012 10 diethelm k an efficient parallel algorithm for the numerical solution of fractional differential equations fractional calculus and applied analysis 14 3 475 490 2011 11 diethelm k ford n freed a a predictor corrector approach for the numerical solution of fractional differential equations nonlinear dynamics 29 1 4 3 22 2002 12 http stacks iop org 1751 8121 43 i 8 a 085002 12 duan j s rach r baleanu d wazwaz a m a review of the adomian decomposition method and its applications to fractional dif ferential equations communications in fractional calculus 3 2 73 99 2012 13 ford n j simpson a c the numerical solution of fractional differ ential equations speed versus accuracy numerical algorithms 26 4 333 346 2001 14 galeone l garrappa r explicit methods for fractional differential equations and their stability properties journal of computational and applied mathematics 228 2 548 560 2009 15 garrappa r on linear stability of predictor corrector algorithms for fractional differential equations international journal of computer mathematics 87 10 2281 2290 2010 16 gong c bao w tang g yang b liu j an efficient parallel solution for caputo fractional reaction diffusion equation the journal of supercomputing 68 3 1521 1537 2014 17 palanivel j suresh k sabarathinam s thamilmaran k chaos in a low dimensional fractional order nonautonomous nonlinear oscillator chaos solitons fractals 95 33 41 2017 18 pedas a tamme e spline collocation methods for linear multi term fractional differential equations journal of computational and applied mathematics 236 2 167 176 2011 19 redbooks i ibm system blue gene solution blue gene p applica tion development vervante 2009 20 song l wang w a new improved adomian decomposition method and its application to fractional differential equations applied mathe matical modelling 37 3 1590 1598 2013 21 zhang w cai x efficient implementations of the adams bashforth moulton method for solving fractional differential equations proceed ings of fda 12 2012 22 zhang w wei w cai x performance modeling of serial and paral lel implementations of the fractional adams bashforth moulton method fractional calculus and applied analysis 17 3 617 637 2014 13 1 introduction 2 preliminaries 3 parallel numerical algorithm 3 1 the classical parallel approach 3 2 parallel implementation using pure mpi 3 3 optimal communication time using mpi and openmp 3 4 memory saving improves computing time 4 simulation results 5 numerical experiment 6 conclusion and future work