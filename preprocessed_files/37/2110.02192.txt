1 reducing gaze distraction for real time vibration monitoring using augmented reality elijah wyckoff 1 marlan ball 2 fernando moreu 2 1 department of mechanical engineering university of new mexico albuquerque nm 87106 united states 2 department of civil construction and environmental engineering university of new mexico albuquerque nm 87106 united states corresponding author fmoreu unm edu 210 university of new mexico albuquerque nm 87131 0001 operators want to maintain awareness of the structure being tested while observing sensor data normally the human s gaze shifts to a separate device or screen during the experiment for data information missing the structure s physical response the human computer interaction provides valuable data and information but separates the human from the reality the sensor data does not collect experiment safety quality and other contextual information of critical value to the operator to solve this problem this research provides humans with real time information about vibrations using an augmented reality ar application an application is developed to augment sensor data on top of the area of interest which allows the user to perceive real time changes that the data may not warn of this paper presents the results of an experiment that show how ar can provide a channel for direct sensor feedback while increasing awareness of reality in the experiment a researcher attempts to closely follow a moving sensor with their own sensor while observing the moving sensor s data with and without ar the results of the reported experiment indicate that augmenting the information collected from sensors in real time narrows the operator s focus to the structure of interest for more efficient and informed experimentation keywords wireless sensor vibration monitoring gaze distraction eye tracking augmented reality acceleration smart sensing 1 introduction researchers quantify the response of structures by measuring and observing vibrations acquiring smart sensor data in real time enables operators to predict failures and make informed decisions on maintenance 1 this is enabled by iot technology which is used for wireless sensor networks wsn for environmental sensing 2 researchers need to track vibration levels to prevent damage to sensitive machines but current technology does not allow for a researcher to work freely without constantly checking a computer monitor 3 smart infrastructure wireless sensors are useful for their reliability low cost low power and fast deployment characteristics 4 wireless sensor networks are used for monitoring and assessing vibration risk in historical buildings and cultural sites 5 forming a network of wireless sensors supports the gathering of data and decision making before during and after a crisis event a wireless sensor network in torre aquila proved the system is an effective tool for assessing the tower stability while delivering data with loss ratios 0 01 with an estimated lifetime over one year 6 often data acquisition occurs prior to processing in wireless sensor systems for structural health monitoring shm which is why researchers have explored implementing real time wireless data acquisition on the imote 2 wireless sensor platform 7 researchers have also developed a vision based mailto fmoreu unm edu 2 tracking method to detect damage to a structural system using cameras already installed in the system 8 wireless and remote sensor systems are optimal for efficient and reliable data feedback but there remain challenges for users to see real time data open challenges remain that would be beneficial to explore in human sensor interfaces ar is useful to researchers in informing of real time data ar has been used to augment different types of wireless sensor data through iot technology 9 researchers augmented displacement data collected by smart sensors however these values were first recorded and stored in a database before they were graphed in ar 10 researchers have also developed a human machine interface which organizes metadata and provides actionable information by visualizing data about the built environment both on and off site using ar 11 ballor et al investigated using ar in infrastructure inspections where the framework uses the headset s sensors to capture a high resolution 3 d measurement of the infrastructure 12 this can be used to analyze the state of the structure over time and track damage progression ar has been used for shm including detecting heat emitted from electronic equipment 13 wang et al presents two mixed reality and ar systems and their application scenarios for the construction industry 14 this study showed how these technologies can be integrated into heavy construction operations and equipment management and they are emphasized for their potential to reduce cost time and levels of risk by augmenting applicable events with digital content implementing automated driving suffers from a problem with lack of trust and user acceptance and ar technology exists as a solution to mitigate these issues the prospect of increasing user acceptance and trust by communicating system decisions through ar is investigated by quantifying user acceptance using the technology acceptance model 15 ar for manufacturing training specifically for welding is evaluated using the technology acceptance model to understand how welders perceive its practicality and ease of use 16 ar has a wide range of uses making it a valuable tool for shm and this research seeks to develop a framework for the direct augmentation of live vibration data gaze distraction is an important obstacle to consider in experimental work and ar is used to address this issue according to a review of ar technology an estimated 80 to 90 of the information humans receive is through vision 17 the ability to absorb and process information is limited by our mental capacity and the same study examines how ar can reduce this cognitive load each mental task we undertake reduces the capacity for other simultaneous tasks ar technology is applied to vehicle operation using ar heads up displays to lay navigational images directly over what the driver sees through the windshield 18 this research proves how this can reduce the mental effort of applying the information and it prevents gaze distraction because the driver focuses their attention on the road ar is also applied to robot teleoperation to reduce gaze distraction where augmenting live video feed from the robot limits the user s view to pertinent information for safer more controlled operation 19 reducing gaze distraction in vibration monitoring looks to manifest safer operation and higher cognition in the same way this paper leverages ar technology to allow researchers to directly interact with the real world through steady real time communication with wsn providing quantitative information ar technology is used to consolidate information in the user s view so that inspectors receive information regardless of where they are looking or positioned in the real world traditional 3 methods of vibration monitoring include a device with a screen that displays data the new interface has been explored in the domain of structural design since it is now possible to interface the structural responses with holograms and other models permitting the researcher to quantify structural dynamics in the augmented interface the interface includes a lewis 5 low cost efficient wireless intelligent sensor which is an arduino metro m 4 microcontroller equipped with an accelerometer to measure vibrations wirelessly this data is sent over wifi using tcp connection to the microsoft hololens gen 2 headset where acceleration values are plotted real time in the user s field of view the proposed application is validated by a series of experiments testing a human s ability to react and maintain awareness of reality with and without ar the human attempts to recreate the motion a moving sensor with their own sensor while also monitoring data where the human s sensor data and eye movement data are collected this work is innovative in human structures interfaces and it enables a new mode of sensing dynamics in real time 2 framework 2 1 motivation out of the five senses humans receive an estimated 80 90 of information from vision 17 understanding where information is best perceived by vision is important in this research according to younis et al 20 central vision has the highest sharpness visually and is where humans pay the most attention to objects of interest human vision perceives a visual field of more than 200 diameter horizontally and 125 vertically but this research is primarily interested in central vision which makes up an area of about 13 around the area of fixation 20 21 this field is modeled below in figure 1 this research seeks to quantify the reduction in gaze distraction by tracking the area covered by the human s eyes with and without the aid of ar figure 1 model of central vision in human perception 20 this project is developed based on a theory of human structure interfaces researchers are interested in measuring vibrations and are informed by the device that receives the sensor feedback if the device receiving sensor data is an ar headset information can be relayed directly to the human 10 this theory proposes that humans can be better informed and maintain better awareness of reality if they directly receive information on nearby structural response andersson et al demonstrate ar in human robot interaction proposing improved training programming 4 maintenance and process monitoring by augmenting information 22 figure 2 illustrates vibration monitoring where it is necessary for the researcher to be present for experimentation in this setup the researcher monitors real time vibration data collected from sensors secured to a frame the researcher maintains focus on the suspended mass while a shaker generates excitations typically data is recorded and plotted on a computer screen which requires the inspector to focus their attention on either the data or the structure monitoring both the data and the structure becomes difficult when the computer screen obstructs the researcher s view the user also depends on the location of the computer for information as it is inefficient and inconvenient to hold and carry around this introduces potential issues with safety and control figure 2 a side view of the researcher s gaze while monitoring vibrations b view from behind the researcher demonstrating obstruction by the screen displaying data 2 2 proposed model by augmenting the plot of the live acceleration data a loop between human and reality is formed that eliminates gaze distraction as a barrier to vibration monitoring figure 3 illustrates gaze distraction as a barrier and figure 4 shows the proposed model aided by ar the user receives direct information on reality via the augmented plot of live data in the ar headset thereby improving cognition of structural response while maintaining an area near central vision in the framework of this research a user reacts to data by attempting to synchronize the acceleration of a handheld sensor with a moving sensor 2 3 new interface the new interface combines hardware and software to improve human cognition of sensor information a connection between the sensor and user is formed by augmenting feedback in the user s vision as shown in figure 4 the ar headset is used to augment information in the form of holograms while maintaining awareness of the structure in the proposed application acceleration data is plotted as a holographic chart in the user s view 5 figure 3 current model figure 4 proposed model 3 hardware ar blends interactive digital elements with a real world environment holograms are generated by a computer and super imposed onto the real world environment which allows the user to interact with the merged environment this is enabled by a device that creates an ar environment via optical see through display the ar headset is a head mounted display that allows for contact free operation by hand gestures and voice commands 3 1 augmented reality device selection there were several factors to be considered in selecting an ar device for use in this research these include the headset s sensing platform system display and interface and general properties including weight durability battery life price and availability mascare as et al 23 gives an overview of these considerations used to make the device selection for this project it was also important to consider the device manufacturer because development of ar applications varies depending on the platform the system considerations include the processing unit random access memory ram system on a chip soc and the device s storage display capabilities include the resolution field of view aspect ratio and refresh rate 6 3 1 2 microsoft hololens 2 selection considering all the device selection criteria the microsoft hololens 2 headset was selected for development and application deployment in this project over the hololens first gen the hololens 2 is the more expensive option but is the best ar device in terms of performance moreu et al 24 summarizes the advantages of the selected device with a comprehensive breakdown of its features and capabilities the microsoft platform allows for universal windows platform uwp development which is supported in unity a significant change in the hololens 2 from the first generation is the move from an x 86 processor to an arm based platform for higher performance and power efficiency 25 the field of view in hololens 2 is also improved up to 52 degrees from 35 degrees in the first gen additionally the hololens 2 enables eye tracking and hand tracking as opposed to the limited gesture tracking of the first gen hololens a more detailed breakdown of the hololens 2 specs from microsoft 26 is included in table 1 figure 5 ar headset microsoft hololens 2 3 2 sensing platform this section describes the sensing platform developed for detecting and recording vibratory data the sensing platform is developed to read acceleration data in a triaxial coordinate system as a wireless shm system this is done with a low cost efficient wireless intelligent sensor abbreviated as lewis 5 the lewis 5 sensor is built by combining a wifi shield and microcontroller with a triaxial accelerometer 3 3 lewis 5 and its components this section provides an overview of the individual components needed to construct the sensor and includes a price breakdown to show the low cost aspect of the sensor a description and price point of each component is included in table 2 the sensor connects via wifi but requires a power source hooked up via micro usb the physical components are shown in figure 5 and the fully assembled sensor is labeled in figure 7 7 table 1 hololens 2 relevant features 26 microsoft hololens 2 general field of view 52 degrees soc qualcomm snapdragon 850 compute platform resolution 2 k 3 2 light engines storage 64 gb ufs 2 1 weight 566 g battery life 2 3 hours active use connectivity wifi usb type c bluetooth software windows holographic operating system microsoft edge dynamics 365 3 d viewer sensors hand tracking 4 visible light cameras eye tracking 2 ir cameras depth 1 mp time of flight depth sensor imu accelerometer gyroscope magnetometer camera 8 mp stills 1080 p 30 video microphone and speakers 5 channels spatial sound table 2 sensor breakdown part description manufacturer price arduino metro m 4 express microcontroller adafruit 27 50 arduino airlift wifi shield shield wifi co processor adafruit 14 95 mma 8451 triaxial accelerometer adafruit 7 95 headers connectors sparkfun 1 50 jump wires connectors sparkfun 1 95 total cost 53 85 3 3 1 metro m 4 express the metro m 4 express is a 32 bit microcontroller with the atsamd 51 microchip 27 the cortex m 4 core runs at 120 mhz with floating point support the board is powered via micro usb or barrel jack connection the board has 25 general purpose input output pins including 8 analog in two analog out and 22 pwm outputs the pins can collect information from sensors for use in this project it also includes a 2 mb quad spi flash storage chip which reads and writes programs from arduino the board is flexible efficient and affordable making it a good option for this project 3 3 2 airlift wifi shield the airlift wifi shield allows the use of the esp 32 chip as a wifi co processor 28 the metro m 4 microcontroller does not have wifi built in so the addition of the shield permits wifi network 8 connection and data transfer from websites as well as the sending of socket based commands the shield includes a microsd card socket used to host or store data the shield is connected to the microcontroller with stack headers in summary the wifi shield is necessary for wireless capabilities 3 2 3 mma 8451 accelerometer the triple axis accelerometer used for this project is the high precision mma 8451 with a 14 bit analog to digital converter 29 the accelerometer is used detect motion tilt and basic orientation designed for use in devices like phones and tablets for the purpose of this project the accelerometer is used to detect motion especially vibrations its usage range varies from 2 g up to 8 g which ideal for its application to this project a b c figure 6 components of the lewis 5 sensor a metro m 4 express b mma 8451 accelerometer c airlift wifi shield figure 7 lewis 5 sensor full assembly 5 software and development programming and development of the ar application is done in unity version 2018 4 19 f 1 taking advantage of the mixed reality toolkit mrtk from microsoft the mrtk is applied to a scene built in the unity application to configure the scene for ar use the application is developed for 9 universal windows platform which allows deployment to the hololens 2 the programming platform is visual studio 2019 and the unity scene programming is written in c figure 8 software components 5 1 arduino programming the sensor programming was performed in the arduino ide an open source software environment that is written in java and based on processing and other software this program facilitates the writing and uploading of code for any arduino board as well as other compatible systems 5 1 1 server creation the wifinina library is available for download in the arduino ide this library enables the lewis 5 sensor to be set up as a transmission control protocol tcp server in the arduino code the board connects to a nearby wifi network and accepts incoming connections on the port it is listening on if the network is private the arduino code includes a secret tab with the network name and password existing scripts for the mma 8451 accelerometer were modified to read print and send the acceleration data at a sampling rate of 20 points per second the arduino serial monitor prints the ssid of the network it is connected to and confirms the wifi connection the board will then wait for a client connection before it begins printing the accelerometer values the serial monitor window begins auto scrolling with the three columns of acceleration data once a client successfully connects there is a slight time delay in the augmented plot of sensor data induced by the network connection which was investigated by the researchers in a series of 12 tests the tests were conducted on a mobile hotspot which is used as the wifi network for the experiment section of this paper it was discovered that the average time delay was about 0 26 seconds on the hotspot which is taken into consideration when reviewing results 5 2 unity development unity game engine version 2018 4 19 f 1 was used for cross platform development as it supports open source programming for headsets and mobile devices the unity scene is configured with microsoft s mrtk library to support the ar features of the application the toolkit includes default scripts for necessary features in the hololens such as gestures commands and interface features 10 5 2 1 client connection modified code from timur kuzhagaliyev 30 is implemented for connecting the hololens and unity to sockets the process implements a tcp client that works for development in the unity editor as well as for development in uwp on hololens functions in the windows sockets namespace system net sockets are used to connect the hololens as a client to the open port on the sensor s server 5 2 2 graph development in unity the graph of the live data is developed as a scatter plot which was chosen as the most effective and efficient solution the graph is developed based on a tutorial from catlike coding 31 points at each appropriate coordinate are generated by unity s default cube game object which are color coordinated based on x y and z acceleration each data point is graphed as a small 3 d cube for visual feedback the transform component is used to position each individual cube which are variably instantiated as clones vector 3 creates a 3 d vector which defines the position of each cube the incoming data is parsed to define each point of vector 3 at any given time there are 100 cubes generating the data lines in the display this is defined by the resolution set in unity as the number of cubes is set to the value of the resolution these cubes are connected with a linerenderer command that makes the displayed data appear as a line chart rather than individual cubes the graph updates with each frame meaning the cubes are adjusted as time progresses defined by the function 5 3 application development the problem addressed in the following section is the lack of a user friendly interface for an ar application for live accelerometer feedback the previous model was a bare plot of the three acceleration lines the developed interface provides the necessary inputs for commands including client connection and disconnection and graph initiation and shut down the interface also includes a means of providing the user with a warning system for the breaching of a user specified threshold value figure 9 illustrates the details of application development in the form of a flowchart figure 9 application flowchart 11 5 4 interface menu and functions this section presents the interface of the ar application and explains the function of its unique features the full view of the interface is shown in figure 10 the application interface consists of six different buttons with specific functionality the following subsections contain a detailed explanation of these functions and their use figure 10 interface menu and graph 5 4 1 client start client start connects the client to the server via tcp in the context of the application the computer running the arduino program acts as the server and the device running the ar application is the client the unity code requires the ip address of the arduino board and the unity code and arduino code are set up on the same port 5 4 2 client stop client stop closes the client connection to the server the live data feed flattens to zero and the arduino program must be rerun to initiate another connection 5 4 3 view start this button initiates the function continueinput incoming data from the server is parsed into x y and z vectors this corresponds to the axes of the accelerometer the graph plots the data from left to right as three color coordinated lines data is converted to terms of the gravitational constant g the x and y data are also offset so that the x line does overlap and hide the y line therefore the graph axis is labeled as z acc for the purpose of the experiment as well as simplicity future work on the application will include the addition of x y and z axes selection 5 4 4 view stop stopping the view zeros out the three data lines but does not disconnect the client the view may be resumed by selecting view start again 12 5 5 positioning the graph in the early development stage of the application the acceleration lines plotted at an arbitrary point in space to verify accurate positioning of the horizontal axis lines the graph was developed using known input from an electrodynamic exciter the exciter vibrates at a user defined frequency to enable exact placement of the axis lines the x axis represents values of time in seconds that are spaced according to the sampling rate by measuring one second intervals the x axis labels were placed accordingly 5 5 1 electrodynamic exciter the smartshaker model k 2004 e 01 electrodynamic exciter from the modal shop is a small portable permanent magnet shaker with a power amplifier integrated in its base 32 the excitation signal from a function generator is plugged directly into the bnc connector at the base of the shaker the framework also includes a separate ar application which can be used to change the input to the shaker wirelessly the smartshaker provides up to 7 pounds pk sine force and is supplied with a dc power supply benefits of the shaker include the integrated power amplifier easy mounting and positioning and 10 32 threaded mounting inserts for payloads up to 2 lbs the lewis 5 sensor is mounted to the shaker by a 10 32 nylon stinger as shown in figure 11 figure 11 graph development and verification sensor shaker configuration 6 investigating reduced gaze distraction 6 1 experimental objective to fully understand reality humans receive information from the physical space while relying on sensors for data and information they cannot detect with their own senses researchers have examined human ability to tap their fingers at frequencies of 1 2 and 3 hz to investigate manual dexterity of elderly subjects 33 for this research a researcher is tasked with following a moving sensor with a second handheld sensor while also maintaining awareness of the data received from the moving sensor the moving sensor is run at 1 1 5 2 2 5 and 3 hz the objective of the experiment is to measure the level of gaze distraction while monitoring and attempting to recreate vibration data with and without ar where it is hypothesized that human has a better sense of reality when the data is augmented in their central vision quantifying the area covered by the 13 user s eyes and the user s ability to follow a moving sensor provides a means of understanding the value of ar as a tool for data visualization and control figure 12 demonstrates the value of ar in reducing gaze distraction by modeling the primary area of interest and its proximity to central vision in the three experimental cases reality monitoring data with a device and monitoring data with ar figure 12 a the researcher maintains maximum awareness of reality in their central vision b the area of interest is not fully in central vision when checking data c the area of interest and data feedback are constrained to the hololens user s central vision 6 2 1 experimental setup and procedure the experiment was set up with two laptop computers two lewis 5 sensors a smart shaker and the microsoft hololens 2 one laptop computer provided power to the shaker sensor and the other laptop computer supplied power to the handheld sensor the shaker sensor the first laptop and the hololens are connected to the mobile wifi hotspot mentioned in section 5 1 1 to send data from sensor to hololens and from hololens to mysql database the second laptop was also used to plot sensor data when measuring gaze distraction without ar the researcher acting as the subject was positioned standing one meter from the sensor shaker setup the shaker was run at 1 1 5 2 2 5 and 3 hz where a second researcher and the subject synchronize the sensors with a vertical excitation the researcher acting as the subject begins following the shaker sensor at their discretion for a period of approximately 12 seconds they were also instructed to maintain 14 awareness of the data while following the moving sensor this generates a sinusoidal plot which can be compared to the plot of the shaker sensor data to obtain time delay additionally the data can be analyzed in the frequency domain to determine how well the user was able to synchronize with the shaker sensor this data is collected using the hololens 2 eye tracking api which from a target of one meter can be plotted in terms of x and y coordinates with an accuracy of 1 56 cm 34 the user must click a button in the application ui to begin eye tracking thus the points at the beginning and end are removed during analysis all analysis and plot generation are done in matlab figure 13 shows the experimental setup with plotted eye tracking and the matlab results of the human s eye movement figure 13 experimental setup and example of eye tracking with ar graph and shaker sensor 6 2 3 experimental results and analysis the eye tracking data is sent from the hololens to a mysql database which is then exported as a json file and converted to a string and parsed in matlab so that the data can be plotted the start and end points are removed by reducing the range of the data each point has a three dimensional coordinate but this research is concerned only with the vertical and horizontal position of the eye movement the string of data can then be graphed in matlab where each point is plotted and connected with a solid line representing the path of eye movement the eye tracking data is sent along with time stamps which allowed the researchers to calculate an average sampling rate the researchers are aware of the variable frame rate in ar applications and especially in ar applications communicating with devices like sensors this influences the sampling rate of eye tracking data and this is taken into account through a method of collecting the real sampling rate for example in five experiments the researchers determined the sampling rate by collecting eye tracking data while running the sensor plot in the same application for multiple iterations the approximated sampling rate for the five experiments was 34 eye tracking points for three experiments at 1 5 hz are collected to demonstrate the importance of gaze distraction researchers conducted the same experiment at the three scenarios and collected the eye tracking points for approximately 50 oscillations the time varied between 30 and 40 seconds depending on the experiment figure 14 shows the results from the eye tracking while the human is trying to match the data by observing the experiment without any dataset the results show that the area of eye tracking is very concentrated apart from four diagonals that can be attributed to the human s eyes drifting to 15 the table nevertheless the eye tracking data of this figure shows how the gaze distraction is minimized for the entire duration of the experiment figure 14 eye tracking results while strictly monitoring the sensor conversely figure 15 shows the results from the eye tracking while the human is trying to match the moving sensor by observing the experiment while data is plotted on a laptop screen the figure shows that eye tracking covers the space in between the screen and the moving sensor as the human attempts to maintain awareness of both this depends on the positioning of the monitor so results vary depending on the experimental setup for the purpose of the experiment the laptop was in front of the human and 1 m from the shaker setup figure 15 eye tracking results monitoring data plotted on a separate screen figure 16 shows the results from the eye tracking while the human is trying to match the moving sensor while monitoring data in ar the results show that the area of eye tracking is extremely 16 concentrated with only one diagonal observed where the human s eyes drifted to the left side of the augmented plot the eye tracking data is heavily concentrated because the hologram of the plotted data is augmented directly on top of the moving sensor figure 16 eye tracking results with the ar plot as expected the eye tracking results shown in figures 14 16 prove the inspector covers an area much closer to central vision than when monitoring data on a separate screen these results help quantify the reduction in gaze distraction when monitoring an augmented graph of sensor data rather than a separate screen the eyes drift 0 24 m from the primary area of focus the shaker sensor as opposed to covering 0 97 m of space outside of central vision when checking a separate screen the human s eyes also drifted even when instructed to remain solely focused on the sensor whereas the user did not get distracted with ar the value lies in the results obtained with ar as the graph can be augmented on top of the area of interest without needing to be supported in some way or blocking the user s view hence the minimal amount of eye movement observed in the results obtained with ar the sinusoidal plots of the handheld sensor and the shaker sensor are plotted from the recorded data according to the sampling rate of the sensor the time vector for the plot is generated from known values of the length of the recorded data and the sampling rate the peak to peak distance between each of the first 10 shaker and human excitations is recorded manually and the average is reported as the time offset for each test as per equation 1 the shaker plot has slight dips that indicate the point at which the shaker briefly pauses at the top and bottom of its motion and the peaks of the human s sensor movement are clearly defined these are the points taken as and 1 10 1 1 17 figure 17 shows the time history of the first 10 excitations for each experiment where the x axis is the time duration of the 10 excitations in seconds the plots are normalized to include the first 10 excitations for each experiment hence the x axis labels are removed and labeled as nondimensional time notably the human s response was inconsistent in both synchronization and amplitude when monitoring the data on the laptop screen the results at 1 hz are the clearest example of the difference between monitoring the laptop screen and monitoring data in ar the response aided by ar closely matches the shaker whereas the response aided by the laptop screen is significantly off for the last nine excitations the results aided by ar also display consistent amplitude for each of the individual experiments when compared to the with screen results and the standard deviation of the amplitude of the peaks of 10 excitations is taken to examine this result figure 17 time history of each experiment 18 figure 18 shows each individual psd generated for the signal in relation to the frequency of the shaker which is indicated by the vertical black line these results are used to understand how well the human synchronized with the moving sensor auto spectral density estimates were generated for each single input signal using welch s method this returns estimates at specified frequencies defined in the range of the sampling rate 35 the truncation window is set to reduce uncertainties where an integer multiple of 16 times the sampling rate is used to set the truncation window for each calculation 36 spikes in the psd indicate that the signal is correlated with itself at regular periods and thus indicate the spectra with the greatest effect 37 this is done to determine the frequency of each signal including that of the shaker since the shaker frequency cannot be assumed to be exact the results for following the shaker while monitoring data on a computer screen termed with screen indicate an asynchronous result in each psd conversely the psd results with ar show that the human was able to generate a signal with a frequency close to that of the shaker sensor the exact value of each offset is reported in figure 20 figure 18 psd of each experiment 19 figures 19 21 display bar graphs of the reported results the results are calculated from the range in which the human attempted to follow the shaker with the first 10 excitations considered as the range for time delay calculations combining the eye tracking results with the results from the handheld sensor prove increased awareness of reality while using ar experiments at higher frequency were considered however the human has difficulty recreating a faster response and the results are less valuable with shorter excitations as expected the human performed the worst when attempting to maintain awareness of data plotted on the computer screen figure 19 reports the average time offset between the response generated by the human and the response from the shaker sensor the human struggled the most at 1 and 2 hz with the separate screen with an average delay of 0 31 and 0 3 seconds respectively figure 19 results of time offset in user s attempt to follow moving sensor figure 20 shows the results of the human s synchronization with the moving sensor calculated from the psd results of figure 18 the human created a response with significantly worse synchronization and consistency when monitoring the computer screen conversely they generated a frequency with less than a 0 1 hz offset for each of the experiments with ar and reality notably the human performed better with ar at 2 hz than solely following reality and had very similar results at the other four frequencies figure 20 results of user synchronization with moving sensor 20 figure 21 displays the results for the standard deviation of the 10 peaks of the signal generated by the human the human generated consistent amplitude at 1 5 hz compared to the other two cases however the standard deviation of the excitation peaks for the other four experiments was much higher in comparison the human was more consistent with ar for each experiment with similar standard deviation compared to the results with reality figure 21 results of user consistency in amplitude for the 10 excitations from the combined results for time offset synchronization and consistency it can be concluded that ar is an improved solution in vibration monitoring compared to the results of the case following reality the results with ar are consistently in a similar range this conclusion was expected as ar provides the ability to focus on both reality and data whereas monitoring data with a separate device does not 7 conclusions this paper developed and tested an ar application for live sensor feedback to reduce gaze distraction in vibration monitoring an experiment was conducted to determine if augmenting data gives a human better awareness of reality by allowing the human to remain focused on the physical space by tracking the human s eyes an experiment proved that gaze remains close to the primary area of focus when monitoring vibration data in ar additionally the human was able to use a handheld sensor to closely replicate the response of a sensor in the primary area of focus while maintaining awareness of the vibration data compared to the same test with the data shown on a separate screen the human performed significantly better which demonstrates the improved sense of reality this project has the potential to expand upon the current model for the inclusion of multiple sensors different types of sensing devices and states and other information pertinent to an inspector s interests this implementation of ar technology reduces gaze distraction in vibration monitoring and allows inspectors to monitor both the physical space and the collected data for awareness and safety 21 acknowledgements the financial support of this research is provided in part by the air force research laboratory afrl grant number fa 9453 18 2 0022 and the new mexico consortium nmsgc sub award no q 02151 the authors would like to extend thanks to dr chris petersen and dr derek doyle for their support and feedback in the project references 1 namuduri s narayanan b n davuluru v s burton l bhansali s 2020 january 28 review deep learning methods for sensor based predictive maintenance and future perspectives for electrochemical sensors journal of the electrochemical society 167 3 037552 doi 10 1149 1945 7111 ab 67 a 8 2 raj a steingart d 2018 april 25 review power sources for the internet of things journal of the electrochemical society 165 8 doi 10 1149 2 0181808 jes 3 karlsson m h rnqvist f 2018 robot condition monitoring and production simulation dissertation retrieved from http urn kb se resolve urn urn nbn se ltu diva 69024 4 morimoto r 2013 a socio economic analysis of smart infrastructure sensor technology transportation research part c emerging technologies 31 18 29 doi 10 1016 j trc 2013 02 015 5 abruzzese d angelaccio m giuliano r miccoli l vari a 2009 monitoring and vibration risk assessment in cultural heritage via wireless sensors network 2009 2 nd conference on human system interactions doi 10 1109 hsi 2009 5091040 6 m ceriotti et al monitoring heritage buildings with wireless sensor networks the torre aquila deployment 2009 international conference on information processing in sensor networks san francisco ca 2009 pp 277 288 7 linderman l e mechitov k a spencer b f 2012 tinyos based real time wireless data acquisition framework for structural health monitoring and control structural control and health monitoring 20 6 1007 1020 https doi org 10 1002 stc 1514 8 harvey p s elisha g 2018 vision based vibration monitoring using existing cameras installed within a building structural control and health monitoring 25 11 https doi org 10 1002 stc 2235 9 kumar p 2021 january 20 augmented reality based sensor data display retrieved from https www engineersgarage com electronic projects augmented reality based sensor data display 10 aguero m maharjan d rodriguez m d mascarenas d d moreu f 2020 design and implementation of a connection between augmented reality and sensors robotics 9 1 3 doi 10 3390 robotics 9010003 11 napolitano r liu z sun c glisic b 2019 combination of image based documentation and augmented reality for structural health monitoring and building pathology frontiers in built environment 5 doi 10 3389 fbuil 2019 00050 12 ballor j p et al 2019 augmented reality for next generation infrastructure inspections in barthorpe r eds model validation and uncertainty quantification volume 3 conference proceedings of the society for experimental mechanics series springer cham https doi org 10 1007 978 3 319 74793 4 23 22 13 morales garcia j e gertsen h j liao a s n mascarenas d d l augmented reality for smart infrastructure inspection technical report los alamos national lab lanl los alamos nm usa 2017 14 wang x 2008 improving human machine interfaces for construction equipment operations with mixed and augmented reality robotics and automation in construction doi 10 5772 5850 15 wintersberger p frison a riener a sawitzky t v 2019 fostering user acceptance and trust in fully automated vehicles evaluating the potential of augmented reality presence virtual and augmented reality 27 1 46 62 doi 10 1162 pres a 00320 16 papakostas c troussas c krouska a et al user acceptance of augmented reality welding simulator in engineering training educ inf technol 2021 https doi org 10 1007 s 10639 020 10418 7 17 porter m e heppelmann j e 2017 a manager s guide to augmented reality harvard business review 46 57 https hbr org 18 park h s park m w wong k h kim k jung s k 2013 in vehicle ar hud system to provide driving safety information etri journal 35 6 1038 1047 doi 10 4218 etrij 13 2013 0041 19 hedayati h walker m szafir d 2018 improving collocated robot teleoperation with augmented reality proceedings of the 2018 acm ieee international conference on human robot interaction doi 10 1145 3171221 3171251 20 younis ola al nuaimy waleed alomari mohammad rowe fiona 2019 a hazard detection and tracking system for people with peripheral vision loss using smart glasses and augmented reality international journal of advanced computer science and applications 10 1 9 10 14569 ijacsa 2019 0100201 21 loschky l c nuthmann a fortenbaugh f c levi d m 2017 scene perception from central to peripheral vision journal of vision 17 1 6 doi 10 1167 17 1 6 22 andersson n argyrou a n gele f ubis f campos u e zarate m o wilterdink r 2016 ar enhanced human robot interaction methodologies algorithms tools procedia cirp 44 193 198 doi 10 1016 j procir 2016 03 022 23 mascare as dd ballor jp mcclain ol et al augmented reality for next generation infrastructure inspections structural health monitoring 2021 20 4 1957 1979 doi 10 1177 1475921720953846 24 moreu f lippitt c maharjan d aguero m yuan x 2019 augmented reality enhancing the inspections of transportation infrastructure research education and industry implementation retrieved from https digitalcommons lsu edu transet data 57 25 pilkington i 2019 october 8 microsoft hololens 2 brings ar to industry arm blueprint https www arm com blogs blueprint microsofts hololens 2 brings augmented reality to industry 26 hololens 2 overview features and specs microsoft hololens overview features and specs microsoft hololens n d https www microsoft com en us hololens hardware 27 industries a n d adafruit metro m 4 feat microchip atsamd 51 retrieved from https www adafruit com product 3382 28 industries a n d adafruit airlift shield esp 32 wifi co processor retrieved from https www adafruit com product 4285 https doi org 10 1007 s 10639 020 10418 7 https www microsoft com en us hololens hardware https www microsoft com en us hololens hardware 23 29 industries a n d adafruit triple axis accelerometer 2 4 8 g 14 bit mma 8451 retrieved from https www adafruit com product 2019 30 kuzhagaliyev t 2018 may 27 tcp client in a uwp unity app on hololens foxy panda https foxypanda me tcp client in a uwp unity app on hololens 31 flick j 2020 september 23 building a graph retrieved from https catlikecoding com unity tutorials basics building a graph 32 mts systems n d smartshaker with integrated power amplifier the modal shop inc https www modalshop com excitation smartshaker with integrated power amplifier id 272 33 carment l abdellatif a lafuente lafuente c pariel s maier m a belmin j lindberg p g 2018 manual dexterity and aging a pilot study disentangling sensorimotor from cognitive decline frontiers in neurology 9 https doi org 10 3389 fneur 2018 00910 34 kapp s barz m mukhametov s sonntag d kuhn j 2021 arett augmented reality eye tracking toolkit for head mounted displays sensors 21 6 2234 https doi org 10 3390 s 21062234 35 cross spectral density estimates cross spectral density estimates ltpda toolbox n d https www lisamission org ltpda usermanual ug sigproc cpsd html references 36 w d s c 2015 vibration fundamentals and practice crc press 37 hunter n f cross k r nelson g 2018 the cross spectrum in multiple input multiple response vibration testing topics in modal analysis testing volume 9 91 102 https doi org 10 1007 978 3 319 74700 2 10 https www adafruit com product 2019 https catlikecoding com unity tutorials basics building a graph https www modalshop com excitation smartshaker with integrated power amplifier id 272 https www modalshop com excitation smartshaker with integrated power amplifier id 272 https doi org 10 3390 s 21062234 https www lisamission org ltpda usermanual ug sigproc cpsd html references