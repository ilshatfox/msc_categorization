on explaining the surprising success of reservoir computing forecaster of chaos the universal machine learning dynamical system with contrasts to var and dmd erik bollt 1 2 1 department of electrical and computer engineering clarkson university potsdam ny 13699 usa 2 clarkson center for complex systems science c 3 s 2 potsdam ny 13699 usa abstract machine learning has become a widely popular and successful paradigm including in data driven science and engineering a major application problem is data driven forecasting of future states from a complex dynamical artificial neural networks ann have evolved as a clear leader amongst many machine learning approaches and recurrent neural networks rnn are considered to be especially well suited for forecasting dynamical systems in this setting the echo state networks esn or reservoir computer rc have emerged for their simplicity and computational complexity advantages instead of a fully trained network an rc trains only read out weights by a simple efficient least squares method what is perhaps quite surprising is that nonetheless an rc succeeds to make high quality forecasts com petitively with more intensively trained methods even if not the leader there remains an unanswered question as to why and how an rc works at all despite randomly selected weights to this end this work analyzes a further simplified rc where the internal activation function is an identity function our simplification is not presented for sake of tuning or improving an rc but rather for sake of analysis of what we take to be the surprise being not that it doesn t work better but that such random methods work at all we explicitly connect the rc with linear activation and linear read out to well developed time series literature on vector autoregressive averages var that includes theorems on representability through the wold theorem which already perform reasonably for short term forecasts in the case of a linear activation and now popular quadratic read out rc we explicitly connect to a nonlinear var nvar which performs quite well further we associate this paradigm to the now widely popular dynamic mode decomposition dmd and thus these three are in a sense different faces of the same thing we illustrate our observations in terms of popular benchmark examples including mackey glass differential delay equations and the lorenz 63 system key words linear reservoir computing rc neural network recurrent neural network rnn machine learning vector autoregression var wold theorem dynamic mode decomposition dmd the power and success of artificial neural networks has been profound across many disciplines including in dynamical systems a leader amongst methodologies for forecasting has been the recurrent neural network rnn for aspects of memory however because of the large number of parameters to train to data observations and likewise the nonlinear nature of the associated optimization process the training phase can be computationally extremely intensive the echo state reservoir computing rc concept is a significant simplification where only the corresponding author bolltem clarkson edu 1 ar x iv 2 00 8 06 53 0 v 6 ph ys ic s da ta a n 1 7 m ar 2 02 1 output weights are trained and in a manner that allows for a straight forward and cheap least squares method the rest of the weights those of the input layer and those of inner layers are simply selected randomly it is clear that this would be cheaper to train but what is not clear and perhaps a surprise is that it would work at all but work it does with a simplification of the concept to allow for a linear activation function while the performance is not quite as good it does still work and now we are able to analyze in detail the role of the randomly selected parameters and how there is still freedom in fitting a well defined time series forecasting model which in fact is equivalent to the well developed theory of vector autoregression var within the var and related vma theory we recall the wold theorem that allows us to discuss representation and now as we show it is relevant to the rc for machine learning also with this description we are able to connect to the recently highly popular dmd concept while we do observe that the fully linear version of the rc and so corresponding var does make reasonable short term forecasts a linear rc with quadratic readout significantly improves forecasts and even apparently once errors do occur they seem more true to the true nature of the original system in the spirit of the linear rc plus linear readout yields a var we show that linear rc with a hadamard quadratic readout yields a nonlinear var nvar that includes monomials of all quadratic forms 1 introduction artificial neural networks ann have emerged as a core and powerful technology in machine learning 25 52 62 63 65 that is well suited for the supervised learning in data driven science and engineering specifically including for forecasting problems in complex dynamical systems 28 48 42 53 15 50 40 however the most straight forward feedforward ann with back propagation for training concepts can be extremely expensive to optimize to the data even considering important recent innovations such as stochastic gradient descent or hardware break throughs such as gpu based processing recurrent neural network concepts rnn are especially suitable for temporal data from a dynamical system 28 47 5 6 22 74 as they naturally embed temporal information and especially the long short term memory lstm approach demonstrate excellent fidelity 89 41 88 22 20 94 but these are especially expensive to fully train 69 the reservoir computing rc 44 58 87 and the closely related echo state network esn 43 57 and liquid state machine lsm 60 35 have emerged as a special variant of an rnn where only the output layer is trained rather than the entire network of weights as such this requires only a simple and efficient least squares estimation rather than the more expensive full nonlinear optimization associated with a fully training an rnn nonetheless and perhaps a most surprising outcome is that despite this gross simplification the forecasting capability can still be competitive even for chaotic or spatiotemporally complex problems 89 70 96 56 18 19 30 specifically an rc thrives when a full state observation is available while fuller and more expensive variants of rnn especially the lstm would considered higher perming especially when only a reduced variable set is available 88 89 still the rc are popular surely because of their simplicity to train and perhaps in part because of their undeniable even if surprising fidelity the purpose of this work is to offer at least a partial explanation as to how an rc can be such a successful and general universal dynamical system for forecasting such a wide array of systems despite randomly trained read in and inner weights in other words we work to better understand where does the randomness go the purpose of this paper is not specifically to build a new method or to improve the current method but to explain what is perhaps a surprising that the rc method works at all in so doing we challenge the concept with a simplified linear activation function version for sake that this allows our simplified analysis throughout and even if this version has reduced fidelity we show it does still have theoretic reasons it still works which we are now in a position to describe in detail our simplification to a linear activation function also allows us to explicitly write the rc as a var serving as a bridge to the more theoretically well established time series theory and also to dmd nonetheless we do describe a simple scenario where the linear rc is expected to perform no worse than the widely used nonlinear counterpart in terms of efficiency but the linear version we can now show in detail without ambiguity when there will be a 2 good forecasting version by tying it to the representation theory of var as seen through the wold theorem while the linear activation reservoir and linear read out is explicitly connected to a var and results at least for short term forecasts are reasonable we also show that linear reservoir with quadratic read out as quadratic read out has become popular 70 is equivalent to a nvar and this turns to perform quite well there have been few explanations as to how despite the random construction an rc works so well but notably 32 17 24 usually instead we find in the literature a collection of descriptions as to how to choose random networks as the inner layers regarding sparsity 81 34 or regarding design of the spectral radius for linear stability 19 45 34 and the echo property 29 an especially strong result comes from study of volterra series by boyd and chua 14 where it was proven that a finite dimensional linear dynamical with a nonlinear read out even a polynomial read out can approximate very general signals a new analysis by hart hook and dawes show that regularized echo state machines make good universal approximators of ergodic dynamical systems 39 and furthermore give generalized embedding results 38 reminiscent of the classical taken s embedding theorem 83 also recently it has been shown that fading memory leads to universality in gonan and ortega 33 in the spirit of still incomplete theoretical basis as to the underlying success of rc we allow a simplified version of rc with linear activation functions for which we are able to more fully identify the inner workings of how the rc can be a universal forecasting machine even for time series from complex and chaotic dynamical systems we show that by this simplification the rc still works albeit with reduced performance quality but nonetheless the purpose here being theoretical explanation of how such a simple system of only training the read out is possible we offer this variant as a theoretical construction by this interpretation we will also be able to connect the rc concept to other theoretically more matured theories specifically the theory of autoregression ar from time series analysis and moving averages ma and together called arma 68 21 13 72 84 80 are founded on the wold theorem 92 that we show are directly related to the rc concept the vector formulation of these 73 59 called vector autogregression var and vector moving averages vma are also connected by a corresponding wold theorem further we describe a relationship to the recently highly popular dynamic mode decomposition dmd 78 91 49 53 8 which is an empirical formulation of koopman spectral theory 8 3 11 so while we do not offer this simplified rc for performance over other approaches we hope that this work will serve to shed light on how the simplified rc approach is capable of providing useful time series forecasts and likewise as a suggestion as to how the general rc is successful there are related concepts concerning how an rnn is closely related to a narma model of a stochastic process nonlinear autoregressive moving average found in 23 while this paper is mostly motivated to describe connections between different approaches the machine learning rc approaches the econometrics time series var approach and also the dynamical systems operator theoretic dmd approach we show reasonable but not excellent forecasting ability of the linear rc with linear read out equivalent of a var however we do go on to connect a linear rc with quadratic read out as quadratic read out popular for reasons of matching signal parity so it is described 70 which we show explicitly can be written as a quadratic nvar that nonlinearities of the reservoir may be usefully moved to the output layer has been pointed out as a possibility and of practical use when building a photonic device implementation in 86 this paper is arranged as follows in sec 2 we describe the nature of the data as derived from a stochastic process in sec 3 we review the standard rc concept and we demonstrate it already with time series data from a mackey glass differential delay equation in sec 4 is the heart of this paper where we first present that a linear activiation function allows the rc to be stated as a linear recursion and therefore fitting just the read out weights can proceed in a manner such that despite random read in and inner weights there is a well posed problem then in this form we are able to directly relate the linear rc solution to the classical var k solution as such we are then able to enlist statistical time series forecasting theory for forecasting stochastic processes so that the wold theorem that guarantees a vma can then be translated to a var furthermore the associated companion form of a var 1 usefully states the full vectorized problem in sec 7 we note that the companion form of the var 1 is reminiscent of prior work for another famous concept in data driven dynamical systems which is the time delay formulation of a dmd koopman analysis in the examples sec 8 we present two classical examples the mackey glass differential delay equation and the lorenz 63 ordinary differential equation with examples comparing aspects of a full nonlinear rc and 3 the linear variant of an rc we will consider the issue of fading memory in sec 9 finally in sec 6 show that a linear rc but with hadamard quadratic readout is equivalent to a quadratic nvar of all monomial quadratic terms analogous to the earlier result of a var 2 the data as sampled from a stochastic process figure 1 time series acquired from the mackey glass differential delay equation eq 60 has become a standard example for time series forecasting for benchmarking data driven methods since it is dynamical rich and high dimensional and therefore challenging top time series index bottom three dimensional projection in delay coordinates x t x t x t 2 20 a sample of n 10 000 data points is chosen as the training data set for data driven forecasting problems we require data from a process including from a deterministic or otherwise from a stochastic dynamical system 12 a process stated xt t t 1 is in terms of a collection of random variables xt on a common probability space b p where is the sample space b the alebra and p a corresponding probability measure t is a time index set and commonly it is chosen as either r or z or subsets for sake of discussing finite samples of data we emphasize maps which may well be from discretely sampling a flow a data set from such a process samples xti of xti stated as a time sorted sample xi ni 1 t 1 t 2 tn using indexing notation xi xti uniform timing is also a simplifying assumption h ti 1 ti for all ti t assuming a vector real valued time series of dimension dx xi ni 1 r dx data derived from a flow say x f x 2 may be collected by stroboscopic map xi 1 ft xi x t x t t t f x s ds 3 suppressing the stroboscopic time t this is a discrete time map f and likewise other poincare maps may be useful for flight between surface of section and random dynamical systems may also be relevant 79 12 an underlying principle here is that the data should be long enough and likewise a general failing of any data driven machine learning method for forecasting a stochastic process will tend to do much better in terms of interpolation than extrapolation generalizing to allow for out of sample forecasts will tend to fare much better when the point to be forecasts is close to other observed inputs said another way the quality of results can be brittle depending as much upon curating a representative data set as the details of the 4 method used to avoid that struggle between fitting between observations and overfitting and too far out of sample as a matter of presenting examples we will highlight two classic problems that remain popular in benchmarking for machine learning in recent literature these will be the mackey glass differential delay equations eq 60 and the lorenz 63 system eq 61 both of which will be presented in fuller detail in sec 8 in fig 1 we show early in this presentation for sake of context a time series data set of the mackey glass system from eq 60 to stand in as a typical data set this problem is a useful benchmark and it is often used as such 61 36 2 66 9 27 93 perhaps because it is a well known chaotic process but also for sake of dimensional complexities that we recall in sec 8 1 figure 2 reservoir computing rc as defined eq 4 including a randomly selected dr dx read in matrix win from dx 1 states vector x a randomly selected dr dr inner layer recurrence matrix a for inner states dr 1 vector r and the dx dr trained read out matrix matrix wout 3 review of the traditional rc with nonlinear sigmoidal acti vation function in this section we review the standard and fully nonlinear rc method by which we mean including the use of a nonlinear activation function q s in this context q s is usually taken to be a sigmoidal function such as the hyperbolic tangent function however in the next section we will challenge these steps including simplifying to the identity function q s s assuming the training data xi ni 1 r dx the reservoir computing rnn is stated ri 1 1 ri q ari ui b yi 1 w outri 1 4 5 the hidden variable ri rdr is generally taken to be of a much higher dimension dr dx by a linear lifting transformation ui w inxi 5 and win is a randomly selected matrix dr dx of weights see fig 2 a is also a linear transformation as randomly chosen square matrix dr dr of weights that should be designed with certain properties such as spectral radius for convergence 19 45 or sparsity 56 70 89 or otherwise consideration of the echo state property 17 likewise the read out is by a linear transformation using a dx dr matrix of weights wout however wout and only wout is trained to the data allowing for forecasts yi given data xi rdx which is the major simplify aspect of rc since it can be done by a simple and cheap least squares computation finally q r r is an activation function using the phrasing from machine learning in the neural network community to mimic the concept of a biological network that fires when a voltage has reached a threshold popular choices include q s tanh s meaning a componentwise application of the scalar hyperbolic tangent function when s is multivariate other activations are popular in general neural network theory including other sigmoidal functions and also the relu function in certain contexts but not so commonly in rc 30 0 1 serves to slow down the rc to moderate stability of the fitting but we will restrict to 1 in this paper as outside the purpose of challenging the concept of explaining how the rc may work in a special case of identity q in which case nonzero can be considered as absorbed into the random a 1 r ar 1 i a r and since a is chosen randomly then 1 i a may be an alternative random selection finally b serves as an offset for activation that is useful in some contexts but it is also not relevant for our needs for the same reason we choose 1 and we choose b 0 what is remarkable about rc is that the usual hard work of optimally developing a full rnn is almost entirely skipped instead of learning win and a optimally fitted to the data these seemingly very important matrices are simply picked randomly this is an enormous savings over what would usually be inherently hard to handle since the parameters are composed within the nonlinear activation q and require at least a gradient descent optimization of back propagation in a high dimensional and likely multi peaked optimization space almost any matrix distribution may plausibly due but several different recipes are suggested we say recipe rather than algorithm since these are descriptions of successful observations in practice rather than a product of mathematical theory that is still not complete here we choose the entries of a uniformly ai j u with to scale the spectral radius but other choices are common notably for sparsity the read in matrix is also chosen uniformly randomly wini j u 0 with 0 chosen to scale the inner variables r the crucial aspect of the simplification that makes reservoir computing so easy and computationally efficient is that training to the output becomes just a linear process the cheap and simple least squares solution is easily handled directly by matrix computations let wout arg min v rdx dr x vr f arg min v rdx dr n i k xi vri 2 k 1 6 notation here is standard that f denotes the frobenius norm of the matrix which is the least squares equivalent of the least squares matrix parameter estimation problem the data xi ni 1 is stated as a dx n k array x xk 1 xk 2 xn vrk 1 vrk 2 vrn vr k 1 7 are the forecasts to x to be optimized in least squares by wout processed through the rc r rk 1 rk 2 rn k 1 8 while k 1 is allowable here for theoretical development in subsequent sections we allow for larger k 1 describing memory in practice a ridge regression tikhonov regularization with least squares regularity 30 70 31 7 is used to mitigate overfitting the solution of which may be written formally wout xrt rrt i 1 9 6 figure 3 standard nonlinear rc one time step forecasts from the mackey glass differential delay equation eq 60 using a training data set from n 10 000 samples as shown in fig 1 top time series data n 5 000 shown for clearer illustration middle reservoir trained across the data set and 500 samples are shown for clarity where we see the error is sufficiently small that the one time step forecasts and the true data are almost the same so that the plot is indistinguishable both shown but curves overlay regularity is chosen to be 1 e 6 bottom some randomly selected 7 of the usually hidden dr 500 activation functions illustrate the general appearance contrast to forecasting into the future as shown in fig 4 and linear method in fig 5 7 notation includes t is the matrix transpose i is the identity matrix and the choice of regularity parameter is 0 we will write a regularized pseudo inverse with the notation r r t rrt i 1 10 in appendix 13 we review the matrix theory as to how to form regularized pseudo inverses such as r by a regularized singular value decomposition svd in terms of regularized singular values such as i 2 i obtained from the singular values i from the svd of r in fig 3 we show an example of an rc machine obtained from data obtained from the mackey glass differential delay equations eq 60 we see fitting for n 10 000 data points x t dx 1 regularizing parameter 1 0 10 8 and fitting for constant time offset fit and true data are shown to be so close that in fact the blue fit curve hides the red true data curve also shown are several 7 of the dr 500 hidden variables r t the fit matrix a is randomly chosen with entries from a uniform distribution and then scaled so that the spectral radius a 1 the random random matrix win is also chosen uniformly scaled so that x values lead to r in 0 6 0 6 in fig 4 the trained rc are used to forecast into the future we see small errors grow in scale as illustrated by the bottom error curve results from an rc forecasting for the lorenz 63 system are presented in sec 8 2 and notably the forecasting quality degrades more quickly in part due to known large lyapunov instability of that system figure 4 standard nonlinear rc forecasts into the future from the mackey glass differential delay equation eq 60 using a training data set from n 10 000 samples as shown in figs 1 3 top time series data 0 t 500 zoom plotted for clearer illustration top forecasts into the future red diverge from true blue and bottom error is shown all forecasts shown will be closed loop once trained on training set style feeding rc output to input but comparing for error to true evolution as is standard such as 56 what is amazing is that despite that rc may seemingly be a gross oversimplification of the rnn concept it still seems to work quite well also from experience it is generally stable in that it is somewhat insensitive to the parameters and hyperparameters of the fitting process even if the level of quality does depend on these furthermore once it starts to make larger errors the kind of dynamics it produces are still plausible alternative wave forms of the process nonetheless there are some parameter choices to make notably 8 dr dx must be large enough but how big is not well understood furthermore the nature of the underlying distribution of matrices win and a is not fully understood we hope to contribute some general perspective as to why an rc may work at all our goal here is not specifically to improve performance and admitting that the corresponding var makes reasonable short term forecasts but perhaps no better than that as illustrated in our examples however we go on in sec 6 with details in appendix 14 to show that fitting a quadratic read out that is extending eq 8 to also include terms r r componentwise multiplication is called the hadamard product yields a quadratic nvar of all monomial quadratic terms which we observe performs quite well 4 rc with a fully linear activation q s s yields a var k now we attempt to challenge a central typical assumption of the rc method instead of choosing the activation function to be a sigmoid function instead we use the identity function q x x with this assumption we can show that the resulting linear rc machine is equivalent to a vector autoregressive process var 73 37 which is extremely popular and successful in the timeseries forecasting field particularly in econometrics 1 with this simplification we find that not only can the linear rc still make useful forecasts but we are able to connect the rc concept to this well established theory associated with var time series analysis notably the existence of representation wold theorem 92 68 however while this gives some explanation as to why a standard nonlinear rc may work despite the seemingly oversimplification of a full rnn we show that that the linear rc does still performs and furthermore now with theoretical underpinnings even if the full nonlinear rc may still perform better so it is for the theoretical connections that we make this simplification rather than a suggestion that it may be a new or simpler method before proceeding with a discussion of q s s notice that r is related to the scale of the read in matrix win proceed by initializing the process by eq 5 u 1 w inx 1 but also we choose r 1 0 11 consider that since win is randomly chosen and we choose uniformly win u 0 then the parameter 0 moderates the subsequent scale of terms ui and then ri see for example fig 3 where the native data x from the mackey glass system is translated to scaled internal variables recall the power series of the nonlinear activation function q s tanh s s s 3 3 s 5 5 12 clearly for s 1 then q s s even if chosen as a sigmoid and the choice of read in scale could be designed to put us in this regime as long as a is designed to keep us in this regime that is if we choose the scale of the read in matrix 0 1 giving small values of the matrix wouti j then at least for a stable rc such as when a has sufficiently small spectral radius then the arguments of s from ar u b in eq 4 remain small so in practice tanh s s stated roughly of is small then at least for some short time we might expect that the fully nonlinear rc is close to a fully linear rc to advance beyond that as an idea for now we believe the insights gained for a linear activation rc should be relevant to the general problem in the following we proceed to study the consequences of stating the activation exactly as the identity q s s 13 with this assumption the first several iterations follow from eq 4 and eq 11 as a forward propagation 9 for which we explicitly observe the following recursion r 2 ar 1 u 1 u 1 w inx 1 14 r 3 ar 2 u 2 awinx 1 w inx 2 15 r 4 ar 3 u 3 a ar 2 u 2 u 3 a 2 winx 1 aw inx 2 w inx 3 16 rk 1 ark uk a ark 1 uk 1 uk ak 1 winx 1 a k 2 winx 2 aw inxk 1 w inxk 17 k j 1 aj 1 uk j 1 k j 1 aj 1 winxk j 1 18 using notation a 0 i the identity matrix since the read out of this process is by eq 4 yi w outri then we may rewrite the final equation eq 18 by left multiplying by wout y 1 w outr 1 wout j 1 aj 1 winx j 1 wouta 1 winx 1 w outa 2 winx 2 w outawinx 1 w outwinx a x 1 a 1 x 2 a 2 x 1 a 1 x 19 with notation aj w outaj 1 win j 1 2 20 each of these coefficients aj are dx dx matrices this follows simply by eq 20 collecting products between dx dr to dr dr and then dr dx matrices and notation al li 1 a a a a l times if l 0 or the identity matrix when l 0 in some sense eq 20 and quadratic generalization eqs 52 53 are the heart of this paper as it is an explicit representation of the coefficient matrices of a var or nvar but as found in terms of projection onto iterations of the random matrices involved in developing a linear activation function version of an rc with exactly k var matrices aj the randomness of the d 2 r free parameters of the random matrix a collapses onto kd 2 x parameters meaning it yields only the finitely many fitted parameters of the matrices of a 1 ak however for a longer time observations which is the more usual way an rc is trained in practiced eq 20 implies that when condition dr kdx 21 then randomness of the choice of a and win is completely specified by stating matrices aj 1 win for many j we will expand upon this statement in the next section and then how it relates to vanishing memory in sec 9 by eq 19 a linear rc yields a classical var k a vector autoregression model of k delays that in a general form is 73 yk 1 c akx 1 ak 1 x 2 a 2 xk 1 a 1 xk k 1 22 in this writing c allows for a general offset term a dx 1 vector that here we do not pursue the k 1 is underlying noise of the stochastic process which is part of the stability theory we review in the next 10 section must be assumed to come from a covariance stationary process this relationship between an rc and a var k allows us to relate to the corresponding theoretical discussions of relevant alternative forms and stability and convergence from the stochastic process time series literature that we will also expand upon in the next section considering the complete data set of vector time series xi ni 1 yields yk 1 yk 2 yn a 1 a 2 ak xk xk 1 xn 1 xk 1 xk xn 2 x 1 x 2 xn k 23 restating this as a single linear equation y ax 24 again remembering that xi are dx 1 vectors and that ai are dx dx matrices a a 1 a 2 ak is a dx kdx matrix y yk 1 yk 2 yn is a dx n k matrix and x is a kdx n k matrix formally minimizing in least squares with regularization j a y ax f a f 25 with y being the target output of the right hand side of eq 23 by best fitted matrix a the solution of this regularized least squares problem may be written in its matrix form a yxt xxt i 1 yx 26 where the symbol refers to the penrose pseudo inverse with notation described in detail in eqs 68 69 when formulating the ridge tikhonov regularized pseudo inverse x 4 1 decomposing the var k solution explicitly relates to rc now we will further decompose the derived var k coefficients found in eq 26 to emphasize the training of just the output matrix wout of an associated rc in terms of randomly pre choosing a and w in referring to eqs 19 20 we can rewrite eqs 23 24 as y ax vax 27 with the matrix defined a win awin ak 2 win ak 1 win 28 this a is a combination of exponents of the random dr dr matrix a and the random dr dx matrix win and so it is itself a dr kdx random matrix interestingly considering just one column at a time of the winl l 1 2 dr a k 1 can be understood as a collection of columns from a krylov space and this entire process can be discussed as an arnoldi iteration which is something we will explore further in section 7 consider that the least squares objective eq 30 can be expanded to split a va 29 11 figure 5 the fully linear rc q s s forecasts from the mackey glass differential delay equation eq 60 using the same training data set from n 10 000 samples as shown in fig 1 contrasting to forecasts into the future as shown in figs 4 we see that clearly the nonlinear rc outperforms the linear rc and by a wide margin but that is not the message here rather which is one of explaining the relationships and fitting of the parameters and so that fitting just the read out matrix wout is relevant is established by eq 35 to emphasize that since if we pre choose a and win then only the read out matrix v is a free parameter j v y ax f y vax f 30 optimizing for v yields wout v y ax yx a 31 comparing this equation with eq 26 defining a we see xx formally appears in both expressions only the associative property of matrix multiplication is needed to emphasize the role of a more importantly this expression eq 31 for wout is written so as to emphasize that the reservoir computing process is designed with a and x combined through the iteration as ax is the data that results from eq 17 r 1 a 1 winx 1 a 2 winx 2 aw inx 1 w inx 32 this is written naturally r ax 33 by the simple way eq 31 uses a matrix identity of pseudo inverses 31 ax x a 34 associativity emphasizes that since a is deterministically defined once a and win are chosen and separately from the data x then the fitting of only the parameters of wout are sufficient if we want the var k parameters we could either ignore the prior knowledge of choice of a and win and compute a directly from eq 30 or from eq 31 defining wout v a a yx a 35 12 we summarize that these manipulations concluding with eq 35 serve directly as the connection between the rc fitted read out and the coefficient matrices of a var k the roles of pre choosing a and win relate directly to wout coefficients or indirectly to the fitted data considering the training of wout eqs 29 31 35 in terms of geometric description of least squares estimation 31 woutbest estimates orthogonal projections of rows of a into the row space of a which has dr row vectors of dimension kdx so no more than dr dimensions can remain free or as described similarly in inequality eq 21 concluding this section with the an example we simplify the nonlinear rc of the mackey glass data from figs 1 3 to a purely linear rc fit shown in fig 5 which clearly is not as well performing but it does still make some forecast into the future further discussion of this example and also likewise a lorenz 63 example in sec 8 said similarly a has no more than rank dr 5 var k theory suggests convergence with k since the var k model of vector autoregression appears naturally in our discussion from the simplified activation function q x x as summarized by eqs 19 and 22 we now recall some of the classical un derlying theory from the statistical time series analysis literature 92 73 that describes sufficient conditions under which we expect existence of a var k representation the wold theorem plays a central role in time series analysis as it describes existence of a vector moving average vma model representation which then under further assumptions for invertibility is equivalent to a var assumptions require a stationary process as a sum of two components 1 a stochastic component consisting of linear combinations of lags from a white noise process and 2 a deterministic component that is uncorrelated with the stochastic component first we recall definitions a d dimensional stochastic process t of zero mean e t 0 is derived from a white noise stochastic process written with zero mean t 1 t 2 t d t wn 0 if e t 0 and e t 1 t t 2 0 for t 1 6 t 2 but e t t t is symmetric positive semi definite a stochastic process is covariance stationary if all terms of the sequence have the same mean and any two terms depend only on their relative positions that is e t e t for all t and for all t 0 there exists t r such that cov t t t t for all t t meaning depending on t t rather than the t or t with these definitions we can state the central theorem of this section that we recall theorem 1 wold decomposititon theorem 92 73 a zero mean covariance stationary vector pro cess xt admits a representation xt c l t t 36 where c l i 0 cil i is a polynomial delay operator polynomial the ci are the moving average matrices and li t t i the term c l is the stochastic part of the decomposition the t term is the deterministic perfectly predictable part as a linear combination of the past values of xt furthermore t is a d dimensional linearly deterministic process t wn 0 is white noise coefficient matrices are square summable i 0 ci 2 37 c 0 i the identity matrix for each t t is called the innovation or the linear forecast errors clarifying notation of the delay operator polynomial with an example let c l 1 1 l 1 2 l 1 2 l 1 1 0 1 2 0 1 1 2 1 l c 0 c 1 l and ci 0 0 0 0 if i 1 38 13 so if for example xt r 2 c l xt 1 1 l 1 2 l 1 2 l x 1 t x 2 t x 1 t x 2 t x 2 t 1 1 2 x 1 t 1 1 2 x 2 t x 2 t 1 39 for interpretation and definition consider if t 0 then this is called a regular process and therefore there is a purely vector moving average vma representation if ci 0 for i p for some finite p 0 then it is called a vma p or otherwise it is a vma representation if xt is regular then the representation is unique now to our point to relate a wold vma representation to our discussion following the linear rc where we saw a var k results eqs 19 when the activation is linear q x x if the delay polynomial c l is invertible with c l 1 c l i and denote c l 1 b l b l b 0 b 1 l b 2 l 2 in terms of matrices bi then writing explicitly b 0 b 1 l b 2 l 2 i c 1 l c 2 l 2 i 40 existence of this inverse implies that the wold implied vma process has a representation xt c l t b l xt t 41 that is a var representation in that this represents the latest xt as a linear combination of prior values of x written succinctly in terms of the delay operator b l in practice when an infinite order vector moving average process vma corresponds to an infinite order vector autoregressive process var then recursion of expanding eq 40 and matching term by term yields b 0 i b 1 c 1 bk ck b 1 ck 1 bk 1 c 1 42 though a var representation may be found from a vma through several methods including a method of moments leading to the walker yule equations 71 or a least squares method in the case of finite presentations often for parsimonious efficiency of presentation a mixed form of a p step ar and a q step ma model might make a suitable approximation for what is called a arma p q model while not allowing ourselves to be drawn entirely into the detailed theory of econometrics and statistical time series analysis pursuing stronger necessary conditions we wish to point out some already apparent relevant points from the stated special sufficient conditions remark 1 summary statements if a vector stochastic process satisfies the hypothesis of a wold theorem then it can be written either as a vma or a var when eq 41 of c l is invertible eq 42 in practice a finite k var k estimates a var as k since the sequence of coefficients matrices ci are square summable eq 37 and considering eq 42 furthermore in practice a least squares estimate of a var k may be used for finite k which relates to an rc by the least squares fit eqs 31 35 finally we separate from the above technical points the following fundamental remark to distinguish existence versus uniqueness of a representation remark 2 while a stochastic process may have a vma representation and if through invertibility a cor responding var which is a linear descriptions of the process it may not taken to be the unique physical underlying description since nonlinear descriptions certainly may exist 14 remark 3 the processes that we may be interested in such as those derived from eq 1 may describe the evolution of a chaotic dynamical system and these may allow a representation eq 3 12 82 10 51 however in many of these natural examples the color or even the nature of the noise may well not be conforming to the white noise assumption of the wold theorem 1 certainly contrasting samples from an invariant measure from a chaotic dynamical system to a white noise process is a well studied 75 46 but still undecided topic while existence of the vma and corresponding var representation by referring to the wold theorem does depend on that hypothesis nonetheless successful constructive fitting of a var k by regression even if implicitly through an rc seems to proceed successfully in practice in a wide array of examples with this last remark we admit that while the details of the rigor guaranteeing existence may in practice break down due to inability to check all hypothesis as often such gaps occur between mathematics applied mathematics and practice as related to real world data we feel that the concept is still highly instructive as underlying explanation despite strong sufficient assumptions used to extend a rigorous theory we summarize this section that the relationship between the wold theorem for the var to our interest in an rc gives two conclusions 1 existence of the vma representation follows the wold which in turn leads to a var when the delay operator is invertible 2 that the coefficient matrices are square summable serves as an upper bound that memory must be fading we describe memory further in sec 9 5 1 stability of the var k and relationship to a var 1 to discuss stability we recall 73 the fact that a var k eqs 19 22 xk 1 c akx 1 ak 1 x 2 a 2 xk 1 a 1 xk k 1 43 can be stated as a var 1 in terms of stacked delayed variables called the companion system this idea is familiar in dynamical systems as we see it is related to stating time delay variables and the taken s embedding theorem 83 67 77 64 9 95 define xk 1 axk c ek where xk xk xk 1 x 1 ek k 0 0 c c 0 0 and a a 1 a 2 ak i 0 0 0 i 0 0 i 0 44 where since ai are each dx dx matrices then a is kdx kdx and xk is kdx 1 for discussion in the next section it will be convenient to consider for contrast to eq 54 a matrix of all the data x xk xk 1 xn k 1 and likewise let x xk 1 xk 2 xn k 45 are kdx n k 1 notice that the data in x also from eq 23 it follows that analysis of stability of a var 1 sufficiently describes the stability of a var k if there is even a small offset c whether by a bias or imperfection of fit then follows the recursion xk c axk 1 ek 1 xk i a al 1 c alxk l l 1 j 0 ajek l 46 this relates the var 1 back to a vma l form clearly even a small constant disturbance c is successively influenced by the delay matrix a in the limit l recall the geometric series of matrices i a 1 lim l i a al 1 47 15 converges if the spectral radius is strictly contained in the complex unit disc a max det a i 0 1 48 equivalently a general var k eq 43 is stable if and only if a characteristic polynomial det i a 1 z a 2 z 2 akzk 0 49 has all its roots outside the unit disc under this stability assumption we conclude that xk c axk 1 ek 1 i a 1 c j 0 ajek l 50 which relates a var 1 form to a wold form through a vma since by eq 20 each matrix aj woutaj 1 win then the magnitude of entries in the matrix a and the read in matrix win each moderate the magnitudes of entries of aj so considerations by the gershgorin disc theorem 31 relates these magnitudes to the magnitudes of z generally sparsity of a magnitude of the spectrum of a and magnitudes of win can be reduced for stability and to moderate the memory associated with converges with k and these magnitudes were already discussed for sake of a regime where the usual sigmoidal q would be close to the identity 6 quadratic nonlinear var in this section as is common practice 70 we investigate an rc that fits the readout wout using not just linear r values from the rc but also terms r r notation is the hadamard product meaning componentwise multiplication r r j r 2 j for each j this yields w out that is dx 2 dr here we state results briefly with detailed derivations given in appendix 14 generalizing the var results of sec 4 here we state that a linear var with a hadamard product quadratic read out is equivalent to a quadratic nonlinear var nvar with all quadratic terms in rirj that is analogously to the var stated eqs 19 22 a quadratic nonlinear var may be stated abbreviated restatement of eqs 80 82 derived in appendix 14 yk 1 akx 1 ak 1 x 2 a 2 xk 1 a 1 xk a 2 k k p 2 x 1 x 1 a 2 k 1 k p 2 x 2 x 1 a 2 1 1 p 2 xk xk 51 with notation for the k linear coefficient dx dx matrices aj w out 1 a j 1 win j 1 2 k 52 now we have k 2 quadratic term dx d 2 x coefficient matrices a 2 i j w out 2 p 2 a i 1 win aj 1 win i j 1 k 53 the notation p 2 v w v 1 w 1 v 1 w 2 vnwn t defines a n 2 vector of all quadratic terms stated between vectors v v 1 vn t and w w 1 wn t p 2 ai 1 win aj 1 win is a dx d 2 x coefficients matrix built from columnwise hadamard products both of these are expanded upon further in eq 72 75 and the form of a 2 i j eq 53 is derived in eq 82 we summarize in this brief section that the discussion of nonlinear quadratic var from a linear rc with hadamard quadratic read out is similar to that of linear var with linear rc and linear read out as discussed in sec 4 however eq 35 generalizes so that we may still write wout v a a yx a but now a a 1 a 2 in this statement a 1 is a renaming of what was a in eq 28 but now a 2 is the dr kd 2 x matrix defined in eq 78 in the appendix and likewise x x 1 x 2 defined in eq 79 16 while the linear reservoir linear readout var forecasting seems to only give reasonable results for short time forecasting fig 7 linear reservoir quadratic readout nvar seems to give better and longer range forecasting which also seem to remain true to the statistic of the chaotic attractor once errors have swamped the point forecasts see fig 8 7 is there a connection to dmd koopman to briefly answer the question titling this section the answer is yes there is a connection between var and dmd and so to rc the more nuanced answer is that the connection is not complete throughout the discussion so far a specialized version of an rc using an identity activation function yields a linear process that is shown to relate to a var that is also a linear process in this section we ask if it also relates to the dynamic mode decomposition dmd 78 76 49 91 a concept that is also premised on a linear process model as a finite estimation of the infinite dimensional linear action of the koopman operator on a function space of observables 3 in koopman theory instead of describing the evolution and geometry of orbits in the phase space the transfer operator methods generally describe evolution of functions whose domain is the phase space 51 12 recently this approach has excited a huge trend in applied dynamical systems with many excellent research papers 91 49 53 8 review papers 3 16 and books 49 toward theory numerical implementation and scientific application practice our focus here will remain narrow the goal being to simply identify a connection to the rc and its related var as discussed above a primary purpose of dmd methods are for modal analysis of the system to describe coherent and typical behaviors but it also can be used for forecasting and for this sake the analogy is drawn here for direct comparison first allow some minor manipulations to relate the var k eq 43 and eq 23 to a typical dmd form a time delay version of a linear evolution is a special case of an exact dmd written as follows with notation used as above xk 1 xk 2 xn xk xk 1 xn 1 x 2 x 3 xn k k xk xk 1 xn 1 xk 1 xk xn 2 x 1 x 2 xn k 1 54 or simply x kx 55 where x and x are the kdx n k 1 data matrices in eq 45 and k is a kdx kdx dmd matrix approximating the action of the infinite dimensional koopman operator abusing notation slightly the least squares problem k arg min k x kx f 56 has the solution k x x 57 which is called the exact dmd solution while there are many variants of dmd this one called exact dmd is popular for its simplicity of implementation while still useful for interpreting the system in terms of modal behaviors 17 contrasting k derived by exact dmd eq 54 versus a for the var 1 form described in eqs 44 45 reveals clear similarities since each states a linear relationship between the same data x kx versus x ax but these are ill posed equations and the a need not be the same as k closer inspection reveals that eq 57 allows freedom for best least squares fit considering the entire matrix k and so differences relative to eqs 23 26 whereas only the first k rows of a are free parameters in the regression the subsequent rows of a are sparsely patterned with either zero s or the identity matrix eq 44 a similar but not identical structural difference appears when contrasting the svd based exact dmd to the original dmd method of schmidt 78 and also rowley and mezic 76 which is an arnoldi like version of dmd in terms of iterations in a krylov space 4 85 reviewing that arnoldi version of dmd using the nota tion of 76 observations xk rd are assumed fitted to be from a linear process but also by considering the iterations are to be fitted in the krylov space assuming that xm krym x 0 span x 0 ax 0 am 1 x 0 for data k x 0 x 1 xm x 0 ax 0 am 1 x 0 stating the linear combination xm axm 1 c 0 x 0 cm 1 xm 1 kc where c c 0 c 1 cm 1 is the vector of coefficients then a key and clever observation was to rewrite this in terms of a companion matrix c 0 0 0 c 1 1 0 0 c 1 0 1 0 c 2 0 0 1 cm 1 58 so that results ak kc 59 from there exploiting the theme of arnoldi methods the eigenvalues of c are related as a subset of the eigenvalues of a and with a direct linear relationship between eigenvectors of c and a ritz vectors and the unknown coefficients c of c can be computed by a least squares procedure keeping in mind that power iterations as one does in krylov spaces emphasize just the dominant direction the arnoldi methods take care to orthogonalize at each step in the algorithm for stabilization an otherwise unstable search for large sparse matrices and these make deliberate use of qr decompositions our interest here is only to point out analogies between a from reservoir computing and var 1 and k rather than to continue toward discussion of modal analysis as one does in dmd analysis summarizing the analogy we see that the companion matrix c in eq 58 reminds us of the companion matrix a in eq 44 however the most significant difference is that while ci are scalars that ai are dx dx matrices 8 examples the example figs 1 3 5 already threaded in the above presentation of methods were in terms of the mackey glass differential delay equation system which we now recall then in the subsequent we will show similar figures highlighting the concepts in a different system the famous lorenz 63 odes 8 1 example 1 mackey glass differential delay equation the mackey glass differential delay equation 61 x t ax t td 1 x t td c bx t 60 has become a now classic standard example in time series analysis 26 54 of a high infinite dimensional dynamical system with a low dimensional attractor which we have used as a benchmark in our own previous work for machine learning 9 dynamical systems the problem is physiologically relevant for describing dynamic diseases a differential delay equations can be described as infinite dimensional dynamical systems 18 a concept that is more easily understandable in terms of the notion that an initial condition state advances not just a finite dimensional vector but rather an entire interval t 0 t 0 td of initial values of x t are required however the mg equations have a nice property for the its practical use as a benchmark problem which is that there is essentially an attractor whose fractal dimension varies with respect to the parameters chosen allowing for a complexity tunable test problem we have chosen parameters td 17 a 0 2 b 0 1 c 10 0 for which if pursing time delay embedding gives an embedding dimension of d 4 we use integration steps of t td 100 throughout we show time series in fig 1 a standard nonlinear rc forecast of the system in fig 3 and the linear rc var forecast of the system in figs 4 5 8 2 example 2 lorenz 63 the lorenz 63 system 55 is the three coupled ordinary differential equations x 10 y x y x 28 z y z xy 8 3 z 61 while these lorenz equations may have been originally posed as time varying fourier coefficients for de scribing a partial differential equation system describing convection rolls of heated fluid in an atmospheric system they have become been a popular paradigm in the study of chaotic systems for foundation principles of chaos historically and ongoing as a simple and familiar benchmark problem and also in the pedagogy of dynamical systems the chaotic attractor in the phase space x t y t z t illustrates a familiar butterfly but we show a segment of the x t time series that will be used as our data set in figs 6 8 also shown are nonlinear rc q x tanh x activation forecasts in fig 6 using the usual nonlinear reservoir computing with excellent success in fig 7 we show forecasting results using a linear q x x activation rc with still good results for short term forecasting agreeable with the expectation with var theory consider the following summary of results of three experiments shown in figs 6 8 1 fig 6 shows a standard fully nonlinear rc with activation q x tanh x and hadamard quadratic read out wout of the reservoir variable r r r forecasting is excellent and interestingly apparently even once errors have accumulated the rc continues to produce a lorenz like attractor 2 fig 7 shows a fully linear rc with linear activation q x x and linear readout wout of the reservoir variable r is equivalent to a var and it produces good short term forecasts but for shorter time than the fully nonlinear rc also once errors accumulate the long term forecasts do not produce a lorenz like attractor but rather seem to converge to zero 3 figs 8 shows a linear rc so q x x but with a hadamard quadratic read out wout of r r r forecast performance is good and the attractor seems to be well reproduced even once error has grown comparably to the fully nonlinear case of fig 6 9 on fading memory there are several ways to consider memory in the system memory as it turns out is an important property for an echo state machine since it was recently shown by gonan and ortega 33 that these are universal the var representation shown here allow for own discussion of this property 9 1 fading memory in terms of the role of the internal layers one way is to think of the connection between the reservoir and the coefficient matrices aj may become increasingly small in a way that is moderated in part by the randomly chosen a yields a fading bound on 19 0 1 2 3 4 5 6 7 8 9 10 t 20 0 20 x t full sim blue is true 0 1 2 3 4 5 6 7 8 9 10 t 20 0 20 y t 0 1 2 3 4 5 6 7 8 9 10 t 0 50 z t 0 1 2 3 4 5 6 7 8 9 10 t 0 10 20 e rr o r t figure 6 lorenz time series with nonlinear rc so with q x tanh x of size dr 1000 and quadratic read out wout fitted to input r r 1 r 2 from eq 71 the top three time series are state variables and blue curves show forecasts where red shows true data bottom error shown growing from initial seed right the phase space presentation of the forecast variables x t y t z t error performance is excellent and even once error has grown the produced attractor seems true to the forecast lorenz compare to figs 7 8 20 0 1 2 3 4 5 6 7 8 9 10 t 20 0 20 x t full sim blue is true 0 1 2 3 4 5 6 7 8 9 10 t 20 10 0 10 y t 0 1 2 3 4 5 6 7 8 9 10 t 20 40 z t 0 1 2 3 4 5 6 7 8 9 10 t 0 10 e rr o r t figure 7 lorenz time series with fully linear rc so q x x of size dr 1000 and linear read out left the top three time series are state variables and blue curves show forecasts where red shows true data bottom error shown growing from initial seed this fully linear rc being equivalent to a var does produce good forecasts for a finite time even if for a shorter time than in the nonlinear methods of figs 6 8 right the phase space presentation of the forecast variables x t y t z t now unlike the nonlinear rc or the nonlinear read out cases despite forecasts for a little while once errors have occurred in forecasting the time series of the fully linear rc the form of the attractor is entirely wrong 21 0 1 2 3 4 5 6 7 8 9 10 t 20 0 20 x t full sim blue is true 0 1 2 3 4 5 6 7 8 9 10 t 20 0 20 y t 0 1 2 3 4 5 6 7 8 9 10 t 0 50 z t 0 1 2 3 4 5 6 7 8 9 10 t 0 20 e rr o r t figure 8 lorenz time series with linear rc so q x x of size dr 1000 but quadratic read out w out fitted to input r r 1 r 2 from eq 71 left the top three time series are state variables and blue curves show forecasts where red shows true data bottom error shown growing from initial seed right the phase space presentation of the forecast variables x t y t z t as in full nonlinear rc method of fig 6 the attractor seems true even once errors have grown compare to figs 6 7 22 the true memory from eq 20 aj w outaj 1 win follows a bound in norm aj woutaj 1 win wout aj 1 win wout a j 1 w in 62 denotes an induced matrix norm inherited by the corresponding vector norm that is allowing a possibly not square matrix b mapping between vector spaces v 1 v 2 by b v 1 v 2 then b v 1 v 2 sup x v 1 1 bx v 2 and vi describes the vector norm in vi for example typical favorite vector norms include the euclidean norm 2 the 1 norm 1 and the norm the specific vector norm is not important thus the noncommital notation for good reason to be described in a moment for simplicity of notation in eq 82 we omit emphasis of domain v 1 and range v 2 vector spaces of each matrix operator understanding that dimensionality of the induced vector norms depends on the matrix sizes and ranks inspecting the term a j 1 in eq 62 we can bound by its eigenvalues for any chosen 0 31 there exists some induced matrix norm which is why we used the noncommittal notation such that a a where a max i i is the spectral radius defined as the largest magnitude of eigenvalues noting that a is square by theorem 31 if a 1 then there exists an induced norm such that in terms of that norm a 1 which implies an a n 0 as n therefore by equivalence of norms an 0 this last statement is stronger than a convergence in norm but rather it is is a componentwise statement of convergence of the matrix to the zero matrix with respect to exponentiation under the condition of spectral radius bounded by 1 a result regarding the var matrix mentioned in eqs 20 62 that as long as the read out matrix is bounded wout c and a 1 then coefficient matrices become increasingly close to the zero matrix aj 0 as j 63 stated strongly as componentwise convergence of the matrices however this is not to say that convergence need to be monotone or that the bound must sharp what is interesting in this statement is that the distribution from which the randomly chosen matrix a is drawn controls the spectral radius a which in turn dominates the var matrices aj this is agreeable with the general var theory reviewed in sec 5 that a general var is well approximated by a finite var k 9 2 experiments on fading memory of the rc consider the actual computed aj matrices and their corresponding induced norms aj 2 as computed from the example of chaotic systems studied in the previous section which were the mackey glass system and the lorenz system in fig 9 top there is shown a k ak 2 curve as computed from eq 62 in the case of a rc model derived from the mackey glass equation corresponding to rc forecasts of the same system in fig 1 we see a pronounced fading memory as the ak 2 have diminished to negligable values by k 6 this is not unexpected since the attractor of these equations shown in fig 1 illustrates primarily rotation and otherwise weak chaos in the sense that while the embedding dimension may be d 4 the lyapunov exponent is relatively small even if positive max 0 0058 90 considering the memory curve k ak 2 of the lorenz system in fig 9 bottom we see that initially ak 2 decreases quickly to small values by k 9 but then the computed values fail to decrease further for larger k we attribute this to difficulties of long range forecasting of this highly chaotic system another way of realizing this point is that the details of the tail of the large k ak values vary with different data samples even though the front part of the series small k are stable across samples interestingly nonetheless as an affirmation of the theoretical discussion the green curve derived from q 62 involving the rc and the blue curve derived as a var fit eq 26 almost entirely coincide the difference being likely due to numerical estimation issues for very large k 23 1 2 3 4 5 6 7 8 9 10 k 0 2 4 6 8 10 12 14 16 18 20 a k 5 10 15 20 25 k 0 0 5 1 1 5 2 2 5 3 3 5 a k figure 9 fading memory as observed by magnitude of var k matrices ak as k vs ak 2 induced norm of top mackey glass system the var 6 model is descriptive since a 6 2 is already relatively small bottom lorenz system where we see that after k 9 coefficient matrices a 9 2 are relatively small suggesting the var 9 description is close to the large k model however we do not see convergence to zero suggested by eq 62 which we attribute to numerical instabilities since the details of the computed aj seem to vary with details of the orbit data whereas the head the early terms remain stable shown in green curve are the ak computed by eq 62 involving the rc but the blue curve directly as a var fit eq 26 and we see that these closely agree as the two curves are almost coincident 24 10 conclusion the success of machine learning and artificial neural networks has lead to a clear and overwhelming widely adopted wave across so many areas where data is relevant and patterns are of interest dynamical systems is no exception and forecasting a dynamical system is a specific application that is broadly relevant and of interest to us here the rnn framework is particularly relevant for a dynamical systems since the reserve memory aspect of the concept allows for a good framework and some aspects of delay embedding however while the rnn tends to have many many parameters to fit with the danger of overfitting always present and in any case the large cost of optimization in the training phase there is a surprising short cut the echo state reservoir computing concepts presume to choose the weights for input layer and internal layer entirely randomly then only the output layer is trained furthermore that output layer training will be by linear algebraic manipulations toward a least squares solution rather than the usual nonlinear optimization necessary for the many parameters of the full nonlinear rnn that this would allow a huge computational savings is clear what is perhaps a surprise is how this gross simplification still yields useful results while there have been a number of studies experimentally describing how to choose better random processes to define the random parameters e g such as to emphasize sparsity or to control the spectral radius and other properties in this work we have taken a different approach which is not specifically to improve the performance of the concept but instead to give a partial explanation as to how the concept can work at all after all at first glance it may seem that it would be impossible that such a simplification could work in this work we have simplified the rc concept allowing for the activation function to be an identity function instead of the more typical sigmoidal function in this case the process is entirely linear and so easier to study as it turns out the rc still performs reasonably well for short term forecasts and it is certainly leads to easier to analysis herein we prove that the linear rc is in factly directly related to the more matured topic of var vector autoregressive time series forecasting and with all the related theory including the wold theorem as a representation theorem which therefore now applies to the rc also we are able to make a direct connection to the increasingly popular dmd theory further the commonly used fitting upon readout of linear and hadamard quadratic observations of the reservoir states yields a nonlinear var nvar allowing for all quadratic monomial terms generalization of the var result this nvar version apparently not only makes competitive forecasts but also errors seem to respect the original attractor in the case of the lorenz system 11 data availability statement the data that support the findings of this study are available from the corresponding author upon reasonable request 12 acknowledgments the author received funding from the army research office n 68164 eg and also darpa i would also like to sincerely thank daniel j gauthier aaron griffith and wendson a s barbosa of ohio state university for their generous and helpful feedback concerning this work 13 appendix review of regularized pseudo inverse we review how to numerically and stably compute the pseudo inverse by the singular value decomposition with regularized singular values svd reviewing the matrix theory of regularized pseudo inverses for general matrices if xb z 64 xn p bp 1 zn 1 then if the svd is x u v with orthogonal matrices n n u satisfies uut utu i and p p v satisfies v v t v tv i and is n p diagonal matrix of singular values 25 1 2 r 0 p 0 1 p if n p 1 p if n p 1 p if n p 65 then x xtx 1 xt v ut where 1 1 1 p in the n p case 66 the least squares estimator of xb z is b xtx 1 xt z x z 67 and we write the ridge regression tikhonov regularized solution b x tx i 1 xtz v t i 1 tut z x z 68 the regularized pseudo inverse x is better stated in terms of the regularized singular values by t i 1 t 1 21 p 2 p in the n p case 69 and then b x z v u t z 70 throughout since we will always refer to regularized pseudo inverses we will not emphasize this by abusing notation allowing that b denotes b even if only a very small 0 is chosen unless otherwise stated 1 0 10 8 this mitigates the tendency of overfitting or likewise stated in terms of zero or almost zero singular values that would otherwise appear in the denominators of the theory is similar for n p and n p as well as the scenario where z is not just a vector but a matrix and likewise as in eq 9 where we refer to the transpose scenario 14 appendix on quadratic nvar connection to rc in this appendix we give details claimed in sec 6 that a linear rc with the hadamard quadratic nonlinear read out also corresponds to a var like entity from stochastic process modeling of time series now however a quadratic type nonlinear var results an nvar this is a generalization of the linear var discussion of sec 4 a commonly used scenario of rc 56 is to fit w out not just to r data but also to r r where denotes the hadamard product implemented in array languages such as matlab by array arithmetic using the notation with the dot in place of what would otherwise be for standard matrix multiplication for a vector r r 1 r 2 rdr t this is defined as component wise operations r r r 21 r 22 r 2 dr t the reason for using nonlinear terms is cited as improved performance allowing for matching the parity of the process in this case we rename what before we called r to now be called r 1 so eq 8 is replaced with r 1 rk rk 1 rn r 2 rk rk rk 1 rk 1 rn rn r r 1 r 2 71 26 then eq 9 remains written as before wout xrt rrt i 1 but now since r is 2 dr n k then wout is dx 2 dr for convenience of the rest of this section partition these matrices wout into top and bottom half portions these we show act on linear and quadratic terms of the corresponding nvar wout wout 1 wout 2 72 each of size dx dr first note an identity of how the hadamard product distributes with standard matrix vector multipli cation let w w 1 w 2 wn t a vector with scalar vector components wi and b a general m n matrix let b b 1 b 2 bn written in terms of the column vectors bj of b then bw bw w 1 b 1 w 2 b 2 wnbn w 1 b 1 w 2 b 2 wnbn b 1 b 1 b 1 b 2 bn bn w 21 w 1 w 2 w 1 wn w 2 w 1 w 22 w 2 n p 2 b b p 2 w w 73 thus the hadamard operator distributes through matrix multiplication to be written purely as matrix multiplication with carefully stated matrices we have defined the matrix of hadamard products as p 2 b b which is a m n 2 matrix by the matrix function defined in eq 73 p 2 rm n rm n rm n 2 74 and vector function also in eq 73 p 2 v w rn rn rn 2 v w 7 v 1 w 1 v 1 w 2 v 1 wn v 2 w 1 v 2 w 2 vnwn t 75 to be the n 2 1 vector of all quadratic combinations suggested in the equation above p 2 rn rn rn 2 by this notation we will state for convenience identity operators p 1 b b and p 1 w w higher order operators follow similarly but we will not need these here with this notation we can proceed comparably to eqs 14 18 by tracking iterations of the rc but with quadratic read out and with the terms in the product for use in building wout to be used in the 27 read out let r 1 0 then r 2 r 2 winx 1 winx 1 p 2 w in win p 2 x 1 r 3 r 3 awinx 1 winx 2 awinx 1 winx 2 awinx 1 awinx 1 awinx 1 winx 2 winx 2 awinx 1 winx 2 winx 2 p 2 aw in awin p 2 x 1 x 1 p 2 aw in win p 2 x 1 x 2 p 2 w in awin p 2 x 2 x 1 p 2 w in win p 2 x 2 x 2 rk 1 rk 1 k i 1 ai 1 winxk 1 i k j 1 aj 1 winxk 1 j k i j 1 p 2 a i 1 win aj 1 win p 2 xk 1 i xk 1 j 76 a 2 x 2 k 77 that is a defined in eq 28 is dr kdx and analogously a 2 p 2 win win p 2 awin win p 2 a 2 win win p 2 ak 1 win win p 2 win awin p 2 awin awin p 2 a 2 win awin p 2 ak 2 win ak 1 win p 2 ak 1 win ak 1 win 78 is a dr kd 2 x matrix similarly where x is a kdx n k matrix of data x 2 is a kdx 2 n k matrix of data but quadratic forms analogous to the kdx n k array x from eqs 23 24 and x 2 k is the kth column x 1 xk xk 1 xn 1 xk 1 xk xn 2 x 1 x 2 xn k x 2 p 2 xk xk p 2 xk 1 xk 1 p 2 xn 1 xn 1 p 2 xk 1 xk p 2 xk xk 1 p 2 xn 2 xn 1 p 2 x 1 xk p 2 x 2 xk 1 p 2 xn k 1 xn 1 p 2 xk xk 1 p 2 xk 1 xk p 2 xn 1 xn 2 p 2 xk 1 xk 1 p 2 xk 1 xk 1 p 2 xn 2 xn 2 p 2 x 1 x 1 p 2 x 2 x 2 p 2 xn k xn k 79 28 now we write x x 1 x 2 with eq 9 and eq 71 in mind and with r 2 the 2 nd column of r being r 2 rk 1 rk 1 rk 1 we generalize the var stated in eq 19 the quadratic nvar follows y 1 w out r 2 wout 1 j 1 aj 1 winx j 1 w out 2 i j 1 p 2 a i 1 win aj 1 win p 2 x 1 i x 1 j wout 1 a 1 winx 1 w out 1 a 2 winx 2 w out 1 aw in 1 x 1 w out 1 w inx i j 1 wout 2 p 2 a i 1 win aj 1 win p 2 x 1 i x 1 j a x 1 a 1 x 2 a 2 x 1 a 1 x a 2 p 2 x 1 x 1 a 2 1 p 2 x 2 x 1 a 2 1 1 p 2 x x 80 with notation for the linear coefficient dx dx matrices as before aj w out 1 a j 1 win j 1 2 81 and now we have 2 quadratic term coefficient dx d 2 x matrices a 2 i j w out 2 p 2 a i 1 win aj 1 win i j 1 82 this is the generalization of the var equation coefficients written explicitly in eq 20 to these coefficient matrices of a quadratic nvar that results a linear rc with hadamard quadratic readout references 1 gianni amisano and carlo giannini topics in structural var econometrics springer science business media 2012 2 piotr antonik marvyn gulina jae l pauwels and serge massar using a reservoir computer to learn chaotic attractors with applications to chaos synchronization and cryptography physical review e 98 1 012215 2018 3 hassan arbabi and igor mezic ergodic theory dynamic mode decomposition and computation of spectral properties of the koopman operator siam journal on applied dynamical systems 16 4 2096 2126 2017 4 walter edwin arnoldi the principle of minimized iterations in the solution of the matrix eigenvalue problem quarterly of applied mathematics 9 1 17 29 1951 5 coryn al bailer jones david jc mackay and philip j withers a recurrent neural network for modelling dynamical systems network computation in neural systems 9 4 531 547 1998 6 thanasis g barbounis john b theocharis minas c alexiadis and petros s dokopoulos long term wind speed and power forecasting using local recurrent neural network models ieee transactions on energy conversion 21 1 273 284 2006 7 erik bollt regularized kernel machine learning for data driven forecasting of chaos 29 8 erik bollt geometric considerations of a good dictionary for koopman analysis of dynamical systems arxiv preprint arxiv 1912 09570 2019 9 erik m bollt model selection confidence and scaling in predicting chaotic time series international journal of bifurcation and chaos 10 06 1407 1422 2000 10 erik m bollt lora billings and ira b schwartz a manifold independent approach to understanding transport in stochastic dynamical systems physica d nonlinear phenomena 173 3 4 153 177 2002 11 erik m bollt qianxiao li felix dietrich and ioannis kevrekidis on matching and even rectifying dynamical systems through koopman operator eigenfunctions siam journal on applied dynamical systems 17 2 1925 1960 2018 12 erik m bollt and naratip santitissadeekorn applied and computational measurable dynamics siam 2013 13 george ep box gwilym m jenkins and g reinsel time series analysis forecasting and control holden day san francisco boxtime series analysis forecasting and control holden day 1970 1970 14 stephen boyd and leon chua fading memory and the problem of approximating nonlinear operators with volterra series ieee transactions on circuits and systems 32 11 1150 1161 1985 15 steven l brunton and j nathan kutz data driven science and engineering machine learning dynam ical systems and control cambridge university press 2019 16 marko budi sic ryan mohr and igor mezic applied koopmanism chaos an interdisciplinary journal of nonlinear science 22 4 047510 2012 17 michael buehner and peter young a tighter bound for the echo state property ieee transactions on neural networks 17 3 820 824 2006 18 daniel canaday aaron griffith and daniel j gauthier rapid time series prediction with a hardware based reservoir computer chaos an interdisciplinary journal of nonlinear science 28 12 123119 2018 19 thomas l carroll and louis m pecora network structure effects in reservoir computers chaos an interdisciplinary journal of nonlinear science 29 8 083130 2019 20 ashesh chattopadhyay pedram hassanzadeh devika subramanian and krishna palem data driven prediction of a multi scale lorenz 96 chaotic system using a hierarchy of deep learning methods reservoir computing ann and rnn lstm 2019 21 jiann fuh chen wei ming wang and chao ming huang analysis of an adaptive time series autore gressive moving average arma model for short term load forecasting electric power systems research 34 3 187 196 1995 22 edward choi andy schuetz walter f stewart and jimeng sun using recurrent neural network models for early detection of heart failure onset journal of the american medical informatics association 24 2 361 370 2017 23 jerome connor les e atlas and douglas r martin recurrent networks and narma modeling in advances in neural information processing systems pages 301 308 1992 24 david darmon christopher j cellucci and paul e rapp information dynamics with confidence using reservoir computing to construct confidence intervals for information dynamic measures chaos an interdisciplinary journal of nonlinear science 29 8 083113 2019 25 philippe de wilde neural network models an analysis springer 1996 30 26 j doyne farmer chaotic attractors of an infinite dimensional dynamical system physica d nonlinear phenomena 4 3 366 393 1982 27 mohammad farzad hanif tahersima and hamid khaloozadeh predicting the mackey glass chaotic time series using genetic algorithm in 2006 sice icase international joint conference pages 5460 5463 ieee 2006 28 ken ichi funahashi and yuichi nakamura approximation of dynamical systems by continuous time recurrent neural networks neural networks 6 6 801 806 1993 29 claudio gallicchio chasing the echo state property arxiv preprint arxiv 1811 10892 2018 30 daniel j gauthier reservoir computing harnessing a universal dynamical system phys rev lett 120 2018 024102 2018 31 gene h golub and charles f van loan matrix computations 4 th johns hopkins 2013 32 lukas gonon and juan pablo ortega reservoir computing universality with stochastic inputs ieee transactions on neural networks and learning systems 31 1 100 112 2019 33 lukas gonon and juan pablo ortega fading memory echo state networks are universal arxiv preprint arxiv 2010 12047 2020 34 aaron griffith andrew pomerance and daniel j gauthier forecasting chaotic systems with very low connectivity reservoir computers chaos an interdisciplinary journal of nonlinear science 29 12 123108 2019 35 beata j grzyb eris chinellato grzegorz m wojcik and wieslaw a kaminski which model to use for the liquid state machine in 2009 international joint conference on neural networks pages 1018 1024 ieee 2009 36 min han zhi wei shi and wei guo reservoir neural state reconstruction and chaotic time series prediction acta physica sinica 56 1 43 50 2007 37 l harrison william d penny and karl friston multivariate autoregressive modeling of fmri time series neuroimage 19 4 1477 1491 2003 38 allen hart james hook and jonathan dawes embedding and approximation theorems for echo state networks neural networks 2020 39 allen g hart james l hook and jonathan hp dawes echo state networks trained by tikhonov least squares are l 2 mu approximators of ergodic dynamical systems arxiv preprint arxiv 2005 06967 2020 40 david hartman and lalit k mestha a deep learning framework for model reduction of dynamical systems in 2017 ieee conference on control technology and applications ccta pages 1917 1922 ieee 2017 41 sepp hochreiter and ju rgen schmidhuber long short term memory neural computation 9 8 1735 1780 1997 42 jin quan huang and frank l lewis neural network predictive control for nonlinear dynamic systems with time delay ieee transactions on neural networks 14 2 377 389 2003 43 herbert jaeger the o echo stateo approach to analysing and training recurrent neural networks with an erratum note bonn germany german national research center for information technology gmd technical report 148 34 13 2001 31 44 herbert jaeger and harald haas harnessing nonlinearity predicting chaotic systems and saving energy in wireless communication science 304 5667 78 80 2004 45 junjie jiang and ying cheng lai model free prediction of spatiotemporal dynamical systems with recurrent neural networks role of network spectral radius physical review research 1 3 033056 2019 46 matthew b kennel and steven isabelle method to distinguish possible chaos from colored noise and to determine embedding parameters physical review a 46 6 3111 1992 47 masahiro kimura and ryohei nakano learning dynamical systems by recurrent neural networks from orbits neural networks 11 9 1589 1599 1998 48 s narendra kumpati parthasarathy kannan et al identification and control of dynamical systems using neural networks ieee transactions on neural networks 1 1 4 27 1990 49 j nathan kutz steven l brunton bingni w brunton and joshua l proctor dynamic mode decom position data driven modeling of complex systems siam 2016 50 martin la ngkvist lars karlsson and amy loutfi a review of unsupervised feature learning and deep learning for time series modeling pattern recognition letters 42 11 24 2014 51 andrzej lasota and michael c mackey chaos fractals and noise stochastic aspects of dynamics volume 97 springer science business media 2013 52 daniel s levine introduction to neural and cognitive modeling routledge 2018 53 qianxiao li felix dietrich erik m bollt and ioannis g kevrekidis extended dynamic mode de composition with dictionary learning a data driven adaptive spectral decomposition of the koopman operator chaos an interdisciplinary journal of nonlinear science 27 10 103111 2017 54 a lichtenberg j and lieberman ma 1983 regular and stochastic motion applied mathematical sci ences 38 85 55 edward n lorenz deterministic nonperiodic flow journal of the atmospheric sciences 20 2 130 141 1963 56 zhixin lu brian r hunt and edward ott attractor reconstruction by machine learning chaos an interdisciplinary journal of nonlinear science 28 6 061104 2018 57 mantas lukos evic ius a practical guide to applying echo state networks in neural networks tricks of the trade pages 659 686 springer 2012 58 mantas lukos evic ius and herbert jaeger reservoir computing approaches to recurrent neural network training computer science review 3 3 127 149 2009 59 helmut lu tkepohl new introduction to multiple time series analysis springer science business media 2005 60 wolfgang maass thomas natschla ger and henry markram real time computing without stable states a new framework for neural computation based on perturbations neural computation 14 11 2531 2560 2002 61 michael c mackey and leon glass oscillation and chaos in physiological control systems science 197 4300 287 289 1977 62 stephen marsland machine learning an algorithmic perspective crc press 2015 32 63 donald michie david j spiegelhalter cc taylor et al machine learning neural and statistical classification 13 1994 1 298 1994 64 mark r muldoon david s broomhead jeremy p huke and rainer hegger delay embedding in the presence of dynamical noise dynamics and stability of systems 13 2 175 186 1998 65 marilyn m nelson and william t illingworth a practical guide to neural nets 1991 66 silvia ort n gonza lez miguel c soriano luis pesquera gonza lez daniel brunner daniel san mart n segura ingo fischer claudio mirasso jose manuel gutie rrez llorente et al a unified frame work for reservoir computing and extreme learning machines based on a single time delayed neuron 2015 67 norman h packard james p crutchfield j doyne farmer and robert s shaw geometry from a time series physical review letters 45 9 712 1980 68 sudhakar madhavrao pandit shien ming wu et al time series and system analysis with applications volume 3 wiley new york 1983 69 razvan pascanu tomas mikolov and yoshua bengio on the difficulty of training recurrent neural networks in international conference on machine learning pages 1310 1318 2013 70 jaideep pathak brian hunt michelle girvan zhixin lu and edward ott model free prediction of large spatiotemporally chaotic systems from data a reservoir computing approach physical review letters 120 2 024102 2018 71 jostein paulsen and dag tj stheim on the estimation of residual variance and order in autoregressive time series journal of the royal statistical society series b methodological 47 2 216 228 1985 72 daniel pena george c tiao and ruey s tsay a course in time series analysis volume 322 john wiley sons 2011 73 duo qin rise of var modelling approach journal of economic surveys 25 1 156 174 2011 74 akhter mohiuddin rather arun agarwal and vn sastry recurrent neural network and a hybrid model for prediction of stock returns expert systems with applications 42 6 3234 3241 2015 75 oa rosso ha larrondo mt martin a plastino and ma fuentes distinguishing noise from chaos physical review letters 99 15 154102 2007 76 clarence w rowley igor mezi shervin bagheri philipp schlatter dans henningson et al spectral analysis of nonlinear flows journal of fluid mechanics 641 1 115 127 2009 77 tim sauer james a yorke and martin casdagli embedology journal of statistical physics 65 3 4 579 616 1991 78 peter j schmid dynamic mode decomposition of numerical and experimental data journal of fluid mechanics 656 5 28 2010 79 alwyn scott encyclopedia of nonlinear science routledge 2006 80 c serio autoregressive representation of time series as a tool to diagnose the presence of chaos epl europhysics letters 27 2 103 1994 81 qingsong song and zuren feng effects of connectivity structure of complex echo state network on its prediction performance for nonlinear time series neurocomputing 73 10 12 2177 2185 2010 82 jie sun dane taylor and erik m bollt causal network inference by optimal causation entropy siam journal on applied dynamical systems 14 1 73 106 2015 33 83 floris takens detecting strange attractors in turbulence in dynamical systems and turbulence war wick 1980 pages 366 381 springer 1981 84 george c tiao and ruey s tsay consistency properties of least squares estimates of autoregressive parameters in arma models the annals of statistics pages 856 871 1983 85 henk a van der vorst iterative krylov methods for large linear systems volume 13 cambridge university press 2003 86 kristof vandoorne pauline mechet thomas van vaerenbergh martin fiers geert morthier david verstraeten benjamin schrauwen joni dambre and peter bienstman experimental demonstration of reservoir computing on a silicon photonics chip nature communications 5 1 1 6 2014 87 david verstraeten benjamin schrauwen michiel do haene and dirk stroobandt an experimental unification of reservoir computing methods neural networks 20 3 391 403 2007 88 pantelis r vlachas wonmin byeon zhong y wan themistoklis p sapsis and petros koumout sakos data driven forecasting of high dimensional chaotic systems with long short term memory networks proceedings of the royal society a mathematical physical and engineering sciences 474 2213 20170844 2018 89 pantelis r vlachas jaideep pathak brian r hunt themistoklis p sapsis michelle girvan edward ott and petros koumoutsakos forecasting of spatio temporal chaotic dynamics with recurrent neural networks a comparative study of reservoir computing and backpropagation algorithms arxiv preprint arxiv 1910 05266 2019 90 hendrik wernecke bulcsu sa ndor and claudius gros chaos in time delay systems an educational review physics reports 824 1 40 2019 91 matthew o williams ioannis g kevrekidis and clarence w rowley a data driven approximation of the koopman operator extending dynamic mode decomposition journal of nonlinear science 25 6 1307 1346 2015 92 herman ole andreas wold a study in the analysis of stationary time series with an appendix almqvist wiksell 1954 93 kyongmin yeo model free prediction of noisy chaotic time series by deep learning arxiv preprint arxiv 1710 01693 2017 94 kyongmin yeo and igor melnyk deep learning algorithm for data driven simulation of noisy dynamical system journal of computational physics 376 1212 1231 2019 95 koji yonemoto and takashi yanagawa estimating the embedding dimension and delay time of chaotic time series by an autoregressive model bulletin of informatics and cybernetics 33 1 2 53 62 2001 96 roland s zimmermann and ulrich parlitz observing spatio temporal dynamics of excitable media using reservoir computing chaos an interdisciplinary journal of nonlinear science 28 4 043118 2018 34 1 introduction 2 the data as sampled from a stochastic process 3 review of the traditional rc with nonlinear sigmoidal activation function 4 rc with a fully linear activation q s s yields a var k 4 1 decomposing the var k solution explicitly relates to rc 5 var k theory suggests convergence with k 5 1 stability of the var k and relationship to a var 1 6 quadratic nonlinear var 7 is there a connection to dmd koopman 8 examples 8 1 example 1 mackey glass differential delay equation 8 2 example 2 lorenz 63 9 on fading memory 9 1 fading memory in terms of the role of the internal layers 9 2 experiments on fading memory of the rc 10 conclusion 11 data availability statement 12 acknowledgments 13 appendix review of regularized pseudo inverse 14 appendix on quadratic nvar connection to rc