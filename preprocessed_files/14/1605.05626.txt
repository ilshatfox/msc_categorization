ar x iv 1 60 5 05 62 6 v 2 m at h a g 2 9 s ep 2 01 6 new classes of matrix decompositions ke ye abstract the idea of decomposing a matrix into a product of structured matrices such as tri angular orthogonal diagonal matrices is a milestone of numerical computations in this paper we describe six new classes of matrix decompositions extending our work in 8 we prove that every n n matrix is a product of finitely many bidiagonal skew symmetric when n is even companion matrices and generalized vandermonde matrices respectively we also prove that a generic n n centrosymmetric matrix is a product of finitely many symmetric toeplitz resp persymmetric hankel matrices we determine an upper bound of the number of structured matrices needed to decompose a matrix for each case 1 introduction matrix decomposition is an important technique in numerical computations for example we have classical matrix decompositions 1 lu a generic matrix can be decomposed as a product of an upper triangular matrix and a lower triangular matrix 2 qr every matrix can be decomposed as a product of an orthogonal matrix and an upper triangular matrix 3 svd every matrix can be decomposed as a product of two orthogonal matrices and a diagonal matrix these matrix decompositions play a central role in engineering and scientific problems related to matrix computations 1 6 for example to solve a linear system ax b where a is an n nmatrix and b is a column vector of length n we can first apply lu decomposition to a to obtain lux b where l is a lower triangular matrix and u is an upper triangular matrix next we can solve ly b ux y to obtain the solution of the original linear equation the advantage of decomposing a into the product of l and u first is that solving linear equations with triangular matrix coefficient is much easier than solving the one with general matrix coefficient similar idea applies to qr and svd decompositions those classical matrix decompositions lu qr and svd decompositions correspond to bruhat iwasawa and cartan decompositions of lie groups 5 2 other than those classical ones there are other matrix decompositions for instance 1 every n n matrix is a product of 2 n 5 toeplitz resp hankel matrices 8 2 every matrix is a product of two symmetric matrices 3 ky is partially supported by afosr fa 9550 13 1 0133 darpa d 15 ap 00109 nsf iis 1546413 dms 1209136 and dms 1057064 1 http arxiv org abs 1605 05626 v 2 2 ke ye as we have seen for classical matrix decompositions toeplitz hankel and symmetric matrix de compositions are important in the sense that structured matrices are well understood for example a toeplitz linear system can be solved in o n log n using displacement rank 10 compared to at least o n 2 for general linear systems sometimes the matrix decomposition refers to the decom position of a matrix into the sum of two matrices see for example 16 17 18 however whenever we mention the matrix decomposition in this paper we always refer to the multiplicative version in this article we study matrix decompositions beyond those mentioned above we use algebraic geometry as our tool to explore the existence of matrix decompositions for various structured matrices we define necessary notions in section 2 and we prove some general results for the matrix decomposition problem and establish a strategy to tackle the matrix decomposition problem in section 3 in section 4 we discuss the matrix decomposition problem with two factors and recover the lu decomposition and the qr decomposition for generic matrices using our method here the lu resp qr decomposition for generic matrices means that the set of matrices which can be written as the product of a lower triangular resp orthogonal matrix and an upper triangular matrix is a dense subset with the zariski topology of the space of all n n matrices in section 5 we apply the strategy built in section 3 4 to matrix decomposition problem for linear subspaces lastly in section 6 we apply the strategy to matrix decomposition problem for non linear varieties we summarize our contributions in the following list 1 bidiagonal decomposition section 5 1 2 skew symmetric decomposition section 5 2 3 symmetric toeplitz decomposition and persymmetric hankel decomposition section 5 3 4 generic decomposition section 5 4 5 companion decomposition section 6 1 6 generalized vandermonde decomposition section 6 2 for each type of matrices in the list above we first prove the existence of the matrix decomposition for a generic matrix in the sense that the set of all matrices which can not be decomposed as a product of matrices of the given type is contained in a proper algebraic subvariety of cn n then we use a result from topological group theory to prove the existence of the matrix decomposition for every matrix the price we need to pay is to increase the number of factors our method can only show the existence of the matrix decomposition and no algorithm can be obtained in general however we do find an algorithm for companion decomposition in 6 1 2 algebraic geometry in this section we introduce necessary notions in algebraic geometry needed in this paper we work over complex numbers but all results hold over algebraically closed fields of characteristic zero every notion we define in this section can be generalized to a more abstract version but we only concentrate on a simplified version main references for this section are 7 9 12 13 19 let c x 1 xn be the polynomial ring of n variables over c we say that a subset x c n is an algebraic subvariety if x is the zero set of finitely many polynomials f 1 fr c x 1 xn and we say that x is defined by f 1 fr for example any linear subspace of c n is an algebraic subvariety because they are all defined by polynomials of degree one less nontrivial examples are algebraic groups such as gln c the group of all n n invertible matrices and o n the group of all n n orthogonal matrices we remark here that gln c is an algebraic subvariety of c n 2 1 defined by f xij t t det xij 1 c xij t where t xij 1 i j n are variables and det xij is the determinant of the n n matrix xij also o n is an algebraic subvariety of cn 2 because o n is the defined by xij t xij 1 c xij where xij 1 i j n are variables new classes of matrix decompositions 3 let x be an irreducible algebraic subvariety of cn we say that x is irreducible if x cannot be written as the union of two algebraic subvarieties properly contained in x in other words whenever x x 1 x 2 for algebraic subvarieties x 1 and x 2 we have x 1 x or x 2 x it is clear that linear subspaces gln c and o n are all irreducible the algebraic subvariety x defined by the equation x 1 x 2 0 is not irreducible since x is the union of xi which is defined by the equation xi 0 i 1 2 let x cn be an algebraic subvariety let i x be the ideal consisting of all polynomials f c x 1 xn such that f x 0 for any x x it is well known 9 that the ideal i x must be finitely generated assume that i x is generated by polynomials f 1 fr let jp be the jacobian matrix jp fi xj p where 1 i r and 1 j n we define the dimension of x to be dimx min p x dimker jp the notion of dimension coincides with the intuition for example the dimension of a linear subspace of cn is the same as its dimension as a linear space the dimension of gln c is n 2 and the dimension of o n is n 2 if p x is a point such that dimx ker jp we say that p is a smooth point of x and we define the tangent space tpx of x at p to be tpx ker jp for example the tangent space tpx of a linear subspace x c n at a point p x can be identified with x itself the tangent space of o n at the identity e is simply the lie algebra o n the lie algebra consisting of all n n skew symmetric matrices 14 let u cn be a subset we define the zariski closure u of u to be the common zero set of polynomials vanishing on u namely u x cn f x 0 f i u for example the zariski closure of r in c is the whose space c we remark that the zariski closure could be much larger than the euclidean closure for example the euclidean closure of r in c is just itself let x cn and y cm be two algebraic subvarieties we say a map f x y is a polynomial map if f can be represented as f x 1 xn f 1 x 1 xn fm x 1 xn where f 1 fm are polynomials in n variables for example we denote c n n by the space of all n n matrices it is clear that cn n cn 2 then the map r c n n cn n r copies cn n defined by r a 1 ar a 1 ar is a polynomial map if w 1 wr are algebraic subvarieties of cn n then the restriction of r is by definition also a polynomial map we denote the set of all k dimensional linear subspaces of cn by gr k n and call it the grass mannian of k planes in cn in particular when k 1 we have gr 1 n pn 1 the projective space we say that a subset x of pn 1 is a projective subvariety if x is defined by homogeneous polynomials i e there are homogeneous polynomials f 1 fr c x 1 xn such that x p pn 1 f 1 p fr p 0 4 ke ye here p is the element in pn 1 which corresponds to the line joining the origin and p cn it is a fundamental fact 9 11 12 13 that gr k n is a projective subvariety in pn 1 where n n k furthermore gr k n is smooth i e every point in gr k n is a smooth point hence we can define the tangent bundle t gr k n whose fiber over a point w gr k n is simply the tangent space t w gr k n we define e w w w gr k n w w and a projection map e gr k n w w w it turns out that e is a vector bundle on gr k n 9 11 12 13 we say that e is the tautological vector bundle on gr k n by definition the fiber 1 w over a point w gr k n is simply the vector space w let x be an algebraic subvariety in cn let p be a property defined for points in x we say that p is a generic property if there exists an algebraic subvariety xp x such that if x x does not satisfy the property p then x xp if the property p is understood we say that x x is a generic point if x satisfies the property p for example we can say that for a fixed hyperplane h in cn a generic point in x cn is not contained in h a generic n n matrix is invertible since matrices that are not invertible are defined by the vanishing of their determinants we also say that for a fixed m plane l a generic k plane intersects with l in a m k n dimensional subspace if x cn is a generic point for property p then by definition the set of points in cn that does not satisfy p is contained in an algebraic subvariety xp c n if we equip cn with the lebesgue measure then it is clear that xp always has measure zero in other words the set of generic points has the full measure in particular when we say that a generic n n matrix can be decomposed into the product of finitely many structured matrices for example toeplitz matrices we mean the set of all matrices which admit such a decomposition is an open dense subset with the zariski topology of the space of all n n matrices for those who are not familiar with the notion of generic objects one can replace generic by random to obtain the intuition though this is not rigorous in mathematics 3 general method let x be an algebraic subvariety of cn n which is closed under matrix multiplication i e for any a b x we have ab x let r be a positive integer and let w 1 wr be subvarieties of x we consider a map r w r w 1 wr x defined by the matrix multiplication r a 1 ar a 1 ar we can rephrase the matrix decomposition problem in terms of question 3 1 1 does there exist r such that r is a dominant map i e r w r x 2 does there exist r such that r is a surjective map i e r w r x here the closure w r is the zariski closure of w r in x in general the fist question in question 3 1 is weaker than the second however we will see later that with some assumptions on x and w we can conclude that if r is dominant r is surjective for some r r new classes of matrix decompositions 5 3 1 lower bound of r first we can do a naive dimension counting to get a lower bound on r to do this we need proposition 3 2 9 if f y z is a dominant polynomial map between two irreducible algebraic varieties then dimy dimz dim f 1 z for a generic point z z in particular dimy dimz apply proposition 3 2 to our case we obtain corollary 3 3 if r w r x is dominant then r i 1 dimwi dimx 1 corollary 3 4 if dimw 1 dimwr m and r is dominant then r dimx m we say that an algebraic subvariety w cn n is a cone if x w implies that x w for any c assume that w 1 wr x are cones for any a x with a decomposition a a 1 ar where a 1 w 1 ar wr 1 r a contains the subvariety za 1 a 1 rar w r i c r i 1 i 1 it is clear from the definition of za that dimza r 1 apply proposition 3 2 to this case we obtain corollary 3 5 if w 1 wr are cones and r w r x is dominant then r i 1 dimwi r 1 dimx corollary 3 6 if w 1 wr are cones of the same dimension m and r is dominant then r dimx 1 m 1 3 2 criterion for dominant maps proposition 3 7 let r be an integer and w 1 wr be subvarieties of x if there is a point a a 1 ar w r such that the differential d r a ta 1 w 1 tarwr t a x has full rank dimx then the map r is dominant proof suppose that r w r is not equal to x then it is a proper subvariety of x and hence it has dimension strictly smaller than that of x therefore we have that the rank of d r a is strictly smaller than dimx for generic a w r however the assumption that there exists some a w r such that d r a has the maximal rank implies that for a generic point a w r we must have that the rank of d r a is equal to dimx for readers unfamiliar with the calculation of the differential d r a we record the following formula d r a x 1 xr r i 1 a 1 ai 1 xiai 1 ar 2 where xi taiwi if in particular wi is a linear subspace of c n n then we may identify the tangent space taiwi as wi itself we will apply formula 2 repeatedly the the rest of this paper 6 ke ye 3 3 criterion for surjective maps proposition 3 8 2 let g be a topological group and let u be an open dense subset of g assume that u contains the identity element of the group g then g u u i e every element g g is of the form hh for some h h u theorem 3 9 open mapping theorem 7 let x y be two irreducible varieties and let f x y be a polynomial map if f is dominant then there is some u f x which is open and dense in y proposition 3 10 let w 1 wr w be a linear subspace and x c n n suppose that w contains all diagonal matrices and that r is dominant then the map r w r w w r copies cn n defined by matrix multiplication is surjective for r 4 r 1 proof since r is dominant by theorem 3 9 its image r w r w w r copies contains an open dense subset of cn n this implies that r w r contains an open dense subset of the group gln c because gln c is an open dense subset of c n n by proposition 3 8 we see that gln c r w r r w r lastly if a cn n then there exists p q gln and a diagonal matrix d c n n such that a pdq hence we see that cn n r w r w r w r 3 4 our strategy let x be an algebraic subvariety of cn n which is closed under matrix mul tiplication and let w 1 wr be r algebraic subvarieties of x we define r w r w 1 wr x in general we may answer question 3 1 by the following strategy 1 we first calculate the lower bound r 0 of r for r to be dominant according to corollaries 3 4 3 5 and 3 6 if r r 0 then r can not be dominant 2 if r r 0 we calculate the differential d r a of r at a point a w r if d r a has the maximal rank dimx then r is dominant by proposition 3 7 3 if w 1 wr w is a linear subspace of x c n n containing all diagonal matrices and r is dominant then by proposition 3 10 r is surjective where r 4 r 1 the main step in our strategy is to find a point a w r such that the differential of r at a has the rank dimx the rest of this paper is concentrating on applying the above strategy to answer question 3 1 for various choices of wi and x new classes of matrix decompositions 7 4 toy examples lu and qr decompositions in this section we will discuss the matrix decomposition for two factors which is the simplest case we can recover the existence of lu and qr decompositions for a generic matrix using our method we know that a generic matrix has the lu decomposition and every matrix has the qr de composition although the existence of the lu decomposition and the qr decomposition is quit elementary and clear from the linear algebra point of view it is interesting to recover it from other point of view in fact we will prove a more general result theorem 4 1 let w 1 and w 2 be two algebraic subvarieties of c n n such that 1 both w 1 and w 2 contain the identity matrix in as a smooth point and 2 tinw 1 tinw 2 c n n then a generic n n matrix is a product of some a 1 w 1 and a 2 w 2 moreover every n n matrix m is a product of some a 1 b 1 c 1 d 1 w 1 a 2 b 2 c 2 d 2 w 2 and a diagonal matrix e i e m a 1 b 1 a 2 b 2 ec 1 c 2 d 1 d 2 here by definition we can identify the tangent space taiwi at a smooth point ai with a linear subspace in cn n for i 1 2 proof consider the differential d 2 in in of 2 at in in then by formula 2 we must have d 2 in in x 1 x 2 x 1 x 2 for any xi wi i 1 2 by assumption we see that d 2 in in is surjective hence 2 is dominant by proposition 3 7 the moreover part follows from proposition 3 10 here we remind readers that by generic we mean there are polynomials f 1 fr c xij such that whenever a aij is a matrix such that a cannot be expressed as a product of some a 1 w 1 and a 2 w 2 then we must have f 1 a fr a 0 we warn readers that a generic matrix has a decomposition a a 1 a 2 for ai wi i 1 2 does not imply that every matrix has such a decomposition intuitively speaking a generic matrix has decomposition means most matrices admit such a decomposition for example a generic matrix has the lu decomposition but it is not true that every matrix has the lu decomposition we will see this phenomenon again in section 6 1 where we prove that a generic n n matrix is a product of n companion matrices but there exits n n matrices that do not admit such a decomposition 4 1 triangular decomposition we apply theorem 4 1 to the lu decomposition and its vari ants corollary 4 2 let a be a generic n n matrix then 1 there exist a lower triangular matrix l and an upper triangular matrix u such that a lu 2 there exist a lower triangular matrix l and an upper triangular matrix u such that a ul 3 there exist a top triangular matrix t and a bottom triangular matrix b such that a tb 4 there exist a top triangular matrix t and a bottom triangular matrix b such that a bt 8 ke ye remark 4 3 on the one hand corollary 4 2 does not specify what generic matrices are it only guarantees that if we equip cn n with lebesgue measure the probability that a randomly picked n n matrix can be written as the product of a lower triangular matrix and an upper triangular matrix is one on the other hand it is known 20 that a nonsingular matrix admit an lu decomposition if and only if all its leading principal minors are nonzero this implies that a nonsingular matrix whose leading principal minors are all nonzero is a generic matrix in this case however we have more generic matrices for example matrices of rank k whose first k principal minors are nonzero are also generic matrices 20 it is also known 21 that there exist n nmatrices which do not admit lu decompositions hence generic matrices for the lu decomposition form a proper subset of the space of n n matrices corollary 4 4 let a be an n n matrix then 1 there exist lower triangular matrices l 1 l 2 l 3 l 4 and upper triangular matrices u 1 u 2 u 3 u 4 such that a l 1 u 1 l 2 u 2 l 3 u 3 l 4 u 4 2 there exist lower triangular matrices l 1 l 2 l 3 l 4 and upper triangular matrices u 1 u 2 u 3 u 4 such that a u 1 l 1 u 2 l 2 u 3 l 3 u 4 l 4 3 there exist top triangular matrices t 1 t 2 t 3 t 4 and bottom triangular matrices b 1 b 2 b 3 b 4 such that a t 1 b 1 t 2 b 2 t 3 b 3 t 4 b 4 4 there exist top triangular matrices t 1 t 2 t 3 t 4 and bottom triangular matrices b 1 b 2 b 3 b 4 such that a b 1 t 1 b 2 t 2 b 3 t 3 b 4 t 4 4 2 qr decompositions assume o n is the group of complex orthogonal matrices and let u be the space of upper triangular matrices since the tangent space of o n at in is simply the linear space of all n n skew symmetric matrices 14 and both o n and u contain in as a smooth point we can apply theorem 4 1 directly to o n and u corollary 4 5 let a be a generic n n matrix then 1 there exist an orthogonal matrix q an upper triangular matrix r such that a rq 2 there exist an orthogonal matrix q an upper triangular matrix r such that a qr 3 there exist an orthogonal matrix q a lower triangular matrix s such that a qs 4 there exist an orthogonal matrix q a lower triangular matrix s such that a sq corollary 4 6 let a be an n n matrix then 1 there exist orthogonal matrices q 1 q 2 q 3 q 4 and upper triangular matrices r 1 r 2 r 3 r 4 such that a r 1 q 1 r 2 q 2 r 3 q 3 r 4 q 4 2 there exist orthogonal matrices q 1 q 2 q 3 q 4 and upper triangular matrices r 1 r 2 r 3 r 4 such that a q 1 r 1 q 2 r 2 q 3 r 3 q 4 r 4 new classes of matrix decompositions 9 3 there exist orthogonal matrices q 1 q 2 q 3 q 4 and lower triangular matrices s 1 s 2 s 3 s 4 such that a q 1 s 1 q 2 s 2 q 3 s 3 q 4 s 4 4 there exist orthogonal matrices q 1 q 2 q 3 q 4 and lower triangular matrices s 1 s 2 s 3 s 4 such that a s 1 q 1 s 2 q 2 s 3 q 3 s 4 q 4 remark 4 7 corollary 4 6 is redundant because it is known that every n n matrix admit a qr decomposition furthermore this implies that generic matrices for qr decomposition in corollary 4 5 are actually all matrices combining this fact with remark 4 3 we see that generic matrices are not necessarily the same for different matrix decompositions one might ask whether or not the same method applies to the svd but unfortunately since the svd involves complex conjugation of a matrix which makes the decomposition non algebraic we are not allowed to use the same argument to recover the svd even for generic matrices 5 matrix decomposition for linear spaces 5 1 bidiagonal decomposition let a aij be an n nmatrix we say that a is a k diagonal matrix if aij 0 if i j k in particular 1 diagonal matrices are simply diagonal matrices 2 diagonal matrices are called bi diagonal matrices for example a 3 3 bidiagonal matrix is of the form a b 0 c d e 0 f g an upper k diagonal matrix a aij is a k diagonal matrix with further restriction aij 0 if i j 1 a 3 3 upper bidiagonal matrix is of the form a b 0 0 c d 0 0 e a matrix a is called lower k diagonal if at is upper k diagonal we denote the set of all k diagonal matrices by dk the set of all upper k diagonal matrices by dk 0 and the set of all lower k diagonal matrices by dk 0 lemma 5 1 let 2 k n be an integer a generic n n upper resp lower k diagonal matrix is a product of k 1 upper resp lower bidiagonal matrices in particular a generic n n upper resp lower matrix is a product of n upper resp lower bidiagonal matrices proof we will prove the lemma for upper triangular matrix case for a positive integer 2 k we recall that dk 0 is the space of upper k diagonal matrices it is clear that the product of k 1 bidiagonal matrices is a k diagonal matrix we want to show that the map k 1 d 2 0 d 2 0 k 1 copies dk 0 defined by matrix multiplication is dominant we proceed by induction on k when k 2 it is clear that 1 is dominant assume that the map k 1 is dominant where k 1 n 1 then we need to prove that k is also dominant to this end we can factor the map k as k d 2 0 d 2 0 d 2 0 k 1 copies idn k 1 d 2 0 dk 1 0 dk 1 0 10 ke ye where idn is the identity map on d 2 0 and is the map defined by multiplication of two matrices by the induction hypothesis we see that id k 1 is dominant hence it is sufficient to show that is dominant now to see that is dominant we calculate the differential of by formula 2 the differential of at a b is given by d a b x y xb ay for all x y d 2 0 dk 1 0 on the other hand given any c dk 1 0 we can write c c c where c dk 0 and c dk 1 0 dk 0 we take b i i k 1 dk 0 where i j is the kronecker delta and a idn then one can easily find x y d 2 0 dk 0 such that xb c y c this implies that d a b is surjective and hence is dominant by proposition 3 7 corollary 5 2 every invertible upper resp lower triangular matrix is a product of 2 n upper resp lower bidiagonal matrices proof since a generic upper triangular matrix is a product of n upper bidiagonal matrices the corollary follows from proposition 3 8 proposition 5 3 a generic n n matrix can be decomposed into a product of 2 n tridiagonal matrices an invertible n n matrix is a product of 4 n bidiagonal matrices proof by lemma 5 1 a generic upper resp lower triangular matrix is a product of n upper resp lower bidiagonal matrices a generic n n matrix has an lu decomposition hence we see that a generic matrix can be decomposed as a product of n upper and n lower bidiagonal matrices the last statement follows from 3 8 theorem 5 4 let r be the smallest number such that every n n matrix is a product of r bidiagonal matrices then n 1 r 8 n proof notice that every matrix a can be written as a pdq where p q are invertible and d is diagonal by proposition 5 3 we see that p q are products of 8 n bidiagonal matrices respectively since diagonal matrices are also bidiagonal we see that every n n matrix is a product of 8 n bidiagonal matrices this gives the upper bound of r for the lower bound we simply notice that a product of k 1 bidiagonal matrices is k diagonal hence r must be at least n 1 since dimd 2 3 n 2 by corollary 3 6 the expected value of r is n 1 3 while the lower bound of r is n 1 this shows that proposition 5 4 gives us an example that the expected value may not be achieved roughly speaking this is because entries on the diagonal of a tridiagonal matrix do not contribute to expand the product to be more precise if x is a diagonal matrix and y is a bidiagonal matrix then xy is still a bidiagonal matrix 5 2 skew symmetric decomposition we consider skew symmetric matrix decomposition prob lem in this section an n n skew symmetric matrix a is defined by the condition a at we denote the space of all n n skew symmetric matrices by n it is clear that n is a linear subspace of cn n and dim n n 2 new classes of matrix decompositions 11 on the one hand since n 2 1 n 2 1 3 if n 3 we see that if the map r n n r copies cn n is dominant then r is at least three on the other hand from the definition one can see that for any a n we have det a det at 1 n det a in particular if n is odd we obtain det a 0 this implies that when n is odd the map can never be dominant regardless how large r is however when n is odd we can expect that r n n r copies detn is dominant for r 3 where detn is the hypersurface of all n n matrices whose determinants are zero proposition 5 5 we have the following two cases 1 n is even a generic n n matrix is a product of r skew symmetric matrices for n r where a n 8 r 3 or b n 6 r 4 or c n 4 r 5 2 n is odd a generic n n matrix is a product of r skew symmetric matrices whose deter minants are zero for n r where a n 5 r 3 or b n 3 r 4 again we consider the map r n n r copies cn n when n is even and r n n r copies detn when n is odd example 5 6 using macalay 2 22 we can calculate the dimension d of the closure of the image of for small n and r we list some results 1 n 2 d 1 for any r 2 n r 3 3 d 7 3 n r 3 4 d 8 im r det 3 4 n r 4 3 d 13 5 n r 4 4 d 15 im r is a hypersurface in c 16 6 n r 4 5 d 16 im r c 16 7 n r 5 3 7 3 9 3 or 11 3 d n 2 1 im r detn 8 n r 6 3 d 35 im r is a hypersurface in c 36 9 n r 6 4 d 36 im r c 36 10 n r 8 3 10 3 12 3 or 14 3 d n 2 im r c n 2 proof of proposition 5 5 it left to show that r is dominant for n 16 r 3 when n is even resp n 13 r 3 when n is odd we may proceed by induction on n case 1 we assume n 16 is even we consider three block diagonal matrices a o o a b o o b and c o o c 12 ke ye where a b c are n 8 n 8 skew symmetric matrices and a b c are 8 8 skew symmetric matrices such that differentials of n 8 3 n 8 n 8 n 8 c n 8 n 8 and 8 3 8 8 8 c 8 8 are surjective at a b c and a b c respectively we consider the differential of 3 n n n c n n at a a o o a b b o o b and c c o o c we parametrize tangent spaces of n at a b and c by x x u ut x y y v vt y and z z w wt z then we have by formula 2 d 3 a b c x y z ab z w wt z a y v vt y c x u ut x bc abz ay c xbc abw avc ub c a b wt a vtc utbc a b z a y c x b c by choice of a b c and a b c we know that abz ay c xbc can be any n 8 n 8 matrix and that a b z a y c x b c can be any 8 8 matrix lastly it is clear that abw avc ub c and a b wt a vtc utbc can be any n 8 8 and 8 n 8 matrix respectively this shows that d 3 a b c is surjective hence is dominant case 2 we assume n 13 is odd then we can choose a a o o a b b o o b and c c o o c where a b c are n 5 n 5 skew symmetric matrices and a b c are 5 5 skew symmetric matrices such that n 5 3 n 5 n 5 n 5 c n 5 n 5 and 5 3 5 5 5 det 5 are dominant at a b c and a b c respectively the similar calculation as in the previous case shows that 3 is dominant we remark that when n 2 a skew symmetric matrix is of the form 0 a a 0 a c therefore we see that if r is even the image of r is simply the space of all 2 2 diagonal matrices and if r is odd the image of r is the space of skew symmetric matrices by proposition 3 10 we can derive from 5 5 the following theorem 5 7 for n 4 every 2 n 2 n matrix is a product of 13 skew symmetric matrices every 6 6 matrix is a product of 17 skew symmetric matrices every 4 4 matrix is a product of 21 skew symmetric matrices notice that we are not able to apply proposition 3 10 when n is odd this is because the image of r is contained in detn which does not contain the group of invertible matrices new classes of matrix decompositions 13 5 3 symmetric toeplitz matrix decomposition a symmetric toeplitz matrix a aij is defined by aij ai p j p aij aji 1 i j i p j p n we denote the space of all symmetric toeplitz matrices by stn a centrosymmetric matrix b bij is defined by bij an i 1 n j 1 1 i j n it is easy to verify that the product of two centrosymmetric matrices is again a centrosymmetric matrix hence the space csn of all centrosymmetric matrices is an algebra moreover we have stn csn we say that a matrix a is a persymmetric hankel if ja is a symmetric hankel where j 0 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 we denote the space of all n n persymmetric hanekl matrices by phn it is clear that phn csn we will consider symmetric toepliz resp persymmetric hankel matrix decomposition problem of a centrosymmetric matrix it is clear that dim stn dim phn n dim csn n 2 2 hence by corollary 3 6 we see that if r stn stn r copies csn or r phn phn r copies csn is dominant then r n 2 2 1 n 1 n 1 2 if n 3 1 if n 2 proposition 5 8 let n 3 be an integer a generic n n centrosymmetric matrix is a product of n 1 2 symmetric toepitz resp persymmetric hankel matrices the proof of proposition 5 8 is similar to the proof of toeplitz matrix decomposition theorem 8 hence we will just give a sketch of the proof for proposition 5 8 here sketch of proof of proposition 5 8 it is sufficient to prove the statement for symmetric toeplitz indeed since we have ja aj j 2 1 if a is symmetric toeplitz if x csn has a decomposition x a 1 ar where a 1 ar stn and r n 1 2 then we see that jx j a 1 ar ja 1 jar if r is odd and x ja 1 jar if r is even in either case this implies that a generic centrosymmetric matrix is a product of r persymmetric hankel matrices 14 ke ye let bk i j k n i j 1 k n 1 n 2 n 1 be a basis of the linear space of all n n toeplitz matrices then sk 1 1 k k bk b k k 0 1 n 1 is a basis of stn precisely we have b 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 b 1 0 1 0 0 1 0 0 1 0 b 2 0 0 1 0 0 0 1 0 0 bn 1 0 0 0 1 0 0 0 0 0 0 and s 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 s 1 0 1 0 1 0 1 0 1 0 0 1 0 1 0 s 2 0 0 1 0 0 0 1 0 0 1 0 1 0 0 sn 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 to prove that r is dominant it suffices to find a point a an r an 1 stn stn r copies such that the differential of r at a has the maximal rank n 2 2 in stead of choosing a point a explicitly we will show that such a point exits to this end we write an i s 0 tn isn i i 1 2 r where tn 1 tn r are indeterminants for such an i we see that the differential d r a can be represented as an n 2 2 rn matrix m whose entries are polynomials in tn 1 tn r now to see that m has rank n 2 2 we need to find a nonzero n 2 2 n 2 2 minor of m claim 5 9 by the same type of argument as in 8 we can show 1 any n 2 2 n 2 2 of m is a polynomial in t s of degree at least n 2 2 n 2 there exists a n 2 2 n 2 2 minor of m contains a monomial of degree exactly n 2 2 n and whose coefficient is non zero indeed the desired monomial is tn 1 n 1 t n 1 n 2 t n 1 n r 1 if n is odd and tn 1 n 1 t n 1 n 2 t n 1 n r 2 t n 2 1 n r 1 if n is even this shows that for a fixed n there exist some values of tn 1 tn r such that the differential d r a has the maximal rank n 2 2 we work out an example to illustrate how the proof of proposition 5 8 works we adopt notations in 8 for the map r stn stn r copies csn we define xn i n 1 k 0 xn i ksk i 1 2 r to be the matrix occuring in the i th argument of r then by formula 2 the differential d r a is simply a linear map defined by d r a xn r xn 1 r i 1 an r an i 1 xn ian i 1 an 1 for xn i stn we denote the entries of d r a xn r xn 1 by lp q 1 p q n then it is clear that the matrix lp q is a centrosymmetric matrix i e lp q ln p 1 n q 1 1 p q n we new classes of matrix decompositions 15 case 1 we consider the case n 3 this gives r n 1 2 2 n 2 2 5 we will see that any 5 5 minor of the 5 6 matrix m is a polynomial in t s of degree at least 3 we can simply write lp q x 1 q p x 2 q p t p q 1 2 3 here t stands for terms of lp q of degree at least one in t s with this notation we express m as x 1 2 x 2 2 x 1 1 x 1 0 x 2 0 x 2 1 l 3 3 l 1 1 1 1 l 3 2 l 1 2 1 1 l 3 1 l 1 3 1 1 l 2 2 1 1 l 2 1 l 23 1 1 here means that the entry if of the form t and 1 means that the corresponding lp q contains x 1 k or xk for example since l 3 3 l 1 1 is of the form x 1 0 x 2 0 t we put 1 in 1 4 th and 1 5 th entry of m and elsewhere in the first row it is not hard to see that any 5 5 minor of m has degree at least 5 3 2 this verifies the first statement of claim 5 9 case 2 next we consider the case n 5 in this case we have r n 1 2 3 n 2 2 13 we consider the table li j 1 2 3 4 5 1 t 4 t 3 1 1 1 2 t 4 t 3 1 t 3 t 4 3 1 t 3 t 4 indicating the way we obtain t 44 t 4 3 namely we pick t 4 from l 1 1 l 2 1 l 2 5 and l 3 5 we pick t 3 from l 1 2 l 2 3 l 2 4 and l 3 4 and one form rest five lij s by definition of lij we see that this is the unique way to obtain the monomial t 44 t 4 3 this verifies the second statement of claim 5 9 5 4 generic matrix decomposition in this section we consider the decomposition problem for generic linear subspaces of cn n let r be a positive integer assume that for i 1 2 r wi is a ki dimensional subspace of c n n we define w r w 1 wr let r w r cn n be the map sending a 1 ar to their product a 1 ar we want to determine r such that r is dominant consider the following diagram te 1 ter d r t gr k 1 n 2 t gr kr n 2 cn n e 1 er r gr k 1 n 2 gr kr n 2 cn n gr k 1 n 2 gr kr n 2 id gr k 1 n 2 gr kr n 2 16 ke ye here ei is the tautological vector bundle over gr ki n 2 tei is the tangent bundle of ei and r is the bundle map induced by r w r cn 2 and d r is the differential of r to be more precise for any wi gr ki n 2 the fiber e wi over wi is wi and the fiber of tei over wi ai is t wi gr ki n 2 wi where ai wi if we restrict r to the fiber e w 1 e wr we obtain the map r w r cn n defined before and the restriction of d r to te 1 w 1 a 1 ter wr ar becomes the differential of r at the point a 1 ar lemma 5 10 let r be a positive integer for each i 1 r let ki be a fixed integer such that 1 ki n 2 assume that r w r cn n is dominant for some ki dimensional subspace wi of c n n i 1 2 r then for a generic ki dimensional subspace w i of c n n the map r w r cn n is dominant where w r w 1 w r proof since r w r cn n is dominant we see that the jacobian matrix of r at a generic point a 1 ar in w r has the maximal rank i e the jacobian matrix of r at w 1 a 1 wr ar has the maximal rank by proposition 3 7 we see that r is dominant i e r is dominant for generic w i gr ki n 2 i 1 2 r we shall make use of the following result proposition 5 11 8 a generic n n matrix can be decomposed as the product of n 2 1 toeplitz matrices proposition 5 12 let cn n be the space of all n n matrices then i for generic 2 n 1 dimensional subspaces w 1 wr of c n n a generic n n matrix is a product of elements in wi i 1 2 r if r n 2 1 ii for generic n 1 2 dimensional subspaces w 1 wr of c n n a generic n n matrix is a product of elements in wi i 1 2 r if r 2 iii for generic 3 n 2 dimensional subspaces w 1 wr of c n n a generic n n matrix is a product of elements in wi i 1 2 r if r 2 n iv for generic 2 n 2 dimensional subspaces w 1 wr of c 2 n 2 n a generic n n matrix is a product of elements in wi i 1 2 r if a r 3 when n 4 b r 4 when n 3 c r 5 when n 2 proof the first statement follows from proposition 5 11 and lemma 5 10 the second statement follows from corollary 4 2 and lemma 5 10 the third statement follows from proposition 5 3 and lemma 5 10 the last statement follows from proposition 5 5 and lemma lemma 5 10 the combination of proposition 3 10 and proposition 5 12 implies theorem 5 13 let cn n be the space of all n n matrices then i for generic 2 n 1 dimensional subspaces w 1 wr of c n n an n n matrix is a product of elements in wi i 1 2 r if r 2 n 5 ii for generic n 1 2 dimensional subspaces w 1 wr of c n n an n n matrix is a product of elements in wi i 1 2 r if r 9 iii for generic 3 n 2 dimensional subspaces w 1 wr of c n n an n n matrix is a product of elements in wi i 1 2 r if r 8 n 1 iv for generic 2 n 2 dimensional subspaces w 1 wr of c 2 n 2 n an n n matrix is a product of elements in wi i 1 2 r if a r 13 when n 4 b r 17 when n 3 c r 21 when n 2 new classes of matrix decompositions 17 we close this section by remarking that proposition 5 12 resp theorem 5 13 only hold for generic subspaces w 1 wr of c n n i e there is a proper algebraic subvariety zi gr ki n 2 such that if w 1 wr gr k 1 n 2 z 1 gr kr n 2 zr then r w 1 wr c 2 is dominant resp surjective however we do not know any infor mation about algebraic subvarieties zi the main contribution of proposition 5 12 and theorem 5 13 is that if the matrix decomposition both dominant and surjective versions holds for some w 1 wr then it also holds for almost all linear subspacesw i of dimensions dimwi i 1 2 r respectively 6 matrix decomposition for nonlinear spaces we consider matrix decompositions for non linear algebraic subvarieties in this section in 6 1 we discuss the matrix decomposition into the product of companion matrices and in 6 2 we discuss the matrix decomposition for generalized vandermonde matrices 6 1 companion decomposition an n n companion matrix is a matrix of the form 0 0 0 c 1 1 0 0 c 2 0 0 1 cn where c 1 cn are arbitrary complex numbers we denote cn by the set of all companion matrices then it is clear that cn is an affine varitey of dimension n proposition 6 1 a generic n n matrix is a product of n companion matrices proof we need to prove that the map n cn cn n copies cn n is dominant let be the matrix corresponding to the permutation 12 n i e is the matrix 0 0 0 1 1 0 0 0 0 0 1 0 for an n n matrix a the matrix a is obtained by shifting the i th row of a to the i 1 th row i 1 n similarly the matrix a is obtained by shifting the i th column of a to the i 1 th column i 1 n here we adopt the convention that the n 1 th row is actually the first row and the 0 th column is actually the n th column we calculate the rank of d n at the point first notice that the tangent space t cn of cn at is the linear space consisting of matrices of the form 0 0 c 1 0 0 c 2 0 0 cn where c 1 cn are arbitrary complex numbers let y 1 yn be n elements of t cn then by formula 2 we have d y 1 yn n i 1 i 1 yi n i 18 ke ye since corresponds to 12 n it is easy to see that i 1 yi n i is a matrix with zero entries everywhere except the i th column on the other hand yi s are independent from each other this suffices to show that the rank of d is n 2 proposition 3 10 together with proposition 6 1 imply theorem 6 2 every n n invertible matrix is a product of 2 n companion matrices every n n matrix is a prodcut of 4 n companion matrices and a diagonal matrix since the map n cn cn n copies cn n is dominant by proposition 3 2 we see that for a generic matrix a cn n the fiber 1 n a is of dimension zero and hence 1 n a is a finite set in fact we can prove more theorem 6 3 the decomposition of a generic n n matrix into the product of n companion matrices is unique i e for a generic n n matrix a if a c 1 cn c 1 c n where ci c i i 1 2 n are companion matrices then ci c i for all i 1 n proof we consider n companion matrices c 1 cn and write ci 0 0 0 ci 1 1 0 0 ci 2 0 0 1 ci n and calculate the product xk c 1 ck we claim that the p q th entry xkp q of x k is a polynomial in cij where 1 i n and 1 j q 1 and it is of the form xkp q q k n 1 j 1 x k p q jcq k n n j 1 cq k n n 1 p q k ifq n k 1 1 if p q k and q n k 1 0 otherwise if k n then xnp q q 1 j 1 xnp q jcq n j 1 cq p q 1 now given a generic n n matrix a aij we can find cij 1 i j n such that ap 1 c 1 p ap 2 ap 1 c 2 n c 2 p 1 ap 3 ap 2 c 3 n ap 1 c 3 n 1 c 3 p 2 ap n ap n 1 cn n ap n 2 cn n 1 ap 1 cn 2 cn p n 1 3 and those cij s are uniquely determined by 3 those ci where ci 0 0 0 ci 1 1 0 0 ci 2 0 0 1 ci n new classes of matrix decompositions 19 are the desired companion matrices such that a c 1 cn the proof of theorem 6 3 actually gives another way to show that a generic n n matrix is a product of n companion matrices moreover it also gives an algorithm to decompose a generic n n matrix into the product of n companion matrices in algorithm 1 the input is an n n matrix a aij with entries aij 1 i j n and the out put is a sequence of n companion matrices c 1 cn such that a c 1 cn if such a decomposition exists and is unique algorithm 1 companion matrix decomposition 1 for p q 1 2 n do 2 solve the linear system ap q q 1 j 1 ap q jcq n j 1 cq p q 1 for cq 1 cq n here we adopt the convention that ai j ci j 0 if either i 0 or j 0 3 if the solution does not exist or is not unique the decomposition of a does not exist or is not unique stop the algorithm otherwise define a matrix cq 0 0 0 cq 1 1 0 0 cq 2 0 0 1 cq n and continue the algorithm 4 end for lastly we remark that it is not true that every n n matrix is a product of n companion matrices indeed if we consider a matrix a aij where a 11 0 a 12 1 then from 3 that we must have c 11 a 11 0 a 11 c 2 n a 12 1 which is impossible hence the companion matrix decomposition is an example where the map r is dominant but not surjective as we have remarked in section 4 6 2 generalized vandermond decomposition now we consider generalized vandermonde matrices first we need to define generalized vandermonde matrices definition 6 4 let s be an integer we call a matrix of the form xq s p 1 xs 1 x s 2 x s n 1 x s n x s 1 1 x s 1 2 x s 1 n 1 x s 1 n xs n 11 x s n 1 2 x n 1 s n 1 x s n 1 n a generalized vandermonde matrix of type s we denote the set of all generalized vandermonde matrix of type s by vands and the set of transpose of generalized vandermonde matrices of type s by vandts by the definition a vandermonde matrix is a generalized vandermonde matrix of type 0 we consider the matrix decomposition for generalized vandermonde matrices and their transpose 20 ke ye proposition 6 5 let s 1 s 2 sn be n integers such that si 6 sj mod n if i 6 j and n j 1 sj 6 0 a generic n n matrix is a product of elements in vandsi and vand t si i 1 2 n proof again we need to prove that the map 2 n vand t s 1 vands 1 vand t sn vandsn c n n is dominant let w be a primitive n th root of unity let wi bi ai bi vand t si ai w q p 1 si vandsi it is clear that wi vand t si vandsi and that id wi then it suffices to show that n w 1 wn c n n is dominant for this we will show that the differential d n at id id id is surjective consider the differential d n id id id x 1 xn n i 1 xi where xi q si 1 w p q si 2 xp i w q p si 1 tidwi and xp i s are variables then a simple calculation shows that xi n k 1 k si 1 w p q k wp si 2 q si 1 xp si let p q i n k 1 k si 1 w p q k wp si 2 q si 1 we regard cn n as cn 2 by the linear isomor phism h cn n cn 2 defined by h xi j x 1 1 x 1 2 x 1 n x 2 1 x 2 2 x 2 n xn 1 xn 2 xn n let m be the coefficient matrix of d 2 n id id id then m is an n 2 n 2 matrix and m 1 1 1 0 0 1 1 2 0 0 1 1 n 0 0 1 2 1 0 0 1 2 2 0 0 1 2 n 0 0 1 n 1 0 0 1 n 2 0 0 1 n n 0 0 0 2 1 1 0 0 2 1 2 0 0 2 1 n 0 0 2 2 1 0 0 2 2 2 0 0 2 2 n 0 0 2 n 1 0 0 2 n 2 0 0 2 n n 0 0 0 n 1 1 0 0 n 1 2 0 0 n 1 n 0 0 n 2 1 0 0 n 2 2 0 0 n 2 n 0 0 n n 1 0 0 n n 2 0 0 n n n new classes of matrix decompositions 21 where d 2 n id id id x 1 xn n i 1 xi can be expressed as 1 1 1 0 0 1 1 2 0 0 1 1 n 0 0 1 2 1 0 0 1 2 2 0 0 1 2 n 0 0 1 n 1 0 0 1 n 2 0 0 1 n n 0 0 0 2 1 1 0 0 2 1 2 0 0 2 1 n 0 0 2 2 1 0 0 2 2 2 0 0 2 2 n 0 0 2 n 1 0 0 2 n 2 0 0 2 n n 0 0 0 n 1 1 0 0 n 1 2 0 0 n 1 n 0 0 n 2 1 0 0 n 2 2 0 0 n 2 n 0 0 n n 1 0 0 n n 2 0 0 n n n x 1 s 1 x 2 s 1 xn s 1 x 1 s 2 x 2 s 2 xn s 2 x 1 sn x 2 sn xn sn now we will prove that m is of the full rank n 2 since by permutation of columns m becomes a block diagonal matrix where blocks mp on the diagonal are mp p 1 1 p 1 2 p 1 n p 2 1 p 2 2 p 2 n p n 1 p n 2 p n n therefore it suffices to prove that mp is of rank n for each p 1 2 n note that we have a formulas n k 1 kwk nw 1 w n k 1 wk 0 for any n th root of unity w 6 1 hence p i j nw p i 1 wp i w p i sj 2 p i if p 6 i 2 sj n 1 nw p 2 if p i then it is easy to see that the rank of mp is the same as the matrix m p p i j where p i j w isj if i 6 p sjw psj if i p 22 ke ye up to a permutation of rows m p is of the form 1 1 1 w s 1 w s 2 w sn w p 1 s 1 w p 1 s 2 w p 1 sn 1 w ps 1 2 w ps 2 nw psn w p 1 s 1 w p 1 s 2 w p 1 sn w n 1 s 1 w n 1 s 2 w n 1 sn where 1 n are fixed integers we still denote this matrix by m p where p 0 1 n 1 we will compute the determinant of m p we have det m p n j 1 1 p j 1 jw psjv w sj sj n 1 p where v w sj is the determinant of the vandermonde matrix determined by w s 1 w sj 1 w sj 1 w sn and sj n 1 p is the n 1 p th symmetric function on w s 1 w sj 1 w sj 1 w sn note that t w s 1 t w sn tn 1 so t w s 1 t w sj 1 t w sj 1 t w sn tn 1 t w sj n 1 k 0 w sj ktn 1 k hence sj n 1 p 1 p w sj n 1 p w sj 1 p on the other hand we know that v w sj a b a b 6 j w sa w sb 1 j 1 a b w sa wsb c 6 j w sc w sj let v be the determinant of the vandermonde matrix determined by w s 1 w sn it is obvious that v 6 0 also from tn 1 t w sj c 6 j t w sc we have c 6 j w sc w sj lim t w sj tn 1 t w sj nwsj new classes of matrix decompositions 23 therefore det m p n j 1 1 p j 1 jw psj 1 p w sj 1 p 1 j 1 a b w sa wsb c 6 j w sc w sj v n j 1 jw sj c 6 j w sc w sj v n j 1 jw sj nwsj v n n j 1 j in particular we set j sj then we see that det m p 6 0 if and only if j 1 sj 6 0 this implies that the map n is dominant for all s 1 sn such that si 6 sj mod n and n j 1 sj 6 0 from the proof of proposition 6 5 we have corollary 6 6 let s 1 sn be as in proposition 6 5 and let wi bi ai bi vand t si ai w q p 1 si a generic n n matrix is a product of elements in wi i 1 2 n combining proposition 3 10 and proposition 6 5 we obtain theorem 6 7 let s 1 sn be as in proposition 6 5 for every n n invertible matrix m there are ai a i vandsi and bi b i vand t si i 1 2 n such that m b 1 a 1 bnanb 1 a 1 b na n for every n n matrix m there are ai a i ci c i vandsi bi b i di d i vand t si i 1 2 n and a diagonal matrix e such that m b 1 a 1 bnanb 1 a 1 b na ned 1 c 1 dncnd 1 c 1 d nc n theorem 6 8 let wi i 1 2 n be as in corollary 6 6 for every n n invertible matrix m there is an element ai in wi for each i 1 2 n such that m a 1 an for every n n matrix m there are ai bi wi for each i 1 2 n and a diagonal matrix c such that m a 1 ancb 1 bn 7 conclusion we discuss the existence of matrix decompositions for bidiagonal skew symmetric symmetric toeplitz persymmetric hankel generic companion generalized vandermonde matrix decomposi tions for both generic and arbitrary matrices it is natural to ask for example if the number of bidiagonal matrices needed to decompose a generic resp arbitrary matrix is the smallest for most types of matrix decompositions we discussed in this paper the number we obtain is already the smallest for a generic matrix it is 24 ke ye still open if the number we obtain is the smallest for an arbitrary matrix we summarize our main results in the following table r generic sharpness r arbitrary algorithm bidiagonal 2 n unknown 8 n unknown skew symmetric n 8 even 3 yes 13 unknown symmetric toeplitz n 1 2 yes unknown unknown companion n yes 4 n 1 yes generalized vandermonde 2 n unknown 8 n 1 unknown references 1 algorithms for the ages science 287 2000 no 5454 pp 799 2 a borel linear algebraic groups 2 nd ed graduate texts in mathematics 126 springer verlag new york ny 1991 3 a j bosch the factorization of a square matrix into two symmetric matrices amer math monthly 93 1986 no 6 pp 462 464 4 r howe very basic lie theory amer math monthly 90 1983 no 9 pp 600 623 5 a w knapp lie groups beyond an introduction 2 nd ed progress in mathematics 140 birkha user boston ma 2002 6 g w stewart the decompositional approach to matrix computation comput sci eng 2 2000 no 1 pp 50 59 7 j l taylor several complex variables with connections to algebraic geometry and lie groups graduate studies in mathematics 46 american mathematical society providence ri 2002 8 k ye and l h lim every matrix is a product of toeplitz matrices found comput math 15 2015 no 6 pp 1 22 9 i r shafarevich basic algebraic geometry 1 springer heidelberg 2013 10 r b bitmead and b d o anderson asymptotically fast solution of toeplitz and related systems of linear equations linear algebra appl 34 1980 pp 103 116 11 j weyman cohomology of vector bundles and syzygies cambridge tracts in mathematics 149 cambridge university press cambridge 2003 12 j harris algebraic geometry graduate texts in mathematics 133 springer verlag new york 1995 13 p griffiths and j harris principles of algebraic geometry wiley classics library john wiley sons inc new york 1994 14 w fulton and j harris representation theory graduate texts in mathematics 129 a first course readings in mathematics springer verlag new york 1991 15 g s ammar and w b gragg superfast solution of real positive definite toeplitz systems siam j matrix anal appl 9 1988 no 1 pp 61 76 16 chandrasekaran venkat and sanghavi sujay and parrilo a pablo and willsky s alan rank sparsity inco herence for matrix decomposition siam j optim 21 2011 no 2 pp 572 596 17 d l donoho and x m huo uncertainty principles and ideal atomic decomposition ieee trans inform theory 47 2001 no 7 pp 2845 2862 18 j b victor z douglas and c david low rank network decomposition reveals structural characteristics of small world networks phys rev e vol 92 2015 iss 6 062822 19 h robin algebraic geometry 52 graduate texts in mathematics springer verlag new york heidelberg 1977 20 h roger c r johnson matrix analysis cambridge university press 1985 21 p okunev necessary and sufficient conditions for existence of the lu factorization of an arbitrary matrix arxiv 0506382 1997 22 grayson r daniel and m e stillman macaulay 2 a software system for research in algebraic geometry avail able at http www math uiuc edu macaulay 2 department of statistics university of chicago chicago il 60637 1514 e mail address kye galton uchicago edu http www math uiuc edu macaulay 2 1 introduction 2 algebraic geometry 3 general method 3 1 lower bound of r 3 2 criterion for dominant maps 3 3 criterion for surjective maps 3 4 our strategy 4 toy examples lu and qr decompositions 4 1 triangular decomposition 4 2 qr decompositions 5 matrix decomposition for linear spaces 5 1 bidiagonal decomposition 5 2 skew symmetric decomposition 5 3 symmetric toeplitz matrix decomposition 5 4 generic matrix decomposition 6 matrix decomposition for nonlinear spaces 6 1 companion decomposition 6 2 generalized vandermond decomposition 7 conclusion references