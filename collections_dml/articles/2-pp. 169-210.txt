 THE ITERATIVE METHODS FOR MESH VARIATIONAL INEQUALITIES  Introduction  1.1. We consider the finite-dimensional variational inequalities (or inclusions) of one of the following classes:  Au+ Cu 3 f, (1)  which be refered as problem P1 or  Au+Bγ = f ; γ ∈ Cu, (2)  which we call by the problem P2. In these variational inequalities A ∈ L(RN ;RN ) corresponds to a mesh approximation (finite difference or finite element) of linear partial differential operator while nonlinear, generally multivalued, operator C is responsible for the constraints in the problem. As for the term Bγ, B ∈ L(RN ;RN ), it may be viewed as an approximation for the nonlinear convective term in the differential problem under consideration.  There are a lot of variational inequalities and differential inclusions of practical importance which mesh approximations lead to finite-dimensional problem of the first or second types. Namely, mesh schemes for obstacle problem and dam problem (mathematical model constructed using Baiocchi transformation), implicit mesh schemes on the fixed time level for one-phase or two-phase Stefan problem and  169    some others can be written as the problem P1. On the other hand, mesh approximations on fixed time level for Stefan-like problems with nonlinear convective terms and the problems of fluid flows in porous medium under the gravity forces have the form P2.  Below in section 1.2 we construct finite difference or finite element schemes for some model problems. These mesh schemes have the form P1 or P2.  In S1 of this article we study existence and uniqueness of the solutions for problems P1 and P2. The existence of the solutions for both problems are proved using rather standard technique. The proof of the uniqueness of the solution for the problem P2 is based on the comparison results of [8], which play the crucial role also in the analysis of the convergence for iterative methods of relaxation types.  Iterative methods for P1 and P2 are constructed and studied in S2 and S3. We pay special attention to iterative methods constructed on the basis of domain decomposition technique, both with overlapping and non-overlapping subdomains. In S2 we unit in one class block coordinate relaxation methods, multisplitting methods, alternative Schwarz methods and study their convergence for P1, P2 with M -matricies A and B. Splitting iterative methods and their application to domain decomposition methods with nonoverlapping subdomains are studied for the problem P1 with positive definite matrix A in S3. The main results here are the rate of convergence and theoretically optimal choice of iterative parameters.  The article ends with the Appendix, in which some results on the monotone operators, M -matricies and convex functions theories are collected.  The iterative methods based on the domain decomposition are thoroughly studied for linear equations, see the books [15],[16], survey articles [2], [10] and proceedings of conferences on DDM. A few articles deal with the Schwarz alternating iterative methods for some classes of variational inequalities, which can be written in the form P1 (or its generalization for nonlinear A), namely [4], [5], [11], [12], [18], [19], [20], [21]. Moreover, as a rule, convergence results are proved if starting from specially choosen initial guess (super- or subsolution for the studied problem according to terminology of this article). More general results on the convergence of iterative methods for P1 as well as several results for problem P2 are studied in [8].  170    The splitting algorithms for finding a zero of the sum of two maximal monotone operators were studied in [13], [3]. Some error bounds for these methods were obtained in the case when A is single-valued monotone and Lipschitz-continuous operator. Better error bounds were derived in [6] for the case of linear self-adjoint operator A and in [9] for the case of nonlinear operator A. .  Two-lewel iterative method with positive definite, non self-adjoint, preconditioner for P1 where C = ∂IK (∂IK is the subdifferential of indicator function for a convex set K ) was studied in [17]. It was shown that this method can be treated as the partial case of splitting method for sum of two maximal monotone operators and its convergence was proved. We refer to [17] also for bibliograthy of the papers concerning the splitting methods for variational inequalities.  1.2. Several examples of variational inequalities (VI’s) and their mesh approximations  Let Ω be a polygonal domain in R2 with boundary ∂Ω. By H1(Ω) we denote the Sobolev space of the functions u ∈ L2(Ω) such that ∂u ∂xi ∈ L2(Ω) ∀i. We equip this space by the inner product (u, v) =∫  Ω (uv + ∇u∇v)dx and the corresponding norm ||u||H1 =  √ (u, u).  The functions from H1(Ω) which traces in the boundary ∂Ω are equal to zero form the space H10 (Ω).  Example 1. Let K = {u ∈ H10 (Ω) : u(x) ≥ 0 a.e. in Ω} be the convex closed set in H10 (Ω) . We look for the following VI which is traditionally called by the obstacle problem: find u ∈ K such that  ∫  Ω  ∇u∇(v − u)dx ≥ ∫  Ω  f(v − u)dx ∀v ∈ K, (3)  where f ∈ L2(Ω) is known right-hand side. The pointwise form of (3) is the following one:  (−∆u− f)(x) ≥ 0; u(x) ≥ 0; (∆u+ f)(x)u(x) = 0 a.e. in Ω.  The domain Ω is divided into two parts, namely, the coincidence set Ω0 = {x ∈ Ω : u(x) = 0} and the set Ω+ = {x ∈ Ω : u(x) > 0}, where the equation −∆u(x) = f(x) is valid. The unknown boundary ∂Ω0 \ ∂Ω is free boundary.  171    Let Th = {δi}i be triangulation of the polygonal domain Ω ∈ R2, hi be the diameter of δi and h = maxi hi. We recall that in the case of triangle elements δi triangulation Th is one of accute type if the following assumption is valid: the orthogonal projection of any vertice of δi to the opposite side belongs to the closure of this side, or in other words, all the angles of triangles δi are less or equal to π/2.  We denote by ωh = {xi}Ni=1 the set of all internal for Ω verticies of triangles δ ∈ Th, N = cardωh and use the following notations for the spaces of polinomials:  P1 = {p = ∑  0≤i+j≤1  cijx i 1x  j 2}; Q1 = {p =  ∑  0≤i,j≤1  cijx i 1x  j 2}, ci,j ∈ R1.  Let us denote by Vh = {uh ∈ H1(Ω) : uh ∈ P1 ∀δ ∈ Th}, V 0h = {uh ∈ Vh : uh(x) = 0 ∀x ∈ ∂Ω}, Kh = {uh ∈ V 0h : uh(x) ≥ 0 ∀x ∈ Ω}. We mark that in the case of P1 - interpolation the constraint V 0h 3 uh(x) ≥ 0 ∀x ∈ Ω is equivalent to the constraints for the nodal values of uh : uh(xi) ≥ 0 for all nodes xi ∈ ωh.  The mesh scheme approximating the obstacle problem (3) is the following finite-dimensional VI:  uh ∈ Kh : ∫  Ω  ∇uh∇(vh − uh)dx ≥ ∫  Ω  f(x)(vh − uh)dx ∀vh ∈ Kh. (4)  Let us put in the correspondence to vh ∈ V 0h the vector v ∈ RN with coordinates vi = vh(xi), xi ∈ ωh, and use the notation v ⇔ vh. Let further K = {u ∈ RN : ui ≥ 0} and (., .) be the usual inner product of RN . We define matrix A and vector f by the following equalities:  (Au, v) =  ∫  Ω  ∇uh(x)∇vh(x)dx, (f, v) = ∫  Ω  f(x)vh(x)dx,  where u⇔ uh, v ⇔ vh. Then we can write (4) as the variational inequality in RN :  u ∈ K : (Au, v − u) ≥ (f, v − u) ∀v ∈ K (5)  or in the form of complementarity problem:  (Au− f)i ≥ 0, ui ≥ 0, (Au− f)iui = 0 ∀i.  172    Let us note that we use the notations u, v, f for the vectors while above we used these notations for the functions. It will not lead to the ambiguity because we study in this article only finite-dimensional problems.  It is easy to check that the rigidity matrix A is symmetric and positive definite. Moreover, if the triangulation is of accute type, then A is M-matrix:  aij ≤ 0, i 6= j and A−1 À 0, i.e. all entries of A−1 are non-negative,  and A has weak diagonal dominance:  aii ≥ ∑  j 6=i  |aij | ∀i and aii > ∑  j 6=i  |aij | for at least one i.  Example 2. Let now φ(x), ψ(x) ∈ C(Ω) and φ(x) ≤ ψ(x) ∀x ∈ Ω; φ(x) ≤ 0 ≤ ψ(x) ∀x ∈ ∂Ω. We introduce the closed convex set K = {u ∈ H10 (Ω) : φ(x) ≤ u(x) ≤ ψ(x) a.e. in Ω} and consider the VI of the form ∫  Ω  ∇u∇(v−u)dx+ ∫  Ω  ā∇u(v−u)dx ≥ ∫  Ω  f(v−u)dx ∀v ∈ K (6)  with given vector ā. This problem is so-called two-sided obstacle problem.  As in the previous example we can divide the domain Ω into the subsets where u(x) = ψ(x) or u(x) = φ(x) and one where the solution u(x) satisfies the equation −∆u(x) + ā∇u(x) = f(x) .  To approximate the problem (6) we use the accute-type triangulation of the domain Ω and the same finite element space Vh as in the previous example. Moreover for approximation of the convective term ā∇u we use so-called up-wind technique. To this end we construct, firstly, one more decomposition of the domain Ω into set Eh of non-intersected subdomains (elements) ei, i = 1, ..., n. Further we denote by xδ the barycenter of δ ∈ Th, the numbers of its verticies being i, j, k. Let also elδ, l = i, j, k, be the quadrangles which split δ into three parts with equal squares and which are bounded by lines connecting xδ with midpoints of the sides of δ.  We put ei = ⋃  δ∈Th  eiδ, n⋃ i=1  ei = Ω and denote by χ i h the characteristic  173    function of the domain ei and by Lh = {uh = n∑ i=1  uiχ i h, ui ∈ R}  the space of piecewise constant functions. Let Ai be the union of all elements δ ∈ Th such that δ has the node with number i and such that vector ā with origine in the node with number i intersects δ.  Let us define the operators Dh, πh from Vh to Lh as follows:  Dhuh = n∑  i=1  ā∇uhχih, πhuh = n∑  i=1  uhχ i h.  Now let Kh = {uh ∈ Vh : φh(x) ≤ uh(x) ≤ ψh(x) ∀x ∈ Ω}, where φh(x) and ψh(x) are the Vh -approximations of φ, ψ.  The mesh scheme approximating the two-sided obstacle problem (6) is the following finite-dimensional VI: find uh ∈ Kh such that for allvh ∈ Kh ∫  Ω  ∇uh∇(vh−uh)dx− ∫  Ω  πhuhD h(vh−uh)dx ≥  ∫  Ω  f(x)(vh−uh)dx. (7)  We define vector f as in Example 1 and matrix A by the following equality:  (Au, v) =  ∫  Ω  ∇uh(x)∇vh(x)dx− ∫  Ω  πhuhD hvhdx, u⇔ uh, v ⇔ vh.  Moreover we put K = {u ∈ RN : φi ≤ ui ≤ ψi} where φi = φ(xi) = φi(xi) and similar for ψi.  Now the mesh scheme (7) can be written in algebraic form (5) with M -matrix A, which has weak diagonal dominance as well as its transpose At has.  Example 3. Find u ∈ H10 (Ω) such that ∫  Ω  ∇u∇(v−u)dx+ ∫  Ω  (|v|−|u|)dx ≥ ∫  Ω  f(v−u)dx ∀v ∈ H10 (Ω). (8)  In contrast with (3) and (6) now we have no set of constraints but the non-differentiable functional φ(u) =  ∫ Ω |u(x)|dx. As above the  domain Ω can be divided into the subsets  Ω+ = {x ∈ Ω : u(x) > 0}; Ω− = {x ∈ Ω : u(x) < 0};  174    Ω0 = {x ∈ Ω : u(x) = 0} and the solution of (8) satisfies the equations in Ω+ and Ω− : −∆u(x) = f(x)− 1 ∀x ∈ Ω+,−∆u(x) = f(x) + 1 ∀x ∈ Ω−. To construct the mesh scheme for (8) we keep all the assumptions  and the notations of Example 1 and approximate the nondifferentiable functional  ∫ Ω |u(x)|dx by using the simplest quadrature  rule: ∫  Ω  |uh(x)|dx ∼= ∑  δ∈Th  Sδ(|uh|) =: F (u).  Here Sδ(|uh|) = mesδ/3 ∑3  i=1 |uh(ai)|, {ai}3i=1 are the verticies of δ. Using of the quadrature formulat leads to the separable function F (u) =  ∑N i=1 Fi(ui) and the mesh VI has the form:  u ∈ RN : (Au, v − u) + F (v)− F (u) ≥ (f, v − u) ∀v ∈ RN . (9)  Let IK(x) = {0, x ∈ K; +∞, x /∈ K} be the indicator function of K for Examples 1,2 and C = ∂IK(x) be its subdifferential. For the case of Example 3 we put C = ∂F. In all cases C is maximal monotone operator and A ∈ L(RN ;RN ), thus finite-dimensional VI’s (5) and (9) are the partial cases of problem P1.  Example 4. We consider the variational inequality which can be viewed as the simplified model for the problem of fluid flow in porous medium under the gravity forces (acting in x1 -direction).  Let Ω = (0, 1) × (0, 1) be the unit square, V = H1(Ω), V z = {u ∈ V : u(x) = z(x) in ∂Ω}, V 0 = H10 (Ω), where z(x) ≥ 0 is given continuous function. We look for the problem:  find u ∈ V z, γ ∈ L∞(Ω) such that ∫  Ω  k(x)∇u∇vdx+ ∫  Ω  γx1vdx = 0 ∀v ∈ V 0;  gamma(x) ∈ H(u(x)) for a.a. x ∈ Ω. (10) Here H(.) is maximal monotone Heavyside graph, k(x) ∈ C(Ω), k(x) ≥ k0 > 0 ∀x ∈ Ω.  Formally (or in distributions) we can write the “equations” that (u, γ) satisfy:  −div(k(x)∇u(x)) + ∂γ/∂x1 = 0; γ(x) ∈ H(u(x)).  175    We approximate the problem (10) by finite difference scheme on uniform mesh of size h. Difference scheme we treat as the finite element scheme on the square elements when using simplest quadrature rules to approximate the integrals over the finite elements. More precisely, let Th be “triangulation” of Ω in squares δ of dimensions h × h , Vh = {uh(x) ∈ H1(Ω) : uh(x) ∈ Q1 ∀δ ∈ Th}, V zh = {uh(x) ∈ Vh : uh(x) = zh(x), x ∈ ∂Ω}, zh is the interpolant from Vh of function z.  We use the quadrature rules:  ∫  δ  uh(x)dx ≈ Sδ(uh) = 1/4h2 4∑  i=1  uh(dj),  ∫  δ  uh(x)dx ≈ Eδ(uh) = 1  2 h2(uh(d3) + uh(d4)),  where dj are verticies of δ ∈ Th : d1 = (x1, x2), d2 = (x1, x2 + h), d3 = (x1 + h, x2), d4 = (x1 + h, x2 + h). Let further Sh(uh) =∑  δ∈Ω Sδ(uh), Eh(uh) = ∑  δ∈ΩEδ(uh). Below we use the following notations for the sets of mesh points,  mesh functions and their derivatives (difference quotients):  ω̄ = {(xi1, x j 2) : x  i 1 = ih, 0 ≤ i ≤ N1; x  j 2 = jh, 0 ≤ j ≤ N1; hN1 = 1},  ω = {x ∈ ω̄ : 0 < x1 < 1, 0 < x2 < 1}, ∂ω = ∂Ω ∩ ω̄, ∂̄1uh = h  −1 1 (uh(x1, x2)− uh(x1 − h1, x2)),  ∂1uh = h −1 1 (uh(x1 + h1, x2)− uh(x1, x2)).  The finite difference scheme for (10) can be written in the following implicit form:  find uh ∈ Vh, γh(x) ∈ Vh such that  Sh(k(x)∇uh∇vh) + Eh(∂γh/∂x1vh) = 0 ∀vh ∈ V 0h ;  γh(x) ∈ Hh(u(x)) ∀x ∈ ω (11) or in the explicit form:  −1/2 2∑  i=1  ∂̄i(k(x)∂iuh)(x) + ∂i(k(x)∂̄iuh)(x) + ∂̄1γh(x) = 0 ∀x ∈ ω;  176    uh(x) = zh(x) ∀x ∈ ∂ω; γh(x) ∈ Hh(u(x)) ∀x ∈ ω̄. Let us put in the correspondence to the function from Vh the  vector from RN , N = (N1 − 1)2 of its internal nodal values: N = cardω and define matricies A,B of dimensions N ×N :  (Au, v) = Sh(k(x)∇uh∇vh), (Bu, v) = Eh(∂uh/∂x1vh)  for all RN 3 u ⇔ uh ∈ V 0h , RN 3 v ⇔ vh ∈ V 0h . Further we define the vector f ∈ RN as follows. Let wh(x) be the function from Vh, which is equal to zh(x) on the boundary ∂Ω and to zero in internal nodes of triangulation. We define also the function α(x) ∈ Vh which equal to 1 in the nodes on ∂Ω, where zh(x) > 0 and to zero in other nodes. Then  (f, v) = Sh(k(x)∇wh∇vh) + Eh(∂αh/∂x1vh).  Let now Cu = (H(u1), H(u2), ..., H(uN )) t with Heavyside graph  H(.). In these notations the mesh scheme (11) can be written in the form of problem P2 with diagonal maximal monotone operator C and M -matricies A and B.  From definition we deduce the following properties of matricies A,B :  A is symmetric and positive definite M -matrix with weak diagonal dominance;  B is two-diagonal M -matrix with weak diagonal dominance (both in rows and in columns);  A−αB is positive definite for sufficiently small α, depending on the minxk(x) = k0 > 0;  aij − αbij ≤ 0 for i 6= j and ∑N  j (aji − αbji) ≥ 0 ∀i. These properties ensure, in particulary, that A−αB is M -matrix.  S1. Existence and uniqueness of the solutions, comparison theorems  1.1 Problem P1  We consider the nonlinear equation in RN :  Au+ Cu 3 f. (12)  177    If matrix A is positive definite, then operator A + C is unformly maximal monotone and the problem (12) has unique solution.  Now we study (12) under the following assumptions:  A is M-matrix, (13)  C is diagonal maximal monotone operator. (14)  Lemma 1.1. Let the assumptions (13), (14) be fulfilled and Auk + Cuk 3 fk, k = 1, 2; f1 À f2. Then  0¿ u1 − u2 ¿ A−1(f1 − f2).  Proof. We denote by J = {1, 2, ..., N} the set of indicies and suppose that J− = {i ∈ J : u1i < u2i } 6= ∅. As v1i < v2i for any vki ∈ ci(uki ) if i ∈ J−, then  (A(u1 − u2))i = (f1 − f2)i − (v1 − v2)i ≥ 0 ∀i ∈ J−;  (u1 − u2)i ≥ 0 ∀i ∈ J \ J−. A being M -matrix the matrix of the above system is also M -matrix and as a consequence we derive the inequality u1 À u2.  Let now z be the solution of the equation Az = f 1 − f2. Then z À 0. We denote by J0 = {i ∈ J : u1i = u2i }. It is clear that zi ≥ u1i − u2i = 0 ∀i ∈ J0. On the other hand for i ∈ J \ J0 we have v1i ≥ v2i ∀vki ∈ ci(uki ). Then  (A(u1−u2−z))i = (v1−v2)i ≤ 0 ∀i ∈ J\J0; (u1−u2−z)i ≤ 0 ∀i ∈ J0  and again from the property of M -matrix the inequality u1−u2−z ¿ 0 follows.  Theorem 1.1. Let the assumptions(13), (14) be fulfilled. Then the equation (12) has the unique solution for any f ∈ RN . If u1, u2 correspond to the right-hand sides f 1, f2 then  u1 − u2 ¿ A−1(f1 − f2)+. (15)  Proof. Let firstly the matrix A be symmetric M -matrix. Then it is positive definite and A + C is maximal strongly monotone and coercive operator. It ensures the existence of the unique solution for the equation (12).  178    To derive the estimate (15) we denote by f ∈ RN the vector with coordinates fi = max(f  1 i , f  2 i ) and by u the solution of (12) with this  right-hand side. Then the desired estimate immediately follows from Lemma 1.1:  u1−u2 = u1−u+u−u2 ¿ u−u2 ¿ A−1(f −f2) = A−1(f1−f2)+.  Let now A be any M -matrix. We split it as follows: A = D − L − U,D = diag{a11, a22, ..., aNN} = D∗, where L,U are strongly lower and upper triangle submatrices of A . The equation  Du+ Cu 3 (L+ U)w + f  has the unique solution u(w) for any w ∈ RN , i.e. the operator G : RN → RN , u(w) = Gw, is defined. From the inequalities  u(w1)− u(w2)¿ D−1(L+ U)(w1 − w2)+,  u(w2)− u(w1)¿ D−1(L+ U)(w2 − w1)+  we deduce the estimate  |Gw1 −Gw2| ¿ D−1(L+ U)|w1 − w2|.  As A = D−L−U is the regular splitting of the matrix A , then the spectral radius ρ(D−1(L+U)) < 1 . This ensures the existence of the unique fixed point for G, which is the solution of (12).  The inequality (15) in the case of general M -matrix A is proved by the same way as in the symmetric case.  2. Problem P2  We consider the problem: find (u, γ) ∈ RN ×RN such that  Au+Bγ = f ; γ ∈ Cu. (16)  Theorem 1.2. Let A,B be M -matricies and C be diagonal maximal monotone operator. Let also the sub- and supersolutions for the problem (16) exist:  (u, γ) ∈ RN ×RN : Au+Bγ ≤ f ; γ ∈ Cu, (17)  (u, γ) ∈ RN ×RN : Au+Bγ ≥ f ; γ ∈ Cu. (18)  179    Then the equation (16) has a solution (u, γ) for any f ∈ RN . Proof. Let B = B0−B1, where B0 = diag(b11, b22, ..., bNN ) and  B1 À 0 and let also A = A0 − A1 be a regular splitting of matrix A : ∃A−10 À 0, A1 À 0. We look for the auxiliary problem:  A0u+B0γ = A1v +B1η + f ; γ ∈ Cu. (19)  Due to Theorem 1.1 for any (v, η) ∈ RN×RN there exists the unique solution (u(v, η), γ(v, η)) ∈ RN × RN of the problem (19). It means that the single-valued operator G : (v, η) → (u, γ) is defined on RN ×RN . A fixed point of this operator, if it exists, is just a solution for the problem (16). To prove the existence of a fixed point of G we use the Kantorovich lemma and to this end we are to check the properties of G.  Firstly we prove that operator G is monotone:  (v1, η1)À (v2, η2)⇒ G(v1, η1)À G(v2, η2).  In what follows we denote by uk ≡ u(vk, ηk), γk ≡ γ(vk, ηk) the components of G(vk, ηk). The inequality u1 À u2 follows from Lemma 1.1. Moreover, from monotonocity of C we have γ1i ≥ γ2i for all i ∈ J = {1, 2, ...N} such that u1i > u2i . Let now i ∈ J : u1i = u2i . Then for this index we have:  b0ii(γ 1−γ2)i = f1i −f2i +(B1(η1−η2))i+(A1(v1−v2))i−(A0(u1−u2))i.  (20) It is easy to see that all terms in the right-hand side of (20) are nonnegative. In particulary, (A0(u  1 − u2))i = ∑  j 6=i aij(u 1 − u2)j ≤ 0,  because of the inequalities u1j − u2j ≥ 0 ∀j, aij ≤ 0, i 6= j. Thus, γ1i ≥ γ2i also for i ∈ J such that u1i = u2i and the monotonicity of G is proved.  Now, it is easy to check that (u, γ), (u, γ) are the sub- and supersolutions for the equation (u, γ) = G(u, γ) :  G(u, γ)À (u, γ), G(u, γ)¿ (u, γ).  It means that operator G maps the ordered interval < (u, γ), (u, γ) > into itself. It rests to prove the continuity of G on this interval.  Let (vk, ηk)→ (v∗, η∗) in RN ×RN as k →∞ and let (u∗, γ∗) = G(v∗, η∗). Then from the inequality (4) we derive:  |uk − u∗| ¿ A−10 (A1|vk − v∗|+B1|ηk − η∗|)→ 0 if k →∞  180    and from (20) the foolowing statement:  γk−γ∗ = B−10 (B1(ηk−η∗)+A1(vk−v∗)−A0(uk−u∗))→ 0 if k →∞.  Thus, operator G is continouous and monotone on the ordered interval < (u, γ), (u, γ) > and maps it into itself. From Kantorovich lemma the existence of its fixed point follows.  Theorem 1.3. Let A,B be weakly diagonally dominant in columns M -matricies, C be diagonal maximal monotone operator.  Let also one of the following assumptions holds: (a) either A or B is strictly diagonally dominant in columns or (b) C is either continuous monotone or strictly maximal  monotone operator or (c) ∃α > 0 : A − αB is weakly diagonally dominant in columns  M -matrix. Then the inequality f1 À f2 implies the inequalities u1 À u2,  γ1 À γ2. Proof. (a) Let us introduce the subsets J− = {i ∈ J : u1i <  u2i }; L− = {i ∈ I : γ1i < γ2i } and define the vector η as follows:  ηi = {1 for i ∈ J− ∪ L−; 0 otherwise }. (21)  We note that u1i ≤ u2i , γ1i ≤ γ2i for i ∈ J− ∪ L−, while u1i ≥ u2i , γ  1 i ≥ γ2i for i ∈ J \ (J− ∪ L−) ≡ K. Moreover, the following  statements are valid:  (Atη)i ≤ 0, (Btη)i ≤ 0, i ∈ K;  (Atη)i ≥ 0, (Btη)i ≥ 0, i ∈ J− ∪ L−. (22) In fact, (Btη)i =  ∑ j 6=i bjiηj ≤ 0 for i ∈ K, because of inequalities  ηj ≥ 0 and bji ≤ 0, j 6= i. On the other hand, if i ∈ J− ∪ L− then (Btη)i = bii +  ∑ j 6=i bjiηj ≥  ∑N j=1 bji ≥ 0. The statements for vector  Aη are proved similarly. Now we prove that u1 À u2. We argue ad absurdum. Namely,  let J− 6= ∅ and vector η be defined by (21). Multiplying by η the equation (16), written for f 1 and f2, we derive the following inequality:  (u1 − u2, Atη) + (γ1 − γ2, Btη) = (f1 − f2, η) ≥ 0. (23)  181    From (22) we deduce that (u1 − u2, Atη) ≤ 0, (γ1 − γ2, Btη) ≤ 0. Further, for i ∈ J− due to the strong diagonal dominance  property for At and due the inequalities ηi ≤ 1 ∀i we have (Atη)i = aii +  ∑ j 6=i ajiηj ≥  ∑N j=1 aji > 0. It means that (u  1 − u2, Atη) < 0 and we get the contradiction to the unequality (23). Thus, J− = ∅ and u1 À u2.  Let us prove now that γ1 ≥ γ2. We suppose that the set L− is nonempty. Due to the inequality u1 À u2 the equality u1i = u2i for i ∈ L− holds. Then (A(u1 − u2))i ≥ 0 for all i ∈ L−. It means that the right-hand side of the equality  BL−L−(γ 1 − γ2)L− = (f1 − f2)J− − (A(u1 − u2))L− −BJ\L−L−(γ1 − γ2)J\L−  is non-negative. The matrix BL−L− being the submatrix of M matrix B is also M -matrix, so, (γ1− γ2)L− ≥ 0. This contradiction proves that L− = ∅.  The inverse operator C−1 is maximal monotone and diagonal. Thus, we can change the roles of the matricies A and B and derive the second assertion of (a).  (b) Let firstly C be continuous operator. Then using the same notation for the subset J− as before we define the vector η as follows:  ηi = {1, for i ∈ J−; 0, for i ∈ J \ J−}. Then (Atη)i ≤ 0, (Btη)i ≤ 0, i ∈ J \ J−; (Atη)i ≥ 0, (Btη)i ≥ 0, i ∈ J−.  Proceeding as in (a) we derive the inequality (23). Let J− is not empty, then the matrix AtJ−J− as M -matrix with weak diagonal dominance has at least one row i∗ with strict diagonal dominance: (Atη)i∗ =  ∑ j∈J−  aji∗ > 0. Due to this fact (u 1 − u2, Atη) ≤ (u1 −  u2)i∗(A tη)i∗ < 0 while (γ  1 − γ2, Btη) ≤ 0. We get the contradiction with the inequality (23) which proves the statement.  Let now C be multivalued, but strictly monotone operator. Then C−1 is the maximal monotone and single-valued, i.e. continuous. Changing the roles of matricies A and B we come to the previous situation and as consequence derive the second statement of (b).  (c) We denote by Ã = A − αB and by C̃u = αu + Cu. Then C̃ becomes strictly maximal monotone operator while the matrix Ã keeps all the properties of A. Now the result of (c) follows from (b).  182    S2. Coordinate relaxation methods  2.1. Coordinate relaxation methods for problem P1  The comparison Theorem 1.1 together with well-known results for convergence of iterative methods for the linear equation Au = f provide the convergence results of these methods for nonlinear problem P1:  Au+ Cu 3 f (24) with M -matrix A and diagonal maximal monotone operator C. Below we cite some of them.  Theorem 2.1. Let A = D−S be a regular splitting of M -matrix A. Then the iterative method for solving (24)  Duk+1 + Cuk+1 3 Suk + f, k = 0, 1, 2, ... (25)  converges starting from any initial guess u0 ∈ RN . Proof. From the estimate of Theorem 1.1 applied to equation  (25) we deduce the following estimate for the difference uk − u of k − th iteration and the solution for (24):  |uk − u| ¿ (D−1S)k|u0 − u|.  Then |uk − u| → 0 as k →∞ because ρ(D−1S) < 1 . Corollary 2.1. Let A be M-matrix and C be diagonal maximal  monotone operator, A = D̃ − L̃− Ũ , (26)  where D̃ is block diagonal submatrix of A and L̃, Ũ are strongly lower and upper triangle submatrices of A . Then the iterations of block variant of Jacoby method:  u0 ∈ RN ; D̃uk+1 + Cuk+1 3 (L̃+ Ũ)uk + f, k = 0, 1, 2, ... (27)  and block variant of Gauss-Seidel method:  u0 ∈ RN ; (D̃ − L̃)uk+1 + Cuk+1 3 Ũuk + f, k = 0, 1, 2, ... (28)  converge to the unique solution of the equation (24). For proving this corollary we remark that methods (27) and (28)  are constructed using the regular splittings of matrix A.  183    Theorem 2.2. Let A = Di−Si with M -matricies Di and Si À 0 be regular splittings of A, i = 1, 2, ...p. Let also Ei À 0 be diagonal matricies, such that  ∑p i=1Ei = I, (I is the unit matrix). Then the  multisplitting method for solving the problem (24):  Div i + Cvi 3 Siuk + f, i = 1, 2, ..., p; uk+1 =  p∑  i=1  Eivi; k = 0, 1, ...  (29) converges to the unique solution of the equation (24) starting from any initial guess u0 ∈ RN .  Proof. From the estimates  |vi − u| ¿ D−1i Si|uk − u|, i = 1, 2, ..., p,  we derive the inequality  |uk+1 − u| ¿ L|uk − u|, L ≡ p∑  i=1  EiD −1 i Si.  It is known [1] that ρ(L) < 1, so the method (29) converges.  Below we study the convergence for the iterative method with variable splittings of the matrix A. Firstly we derive the estimate for spectral radius of the product of matricies:  Theorem 2.3. Let A be a weakly diagonally dominant M matrix:  aii > 0, aij ≤ 0 for i 6= j; N∑  j=1  aij ≥ 0 ∀i; ∃i∗ : N∑  j=1  ai∗j > 0; (30)  Let further A = Dk−Sk be two regular splittings of A,Rk = D−1k Sk. Then the spectral radius ρ(R2R1) < 1.  Before proving this theorem let us prove several auxiliary results. Lemma 2.1. Let A = D − S be a regular splitting of A. If  e = (1, 1, ..., 1)t ∈ RN and Dz = Se then z ¿ e and the set J(z) ≡ {i : zi < 1} is nonempty.  Proof. From diagonal dominance of A the inequality DeÀ Se = Dz follows, so eÀ z. For i = i∗ we have  ai∗i∗zi∗ = − ∑  j∈J1  ai∗jzj − ∑  j∈J2  ai∗jej , (31)  184    with disjoint sets J1 ∪ J2 ∪ {i∗} = {1, 2, ..., N}. Using (31) and the inequality z ¿ e we deduce  ai∗i∗zi∗ ≤ − ∑  j 6=i∗  ai∗j < ai∗i∗ .  It means that J(z) 3 i∗. Lemma 2.2. Let A = Dk − Sk be two regular splittings of M  matrix A, Rk = D −1 k Sk. Let also y = R1z and z À y À 0. Then  y À R2y ≡ v. Proof. From the equalities D2v = S2y,D1y = S1z the following  statement follows  D2y = (D2 −D1)y + S1z À (D2 −D1)y + S1y  = (S2 − S1)y + S1y = S2y = D2v which implies y À v.  Lemma 2.3. Let y ¿ e, J(y) 6= ∅, J(y) 6= {1, 2, ...N} ≡ J and v = Ry ¿ y. Then J(y) ⊂ J(v) (with strong inclusion.)  Proof. From the assumptions v ¿ y ¿ e we have J(y) ⊆ J(v). Let us suppose that J(v) = J(y). If we denote by L = {i : vi = 1}, then for any i ∈ L the corresponding equation of the system Dv = Sy can be written as follows:  ∑  j∈L  aij + ∑  j∈J1(i)  aijvj + ∑  j∈J2(i)  aijyj = 0, (32)  where disjoint subsets J1(i), J2(i), L compose the whole set of indicies J and vj < 1 for j ∈ J1(i), yj < 1 for j ∈ J2(i). The submatrix ALL = (aij)i∈L,j∈L of matrix A is also M -matrix with weak diagonal dominance. Thus, it has at least one row with the strong diagonal dominance, let it be row with number i = i∗ ∈ L. But from (32) for i = i∗ we derive the inequality  0 = ∑  j∈L  ai∗j + ∑  j∈J1(i∗)  ai∗jvj + ∑  j∈J2(i∗)  ai∗jyj ≥ N∑  j=1  ai∗j  that contradicts to our supposition about the number i∗.  185    Proof of the Theorem 2.3. Lemma 2.1 ensures that R1e¿ e, so R2R1e ¿ R1e due to Lemma 2.2. Proceeding by induction we derive  eÀ R1eÀ R2R1eÀ R1R2R1eÀ ..... From Lemma 2.1 we have J(R1e) 6= ∅ and now from Lemma 2.3 it follows:  J(R1e) ⊂ J(R2R1e) ⊂ J(R1R2R1e) ⊂ ... with strong inclusions. Obviously, for some m ≤ [N/2] + 1 the inequality ((R2R1)  me)i < 1 holds for all i ∈ J. It means that ||(R2R1)m||∞ < 1⇒ ρ(R2R1) < 1.  Remark 2.1. The result of this theorem can be easily extended by induction to the case of any finite number of operators Rk.  As the straightforward consequence of the Theorems 1.1, 2.3 we have the following result:  Theorem 2.4. Let A be a N × N weakly diagonally dominant M -matrix, A = Dk − Sk, k = 1, 2, ...p, be its regular splittings and C be maximal monotone diagonal operator in RN . Then the iterative method  Dj+1u k+1+Cuk+1 3 Sj+1uk+f, k = 0, 1, 2, ...; j = k− [k/p]p (33)  converges starting from any initial guess u0 ∈ RN .  2.2. Coordinate relaxation methods for problem P2  In this section we study the iterative methods to solve the problem P2:  Au+Bγ = f ; γ ∈ Cu, (34) with M -matricies A and B and diagonal maximal monotone operator C.  Let A = Al0−Al1, B = Bl0−Bl1 be for l = 1, 2, ..., p the splittings of matricies A,B such that Al0, B  l 0 are M -matricies, A  l 1 À 0, Bl1 À  0. Let also El À 0 be diagonal matricies and ∑p  l=1El = I. We consider the multisplitting method for solving the problem (34):  Al0v k+1 l +B  l 0η  k+1 l = A  l 1u  k +Bl1γ k + f, ηk+1l ∈ Cv  k+1 l , l = 1, 2, ..., p;  uk+1 =  p∑  l=1  Elv k+1 l ; γ  k+1 =  p∑  l=1  Elη k+1 l k = 0, 1, 2, ... (35)  186    Theorem 2.6. Let A and B be weakly diagonally dominant in columns M -matricies, C be diagonal maximal monotone operator, there exist the subsolution (u, γ) and supersolution (u, γ) for the problem (34). Let further one of the following assumptions be satisfied:  1) either A or B is strictly diagonally dominant in columns or (b) C is either continuous monotone or strictly maximal  monotone operator or 3) ∃α > 0 : A − αB is weakly diagonally dominant in columns  M -matrix and the splittings of matricies A and B are consistent: ∀l ∃αl > 0 such that Al0 − αlBl0 are weakly diagonally dominant in columns M -matricies.  Then: a) iterative method (35) is correctly defined for any initial guess  (u0, γ0) from ordered interval < (u, γ), (u, γ) >,  b) if (u0, γ0) = (u, γ) (or (u0, γ0) = (u, γ)) then the sequence  {(uk, γk)} converges monotonically decreasing (correspondingly, increasing) to the solution (u∗, γ∗) of the problem (34).  Proof. Firstly, we note that matricies Al0, B l 0, l = 1, 2, ..., p keep  all properties of A and B which are sufficient to use the results of Theorem 1.2 for existence of a solution (vkl , η  k l ), ∀k, ∀l and the  results of Theorem 1.3 for its uniqueness. In fact, we need only to observe that (u, γ) and (u, γ) are the sub- and supersolutions for all equations in (35). To this end we proceed by induction supposing that (uk, γk) ∈< (u, γ), (u, γ) > . Then  Al0u+B l 0γ À Al1u+Bl1γ+f À Al1uk+Bl1γk+f = Al0vk+1l +B  l 0η  k+1 l  and from comparison result of Theorem 1.3 it follows that for all l the inequality (vk+1l , η  k+1 l )¿ (u, γ) holds, so, it is for (uk+1, γk+1).  Similarly we prove that (uk+1, γk+1)À (u, γ). Now we prove the convergence of iterations. Let us suppose that  (u0, γ0) = (u, γ). To study the convergence of the method (35) we consider along with it the Jacoby method. Namely, let A = A0 − A1, B = B0 − B1, where A0 = diag(a11, a22, ..., aNN ), B0 =  187    diag(b11, b22, ..., bNN ) and we look for the iterative method:  A0w k+1+B0β  k+1 = A1w k+B1β  k+f ; βk+1 ∈ Cwk+1, k = 0, 1, 2, ... (36)  We will prove for all k and l the following statements:  (a)(vk+1l , η k+1 l )¿ (v  k l , η  k l ), (u  k+1, γk+1)¿ (uk, γk),  (wk+1, βk+1)¿ (wk, βk); (b)(u∗, γ∗)¿ (vkl , ηkl )¿ (wk, βk), (u∗, γ∗)¿ (uk, γk)¿ (wk, βk);  (c)(wk, βk) ↓ (u∗, γ∗) when k →∞. We prove the inequalities (a), (b) only for (vkl , η  k l ) for fixed l  because all other inequalities in (a), (b) follow from them as the consequences.  (a) We proceed by induction. For k = 0 the statements of (a) follow from the definition of supersolution. Let (a) be valid for some k > 0. From the inequality (uk, γk)¿ (uk−1, γk−1) we derive:  Al0v k+1 l +B  l 0η  k+1 l ¿ A  l 1u  k−1 +Bl1γ k−1 + f = Al0v  k l +B  l 0η  k l ,  ηk+1l ∈ Cv k+1 l , η  k l ∈ Cvkl .  Now from comparison result the inequality (vk+1l , η k+1 l ) ¿ (vkl , ηkl )  immedeately follows. (b) We prove that (u∗, γ∗) ¿ (vkl , ηkl ) ¿ (wk, βk) for fixed l by  induction in k. For k = 0 we have (u∗, γ∗) ¿ (v0l , η0l ) = (w0, β0) = (u, γ). Let now the desired inequalities hold for some k ≥ 0 and let us prove them for k+1. We rewrite the equation from (35) as follows:  A0v k+1 l +B0η  k+1 l = A  l 1u  k+Bl1γ k+f+(A0−Al0)vk+1l +(B0−B  l 0)η  k+1 l .  The matricies A0 −Al0, B0 −Bl0, Al1, Bl1 are non-negative. Using this fact and also the supposition of the induction and the inequalities (a) we derive:  A0v k+1 l +B0η  k+1 l ¿ A  l 1u  k+Bl1γ k+f+(A0−Al0)vkl +(B0−Bl0)ηkl ¿  Al1w k +Bl1β  k + f + (A0−Al0)wk + (B0−Bl0)βk = A1wk +B1βk + f = A0w  k+1 +B0β k+1.  188    Then (vk+1l , η k+1 l ) ¿ (wk+1, βk+1) due to comparison result of  Theorem 1.3, applied for the equation with matricies A0 and B0 . The inequality (u∗, γ∗) ¿ (vk+1l , η  k+1 l ) follows also from corresponding  comparison result because of the inequality  Al0u ∗+Bl0γ  ∗ = Al1u ∗+Bl1γ  ∗+f ¿ Al1uk+Bl1γk+f = Al0vk+1l +B l 0η  k+1 l .  (c) The convergence of (wk, βk) to (u∗, γ∗) is the direct consequence of Theorem 1.1. Namely,  |wk − u∗| ¿ A−10 A1|w0 − u∗| → 0  because ρ(A−10 A1) < 1 and  |βk − γ∗| ¿ B−10 A1|wk−1 − u∗| → 0.  As the sequence {(wk, βk)} is monotonically decreasing, we have the statement (c). The monotone convergence of {(uk, γk)} to (u∗, γ∗) now follows from (b) and (c).  The result on the convergence when starting from the subsolution is proved similar.  Let us now consider the iterative method of block relaxation type with permuting blocks. For simplicity we analyse the case when iterative method is constructed using two splittings of matricies A and B.  Let matricies, nonlinear operator and vectors be partitioned in non-overlapping blocks:  A = (Aij) 3 i,j=1, B = (Bij)  3 i,j=1, C = diag(C1, C2, C3),  I = diag(I1, I2, I3), u = (u1, u2, u3) t.  Here Ik are unit matricies of corresponding dimensions. We introduce some more matricies, namely:  A10 =     A11 A12 0 A21 A22 0 0 0 A33    , A20 =     A11 0 0 0 A22 A23 0 A32 A33    ,  B10 =     B11 B12 0 B21 B22 0 0 0 B33    , B20 =     B11 0 0 0 B22 B23 0 B32 B33    .  189    Let for l = 1, 2 A = Al0 − Al1, B = Bl0 − Bl1 be the splittings of matricies A,B, Al0, B  l 0 are M -matricies, A  l 1 À 0, Bl1 À 0.  We consider the following iterative method:  A10ṽ k+1 +B10 η̃  k+1 = A11u k +B11γ  k + f ; η̃k+1 ∈ Cṽk+1, (vk+1l , η  k+1 l ) = (ṽ  k+1 l , η̃  k+1 l ) for l = 1, 2; (v  k+1 3 , η  k+1 3 ) = (u  k 3 , γ  k 3 ); (37)  A20ũ k+1 +B20 γ̃  k+1 = A21v k+1 +B21η  k+1 + f ; γ̃k+1 ∈ Cũk+1, (uk+11 , γ  k+1 1 ) = (v  k+1 1 , η  k+1 1 ); (u  k+1 l , γ  k+1 l ) = (ũ  k+1 l , γ̃  k+1 l ), for l=2,3.  (38) with initial guess (u0, γ0).  Theorem 2.7. Let the assumptions of Theorem 2.6 be fulfilled. Then:  iterative method (37),(38) is correctly defined for any initial guess (u0, γ0) from ordered interval < (u, γ), (u, γ) >,  if (u0, γ0) = (u, γ) ((u0, γ0) = (u, γ)) then the sequence  {(uk, γk)} converges monotonically decreasing (increasing) to the solution (u∗, γ∗) of the problem (34).  Proof. Proceeding as in Theorem 2.6 we find that < (u, γ) and (u, γ) > are the sub- and supersolutions for both systems of equations in (37) and (38) and prove after that the existence of unique solutions (vk+1, ηk+1), (uk+1, γk+1) for all k when (u0, γ0) ∈< (u, γ), (u, γ) > .  Let (u0, γ0) = (u, γ). We prove that the sequencies {(vk, ηk)}, {(uk, γk)} monotonically decrease and that  (vk+1, ηk+1)¿ (uk, γk)¿ (vk, ηk) for all k ≥ 0. (ṽk+1, η̃k+1)¿ (uk, γk)¿ (ṽk, η̃k) for all k ≥ 0.  To do this we use induction and firstly prove that (vk+1, ηk+1) ¿ (vk, ηk) and (uk+1, γk+1)¿ (uk, γk).  Let us denote by G1 and G2 the nonlinear operators defined by (37), (38):  (ṽk+1, η̃k+1) = G1(u k, γk); (ũk+1, γ̃k+1) = G2(v  k+1, ηk+1).  Let also E1 = diag(I1, I2, 0), E2 = diag(0, I2, I3). Using these notations we can write:  (vk+1, ηk+1) = E1G1(u k, γk) + (I − E1)(uk, γk);  190    (uk+1, γk+1) = E2G2(v k+1, ηk+1)+(I−E2)(vk+1, ηk+1) ≡ G(uk, γk).  The operator G inherits the properties of monotonocity and continuity of G1, G2. Moreover (u  1, γ1)¿ (u0, γ0). This implies the inequality (uk+1, γk+1)¿ (uk, γk) for all k.  Similarly,  (uk, γk) = E2G2(v k, ηk) + (I − E2)(vk, ηk);  (vk+1, ηk+1) = E1G1(u k, γk) + (I − E1)(uk, γk) ≡ P (vk, ηk)  with monotone and continuous operator P. As (v1, η1)¿ (v0, η0) ≡ (u0, γ0), we derive the inequality (vk+1, ηk+1) ¿ (vk, ηk) from monotonicity of P.  For proving the inequality (uk, γk)¿ (vk, ηk) we note, first of all, that (uk1 , γ  k 1 ) = (v  k 1 , η  k 1 ) by definition and (v  k 3 , η  k 3 ) = (u  k−1 3 , γ  k−1 3 )À  (uk3 , γ k 3 ) by induction. To prove the corresponding inequality for  second components of vectors we need to use special form of splittings for A and B. From (38) we derive  A22v k 2 +B22η  k 2 = f2 −A21vk1 −A23uk−13 −B21ηk1 −B23γk−13 À  f2 −A21vk1 −A23uk3 −B21ηk1 −B23γk3 = A22uk2 +B22γk2 and comparison result applied to the system with matricies A22, B22 leads to the inequality (uk2 , γ  k 2 )¿ (vk2 , ηk2 ). Thus, (uk, γk)¿ (vk, ηk).  All other inequalities are proved similarly, using the monotonicity arguments.  Thus, the sequences {(uk, γk)}, {(vk, ηk)}, {(ṽk, η̃k)} monotonically decrease and have the unique limit (u∗, γ∗). Passing to the limit in the equation (37) we see that (u∗, γ∗) is the solution of (34).  Remark 2.2. The only argument we used in this section to prove the monotone convergence of iterative methods is the monotone dependence of the solution for the problem (34) upon the righthand side (comparison results). As the problem (24) is the partial case of (34), then all these results are still valid for the equation Au + Cu 3 f. Moreover, the comparison result for (24) holds with only assumption for matrix A to be M -matrix. So, the only condition for monotone convergence of block relaxation, multisplitting and  191    Schwarz alternating methods for (24) is that they start from subor supersolution.  Remark 2.3. Let the operator C be the subdifferential of indicator function for closed convex set K ∈ RN (or, normal cone for K ). Let also a vector u satisfies the conditions  u ∈ K; AuÀ f. Then (u, 0) is a supersolution for (24). Similarly, if u ∈ K; Au¿ f then (u, 0) is a subsolution for (24).  2.3. Domain decomposition methods with overlapping subdomains  a) Let us consider the mesh schemes of the Examples 1 - 3 from Introduction. As we marked overthere these mesh schemes can be written as equation P1 with diagonal maximal monotone operator C and M -matrix A (we use the triangulation of accute type). Let now the domain Ω be decomposed into the overlapping subdomains. For the simplicity but without loss of generality we suppose that there are two subdomains Ω1 and Ω2 , consisiting of the triangles of triangulation Th ; any internal node of the grid in Ω is the internal node of at least one of the subdomains.  We arrange the internal nodes of the mesh as follows: first, we enumerate the nodes lying in Ω1 \Ω1 ∩ Ω2, then the nodes in Ω1∩Ω2 and, at last, the nodes in Ω2 \ Ω1 ∩ Ω2. Vector RN 3 u ⇔ uh ∈ Vh takes the form u = (u1, u2, u3) with subvectors u1, u2, u3 which correspond to chosen enumeration of the nodes. The matrices A, Id and the operator C take the form:  A = (Aij) 3 ij=1, C = diag(C1, C2, C3), I = diag(I1, I2, I3).  Then the mesh problem can be written as the system of nonlinear equations:      A11u1 +A12u2 +C1u1 3 f1, A21u1 +A22u2 +A23u3 +C2u2 3 f2,  A32u2 +A33u3 +C3u3 3 f3. Firstly we study the following multiplicative Schwarz alternating method, which be refered as MSAM:  { A11u  k+1 1 +A12u  k+1/2 2 +C1u  k+1 1 3 f1,  A21u k+1 1 +A22u  k+1/2 2 +A23u  k 3 +C2u  k+1/2 2 3 f2,  192    { A21u  k+1 1 +A22u  k+1 2 +A23u  k+1 3 +C2u  k+1 2 3 f2,  A32u k+1 2 +A33u  k+1 3 +C3u  k+1 3 3 f3  for k = 0, 1, 2, ...; (u01, u 0 2) t is an initial guess. The matricies of these  systems of equations are M -matricies as submatricies of M -matrix A. Thus the iterative method MSAM is correctly defined. Along with this iterative method we consider the following block coordinate relaxation algoritm:      A11v k+1/2 1 +A12v  k+1/2 2 +C1v  k+1/2 1 3 f1,  A21v k+1/2 1 +A22v  k+1/2 2 +A23u  k 3 +C2v  k+1/2 2 3 f2,  A32v k 2 +A33v  k+1/2 3 +C3v  k+1/2 3 3 f3,  (39)    A11v k+1 1 +A12v  k+1/2 2 +C1v  k+1 1 3 f1,  A21v k+1/2 1 +A22v  k+1 2 +A23v  k+1 3 +C2v  k+1 2 3 f2,  A32v k+1 2 +A33v  k+1 3 +C3v  k+1 3 3 f3.  (40)  It is easy to see that the last equation in (39) and the first equation  in (40) are fictitious: we neglect v k+1/2 3 when calculate v  k+1 while  vk+11 = v k+1/2 1 due to the uniqueness of the solution for corresponding  equation. Moreover vk+1 = uk+1. Thus, one step of MSAM is equivalent to two steps of block coordinate relaxation method with different block partitions. Its convergence follows from Theorem 2.3. In fact, the block partitions of the matrix A that we use to construct the equations (39), (40) lead to two its regular splittings with matricies D1, D2 :  D1 =     A11 A12 0 A21 A22 0 0 0 A33    , D2 =     A11 0 0 0 A22 A23 0 A32 A33    ,  and Si = A−Di À 0. The following estimate holds:  |uk+1−u| = |vk+1−u| ¿ D−12 S2|vk+1/2−u| ¿ D−12 S2D−11 S1|uk−u|  and ρ(D−12 S2D −1 1 S1) < 1 due to Theorem 2.3 which ensures the  convergence of MSAM.  193    Let us now consider the additive Schwarz alternating method (ASAM):  { A11v  k+1 1 +A12v  k+1 2 +C1u  k+1 1 3 f1,  A21v k+1 1 +A22v  k+1 2 +A23u  k 3 +C2u  k+1 2 3 f2,  { A21u  k 1 +A22w  k+1 2 +A23w  k+1 3 +C2w  k+1 2 3 f2,  A32w k+1 2 +A33w  k+1 3 +C3w  k+1 3 3 f3,  uk+1 = (vk+11 , 1/2(v k+1 2 + w  k+1 2 ), w  k+1 3 )  t.  This method is the partial case of multisplitting method (29). In fact the splittings for the matrix A are the same as for MSAM, namely, A = Dl − Sl, l = 1, 2 while E1 = diag(I1, 1/2I2, 0), E1 = diag(0, 1/2I2, I3). So, the convergence of ASAM follows from Theorem 2.2.  b) Let us now consider the mesh scheme (11) of the Example 4 from Introduction. It is the partial case of the problem P2 with matricies A and B and operator C satisfying all the assumptions of Theorem 1.3. The only assumption we have to check is the existence of sub- and supersolution. But if we put (u, γ) = (0, 0) then it is the subsolution. On the other hand, (u, γ) = (maxx∈∂ωz(x), 1) is a supersolution for the problem. Due to Theorem 1.3 this mesh scheme has the unique solution.  Moreover, if we decompose as befor the domain Ω into overlapping subdomains, then the corresponding splittings of matricies A,B satisfy the conditions of Theorems 2.6, 2.7. It means that all iterative methods of previous section (Schwarz alternating methods among them) can be used and converge for the mesh scheme (11).  S3. Splitting iterative methods for the problem P1 with positive definite matrix  Let B and D be regular N × N matricies. We consider the following iterative method for solving the problem P1:  D−1B(uk+1 − uk) +Auk +C(B(uk+1 − uk) + uk) 3 f, k = 0, 1, 2, ... (41)  starting from any u0 ∈ RN .  194    If B = I, then we derive from (41) iterative scheme:  u0 ∈ V ;D−1(uk+1 − uk) +Auk + C(uk+1) 3 f, k = 0, 1, 2, ... (42)  which in the linear case (C = 0) becomes well-known one-step iterative process. On the other hand, if D = 1/λ · I,B = I + λA, then the method (41) can be written in the splitting form:  (uk+1/2 − uk)/λ+Auk + Cuk+1/2 3 f, (43)  (uk+1 − uk+1/2)/λ+A(uk+1 − uk) = 0, (44) which is none other than Douglas-Rachford scheme for variational inequality P1.  The solution of the variational inequality (42) for fixed k may be of the same complexity as the initial problem, at least when preconditioner D is "close"to A−1. On the other hand, to realize the Douglas-Rachford procedure (43), (44) we need to solve noncoupled system of equation (44) with linear operator I +λA and the equation (43) with multivalued operator I +λC. For solving (44) we can use well-known iterative procedures or a direct method. As for operator I + λC, then in many practical cases it is easy invertible (see Appendix). Thus, the splitting iterative schemes which need the inversion of the operator I + λC have practical importance.  In 3.1 we derive a priori estimates for two-lewel iterative method (41) with D = 1/λ · I. Then in 3.2 we cite some results on the rate of convergence and optimal iterative parameters splitting methods, which are partial cases of (41). One example of iterative method for mesh approximation for variational inequality is also analysed in 3.2. Splitting iterative method for the mesh scheme constructed via domain decomposition technique is studied in 3.3.  3.1. A priori estimates for the splitting iterative method  We consider the equation  Au+ Cu 3 f (45)  with positive definite matrix A and maximal monotone operator C in RN amd study the following iterative method for its solving:  B(uk+1−uk)/λ+Auk+C(B(uk+1−uk)+uk) 3 f, k = 0, 1, 2, ... (46)  195    where u0 ∈ RN , B is a regular matrix and λ > 0 is an iterative parameter.  We can rewrite (46) in the form of following system:  (uk+1/2 − uk)/λ+Auk + Cuk+1/2 3 f,  B(uk+1 − uk) = uk+1/2 − uk. From this equivalent writing of (46) we derive the existence of unique uk+1 for any k.  Lemma 3.1. Let A be positive definite matrix, B be regular matrix and C be maximal monotone operator. Then the following error estimate for zk = uk − u is valid:  ||Bzk+1|| ≤ (1/2||(I − λA)B−1||+ ||I − 1/2(I + λA)B−1||)||Bzk||. (47)  Proof. From (45), (46) it follows:  B(zk+1 − zk) + zk + λ(C(B(uk+1 − uk) + uk))− Cu) 3 (I − λA)zk.  Multiplying this equation by B(zk+1− zk)+ zk and using the monotonicity of the operator C, we deduce the inequality  ||B(zk+1 − zk) + zk||2 ≤ (B(zk+1 − zk) + zk, zk − λAzk)  which after transformations becomes:  ||Bzk+1||2 − (Bzk+1, (2B − I − λA)zk) + (Bzk, Bzk − zk − λAzk)  +λ(Azk, zk) ≤ 0 or  ||Bzk+1||2 − ||Bzk+1||||(2B − I − λA)zk||+ (Bzk, Bzk − zk − λAzk)  +λ(Azk, zk) ≤ 0. In the left side of the last inequality there is the quadratic trinomial on the ||Bzk+1|| . For its roots we have the estimate:  ||Bzk+1|| ≤ 1/2||(2B − I − λA)zk||+ 1/2(||(2B − I − λA)zk||2−  4(Bzk, Bzk − zk − λAzk)− 4λ(Azk, zk))1/2  196    = 1/2||(2B − I − λA)zk||+ 1/2||(I − λA)zk||. From this estimate (47) follows immediately.  3.2. Iterative methods with splitting of the operator  We consider the following splitting iterative methods for solving the equation (45):  explicit (with reference to A) scheme:  (uk+1 − uk)/λ+Auk + Cuk+1 3 f, (48)  Peaceman-Rachford scheme:  (uk+1/2 − uk)/λ+Auk + Cuk+1/2 3 f,  (uk+1 − 2uk+1/2 + uk)/λ+A(uk+1 − uk) = 0, (49) Douglas-Rachford scheme:  (uk+1/2 − uk)/λ+Auk + Cuk+1/2 3 f,  (uk+1 − uk+1/2)/λ+A(uk+1 − uk) = 0. (50) Theorem 3.1. Let m · I ≤ A = A∗ ≤ M · I and C be maximal  monotone operator, m,M = const > 0. Then: 1) Iterative method (48) converges if λ ∈ (0, 2/M) and for λ =  λ0 = 2/(M +m) the estimate for the rate of its convergence is:  ||zn|| ≤ ( M −m M +m)  )n||z0||.  2) Iterative methods (49), (50) converge for any λ > 0 and for λ = λ0 = 1/  √ Mm the following estimates are valid:  ||(I+λ0A)zn|| ≤ ( √ M −  √ m√  M + √ m)  )n||(I+λ0A)z0|| for the method (49) ;  ||(I+λ0A)zn|| ≤ ( √ M√  M + √ m)  )n||(I+λ0A)z0|| for the method (50).  Proof. The iteration schemes (48) - (50) are the realisations of the general iterative method (46) with B = I, B = 1/2(I+λA), B = I+  197    λA correspondingly. Using the estimate (47) we derive ||zk+1|| ≤ ||I− λA||||zk|| for method (48). Similarely, for method (49) the estimate  ||(I + λA)zk+1|| ≤ ||(I − λA)(I + λA)−1||||(I + λA)zk|| (51)  holds while for method (50) the following one is valid:  ||(I+λA)zk+1|| ≤ (1/2+1/2||(I−λA)(I+λA)−1||||(I+λA)zk||. (52)  Now the estimates cited in the formulation of the Theorem 3.1 are the consequences of well-known estimates for the norms of symmetric matricies. Namely,  ||I−λA|| = max µ∈σ(A)  |1−µλ| ≤ max m≤t≤M  |1−tλ| = max{|1−mt|, |1−Mt|}  = (M −m)/(M +m) for λ = 2/(M +m). Similarly,  ||(I − λA)(I + λA)−1|| ≤ max{| 1−mλ 1 +Mλ  |, |1−Mλ 1 +Mλ  |} < 1  for any λ > 0. It can be easy calculated that λ = λ0 = 1/ √ mM is  theoretically optimal parameter and that the cited estimates for rate of convergence are valid with this choice of λ.  Theorem 3.2. Let C be a maximal monotone operator, matrix A be positive definite and:  ||Au||2 ≤ ∆(Au, u) ∀u ∈ RN , (53)  σ(A+A∗) ∈ [m,M ]. (54) Then the iterative methods (49) and (50) for solving the equation (45) are convergent for any λ > 0 and the following error estimate is valid:  ||(I + λA)zn|| ≤ qn/2||(I + λA)z0||. (55) where  q = q0 = max{| 1− λ(1− λ∆/2)m 1 + λ(1 + λ∆/2)m  |; |1− λ(1− λ∆/2)M 1 + λ(1 + λ∆/2)M  |} < 1  for the method (50) and q = 1/2 + 1/2q0 for the method (49).  198    Proof. Using the estimate (47) with B = 1/2(I + λA) and B = I+λA we derive the inequalities (51) and (52), correspondingly. It is known that under the assumptions (53), (54) the following estimate is true:  ||(I − λA)(I + λA)−1||2 ≤ max t∈σ(A+A∗)  Fλ(t) ≤ max t∈[m,M ]  Fλ(t),  Fλ(t) = |1− λt+ λ2t∆/2| |1 + λt+ λ2t∆/2| .  For any λ > 0 the function Fλ(t) decreases in t > 0 , thus it achieves the maximum in the segment [m,M ] in its boundary. This finishes the proof.  Example. Let Ω be a poligonal domain, differential operator be defined by Lu = −  ∑2 i,j=1 ∂/∂xi(aij∂u/∂xj), aij(x) ∈ C(Ω) and  matrix of coefficients satisfies following assumptions:  2∑  i,j=1  aij(x)tjti ≥ c0||t||2, ||t||2 = 2∑  i,j=1  t2i , (56)  2∑  i,j=1  aij(x)tjsi ≤ c1||t||||s|| ∀x ∈ Ω. (57)  We denote by V = H10 (Ω),K = {u ∈ V : u(x) ≥ 0 ∀x ∈ Ω} and consider variational inequality  u ∈ K : a(u, v − u) = ∫  Ω  Lu(v − u)dx ≥ ∫  Ω  f(v − u)dx ∀v ∈ K.  Let Th be regular triangulation of Ω, the lengths of sides for triangles δ ∈ Th be O(h) , Vh = {uh ∈ V : uh ∈ P1 ∀δ ∈ Th}, Kh = {uh ∈ Vh : uh(x) ≥ 0 ∀x ∈ Ω}. Using the correspondence u ⇔ uh for u ∈ RN , uh ∈ Vh we introduce following notations:  (Au, v) = a(uh, vh), (f, v) =  ∫  Ω  f(x)vh(x)dx,  K = {u ∈ RN : ui ≥ 0 ∀i}, C = ∂IK . With these notations we can write mesh VI approximating the original problem in the form (45).  199    Let now matrix R be defined by equality  (Ru, v) =  ∫  Ω  ∇uh(x)∇vh(x)dx.  Then the assumptions (56),(57) on the coefficients aij ensure the estimates:  (Au, u) ≥ c0(Ru, u), (Au, v) ≤ c1(Ru, u)1/2(Rv, v)1/2. (58)  As the consequence of these inequalities the following estimate holds:  ||Au|| = sup v 6=0 |(Au, v)|/||v|| ≤ c1  c 1/2 0  (Au, u)1/2 sup v 6=0  (Rv, v)1/2/||v||.  Now using Friedrichs-Poincare inequality we have  (Ru, u) =  ∫  Ω  |∇uh|2dx ≥ c ∫  Ω  u2hdx = α0(u, u)  where c = const 6= c(h) and α0 = O(h2). On the other hand  (Ru, u) ≤ const · h−2 ∫  Ω  u2hdx = α1(u, u)  with α1 = O(1). Summarizing the received results we derive:  (Au, u) ≥ c0α0||u||2, (Au, u) ≤ c1α1||u||2, ||Au||2 ≤ α1c21/c0(Au, u). (59)  Thus the assumptions (53), (54) of Theorem 3.2 are satisfied with m = 2c0α0 = O(h  2), M = 2c1α1 = O(1), ∆ = α1c 2 1/c0 = O(1).  If we choose λ = O(h−1) in method (49) (or (50)) then q0 = 1 − O(h) and the number of iterations to achieve accuracy ² is n(²) = O(h−1 ln 1/²).  3.3. Domain decomposition method and splitting iterative process  Let Ω = (0, 1)× (0, 1)Lu = − ∑2  i,j=1 ∂/∂xi(aij∂u/∂xj), aij(x) ∈ C(Ω) and matrix of coefficients satisfies assumptions (56),(57). We denote by V = H10 (Ω),K = {u ∈ V : u(x) ≥ 0 ∀x ∈ Ω} and consider VI  u ∈ K : a(u, v−u) = ∫  Ω  Lu(v−u)dx ≥ ∫  Ω  f(v−u)dx ∀v ∈ K. (60)  200    Let Th be "triangulation"of Ω in squares δ of dimensions h × h , Vh = {uh ∈ V : uh ∈ Q1 ∀δ ∈ Th}. We use also the quadrature rule  ∫  δ  uh(x)dx ≈ Sδ(uh) = 1/4h2 4∑  i=1  uh(dj),  where dj are verticies of δ ∈ Th. Let now Ω be divided into two subdomains Ω1,Ω2, where Ω1 =  {x ∈ Ω : x1 < 1/2}, Ω2 = {x ∈ Ω : x1 > 1/2}. We suppose triangulation to be such that the interface S = int(Ω1 ∩Ω2) consists of the sides of δ ∈ Th. The restriction of functions from Vh on subdomain Ωk form the space V  k h , k = 1, 2. Let us use also th  following notations:  Kih = {uih ∈ V ih : uih(x) ≥ 0, x ∈ Ωi}, i = 1, 2;  Kh = {uih ∈ Kih : u1h(x) = u2h(x), x ∈ S}. To approximate VI (60) by mesh scheme we construct following bilinear and linear forms:  aih(u i h, v  i h) =  ∑  δ∈Ωi  Sδ(  2∑  k,j=1  akj∂uh/∂xj∂vh/∂xk);  f ih(v i h) =  ∑  δ∈Ωi  Sδ(fv i h), i = 1, 2.  Using these notations we are able to write the mesh approximation of (60) as follows:  find uh = (u 1 h, u  2 h) ∈ Kh :  a1h(u 1 h, v  1 h− u1h)+ a2h(u2h, v2h− u2h) ≥ f1h(v1h− u1h)+ f2h(v2h− u2h), (61)  ∀vh = (v1h, v2h) ∈ Kh. Let xi ∈ S, i = 1, 2, ..., N0 be grid nodes (verticies of δ ∈ Th ) lying on S. By N1 we denote number of nodes in Ωi∪S , i.e. internal nodes plus nodes on interface. We put in correspondence to u1h ∈ V 1h vector u1 ∈ RN1 with coordinates u1i = u1h(xi). Let the first N0 coordinates of u1 correspond to grid point in S , while another N1−N0 to internal points in Ω1. By the similar way we define vector u  2 ∈ RN1 .  201    Let now (., .) be inner product in Rm with any m. We get  (Aiu i, vi) = aih(u  i h, v  i h), (f  i, vi) = f ih(v i h) u  i ⇔ uih, vi ⇔ vih and  Ki = {ui ∈ RNi : uij ≥ 0 ∀j = 1, 2, ..., Ni}, K = {u = (u1, u2) ∈ K1 ×K2 : u1i = u2i ∀i = 1, 2, .., N0.}.  Using these notations we write mesh VI (61) in the form (45), where  A =  ( A1 0 0 A2  ) ; f =  ( f1  f2  ) ;C = ∂IK .  The iterative method (46) with B = I + λA for solving (61) leads to algorithm  (uk+1/2 − uk)/λ+Auk + Cuk+1/2 3 f, (62) (Ii + λAi)(u  i,k+1 − ui,k) = ui,k+1/2 − ui,k, i = 1, 2, (63) where Ii is identity in R  Ni , uk = (u1,k, u2,k). Linear equations (62) are solved independently for i = 1, 2 . As  for (63) then for coordinates of u corresponding to internal nodes xj ∈ Ω1 or xj ∈ Ω2 operator C has diagonal form: (Cu)j = ∂Kj (uj), Kj = {uj ∈ R : uj ≥ 0}. It means that u  k+1/2 j =  (ukj + λ(f −Auk)j)+ for such indecies j. For xj ∈ S system (63) contains coupled subsystems of two  equations for coordinates u1j and u 2 j . For any λ > 0 to solve  such subsystem it is enough to execute orthogonal projection in R2 to M = {u = (u1, u2) ∈ R2 : u1 = u2 ≥ 0}. So, if F 1j = (u1,k + λ(f −A1u1,k))j , F 2j = (u2,k + λ(f −A2u2,k))j , F = (F 1j , F 2j ), then for xj ∈ S  u 1,k+1/2 j = u  2,k+1/2 j = PrMF = (F  1 j + F  2 j )  +/2.  Let us do several remarks on the convergence and rate of convergence for iterative method which we study.  Simuilar to Example in previous section we introduce matricies Ri, i = 1, 2 - mesh Laplace operators with corresponding boundary conditions - by the equalities:  (Riui, vi) = ∑  δ∈Ωi  Sδ(∇uih,∇vih), R = diag(R1, R2).  202    Assumptions (56), (57) ensure the validity of inequalities (58). Moreover, due to presence of Dirichlet boundary condition on the part of ∂Ωi Friedrichs - Poincare inequality is valid for functions in V ih . Thus the analogues of estimates (59) are satisfied with α0 = O(h2), α1 = O(1). It means that for λ = O(h  −1) the number of iterations to achive accuracy ² is n(²) = O(h−1 ln 1/²).  References  [1] O. Axelsson. Iterative solution methods. – Cambridge University Press: New York, 1996.  [2] T. Chan. Domain decomposition algorithms// Acta Numerica. – 1994.– V.1.– P. 61-143.  [3] D. Gabay. Applications of the method of multipliers to variational inequalities, In: Augmented Lagrangian methods: applications to the numerical solution of boundary-value problems (Eds. Fortin M., Glowinski R.). – Amsterdam: North– Holland Publishing Company, 1983.  [4] Yu. Kuznetsov, P. Neittaanmäki and P. Tarvainen. Block relaxation methods for algebraic obstacle problem with Mmatrices//East-West J. of Numer. Math. – 1994. – V.2. – P.7589.  [5] Yu. Kuznetsov, P. Neittaanmäki and P. Tarvainen. Schwarz methods for obstacle problems with convection-diffusion operators, In: Domain Decomposition Methods in Scientifical and Engineering Computing. – 1995.– P. 251-256.  [6] A.Lapin, D.Solovyev. Splitting iterative methods for variational inequalities. – Preprint N.783, Center of Calcul., Novosibirsk, 1988, 24 p. (in Russian).  [7] A. Lapin. Relaxation iterative methods for some classes of variational inequalities in Rn // Numer. Anal. and Math. Modelling. – 1989. – P.127-143 (in Russian)  [8] A. Lapin.Iterative solution for two classes of mesh variational inequalities. – Preprint, July 1999, Dep. of Math. Sci., University of Oulu, 1999. – 29 p.  203    [9] A. Lapin. Convergence and error bounds for the splitting iterative methods for variational inequalities with monotone operators// – 1999 (to appear).  [10] P. Le Tallec. Domain decompostion methods in computational mechanics// Computational Mechanics Advances. – 1994. – V.1. – P. 121-220.  [11] P.L. Lions. On the Schwarz alternating method, I,In: Domain Decomposition Methods for PDE’s (eds. Glowinski R., Gloub G. H., Meurant G. A., Périaux J.)// SIAM. – 1988. – P. 1-40.  [12] P.L Lions. On the Schwarz alternating method, II, In: Domain Decomposition Methods for PDE’s (eds. Chan T. F., Glowinski, R., Périaux J., Widlund O. B.)// SIAM. – 1989. – P. 47-70.  [13] P.L. Lions, B. Mercier. Splitting algorithms for the sum of two nonlinear operators// SIAM J. Numer. Anal. – 1979. – V.16. – P. 964–979.  [14] J.M. Ortega and W.C. Rheinboldt. Iterative Solutions of Nonlinear Equations in Several Variables. – Academic Press: New York, 1970.  [15] Y. Saad. Iterative methods for sparce linear systems. – PWS Publ. Comp.: Boston, 1996.  [16] B. Smith, P. Björstad and W. Gropp. Domain decomposition: parallel multilevel methods for elliptic partial differential equations. – Cambridge University Press: New York, 1996.  [17] P.Tseng. Further applications of splitting algorithm to decomposition in variational inequalities and convex programming// Math. Progr. – 1991. – V.48. – P. 249-263.  [18] S.C. Zhou. An additive Schwarz algorithm for variational inequality, In: Domain Decomposition Methods in Science and Engineering (eds. Glowinski R., Périaux J., Shi Z., Widlund O. P.), 1996.  [19] J. Zeng. Geometric convergence of overlapping Schwarz methods for obstacle problems, In: 9-th International conference on DDM (eds. Björstad P., Magne S., Esptal S., Keyes D.L.), 1998.  204    [20] J. Zeng and S.C.Zhou. On momotone and geometric convergence of Schwarz methods for two-side obstacle problems// SIAM J. Numer. Anal. – 1998. – V.35. – N 2. – P. 600-616.  [21] Zhong-Zhi Bai. The Monotone Convergence of Class of Parallel Nonlinear Relaxation Methods for Nonlinear Complementarity Problems//Computers Math. Applic. – 1996. – V.31. – N 12. – P.17-33.  Appendix. Monotone mappings and convex functions  1. Contractions and partial ordering  We use the following notations for u ∈ RN , B ∈ L(RN ;RN ) :  uÀ 0⇔ ui ≥ 0 ∀i, B À 0⇔ bij ≥ 0 ∀i, j, |u| = (|u1|, |u2|, ..., |uN |).  Definition 1. A mapping G : D ∈ Rn → Rn is called a P contraction if there exists a linear mapping (matrix) P such that:  P À 0, ρ(P ) < 1 , |Gx−Gy| ¿ P |x− y| ∀x, y ∈ D,  (ρ(.) is spectral radius ).  Proposition 1. Let G be P -contraction on the closed set D and GD ⊂ D. Then the sequence xk+1 = Gxk, k = 0, 1, 2, ... converges to unique fixed point x∗ of G in D starting from any initial guess x0 ∈ D and  |xk − x∗| ¿ P k|x0 − x∗|. Proposition 2 (Kantorovich lemma). Let G be continuous  and monotone (isotone) on D ⊂ Rn : x À y ⇒ Gx À Gy. Let also x0 ¿ y0, x0 ∈ D, y0 ∈ D, x0 ¿ Gx0, y0 À Gy0. Then for sequences  xk+1 = Gxk, yk+1 = Gyk, k = 0, 1, 2, ..  the following statements are valid: xk ↑ x∗, yk ↓ y∗, where x∗ ¿ y∗ are fixed points of G.  Definition 2. A regular matrix A = (aij) is called an M-matrix if A has non-positive off-diagonal elements and if its inverse has non-negative entries.  Proposition 3. Let A be a matrix with non-positive off-diagonal elements. Then any two of the following statements are equivalent:  205    1) A is M-matrix ; 2) there exists a positive vector f (fi > 0 ∀i) such that A−1f is  positive vector; 3) Reλ > 0 for any eigevalue λ ∈ σ(A) . Consequence. Symmetric M-matrix is positive definite. Proposition 4. Let A1 be a M-matrix, A2 À A1 and off diagonal elements of A2 are non-positive. Then A2 is also M-matrix and A−12 ¿ A−11 .  Definition 3. Let A be a M-matrix and A = B−C be its splitting such that ∃B−1 À 0, C À 0 . Then it is called by the regular splitting.  Proposition 5. If A = B − C is a regular splitting of M-matrix A, then ρ(B−1C) < 1.  2. Monotone operators and convex functions  Let A be a (multivalued) operator in RN with domain D(A) = {x ∈ RN : Ax 6= ∅} and range R(A) = {y ∈ RN : ∃x ∈ D(A), y ∈ Ax}. We denote by graphA = {(x, y) ∈ RN × RN : x ∈ D(A), y ∈ Ax}.  Definition 4. Operator A is called to be monotone if  (y1 − y2, x1 − x2) ≥ 0 ∀xi ∈ D(A) ∀yi ∈ Axi.  and to be maximal monotone if the following property is valid: if the pair (x0, y0), x0 ∈ D(A) is such that (y − y0, x − x0) ≥ 0 ∀x ∈ D(A) ∀y ∈ Ax, then y0 ∈ Ax0.  The monotonocity of A means that graphA is monotone set in RN × RN . If moreover it is not the proper part of other monotone set, the operator A is maximal monotone.  The inverse A−1 of maximal monotone operator A is also maximal monotone operator with D(A−1) = R(A) and R(A−1) = D(A).  Examples of maximal monotone operators in RN : 1) Positive semidefinite matrix: (Ax, x) ≥ 0 ∀x ∈ RN , 2) Single-valued monotone: (Ax−Ay, x−y) ≥ 0 ∀x, y ∈ RN and  continuous: xk → x⇒ Axk → Ax operator. Proposition 6 (Fundamental property of maximal mo notone operators). Let A be a monotone operator. Then A is maximal monotone iff for each λ > 0 ( equivalently, for some positive λ) R(I + λA) = RN , where I is the identity mapping in RN .  206    The single-valued operator I + λA is non-expansive in RN :  ||(I + λA)x− (I + λA)y|| ≤ ||x− y|| ∀x, y ∈ RN .  Proposition 7 (closeness of maximal monotone operator). Let A be a maximal monotone operator in Rn and xn ∈ D(A), xn → x, yn ∈ Axn, yn → y. Then y ∈ Ax.  Definition 5. A function φ : RN → R ∪ {+∞} is called : proper if its effective domain D(φ) = {x ∈ RN : φ(x) < +∞} is  non empty; convex if φ(tx+ (1− t)y) ≤ tφ(x) + (1− t)φ(y) ∀x, y ∈ RN ∀t ∈  (0, 1); lower semicontinuous (l.s.c.)if lim infxk→xφ(xk) ≥ φ(x) ∀x ∈  RN . Definition 6. Given a proper convex function φ in RN and a  point x ∈ RN , we denote by ∂φ(x) the set of all µ ∈ RN such that  (µ, y − x) ≤ φ(y)− φ(x) ∀y ∈ RN .  Such elements are called subgradients of φ at x , and ∂φ(x) is called the subdifferential of φ at x .  The set ∂φ(x) is closed and convex (possibly empty). We note also that  φ(x) = min{φ(y); y ∈ RN} ⇔ 0 ∈ ∂φ(x). Proposition 8. Let φ be a l.s.c. proper convex function. Then:  - ∂φ is maximal monotone operator, called by subpotential operator; - D(∂φ) is dense subset of D(φ) .  Examples of subpotential opeators: 1) Let convex function φ be Gateaux differentiable at x :  ∃ lim t→0  (φ(x+ th)− φ(x))/t = (∇φ(x), h) ∀h ∈ RN .  Then ∂φ(x) = {∇φ(x)}, i.e. subdifferential ∂φ(x) contains the only element — gradient of φ in the point x.  2) Let K be a closed convex set in RN and IK(x) = {0, x ∈ K; +∞, x /∈ K} be its indicator function. Then IK is the proper convex l.s.c. function with subdifferential:  ∂IK(x) = {µ ∈ RN : (µ, y − x) ≤ 0 ∀y ∈ K}, D(∂IK) = D(IK) = K.  207    3) Graph of Heavyside function H(x) = {0, x < 0; [0, 1], x = 0; 1, x > 0} is the subdifferential of the function φ(x) = x+ = max{0, x} . The inverse to H is maximal monotone operator H−1 = {(−∞, 0], x = 0; 0, x ∈ [0, 1]; [0,+∞), x = 1} with D(H−1) = R(H) = [0, 1]. It is easy to prove that H−1 = ∂I[0,1].  More examples of maximal monotone operators can be constructed using the following proposition.  Proposition 9. Let A,B be maximal monotone operators and let intD(A) ∩ D(B) 6= ∅ . Then A + B is also maximal monotone operator.  Here we denote the set of interior points by intD(A) = {x ∈ D(A) : ∃² > 0 ∀y : ||y − x|| < ²→ y ∈ D(A)} .  Consequence. If A is monotone continuous (in particulary, linear positive semidefinite) and B is maximal monotone operator then A+B is maximal monotone operator.  3. Surjectivity of maximal monotone operators  Definition 7. An operator A is called coercive if  ∃x0 ∈ RN : lim n→+∞  (yn, xn − x0)/||xn|| = +∞  ∀xn ∈ D(A) : ||xn|| → ∞ and ∀yn ∈ Axn. Examples of coercive operators: 1) Positive definite matrix; 2) Uniformly monotone: (Ax − Ay, x − y) ≥ m||x − y||2 ∀x, y ∈  RN , m > 0, and bounded in RN operator A . Proposition 10. If A is a maximal monotone operator and A  is coercive or D(A) is bounded then R(A) = RN , i.e. the equation Ax 3 f has a solution for arbitrary f ∈ RN .  Proposition 11. Let φ be a proper, convex, lower semicontinouos function and either φ is coercive:  xn ∈ D(φ) ||xn|| → ∞ ⇒ φ(xn)→ +∞  or D(φ) is bounded. Then there exists a solution u for minimisation problem:  φ(u) = min x∈Rn  φ(x).  208    4. Equivalent formulations of variational inequalities  We consider the following problems:  min{1/2(Ax, x)− (f, x) + φ(x);x ∈ RN}, (64)  (Ax, y − x) + φ(y)− φ(x) ≥ (f, y − x) ∀y ∈ RN , (65) Ax+ Cx 3 f, C = ∂φ. (66)  Proposition 12. If A is symmetric semidefinite positive matrix, φ is proper convex l.s.c. function, then problems (64)-(66) are equivalent.  If A 6= A∗ then variational inequality (65) is not equivalent to a minimisation problem. Similarly, if C is not the subdifferential of a convex function then the inclusion (66) is not equivalent to a variational inequality. Thus, the formulation (66) is the most general one.  5. “Easy invertible” maximal monotone operators  Let C be the maximal monotone operator in RN of the form C = diag(c1, c2, ..., cN ), where ci are one-dimensional maximal monotone operators. We say that C is diagonal maximal monotone operator. In this case the equation  x+ Cx 3 f  decouples in the system of N independent scalar equations  xi + ci(xi) 3 fi. (67)  To solve the scalar equation we can use the following result: Any one-dimensional maximal monotone operator is subpotential,  i.e. it is the subdifferential of a convex l.s.c function ci = ∂φi . So the corresponding equation is equivalent to the minimisation  problem in R1 . Moreover, in many cases of practical interest we can solve the one-dimensional equation (67) by direct calculations, for example, if the graph of ci is piecewise-linear.  The equation Lx+ Cx 3 f  209    with diagonal maximal monotone operator C and triangular matrix L can be solved by recurence using one-dimensional solvers.  This work was supported by grants N 98-01-00200, N 99-01-00173 of RFBR and Scientific Foundation of Republic Tatarstan.  Д. Г. Слугин (Москва)  КОНЕЧНО-РАЗНОСТНЫЙ МЕТОД ДЛЯ СИСТЕМЫ УРАВНЕНИЙ ОДНОМЕРНОГО ДВИЖЕНИЯ  ВЯЗКОГО ЭЛЕКТРОПРОВОДНОГО ГАЗА В ПЕРЕМЕННЫХ ЭЙЛЕРА  Задача об одномерном движении вязкого электропроводного газа сводится к системе дифференциальных уравнений магнитной гидродинамики в переменных Эйлера:  ∂ρ  ∂t + ∂ρv  ∂x = 0, (1.1)  ρ  ( ∂v  ∂t + v  ∂v  ∂x  ) + ∂p  ∂x =  4  3 µ ∂2v  ∂x2 + fx, (1.2)  ρ  ( ∂u  ∂t + v  ∂u  ∂x  ) = µ  ∂2u  ∂x2 + fy, (1.3)  ρ  ( ∂w  ∂t + v  ∂w  ∂x  ) = µ  ∂2w  ∂x2 + fz, (1.4)  Hx = Hx0 = const,  ∂Hy ∂t  + ∂(vHy)  ∂x = Hx  ∂u  ∂x +  1  4πσ  ∂2Hy ∂x2  , (1.5)  ∂Hz ∂t  + ∂(vHz)  ∂x = Hx  ∂w  ∂x +  1  4πσ  ∂2Hz ∂x2  , (1.6)  fx = − ∂  ∂x  ( H2y +H  2 z  8π  ) ,  210   