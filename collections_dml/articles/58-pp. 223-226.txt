 PLACES : DEEP CONVOLUTIONAL NETWORKS FOR PLACES DETECTION F.P.B. Dossou1  1 femipancrace.dossou@gmail.com; Kazan Federal Unirersity  The geolocation services thanks to the precious tool called GPS, are nowadays very present and important in our daily life activities: taxi ordering, place tagging, address finding etc. However, this identification is not always easy because it strongly depends on the activation of this GPS services on the user’s phone settings and also a good internet connection to ensure high efficiency and accuracy in determining the geographical location coordinates of the user. In recent years, with the dazzling advance of Artificial Intelligence, the human being has been able to achieve excellent and unimaginable progresses. This research paper introduces BD97 Places, an ongoing research project to develop a powerful artificial intelligence system for places detection and GPS Services helper using deep convolutional neural networks.  Keywords: Places Detection, Convolutional Neural Networks, Artificial Intelligence, Deep Learning  Introduction to BD97 Places  Our current task is a computer vision task because we have to teach our Artificial Neural Network how to analyze a picture and how to be able to identify probable places where the picture has been taken as we, humans could simply do by using our memory and our eyes. For that, Convolutional Neural Networks architecture has been preferred and used since they are able to successfully establish dependencies between patterns in the image through the use of appropriate filters. The proposed model is called BD-97 Places and at the currently is made of two convolutional layers with precised numbers of filters.  BD97 Places Architecture  The input to BD-97 Places is a picture. In order to get good results from our model, we applied some preliminary transformations to the input data. Firstly, we resized the picture at the input, because we needed to get it shaped so that the model could efficiently extract very important properties from them. We get another array that contains numbers, each of which respectively characterizes each pixel of the input. This last array that we got is a matrix, where height is the number of rows and width is the number of columns. Along the input image, we have a third axis called the channel axis, which corresponds to the RGB (red-green-blue) values of the image. While performing transformations on the input data, the data along that third axis can be easily compressed in quality and as a result we could most likely get very poor data quality. In order to maintain good data quality as in the input data, we performed the zero-padding operation: it consists in adding the maximum possible number of zeros along all axes of our input data matrix. After the zero-padding, the input data is sent to convolutional layers, where the model start filtering the input data. The filtration is performed by taking each of the submatrices with a predetermined size and multiply by the filter matrix to obtain a convoluted matrix of features. These submatrices are called kernels, and    224 «ЛОБАЧЕВСКИЕ ЧТЕНИЯ - 2019»  their size is called kernel’s size. However instead of multiplying each elementary single submatrix by the filter matrices, we can specifically decide which submatrices we need. This is possible thanks to the stride: the stride is the distance between each linear pattern along the axes. For example, stride 2 means that we must go from 2 rows and 2 columns along the axes of our pixel array, moving from one kernel to another. By default, its value is 1. The convoluted matrix of features is the result of the input through the filter obtained after performing multiplication between the kernel and the filter matrix taking into account the value of the stride parameter. If W is the width of the input image, the number of columns of the array of our input image, H is the height of the input image, the number of lines of the array of our input image, D is the input depth that determines the size of the axes of the channels (here the size is 3, since we are working on the RGB system), F - kernel size, S - stride, K - number of filters, P - size of the zero-padding.  Then the output size will be W ′∗H ′∗D, where:  W ′ = (W −F +2∗P )/S +1; H ′ = (H −F +2∗P )/S +1.  The convoluted matrix of features needs to be passed through an activation function, which gives an array essentially characterizing abstract patterns of the input data with pixels. There are many activation functions but in the structure of our model we used reLu and softmax as activation functions. By definition reLu function is a semi-linear function with saturation which has been proven to give very good experimental results. ReLU is simply defined as f (x) = max(0, x) and the Softmax function, also known as the normalized exponential function, calculates the probability distribution of an event over “n” different events. In this case, the events here are the different classes we trained our model on. Between the convolution layers there are pooling layers. Their role is to gradually reduce the spatial size of the representation, in order to reduce the number of parameters and calculations in the network, while performing the conversion of our input data. The main operation performed by the pooling layers is what we call maximum pooling. The maximum pooling’s goal is to reduce the selection of the input representation (image, output matrix of the hidden layer, etc.), reducing its dimension and making assumptions about the properties contained in the selected subregions. How does it work and why? This is done in order to partially help with the adaptation, providing an abstract form of presentation. In addition, it minimizes computational costs by reducing the number of parameters studied and provides basic translation immutability for internal representation. Maximum pooling is accomplished by applying the max filter to the (usually) non-overlapping subregions of the original view. Pooling layers accept a volume of size W ∗H ∗D, requiring two parameters: stride S and kernel size F.  This will produce an output of size W ′∗H ′∗D, where:  W ′∗= (W −F )/S +1; H ′∗= (H −F )/S +1.    F.P.B. Dossou 225  Experiments and results  Experiments  Dataset Creation: For our task, there were no dataset of images that we could have used to directly start training our model, therefore we wrote a helping function in python, which uses the google-images-download built-in python library to load many pictures from the internet matching up with specifics keywords. BD-97 Places is currently able to recognize in total eight (08) places : five (05) popular touristic places in various cities of the world [the Eiffel Tower (Paris, France), the Statue of Liberty (New York, USA), the Colosseum (Rome, Italy), the Statue of Christ the Redeemer (Rio, Brazil) and Taj Mahal (New Delhi, India)] and three (03) countries in Africa : Madagascar, Rwanda and Ivory Coast. Although, we have written a function to collect data (pictures) from the internet; after the their cleaning (removing non-valid pictures from the dataset of each category) and their processing, the whole dataset size was still not enough and we had to apply the data-augmentation technique by taking pictures of the training set and applying them: a 30-degree angle rotation, width-height range shifting, zooming and horizontally flipping.  Network training section: We first trained our network for 200 epochs with a learning rate of 0.001 and a batch size of 64. We started with only two classes: the Eiffel Tower and the Statue of Liberty. The result was good enough and then we proceeded to add more classes: The Colosseum (Roma, Italy) and Christ the Redeemer (Rio de Janeiro, Brazil), Taj Mahal(India). After a series of hyperparameter tuning and data augmentation, we finally achieved an accuracy of over 98% on the test data set.  Fig. 1. Accuracy and loss plot for BD97 Places training.  Conclusion and perspectives  In conclusion, our current task was a computer vision problem. Our main focus was to suggest a solution using artificial intelligence and deep neural networks to reinforce    226 «ЛОБАЧЕВСКИЕ ЧТЕНИЯ - 2019»  or bypass GPS services while trying to identity the probable places where a given picture has been taken. To solve this problem, we used a special type of architecture of neural networks called Convolutional Neural Networks. To train our network, it was necessary first of all to write a python script that helped to automatically download images from Google with the given keywords because we couldn’t find any ready dataset, we had to create our own. After the data collection, cleaning and processing, the data were still not enough and then we had to apply the data-augmentation technique. After the dataaugmentation, the data increased and we could then train our network. We made two convolutional neural network models and we established a voting classifier: this made and helped us to obtain great results. For future references, in order to get a more general and better performing network for recognizing more places, we will need to collect more data and train the network with very fined-tuned parameters and look for an effective way to use the models that we would get.  References  1. Chollet, F., Deep Learning with Python / F. Chollet. — Manning Shelter Island, 2018. — 362p  2. Cornelisse D. - Apr 24: An intuitive guide to Convolutional Neural Networks 2018  3. Abdellatif Abdelfattah - 2017, Jul 28: Image Classification using Deep Neural Network – A beginner friendly approach using TensorFlow  UDC 004.85  SEEGAAI : DEEP REINFORCEMENT LEARNING IN SEEGA C.C. Emezue1, F.P.B. Dossou2  1 chris.emezue@gmail.com; Kazan Federal University 2 femipancrace.dossou@gmail.com; Kazan Federal University  This research paper introduces SeegaAI, a research project to develop a powerful artificial intelligence bot for the game of Seega using deep reinforcement learning. Researchers have long been working on embedding human-like behavior in computers: a field of research popularly called artificial intelligence (hereafter called AI). With the advent of personal, faster and more efficient computers in our present generation, it has become easy to create powerful AIs that can play (and beat humans) at various games like checkers, backgammon, chess, and Go[1]. As a result, several resources concerning the implementation of AI in these gamesresearch papers, python libraries, game bots online, project codes, etc. – are available and easily accessible online. On the contrary, only few resources from African indigenous games (Ayo, Seega, Abula) can be found because little research has beenmade on them. These games are however unique because they involve a complex combination of strategy, psychology, and quick decision making. For example, playing Seega involves two stages: placing the players and moving the players, which both need strategy to win, unlike chess (which only requires strategy in moving the players as they have a fixed position) and Go (which involves only placing the player). It is for this complex gaming structure of Seega that we chose it for our AI project. We hope to achieve two things with this project: successfully implement deep   